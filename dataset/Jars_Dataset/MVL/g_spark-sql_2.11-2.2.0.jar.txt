org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/unapply(org.apache.spark.sql.execution.datasources.InMemoryFileIndex$SerializableFileStatus)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/blockReplication()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/unapply(org.apache.spark.sql.execution.datasources.InMemoryFileIndex$SerializableFileStatus)#scala/runtime/BoxesRunTime/boxToShort(short)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/unapply(org.apache.spark.sql.execution.datasources.InMemoryFileIndex$SerializableFileStatus)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/path()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/unapply(org.apache.spark.sql.execution.datasources.InMemoryFileIndex$SerializableFileStatus)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/length()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/unapply(org.apache.spark.sql.execution.datasources.InMemoryFileIndex$SerializableFileStatus)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/isDir()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/unapply(org.apache.spark.sql.execution.datasources.InMemoryFileIndex$SerializableFileStatus)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/unapply(org.apache.spark.sql.execution.datasources.InMemoryFileIndex$SerializableFileStatus)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/blockSize()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/unapply(org.apache.spark.sql.execution.datasources.InMemoryFileIndex$SerializableFileStatus)#scala/Tuple8/Tuple8(java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/unapply(org.apache.spark.sql.execution.datasources.InMemoryFileIndex$SerializableFileStatus)#scala/runtime/BoxesRunTime/boxToBoolean(boolean)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/unapply(org.apache.spark.sql.execution.datasources.InMemoryFileIndex$SerializableFileStatus)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/unapply(org.apache.spark.sql.execution.datasources.InMemoryFileIndex$SerializableFileStatus)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/modificationTime()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/unapply(org.apache.spark.sql.execution.datasources.InMemoryFileIndex$SerializableFileStatus)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/accessTime()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/unapply(org.apache.spark.sql.execution.datasources.InMemoryFileIndex$SerializableFileStatus)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/blockLocations()
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator/anonfun/receiveAndReply/1/applyOrElse(java.lang.Object,scala.Function1)#scala/Some/x()
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator/anonfun/receiveAndReply/1/applyOrElse(java.lang.Object,scala.Function1)#org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$5/5(org.apache.spark.sql.execution.streaming.state.StateStoreCoordinator$$anonfun$receiveAndReply$1)
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator/anonfun/receiveAndReply/1/applyOrElse(java.lang.Object,scala.Function1)#scala/collection/mutable/HashMap/get(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator/anonfun/receiveAndReply/1/applyOrElse(java.lang.Object,scala.Function1)#org/apache/spark/scheduler/ExecutorCacheTaskLocation/executorId()
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator/anonfun/receiveAndReply/1/applyOrElse(java.lang.Object,scala.Function1)#org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$2/2(org.apache.spark.sql.execution.streaming.state.StateStoreCoordinator$$anonfun$receiveAndReply$1,org.apache.spark.sql.execution.streaming.state.StateStoreId,boolean)
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator/anonfun/receiveAndReply/1/applyOrElse(java.lang.Object,scala.Function1)#scala/collection/Iterable/filter(scala.Function1)
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator/anonfun/receiveAndReply/1/applyOrElse(java.lang.Object,scala.Function1)#org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator$$anonfun$receiveAndReply$1$$anonfun$2/2(org.apache.spark.sql.execution.streaming.state.StateStoreCoordinator$$anonfun$receiveAndReply$1,java.lang.String)
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator/anonfun/receiveAndReply/1/applyOrElse(java.lang.Object,scala.Function1)#scala/collection/TraversableOnce/toSeq()
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator/anonfun/receiveAndReply/1/applyOrElse(java.lang.Object,scala.Function1)#scala/collection/mutable/HashMap/$minus$minus$eq(scala.collection.TraversableOnce)
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator/anonfun/receiveAndReply/1/applyOrElse(java.lang.Object,scala.Function1)#org/apache/spark/rpc/RpcCallContext/reply(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator/anonfun/receiveAndReply/1/applyOrElse(java.lang.Object,scala.Function1)#scala/collection/mutable/HashMap/keys()
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator/anonfun/receiveAndReply/1/applyOrElse(java.lang.Object,scala.Function1)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator/anonfun/receiveAndReply/1/applyOrElse(java.lang.Object,scala.Function1)#org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$3/3(org.apache.spark.sql.execution.streaming.state.StateStoreCoordinator$$anonfun$receiveAndReply$1,org.apache.spark.sql.execution.streaming.state.StateStoreId,scala.Option)
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator/anonfun/receiveAndReply/1/applyOrElse(java.lang.Object,scala.Function1)#scala/runtime/BoxesRunTime/boxToBoolean(boolean)
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator/anonfun/receiveAndReply/1/applyOrElse(java.lang.Object,scala.Function1)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator/anonfun/receiveAndReply/1/applyOrElse(java.lang.Object,scala.Function1)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator/anonfun/receiveAndReply/1/applyOrElse(java.lang.Object,scala.Function1)#org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator$$anonfun$receiveAndReply$1$$anonfun$1/1(org.apache.spark.sql.execution.streaming.state.StateStoreCoordinator$$anonfun$receiveAndReply$1)
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator/anonfun/receiveAndReply/1/applyOrElse(java.lang.Object,scala.Function1)#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator/anonfun/receiveAndReply/1/applyOrElse(java.lang.Object,scala.Function1)#org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$4/4(org.apache.spark.sql.execution.streaming.state.StateStoreCoordinator$$anonfun$receiveAndReply$1,java.lang.String,scala.collection.Seq)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/4/anonfun/5/apply(org.apache.hadoop.fs.FileStatus)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/4/anonfun/5/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/Path/toString()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/4/anonfun/5/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/FileStatus/getReplication()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/4/anonfun/5/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/FileStatus/getBlockSize()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/4/anonfun/5/apply(org.apache.hadoop.fs.FileStatus)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/4/anonfun/5/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/FileStatus/getModificationTime()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/4/anonfun/5/apply(org.apache.hadoop.fs.FileStatus)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$$anonfun$4$$anonfun$5$$anonfun$6/6(org.apache.spark.sql.execution.datasources.InMemoryFileIndex$$anonfun$4$$anonfun$5)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/4/anonfun/5/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/FileStatus/getAccessTime()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/4/anonfun/5/apply(org.apache.hadoop.fs.FileStatus)#scala/Array$/empty(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/4/anonfun/5/apply(org.apache.hadoop.fs.FileStatus)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/SerializableFileStatus(java.lang.String,long,boolean,short,long,long,long,org.apache.spark.sql.execution.datasources.InMemoryFileIndex$SerializableBlockLocation[])
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/4/anonfun/5/apply(org.apache.hadoop.fs.FileStatus)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/4/anonfun/5/apply(org.apache.hadoop.fs.FileStatus)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/4/anonfun/5/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/FileStatus/getPath()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/4/anonfun/5/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/LocatedFileStatus/getBlockLocations()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/4/anonfun/5/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/FileStatus/isDirectory()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/4/anonfun/5/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/FileStatus/getLen()
org/apache/spark/sql/execution/streaming/StateStoreRestoreExec/doExecute()#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/streaming/StateStoreRestoreExec/doExecute()#org/apache/spark/sql/execution/streaming/state/package$StateStoreOps/mapPartitionsWithStateStore(java.lang.String,long,long,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.internal.SessionState,scala.Option,scala.Function2,scala.reflect.ClassTag)
org/apache/spark/sql/execution/streaming/StateStoreRestoreExec/doExecute()#org/apache/spark/sql/execution/streaming/StateStoreRestoreExec/longMetric(java.lang.String)
org/apache/spark/sql/execution/streaming/StateStoreRestoreExec/doExecute()#org/apache/spark/sql/execution/streaming/StateStoreRestoreExec$$anonfun$doExecute$1/1(org.apache.spark.sql.execution.streaming.StateStoreRestoreExec,org.apache.spark.sql.execution.metric.SQLMetric)
org/apache/spark/sql/execution/streaming/StateStoreRestoreExec/doExecute()#org/apache/spark/sql/catalyst/expressions/package$AttributeSeq/toStructType()
org/apache/spark/sql/execution/streaming/StateStoreRestoreExec/doExecute()#org/apache/spark/sql/catalyst/expressions/package$/AttributeSeq(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/StateStoreRestoreExec/doExecute()#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/streaming/StateStoreRestoreExec/doExecute()#org/apache/spark/sql/execution/streaming/StateStoreRestoreExec/sqlContext()
org/apache/spark/sql/execution/streaming/StateStoreRestoreExec/doExecute()#org/apache/spark/sql/execution/streaming/state/package$/StateStoreOps(org.apache.spark.rdd.RDD,scala.reflect.ClassTag)
org/apache/spark/sql/execution/streaming/StateStoreRestoreExec/doExecute()#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/18/apply(scala.Tuple2)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/18/apply(scala.Tuple2)#scala/collection/Seq/apply(int)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/18/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/18/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/isNull()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/18/apply(scala.Tuple2)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/18/apply(scala.Tuple2)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/18/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/18/apply(scala.Tuple2)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/18/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/value()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/18/apply(scala.Tuple2)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/18/apply(scala.Tuple2)#scala/Tuple2/_2$mcI$sp()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/18/apply(scala.Tuple2)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/18/apply(scala.Tuple2)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/python/EvaluatePython/StructTypePickler/pickle(java.lang.Object,java.io.OutputStream,net.razorvine.pickle.Pickler)#org/apache/spark/sql/types/StructType/json()
org/apache/spark/sql/execution/python/EvaluatePython/StructTypePickler/pickle(java.lang.Object,java.io.OutputStream,net.razorvine.pickle.Pickler)#net/razorvine/pickle/Pickler/save(java.lang.Object)
org/apache/spark/sql/execution/python/EvaluatePython/StructTypePickler/pickle(java.lang.Object,java.io.OutputStream,net.razorvine.pickle.Pickler)#java/io/OutputStream/write(int)
org/apache/spark/sql/execution/python/EvaluatePython/StructTypePickler/pickle(java.lang.Object,java.io.OutputStream,net.razorvine.pickle.Pickler)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/python/EvaluatePython/StructTypePickler/pickle(java.lang.Object,java.io.OutputStream,net.razorvine.pickle.Pickler)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/python/EvaluatePython/StructTypePickler/pickle(java.lang.Object,java.io.OutputStream,net.razorvine.pickle.Pickler)#java/io/OutputStream/write(byte[])
org/apache/spark/sql/execution/python/EvaluatePython/StructTypePickler/pickle(java.lang.Object,java.io.OutputStream,net.razorvine.pickle.Pickler)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/python/EvaluatePython/StructTypePickler/pickle(java.lang.Object,java.io.OutputStream,net.razorvine.pickle.Pickler)#org/apache/spark/sql/execution/python/EvaluatePython$/org$apache$spark$sql$execution$python$EvaluatePython$$module()
org/apache/spark/sql/execution/python/EvaluatePython/StructTypePickler/pickle(java.lang.Object,java.io.OutputStream,net.razorvine.pickle.Pickler)#java/lang/String/getBytes(java.nio.charset.Charset)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitGenericFileFormat/1/apply()#scala/Some/x()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitGenericFileFormat/1/apply()#org/apache/spark/sql/catalyst/parser/ParserUtils$/operationNotAllowed(java.lang.String,org.antlr.v4.runtime.ParserRuleContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitGenericFileFormat/1/apply()#org/apache/spark/sql/internal/HiveSerDe$/sourceToSerDe(java.lang.String)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitGenericFileFormat/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$1()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitGenericFileFormat/1/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitGenericFileFormat/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$5()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitGenericFileFormat/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$6()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitGenericFileFormat/1/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitGenericFileFormat/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy(scala.Option,scala.Option,scala.Option,scala.Option,boolean,scala.collection.immutable.Map)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitGenericFileFormat/1/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitGenericFileFormat/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitGenericFileFormat$1/apply()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitGenericFileFormat/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat$/empty()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitGenericFileFormat/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$GenericFileFormatContext/identifier()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitGenericFileFormat/1/apply()#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitGenericFileFormat/1/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitGenericFileFormat/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$IdentifierContext/getText()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitGenericFileFormat/1/apply()#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/command/SetCommand/anonfun/6/anonfun/apply/4/apply(scala.Tuple3)#org/apache/spark/sql/execution/command/SetCommand$$anonfun$6$$anonfun$apply$4$$anonfun$apply$5/5(org.apache.spark.sql.execution.command.SetCommand$$anonfun$6$$anonfun$apply$4)
org/apache/spark/sql/execution/command/SetCommand/anonfun/6/anonfun/apply/4/apply(scala.Tuple3)#scala/Tuple3/_1()
org/apache/spark/sql/execution/command/SetCommand/anonfun/6/anonfun/apply/4/apply(scala.Tuple3)#scala/Tuple3/_2()
org/apache/spark/sql/execution/command/SetCommand/anonfun/6/anonfun/apply/4/apply(scala.Tuple3)#scala/Tuple3/_3()
org/apache/spark/sql/execution/command/SetCommand/anonfun/6/anonfun/apply/4/apply(scala.Tuple3)#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/command/SetCommand/anonfun/6/anonfun/apply/4/apply(scala.Tuple3)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/SetCommand/anonfun/6/anonfun/apply/4/apply(scala.Tuple3)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/command/SetCommand/anonfun/6/anonfun/apply/4/apply(scala.Tuple3)#org/apache/spark/sql/Row$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/command/SetCommand/anonfun/6/anonfun/apply/4/apply(scala.Tuple3)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding/Encoder/gatherCompressibilityStats(org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/execution/columnar/NativeColumnType/actualSize(org.apache.spark.sql.catalyst.InternalRow,int)
org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding/Encoder/gatherCompressibilityStats(org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding$Encoder/_uncompressedSize()
org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding/Encoder/gatherCompressibilityStats(org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding$Encoder/lastValue()
org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding/Encoder/gatherCompressibilityStats(org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding$Encoder/lastRun()
org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding/Encoder/gatherCompressibilityStats(org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding$Encoder/_compressedSize_$eq(int)
org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding/Encoder/gatherCompressibilityStats(org.apache.spark.sql.catalyst.InternalRow,int)#scala/runtime/BoxesRunTime/equals(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding/Encoder/gatherCompressibilityStats(org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/execution/columnar/NativeColumnType/copyField(org.apache.spark.sql.catalyst.InternalRow,int,org.apache.spark.sql.catalyst.InternalRow,int)
org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding/Encoder/gatherCompressibilityStats(org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/execution/columnar/NativeColumnType/getField(org.apache.spark.sql.catalyst.InternalRow,int)
org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding/Encoder/gatherCompressibilityStats(org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/catalyst/expressions/SpecificInternalRow/isNullAt(int)
org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding/Encoder/gatherCompressibilityStats(org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding$Encoder/_uncompressedSize_$eq(int)
org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding/Encoder/gatherCompressibilityStats(org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding$Encoder/lastRun_$eq(int)
org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding/Encoder/gatherCompressibilityStats(org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding$Encoder/_compressedSize()
org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding/Encoder/compress(java.nio.ByteBuffer,java.nio.ByteBuffer)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding/Encoder/compress(java.nio.ByteBuffer,java.nio.ByteBuffer)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding/Encoder/compress(java.nio.ByteBuffer,java.nio.ByteBuffer)#org/apache/spark/sql/catalyst/expressions/SpecificInternalRow/SpecificInternalRow(scala.collection.Seq)
org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding/Encoder/compress(java.nio.ByteBuffer,java.nio.ByteBuffer)#java/nio/ByteBuffer/rewind()
org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding/Encoder/compress(java.nio.ByteBuffer,java.nio.ByteBuffer)#java/nio/ByteBuffer/hasRemaining()
org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding/Encoder/compress(java.nio.ByteBuffer,java.nio.ByteBuffer)#org/apache/spark/sql/catalyst/expressions/SpecificInternalRow/get(int,org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding/Encoder/compress(java.nio.ByteBuffer,java.nio.ByteBuffer)#org/apache/spark/sql/execution/columnar/NativeColumnType/extract(java.nio.ByteBuffer,org.apache.spark.sql.catalyst.InternalRow,int)
org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding/Encoder/compress(java.nio.ByteBuffer,java.nio.ByteBuffer)#org/apache/spark/sql/execution/columnar/NativeColumnType/append(org.apache.spark.sql.catalyst.InternalRow,int,java.nio.ByteBuffer)
org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding/Encoder/compress(java.nio.ByteBuffer,java.nio.ByteBuffer)#java/nio/ByteBuffer/putInt(int)
org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding/Encoder/compress(java.nio.ByteBuffer,java.nio.ByteBuffer)#org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding$/typeId()
org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding/Encoder/compress(java.nio.ByteBuffer,java.nio.ByteBuffer)#scala/runtime/BoxesRunTime/equals(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding/Encoder/compress(java.nio.ByteBuffer,java.nio.ByteBuffer)#org/apache/spark/sql/execution/columnar/NativeColumnType/copyField(org.apache.spark.sql.catalyst.InternalRow,int,org.apache.spark.sql.catalyst.InternalRow,int)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/catalyst/expressions/aggregate/AggregateFunction/children()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/SeqLike/length()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/Tuple2/_2()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/zip(scala.collection.GenIterable,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/generic/GenericTraversableTemplate/unzip(scala.Function1)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$9/9()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/head()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$/createAggregate(scala.Option,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,int,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/length()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$/createAggregate$default$5()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/zipWithIndex(scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$20/20(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$25/25(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/catalyst/expressions/aggregate/AggregateExpression/aggregateFunction()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/Tuple2/_1()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$8/8()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$17/17(scala.collection.immutable.Map)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/Predef$/$conforms()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$/createAggregate$default$1()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$15/15()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$16/16()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$18/18()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$19/19()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$14/14()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$10/10()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$21/21()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$11/11()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$22/22()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$23/23()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$12/12()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$13/13()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$24/24()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithoutDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$2/2()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithoutDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$3/3()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithoutDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$4/4()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithoutDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$5/5()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithoutDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$6/6()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithoutDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$7/7()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithoutDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithoutDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$/planAggregateWithoutDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithoutDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithoutDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithoutDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$/createAggregate(scala.Option,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,int,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithoutDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/length()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithoutDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithoutDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithoutDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$/createAggregate(scala.Option,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,int,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/length()
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$/createAggregate$default$5()
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$/createAggregate$default$1()
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$27/27()
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$28/28()
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$29/29()
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$30/30()
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$31/31()
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$32/32()
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$33/33()
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$34/34()
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$35/35()
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$36/36()
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$26/26()
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$37/37()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$bulkListLeafFiles(scala.collection.Seq,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,org.apache.spark.sql.SparkSession)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$bulkListLeafFiles(scala.collection.Seq,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,org.apache.spark.sql.SparkSession)#org/apache/spark/rdd/RDD/mapPartitions(scala.Function1,boolean,scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$bulkListLeafFiles(scala.collection.Seq,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,org.apache.spark.sql.SparkSession)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$bulkListLeafFiles(scala.collection.Seq,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,org.apache.spark.sql.SparkSession)#org/apache/spark/rdd/RDD/mapPartitions$default$2()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$bulkListLeafFiles(scala.collection.Seq,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$$anonfun$3/3(org.apache.hadoop.fs.PathFilter,org.apache.spark.util.SerializableConfiguration)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$bulkListLeafFiles(scala.collection.Seq,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,org.apache.spark.sql.SparkSession)#scala/Predef$DummyImplicit$/dummyImplicit()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$bulkListLeafFiles(scala.collection.Seq,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,org.apache.spark.sql.SparkSession)#org/apache/spark/sql/internal/SQLConf/parallelPartitionDiscoveryThreshold()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$bulkListLeafFiles(scala.collection.Seq,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,org.apache.spark.sql.SparkSession)#scala/collection/Seq/size()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$bulkListLeafFiles(scala.collection.Seq,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,org.apache.spark.sql.SparkSession)#java/lang/Math/min(int,int)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$bulkListLeafFiles(scala.collection.Seq,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,org.apache.spark.sql.SparkSession)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$bulkListLeafFiles(scala.collection.Seq,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,org.apache.spark.sql.SparkSession)#org/apache/spark/rdd/RDD/collect()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$bulkListLeafFiles(scala.collection.Seq,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$/logInfo(scala.Function0)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$bulkListLeafFiles(scala.collection.Seq,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$$anonfun$org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$bulkListLeafFiles$3/3()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$bulkListLeafFiles(scala.collection.Seq,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,org.apache.spark.sql.SparkSession)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$bulkListLeafFiles(scala.collection.Seq,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,org.apache.spark.sql.SparkSession)#org/apache/spark/sql/internal/SQLConf/parallelPartitionDiscoveryParallelism()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$bulkListLeafFiles(scala.collection.Seq,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,org.apache.spark.sql.SparkSession)#org/apache/spark/SparkContext/parallelize(scala.collection.Seq,int,scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$bulkListLeafFiles(scala.collection.Seq,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$$anonfun$org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$bulkListLeafFiles$2/2(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$bulkListLeafFiles(scala.collection.Seq,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$$anonfun$org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$bulkListLeafFiles$1/1(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,org.apache.spark.sql.SparkSession)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$bulkListLeafFiles(scala.collection.Seq,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,org.apache.spark.sql.SparkSession)#scala/Array$/fallbackCanBuildFrom(scala.Predef$DummyImplicit)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$bulkListLeafFiles(scala.collection.Seq,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$$anonfun$2/2()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$bulkListLeafFiles(scala.collection.Seq,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,org.apache.spark.sql.SparkSession)#org/apache/spark/metrics/source/HiveCatalogMetrics$/incrementParallelListingJobCount(int)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$bulkListLeafFiles(scala.collection.Seq,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$$anonfun$4/4()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$bulkListLeafFiles(scala.collection.Seq,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,org.apache.spark.sql.SparkSession)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$bulkListLeafFiles(scala.collection.Seq,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,org.apache.spark.sql.SparkSession)#org/apache/spark/util/SerializableConfiguration/SerializableConfiguration(org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$bulkListLeafFiles(scala.collection.Seq,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,org.apache.spark.sql.SparkSession)#org/apache/spark/rdd/RDD/map(scala.Function1,scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,scala.Option)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$/logTrace(scala.Function0)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,scala.Option)#org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,scala.Option)#scala/collection/mutable/ArrayOps/filterNot(scala.Function1)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,scala.Option)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$$anonfun$org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles$3/3(org.apache.hadoop.fs.FileSystem)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,scala.Option)#scala/collection/mutable/ArrayOps/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,scala.Option)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,scala.Option)#scala/Tuple2/_2()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,scala.Option)#scala/Array$/fallbackCanBuildFrom(scala.Predef$DummyImplicit)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,scala.Option)#scala/Array$/empty(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,scala.Option)#org/apache/hadoop/fs/FileSystem/listStatus(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,scala.Option)#scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,scala.Option)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,scala.Option)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$$anonfun$9/9(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,scala.Option)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,scala.Option)#scala/collection/mutable/ArrayOps/filter(scala.Function1)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,scala.Option)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,scala.Option)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,scala.Option)#scala/collection/mutable/ArrayOps/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,scala.Option)#scala/Some/x()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,scala.Option)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,scala.Option)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,scala.Option)#scala/Tuple2/_1()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,scala.Option)#scala/Predef$DummyImplicit$/dummyImplicit()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,scala.Option)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$/logWarning(scala.Function0)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,scala.Option)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$$anonfun$org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles$1/1(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,scala.Option)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$bulkListLeafFiles(scala.collection.Seq,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,org.apache.spark.sql.SparkSession)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,scala.Option)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$$anonfun$10/10()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,scala.Option)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$$anonfun$11/11()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,scala.Option)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$$anonfun$12/12()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,scala.Option)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$$anonfun$13/13()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,scala.Option)#scala/collection/mutable/ArrayOps/partition(scala.Function1)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,scala.Option)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$$anonfun$org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles$2/2()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,scala.Option)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$$anonfun$14/14(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,scala.Option)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$listLeafFiles(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,scala.Option)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$$anonfun$15/15(org.apache.hadoop.fs.PathFilter)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/11/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$/readSchemaFromFooter(org.apache.parquet.hadoop.Footer,org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/11/apply(scala.collection.Iterator)#scala/collection/Seq/isEmpty()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/11/apply(scala.collection.Iterator)#scala/collection/Iterator$/empty()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/11/apply(scala.collection.Iterator)#scala/collection/Iterator/toSeq()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/11/apply(scala.collection.Iterator)#scala/collection/IterableLike/foreach(scala.Function1)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/11/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$11$$anonfun$apply$10/10(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$11,org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter,scala.runtime.ObjectRef)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/11/apply(scala.collection.Iterator)#scala/collection/Seq/head()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/11/apply(scala.collection.Iterator)#scala/collection/Iterator/map(scala.Function1)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/11/apply(scala.collection.Iterator)#scala/runtime/ObjectRef/create(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/11/apply(scala.collection.Iterator)#scala/collection/Seq/tail()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/11/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$11$$anonfun$12/12(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$11)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/11/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$/readParquetFootersInParallel(org.apache.hadoop.conf.Configuration,scala.collection.Seq,boolean)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/11/apply(scala.collection.Iterator)#org/apache/spark/util/SerializableConfiguration/value()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/11/apply(scala.collection.Iterator)#scala/package$/Iterator()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/11/apply(scala.collection.Iterator)#scala/collection/Iterator$/single(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitSetConfiguration/1/apply()#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitSetConfiguration/1/apply()#java/lang/String/trim()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitSetConfiguration/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitSetConfiguration$1/apply()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitSetConfiguration/1/apply()#org/antlr/v4/runtime/tree/TerminalNode/getSymbol()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitSetConfiguration/1/apply()#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitSetConfiguration/1/apply()#scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitSetConfiguration/1/apply()#java/lang/String/substring(int,int)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitSetConfiguration/1/apply()#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitSetConfiguration/1/apply()#java/lang/String/indexOf(int)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitSetConfiguration/1/apply()#org/apache/spark/sql/catalyst/parser/ParserUtils$/remainder(org.antlr.v4.runtime.Token)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitSetConfiguration/1/apply()#scala/Predef$/ArrowAssoc(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitSetConfiguration/1/apply()#java/lang/String/substring(int)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitSetConfiguration/1/apply()#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitSetConfiguration/1/apply()#scala/collection/immutable/StringOps/nonEmpty()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitSetConfiguration/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$SetConfigurationContext/SET()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateFunction/1/apply()#scala/collection/mutable/Buffer$/canBuildFrom()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateFunction/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateFunctionContext/qualifiedName()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateFunction/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateFunction$1/apply()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateFunction/1/apply()#org/apache/spark/sql/catalyst/FunctionIdentifier/funcName()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateFunction/1/apply()#scala/collection/convert/Decorators$AsScala/asScala()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateFunction/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateFunctionContext/TEMPORARY()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateFunction/1/apply()#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateFunction/1/apply()#org/apache/spark/sql/catalyst/FunctionIdentifier/database()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateFunction/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateFunction$1$$anonfun$20/20(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateFunction$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateFunction/1/apply()#org/apache/spark/sql/catalyst/parser/ParserUtils$/string(org.antlr.v4.runtime.Token)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateFunction/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateFunctionContext/resource()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateFunction/1/apply()#scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateFunction/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder/visitFunctionName(org.apache.spark.sql.catalyst.parser.SqlBaseParser$QualifiedNameContext)
org/apache/spark/sql/execution/aggregate/HashMapGenerator/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/ExprCode(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/aggregate/HashMapGenerator/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/aggregate/HashMapGenerator/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/isNull()
org/apache/spark/sql/execution/aggregate/HashMapGenerator/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashMapGenerator/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/Expression/dataType()
org/apache/spark/sql/execution/aggregate/HashMapGenerator/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashMapGenerator/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/aggregate/HashMapGenerator/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/aggregate/HashMapGenerator/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/code()
org/apache/spark/sql/execution/aggregate/HashMapGenerator/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/Expression/genCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)
org/apache/spark/sql/execution/aggregate/HashMapGenerator/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/value()
org/apache/spark/sql/execution/aggregate/HashMapGenerator/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addMutableState(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/aggregate/HashMapGenerator/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/HashMapGenerator/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/aggregate/HashMapGenerator/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashMapGenerator/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/freshName(java.lang.String)
org/apache/spark/sql/execution/aggregate/HashMapGenerator/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/javaType(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/aggregate/HashMapGenerator/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashMapGenerator/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/3/apply(scala.Tuple2)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/3/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/3/apply(scala.Tuple2)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/3/apply(scala.Tuple2)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/3/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addNewFunction(java.lang.String,java.lang.String)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/3/apply(scala.Tuple2)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/3/apply(scala.Tuple2)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/3/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/3/apply(scala.Tuple2)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/3/apply(scala.Tuple2)#scala/Tuple2/_2$mcI$sp()
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/3/apply(scala.Tuple2)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/3/apply(scala.Tuple2)#scala/collection/Seq/mkString(java.lang.String)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/3/apply(scala.Tuple2)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/ReuseSubquery/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/execution/ExecSubqueryExpression/plan()
org/apache/spark/sql/execution/ReuseSubquery/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object)
org/apache/spark/sql/execution/ReuseSubquery/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/execution/ExecSubqueryExpression/withNewPlan(org.apache.spark.sql.catalyst.plans.QueryPlan)
org/apache/spark/sql/execution/ReuseSubquery/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#scala/Option/isDefined()
org/apache/spark/sql/execution/ReuseSubquery/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/plans/QueryPlan/schema()
org/apache/spark/sql/execution/ReuseSubquery/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#scala/collection/mutable/HashMap/getOrElseUpdate(java.lang.Object,scala.Function0)
org/apache/spark/sql/execution/ReuseSubquery/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/execution/ReuseSubquery$$anonfun$apply$2$$anonfun$2/2(org.apache.spark.sql.execution.ReuseSubquery$$anonfun$apply$2,org.apache.spark.sql.execution.ExecSubqueryExpression)
org/apache/spark/sql/execution/ReuseSubquery/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/execution/ReuseSubquery$$anonfun$apply$2$$anonfun$1/1(org.apache.spark.sql.execution.ReuseSubquery$$anonfun$apply$2)
org/apache/spark/sql/execution/ReuseSubquery/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/execution/ReuseSubquery/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#scala/collection/mutable/ArrayBuffer/find(scala.Function1)
org/apache/spark/sql/execution/ReuseSubquery/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#scala/Option/get()
org/apache/spark/sql/execution/SparkStrategies/InMemoryScans/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/SparkStrategies$InMemoryScans$$anonfun$2/2(org.apache.spark.sql.execution.SparkStrategies$InMemoryScans$)
org/apache/spark/sql/execution/SparkStrategies/InMemoryScans/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)
org/apache/spark/sql/execution/SparkStrategies/InMemoryScans/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Tuple3/_2()
org/apache/spark/sql/execution/SparkStrategies/InMemoryScans/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Tuple3/_3()
org/apache/spark/sql/execution/SparkStrategies/InMemoryScans/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Option/isEmpty()
org/apache/spark/sql/execution/SparkStrategies/InMemoryScans/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/planning/PhysicalOperation$/unapply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/SparkStrategies/InMemoryScans/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Option/get()
org/apache/spark/sql/execution/SparkStrategies/InMemoryScans/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Tuple3/_1()
org/apache/spark/sql/execution/SparkStrategies/InMemoryScans/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/SparkStrategies$InMemoryScans$$anonfun$3/3(org.apache.spark.sql.execution.SparkStrategies$InMemoryScans$,scala.collection.Seq,org.apache.spark.sql.execution.columnar.InMemoryRelation)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/getValidBatchesBeforeCompactionBatch(long,int)#scala/Predef$/longWrapper(long)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/getValidBatchesBeforeCompactionBatch(long,int)#scala/Predef$/assert(boolean,scala.Function0)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/getValidBatchesBeforeCompactionBatch(long,int)#scala/math/package$/max(long,long)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/getValidBatchesBeforeCompactionBatch(long,int)#scala/runtime/RichLong/until(java.lang.Object)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/getValidBatchesBeforeCompactionBatch(long,int)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/getValidBatchesBeforeCompactionBatch(long,int)#scala/runtime/RichLong/RichLong(long)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/getValidBatchesBeforeCompactionBatch(long,int)#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog$/getValidBatchesBeforeCompactionBatch(long,int)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/getValidBatchesBeforeCompactionBatch(long,int)#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog$/isCompactionBatch(long,int)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/getValidBatchesBeforeCompactionBatch(long,int)#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog$$anonfun$getValidBatchesBeforeCompactionBatch$1/1(long)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/serialize(java.lang.Object[],java.io.OutputStream)#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog$$anonfun$serialize$1/1(org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog,java.io.OutputStream)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/serialize(java.lang.Object[],java.io.OutputStream)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/serialize(java.lang.Object[],java.io.OutputStream)#scala/collection/mutable/ArrayOps/foreach(scala.Function1)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/serialize(java.lang.Object[],java.io.OutputStream)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/serialize(java.lang.Object[],java.io.OutputStream)#java/lang/String/getBytes(java.nio.charset.Charset)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/serialize(java.lang.Object[],java.io.OutputStream)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/serialize(java.lang.Object[],java.io.OutputStream)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/serialize(java.lang.Object[],java.io.OutputStream)#java/io/OutputStream/write(byte[])
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/serialize(java.lang.Object[],java.io.OutputStream)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/batchIdToPath(long)#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog$/COMPACT_FILE_SUFFIX()
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/batchIdToPath(long)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/batchIdToPath(long)#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog$/isCompactionBatch(long,int)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/batchIdToPath(long)#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/metadataPath()
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/batchIdToPath(long)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/batchIdToPath(long)#java/lang/Object/toString()
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/batchIdToPath(long)#org/apache/hadoop/fs/Path/Path(org.apache.hadoop.fs.Path,java.lang.String)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/batchIdToPath(long)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/batchIdToPath(long)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/batchIdToPath(long)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/deserialize(java.io.InputStream)#java/lang/IllegalStateException/IllegalStateException(java.lang.String)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/deserialize(java.io.InputStream)#scala/io/Source$/fromInputStream(java.io.InputStream,java.lang.String)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/deserialize(java.io.InputStream)#java/nio/charset/Charset/name()
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/deserialize(java.io.InputStream)#scala/collection/Iterator/map(scala.Function1)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/deserialize(java.io.InputStream)#scala/io/BufferedSource/getLines()
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/deserialize(java.io.InputStream)#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog$$anonfun$deserialize$1/1(org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/deserialize(java.io.InputStream)#scala/collection/Iterator/next()
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/deserialize(java.io.InputStream)#scala/collection/Iterator/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/deserialize(java.io.InputStream)#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/parseVersion(java.lang.String,int)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/deserialize(java.io.InputStream)#scala/collection/Iterator/hasNext()
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/compactInterval$lzycompute()#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog$$anonfun$3/3(org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/compactInterval$lzycompute()#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog$$anonfun$2/2(org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/compactInterval$lzycompute()#scala/runtime/IntRef/create(int)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/compactInterval$lzycompute()#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog$$anonfun$compactInterval$1/1(org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog,scala.runtime.IntRef)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/compactInterval$lzycompute()#scala/collection/mutable/ArrayOps/sorted(scala.math.Ordering)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/compactInterval$lzycompute()#scala/collection/mutable/ArrayOps/reverse()
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/compactInterval$lzycompute()#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/compactInterval$lzycompute()#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog$/deriveCompactInterval(int,int)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/compactInterval$lzycompute()#scala/Predef$/assert(boolean,scala.Function0)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/compactInterval$lzycompute()#scala/Predef$/longArrayOps(long[])
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/compactInterval$lzycompute()#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/compactInterval$lzycompute()#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/logInfo(scala.Function0)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/compactInterval$lzycompute()#org/apache/spark/sql/execution/streaming/HDFSMetadataLog$FileManager/list(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/compactInterval$lzycompute()#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog$$anonfun$compactInterval$2/2(org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog,scala.runtime.IntRef)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/compactInterval$lzycompute()#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/compactInterval$lzycompute()#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/metadataPath()
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/compactInterval$lzycompute()#scala/reflect/ClassTag$/Long()
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/compactInterval$lzycompute()#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/fileManager()
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/compactInterval$lzycompute()#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/batchFilesFilter()
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/compactInterval$lzycompute()#scala/collection/mutable/ArrayOps/filter(scala.Function1)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/compact(long,java.lang.Object[])#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/compact(long,java.lang.Object[])#scala/collection/Seq/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/compact(long,java.lang.Object[])#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog$$anonfun$5/5(org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/compact(long,java.lang.Object[])#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog$$anonfun$4/4(org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/compact(long,java.lang.Object[])#scala/collection/generic/GenericTraversableTemplate/flatten(scala.Function1)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/compact(long,java.lang.Object[])#scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/compact(long,java.lang.Object[])#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog$/getValidBatchesBeforeCompactionBatch(long,int)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/compact(long,java.lang.Object[])#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/compact(long,java.lang.Object[])#scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/deleteExpiredLog(long)#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog$$anonfun$deleteExpiredLog$1/1(org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog,long)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/deleteExpiredLog(long)#scala/Predef$/assert(boolean,scala.Function0)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/deleteExpiredLog(long)#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog$$anonfun$deleteExpiredLog$3/3(org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog,long)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/deleteExpiredLog(long)#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog$$anonfun$deleteExpiredLog$2/2(org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog,long,long)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/deleteExpiredLog(long)#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog$$anon$1/1(org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog,long)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/deleteExpiredLog(long)#scala/collection/mutable/ArrayOps/foreach(scala.Function1)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/deleteExpiredLog(long)#java/lang/System/currentTimeMillis()
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/deleteExpiredLog(long)#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog$/isCompactionBatch(long,int)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/deleteExpiredLog(long)#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/fileManager()
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/deleteExpiredLog(long)#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/logInfo(scala.Function0)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/deleteExpiredLog(long)#org/apache/spark/sql/execution/streaming/HDFSMetadataLog$FileManager/list(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/deleteExpiredLog(long)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/deleteExpiredLog(long)#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/metadataPath()
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/allFiles()#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/allFiles()#scala/collection/Seq/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/allFiles()#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog$$anonfun$8/8(org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/allFiles()#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog$$anonfun$7/7(org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/allFiles()#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/getLatest()
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/allFiles()#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog$$anonfun$6/6(org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/allFiles()#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog$/nextCompactionBatchId(long,long)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/allFiles()#scala/Array$/empty(scala.reflect.ClassTag)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/allFiles()#scala/collection/generic/GenericTraversableTemplate/flatten(scala.Function1)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/allFiles()#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/allFiles()#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog$/getAllValidBatches(long,long)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/allFiles()#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog$$anonfun$allFiles$1/1(org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog,java.io.IOException)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/allFiles()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/allFiles()#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/allFiles()#scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/allFiles()#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog$$anonfun$1/1(org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog)
org/apache/spark/sql/execution/streaming/BatchCommitLog/deserialize(java.io.InputStream)#java/lang/IllegalStateException/IllegalStateException(java.lang.String)
org/apache/spark/sql/execution/streaming/BatchCommitLog/deserialize(java.io.InputStream)#java/lang/String/trim()
org/apache/spark/sql/execution/streaming/BatchCommitLog/deserialize(java.io.InputStream)#org/apache/spark/sql/execution/streaming/BatchCommitLog$/org$apache$spark$sql$execution$streaming$BatchCommitLog$$VERSION()
org/apache/spark/sql/execution/streaming/BatchCommitLog/deserialize(java.io.InputStream)#scala/collection/Iterator/hasNext()
org/apache/spark/sql/execution/streaming/BatchCommitLog/deserialize(java.io.InputStream)#scala/io/Source$/fromInputStream(java.io.InputStream,java.lang.String)
org/apache/spark/sql/execution/streaming/BatchCommitLog/deserialize(java.io.InputStream)#java/nio/charset/Charset/name()
org/apache/spark/sql/execution/streaming/BatchCommitLog/deserialize(java.io.InputStream)#scala/io/BufferedSource/getLines()
org/apache/spark/sql/execution/streaming/BatchCommitLog/deserialize(java.io.InputStream)#org/apache/spark/sql/execution/streaming/BatchCommitLog$/org$apache$spark$sql$execution$streaming$BatchCommitLog$$EMPTY_JSON()
org/apache/spark/sql/execution/streaming/BatchCommitLog/deserialize(java.io.InputStream)#scala/collection/Iterator/next()
org/apache/spark/sql/execution/streaming/BatchCommitLog/deserialize(java.io.InputStream)#org/apache/spark/sql/execution/streaming/BatchCommitLog/parseVersion(java.lang.String,int)
org/apache/spark/sql/execution/streaming/BatchCommitLog/serialize(java.lang.String,java.io.OutputStream)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/streaming/BatchCommitLog/serialize(java.lang.String,java.io.OutputStream)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/BatchCommitLog/serialize(java.lang.String,java.io.OutputStream)#java/lang/String/getBytes(java.nio.charset.Charset)
org/apache/spark/sql/execution/streaming/BatchCommitLog/serialize(java.lang.String,java.io.OutputStream)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/BatchCommitLog/serialize(java.lang.String,java.io.OutputStream)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/BatchCommitLog/serialize(java.lang.String,java.io.OutputStream)#org/apache/spark/sql/execution/streaming/BatchCommitLog$/org$apache$spark$sql$execution$streaming$BatchCommitLog$$VERSION()
org/apache/spark/sql/execution/streaming/BatchCommitLog/serialize(java.lang.String,java.io.OutputStream)#java/io/OutputStream/write(int)
org/apache/spark/sql/execution/streaming/BatchCommitLog/serialize(java.lang.String,java.io.OutputStream)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/BatchCommitLog/serialize(java.lang.String,java.io.OutputStream)#org/apache/spark/sql/execution/streaming/BatchCommitLog$/org$apache$spark$sql$execution$streaming$BatchCommitLog$$EMPTY_JSON()
org/apache/spark/sql/execution/streaming/BatchCommitLog/serialize(java.lang.String,java.io.OutputStream)#java/io/OutputStream/write(byte[])
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/anonfun/compactInterval/2/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/anonfun/compactInterval/2/apply()#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/anonfun/compactInterval/2/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/anonfun/compactInterval/2/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/anonfun/compactInterval/2/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/anonfun/compactInterval/2/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/anonfun/compactInterval/2/apply()#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog$$anonfun$compactInterval$2/apply()
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/anonfun/compactInterval/2/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/anonfun/compactInterval/2/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#org/apache/spark/sql/execution/stat/StatFunctions$$anonfun$6/6()
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#org/apache/spark/sql/execution/stat/StatFunctions$$anonfun$7/7()
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/collection/mutable/ArrayOps/groupBy(scala.Function1)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/collection/immutable/Map/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#org/apache/spark/sql/execution/stat/StatFunctions$$anonfun$crossTabulate$1/1()
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#org/apache/spark/sql/execution/stat/StatFunctions$$anonfun$8/8(scala.collection.immutable.Map,int)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/collection/mutable/ArrayOps/distinct()
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#org/apache/spark/sql/types/StructField$/apply$default$3()
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/collection/immutable/Map/size()
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/collection/Seq/sortBy(scala.Function1,scala.math.Ordering)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#org/apache/spark/sql/types/StructField$/apply$default$4()
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#org/apache/spark/sql/execution/stat/StatFunctions$$anonfun$9/9()
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#org/apache/spark/sql/functions$/count(java.lang.String)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#org/apache/spark/sql/types/StructField/StructField(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.types.Metadata)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#org/apache/spark/sql/execution/stat/StatFunctions$/logWarning(scala.Function0)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/collection/Seq/$plus$colon(java.lang.Object,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#org/apache/spark/sql/types/StructType/toAttributes()
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/Predef$/require(boolean,scala.Function0)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/collection/mutable/ArrayOps/toMap(scala.Predef$$less$colon$less)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#org/apache/spark/sql/execution/stat/StatFunctions$$anonfun$crossTabulate$2/2(java.lang.String,int)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#org/apache/spark/sql/Dataset$/ofRows(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#org/apache/spark/sql/execution/stat/StatFunctions$$anonfun$10/10()
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/collection/mutable/ArrayOps/zipWithIndex(scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/collection/TraversableOnce/toSeq()
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/collection/immutable/Map/toSeq()
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/collection/immutable/Iterable$/canBuildFrom()
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/Predef$/$conforms()
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#org/apache/spark/sql/execution/stat/StatFunctions$/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/collection/mutable/ArrayOps/sorted(scala.math.Ordering)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#org/apache/spark/sql/types/StructType$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#org/apache/spark/sql/catalyst/plans/logical/LocalRelation/LocalRelation(scala.collection.Seq,scala.collection.Seq)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/stat/StatFunctions/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/stat/StatFunctions/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/stat/StatFunctions/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)#scala/Predef$DummyImplicit$/dummyImplicit()
org/apache/spark/sql/execution/stat/StatFunctions/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)#org/apache/spark/sql/execution/stat/StatFunctions$/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)
org/apache/spark/sql/execution/stat/StatFunctions/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)#scala/Array$/fallbackCanBuildFrom(scala.Predef$DummyImplicit)
org/apache/spark/sql/execution/stat/StatFunctions/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)#scala/Predef$/require(boolean,scala.Function0)
org/apache/spark/sql/execution/stat/StatFunctions/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)#scala/collection/Seq/size()
org/apache/spark/sql/execution/stat/StatFunctions/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)#org/apache/spark/sql/execution/stat/StatFunctions$$anonfun$3/3()
org/apache/spark/sql/execution/stat/StatFunctions/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/stat/StatFunctions/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)#scala/Array$/fill(int,scala.Function0,scala.reflect.ClassTag)
org/apache/spark/sql/execution/stat/StatFunctions/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)#org/apache/spark/sql/execution/stat/StatFunctions$$anonfun$1/1(org.apache.spark.sql.Dataset)
org/apache/spark/sql/execution/stat/StatFunctions/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)#org/apache/spark/rdd/RDD/aggregate(java.lang.Object,scala.Function2,scala.Function2,scala.reflect.ClassTag)
org/apache/spark/sql/execution/stat/StatFunctions/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/stat/StatFunctions/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)#org/apache/spark/sql/execution/stat/StatFunctions$$anonfun$multipleApproxQuantiles$2/2(scala.collection.Seq)
org/apache/spark/sql/execution/stat/StatFunctions/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)#org/apache/spark/sql/execution/stat/StatFunctions$$anonfun$2/2(double)
org/apache/spark/sql/execution/stat/StatFunctions/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)#scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)
org/apache/spark/sql/execution/stat/StatFunctions/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)#org/apache/spark/sql/execution/stat/StatFunctions$$anonfun$multipleApproxQuantiles$1/1(double)
org/apache/spark/sql/execution/stat/StatFunctions/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)#org/apache/spark/sql/execution/stat/StatFunctions$$anonfun$4/4()
org/apache/spark/sql/execution/stat/StatFunctions/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/stat/StatFunctions/collectStatisticalData(org.apache.spark.sql.Dataset,scala.collection.Seq,java.lang.String)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/stat/StatFunctions/collectStatisticalData(org.apache.spark.sql.Dataset,scala.collection.Seq,java.lang.String)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/stat/StatFunctions/collectStatisticalData(org.apache.spark.sql.Dataset,scala.collection.Seq,java.lang.String)#org/apache/spark/sql/execution/stat/StatFunctions$$anonfun$collectStatisticalData$1/1(java.lang.String)
org/apache/spark/sql/execution/stat/StatFunctions/collectStatisticalData(org.apache.spark.sql.Dataset,scala.collection.Seq,java.lang.String)#org/apache/spark/rdd/RDD/aggregate(java.lang.Object,scala.Function2,scala.Function2,scala.reflect.ClassTag)
org/apache/spark/sql/execution/stat/StatFunctions/collectStatisticalData(org.apache.spark.sql.Dataset,scala.collection.Seq,java.lang.String)#org/apache/spark/sql/execution/stat/StatFunctions$$anonfun$collectStatisticalData$3/3(java.lang.String)
org/apache/spark/sql/execution/stat/StatFunctions/collectStatisticalData(org.apache.spark.sql.Dataset,scala.collection.Seq,java.lang.String)#scala/collection/IterableLike/foreach(scala.Function1)
org/apache/spark/sql/execution/stat/StatFunctions/collectStatisticalData(org.apache.spark.sql.Dataset,scala.collection.Seq,java.lang.String)#scala/Predef$/require(boolean,scala.Function0)
org/apache/spark/sql/execution/stat/StatFunctions/collectStatisticalData(org.apache.spark.sql.Dataset,scala.collection.Seq,java.lang.String)#org/apache/spark/sql/execution/stat/StatFunctions$$anonfun$collectStatisticalData$4/4()
org/apache/spark/sql/execution/stat/StatFunctions/collectStatisticalData(org.apache.spark.sql.Dataset,scala.collection.Seq,java.lang.String)#org/apache/spark/sql/execution/stat/StatFunctions$$anonfun$collectStatisticalData$5/5()
org/apache/spark/sql/execution/stat/StatFunctions/collectStatisticalData(org.apache.spark.sql.Dataset,scala.collection.Seq,java.lang.String)#org/apache/spark/sql/execution/stat/StatFunctions$$anonfun$collectStatisticalData$2/2(org.apache.spark.sql.Dataset)
org/apache/spark/sql/execution/stat/StatFunctions/collectStatisticalData(org.apache.spark.sql.Dataset,scala.collection.Seq,java.lang.String)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/stat/StatFunctions/collectStatisticalData(org.apache.spark.sql.Dataset,scala.collection.Seq,java.lang.String)#org/apache/spark/sql/execution/stat/StatFunctions$$anonfun$5/5()
org/apache/spark/sql/execution/stat/StatFunctions/collectStatisticalData(org.apache.spark.sql.Dataset,scala.collection.Seq,java.lang.String)#scala/collection/Seq/length()
org/apache/spark/sql/execution/stat/StatFunctions/collectStatisticalData(org.apache.spark.sql.Dataset,scala.collection.Seq,java.lang.String)#org/apache/spark/sql/execution/stat/StatFunctions$CovarianceCounter/CovarianceCounter()
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/estimatePartitionStartIndices(org.apache.spark.MapOutputStatistics[])#scala/math/package$/ceil(double)
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/estimatePartitionStartIndices(org.apache.spark.MapOutputStatistics[])#scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object)
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/estimatePartitionStartIndices(org.apache.spark.MapOutputStatistics[])#scala/reflect/ClassTag$/Int()
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/estimatePartitionStartIndices(org.apache.spark.MapOutputStatistics[])#scala/Predef$/assert(boolean)
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/estimatePartitionStartIndices(org.apache.spark.MapOutputStatistics[])#scala/math/package$/max(long,long)
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/estimatePartitionStartIndices(org.apache.spark.MapOutputStatistics[])#org/apache/spark/sql/execution/exchange/ExchangeCoordinator$$anonfun$estimatePartitionStartIndices$2/2(org.apache.spark.sql.execution.exchange.ExchangeCoordinator)
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/estimatePartitionStartIndices(org.apache.spark.MapOutputStatistics[])#org/apache/spark/sql/execution/exchange/ExchangeCoordinator$$anonfun$2/2(org.apache.spark.sql.execution.exchange.ExchangeCoordinator)
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/estimatePartitionStartIndices(org.apache.spark.MapOutputStatistics[])#org/apache/spark/sql/execution/exchange/ExchangeCoordinator$$anonfun$estimatePartitionStartIndices$1/1(org.apache.spark.sql.execution.exchange.ExchangeCoordinator,long)
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/estimatePartitionStartIndices(org.apache.spark.MapOutputStatistics[])#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/estimatePartitionStartIndices(org.apache.spark.MapOutputStatistics[])#scala/reflect/ClassTag$/Long()
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/estimatePartitionStartIndices(org.apache.spark.MapOutputStatistics[])#scala/Some/x()
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/estimatePartitionStartIndices(org.apache.spark.MapOutputStatistics[])#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/estimatePartitionStartIndices(org.apache.spark.MapOutputStatistics[])#scala/collection/mutable/ArrayBuffer$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/estimatePartitionStartIndices(org.apache.spark.MapOutputStatistics[])#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/estimatePartitionStartIndices(org.apache.spark.MapOutputStatistics[])#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/estimatePartitionStartIndices(org.apache.spark.MapOutputStatistics[])#scala/Predef$/assert(boolean,scala.Function0)
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/estimatePartitionStartIndices(org.apache.spark.MapOutputStatistics[])#scala/Predef$/intArrayOps(int[])
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/estimatePartitionStartIndices(org.apache.spark.MapOutputStatistics[])#scala/Predef$/longArrayOps(long[])
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/estimatePartitionStartIndices(org.apache.spark.MapOutputStatistics[])#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/estimatePartitionStartIndices(org.apache.spark.MapOutputStatistics[])#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/estimatePartitionStartIndices(org.apache.spark.MapOutputStatistics[])#scala/collection/mutable/ArrayOps/sum(scala.math.Numeric)
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/estimatePartitionStartIndices(org.apache.spark.MapOutputStatistics[])#org/apache/spark/sql/execution/exchange/ExchangeCoordinator$$anonfun$1/1(org.apache.spark.sql.execution.exchange.ExchangeCoordinator)
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/estimatePartitionStartIndices(org.apache.spark.MapOutputStatistics[])#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/estimatePartitionStartIndices(org.apache.spark.MapOutputStatistics[])#scala/collection/mutable/ArrayBuffer/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/estimatePartitionStartIndices(org.apache.spark.MapOutputStatistics[])#org/apache/spark/MapOutputStatistics/bytesByPartitionId()
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/estimatePartitionStartIndices(org.apache.spark.MapOutputStatistics[])#scala/collection/mutable/ArrayOps/distinct()
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/estimatePartitionStartIndices(org.apache.spark.MapOutputStatistics[])#scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/estimatePartitionStartIndices(org.apache.spark.MapOutputStatistics[])#scala/collection/mutable/ArrayOps/head()
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/estimatePartitionStartIndices(org.apache.spark.MapOutputStatistics[])#scala/math/package$/min(long,long)
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/doEstimationIfNecessary()#scala/collection/mutable/ArrayBuffer$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/doEstimationIfNecessary()#scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object)
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/doEstimationIfNecessary()#org/apache/spark/FutureAction/get()
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/doEstimationIfNecessary()#org/apache/spark/rdd/RDD/partitions()
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/doEstimationIfNecessary()#scala/Predef$/assert(boolean)
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/doEstimationIfNecessary()#scala/collection/mutable/ArrayBuffer/length()
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/doEstimationIfNecessary()#java/util/Map/putAll(java.util.Map)
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/doEstimationIfNecessary()#java/util/HashMap/put(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/doEstimationIfNecessary()#java/util/HashMap/HashMap(int)
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/doEstimationIfNecessary()#org/apache/spark/ShuffleDependency/rdd()
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/doEstimationIfNecessary()#java/util/HashMap/size()
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/doEstimationIfNecessary()#org/apache/spark/SparkContext/submitMapStage(org.apache.spark.ShuffleDependency)
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/doEstimationIfNecessary()#java/util/Map/isEmpty()
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/doEstimationIfNecessary()#scala/collection/mutable/ArrayBuffer/apply(int)
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/doEstimationIfNecessary()#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/doEstimationIfNecessary()#org/apache/spark/sql/execution/exchange/ShuffleExchange/sqlContext()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetSchema(org.apache.parquet.schema.MessageType,org.apache.spark.sql.types.StructType)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetSchema(org.apache.parquet.schema.MessageType,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter$/SPARK_PARQUET_SCHEMA_NAME()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetSchema(org.apache.parquet.schema.MessageType,org.apache.spark.sql.types.StructType)#org/apache/parquet/schema/Types/buildMessage()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetSchema(org.apache.parquet.schema.MessageType,org.apache.spark.sql.types.StructType)#org/apache/parquet/schema/Types$MessageTypeBuilder/addFields(org.apache.parquet.schema.Type[])
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetSchema(org.apache.parquet.schema.MessageType,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport$/clipParquetGroupFields(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetSchema(org.apache.parquet.schema.MessageType,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport$/clipParquetSchema(org.apache.parquet.schema.MessageType,org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetSchema(org.apache.parquet.schema.MessageType,org.apache.spark.sql.types.StructType)#org/apache/parquet/schema/Types$BaseGroupBuilder/named(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetSchema(org.apache.parquet.schema.MessageType,org.apache.spark.sql.types.StructType)#org/apache/parquet/schema/MessageType/asGroupType()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetSchema(org.apache.parquet.schema.MessageType,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter$/EMPTY_MESSAGE()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetSchema(org.apache.parquet.schema.MessageType,org.apache.spark.sql.types.StructType)#scala/collection/Seq/isEmpty()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetSchema(org.apache.parquet.schema.MessageType,org.apache.spark.sql.types.StructType)#scala/collection/Seq/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/init(org.apache.parquet.hadoop.api.InitContext)#org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport$/SPARK_ROW_REQUESTED_SCHEMA()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/init(org.apache.parquet.hadoop.api.InitContext)#scala/Predef$/Map()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/init(org.apache.parquet.hadoop.api.InitContext)#scala/collection/convert/Decorators$AsJava/asJava()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/init(org.apache.parquet.hadoop.api.InitContext)#org/apache/spark/sql/types/StructType$/fromString(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/init(org.apache.parquet.hadoop.api.InitContext)#org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport$/clipParquetSchema(org.apache.parquet.schema.MessageType,org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/init(org.apache.parquet.hadoop.api.InitContext)#org/apache/parquet/hadoop/api/InitContext/getFileSchema()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/init(org.apache.parquet.hadoop.api.InitContext)#scala/collection/JavaConverters$/mapAsJavaMapConverter(scala.collection.Map)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/init(org.apache.parquet.hadoop.api.InitContext)#org/apache/parquet/hadoop/api/ReadSupport$ReadContext/ReadContext(org.apache.parquet.schema.MessageType,java.util.Map)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/init(org.apache.parquet.hadoop.api.InitContext)#scala/collection/immutable/Map$/empty()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/init(org.apache.parquet.hadoop.api.InitContext)#org/apache/parquet/hadoop/api/InitContext/getConfiguration()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/init(org.apache.parquet.hadoop.api.InitContext)#org/apache/hadoop/conf/Configuration/get(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/init(org.apache.parquet.hadoop.api.InitContext)#org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport$$anonfun$init$1/1(org.apache.spark.sql.execution.datasources.parquet.ParquetReadSupport)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/init(org.apache.parquet.hadoop.api.InitContext)#scala/Predef$/assert(boolean,scala.Function0)
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/partitionColumnsSchema/1/anonfun/apply/11/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/partitionColumnsSchema/1/anonfun/apply/11/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/partitionColumnsSchema/1/anonfun/apply/11/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/partitionColumnsSchema/1/anonfun/apply/11/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/partitionColumnsSchema/1/anonfun/apply/11/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/partitionColumnsSchema/1/anonfun/apply/11/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/partitionColumnsSchema/1/anonfun/apply/11/apply()#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$partitionColumnsSchema$1$$anonfun$apply$11/apply()
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/partitionColumnsSchema/1/anonfun/apply/11/apply()#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/partitionColumnsSchema/1/anonfun/apply/11/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/partitionColumnsSchema/1/anonfun/apply/11/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/MemorySink/addBatch(long,org.apache.spark.sql.Dataset)#scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object)
org/apache/spark/sql/execution/streaming/MemorySink/addBatch(long,org.apache.spark.sql.Dataset)#scala/Option/isEmpty()
org/apache/spark/sql/execution/streaming/MemorySink/addBatch(long,org.apache.spark.sql.Dataset)#org/apache/spark/sql/execution/streaming/MemorySink$$anonfun$addBatch$1/1(org.apache.spark.sql.execution.streaming.MemorySink,long)
org/apache/spark/sql/execution/streaming/MemorySink/addBatch(long,org.apache.spark.sql.Dataset)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/MemorySink/addBatch(long,org.apache.spark.sql.Dataset)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/execution/streaming/MemorySink/addBatch(long,org.apache.spark.sql.Dataset)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/MemorySink/addBatch(long,org.apache.spark.sql.Dataset)#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/streaming/MemorySink/addBatch(long,org.apache.spark.sql.Dataset)#scala/collection/mutable/ArrayBuffer/clear()
org/apache/spark/sql/execution/streaming/MemorySink/addBatch(long,org.apache.spark.sql.Dataset)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/streaming/MemorySink/addBatch(long,org.apache.spark.sql.Dataset)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/MemorySink/addBatch(long,org.apache.spark.sql.Dataset)#org/apache/spark/sql/execution/streaming/MemorySink$AddedData$/apply(long,org.apache.spark.sql.Row[])
org/apache/spark/sql/execution/streaming/MemorySink/addBatch(long,org.apache.spark.sql.Dataset)#scala/runtime/BoxesRunTime/boxToBoolean(boolean)
org/apache/spark/sql/execution/streaming/MemorySink/addBatch(long,org.apache.spark.sql.Dataset)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/MemorySink/addBatch(long,org.apache.spark.sql.Dataset)#scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)
org/apache/spark/sql/execution/streaming/MemorySink/addBatch(long,org.apache.spark.sql.Dataset)#org/apache/spark/sql/execution/streaming/MemorySink$$anonfun$addBatch$2/2(org.apache.spark.sql.execution.streaming.MemorySink,long)
org/apache/spark/sql/execution/streaming/MemorySink/addBatch(long,org.apache.spark.sql.Dataset)#scala/Option/get()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/infer(org.apache.spark.rdd.RDD,org.apache.spark.sql.catalyst.json.JSONOptions,scala.Function2)#org/apache/spark/rdd/RDD/mapPartitions(scala.Function1,boolean,scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/infer(org.apache.spark.rdd.RDD,org.apache.spark.sql.catalyst.json.JSONOptions,scala.Function2)#org/apache/spark/rdd/RDD/fold(java.lang.Object,scala.Function2)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/infer(org.apache.spark.rdd.RDD,org.apache.spark.sql.catalyst.json.JSONOptions,scala.Function2)#org/apache/spark/rdd/RDD/mapPartitions$default$2()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/infer(org.apache.spark.rdd.RDD,org.apache.spark.sql.catalyst.json.JSONOptions,scala.Function2)#org/apache/spark/sql/catalyst/json/JSONOptions/columnNameOfCorruptRecord()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/infer(org.apache.spark.rdd.RDD,org.apache.spark.sql.catalyst.json.JSONOptions,scala.Function2)#org/apache/spark/sql/execution/datasources/json/JsonInferSchema$$anonfun$1/1(org.apache.spark.sql.catalyst.json.JSONOptions,scala.Function2,org.apache.spark.sql.catalyst.util.ParseMode,java.lang.String)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/infer(org.apache.spark.rdd.RDD,org.apache.spark.sql.catalyst.json.JSONOptions,scala.Function2)#org/apache/spark/sql/types/StructType$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/infer(org.apache.spark.rdd.RDD,org.apache.spark.sql.catalyst.json.JSONOptions,scala.Function2)#org/apache/spark/sql/catalyst/json/JSONOptions/parseMode()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/infer(org.apache.spark.rdd.RDD,org.apache.spark.sql.catalyst.json.JSONOptions,scala.Function2)#org/apache/spark/sql/execution/datasources/json/JsonInferSchema$/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$canonicalizeType(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/infer(org.apache.spark.rdd.RDD,org.apache.spark.sql.catalyst.json.JSONOptions,scala.Function2)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/infer(org.apache.spark.rdd.RDD,org.apache.spark.sql.catalyst.json.JSONOptions,scala.Function2)#scala/Some/x()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/infer(org.apache.spark.rdd.RDD,org.apache.spark.sql.catalyst.json.JSONOptions,scala.Function2)#org/apache/spark/sql/execution/datasources/json/JsonInferSchema$/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$compatibleRootType(java.lang.String,org.apache.spark.sql.catalyst.util.ParseMode)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/infer(org.apache.spark.rdd.RDD,org.apache.spark.sql.catalyst.json.JSONOptions,scala.Function2)#org/apache/spark/sql/execution/datasources/json/JsonInferSchema$/infer(org.apache.spark.rdd.RDD,org.apache.spark.sql.catalyst.json.JSONOptions,scala.Function2)
org/apache/spark/sql/execution/joins/HashJoin/anonfun/existenceJoin/1/apply(org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/sql/catalyst/expressions/UnsafeProjection/apply(org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/joins/HashJoin/anonfun/existenceJoin/1/apply(org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/sql/catalyst/expressions/JoinedRow/apply(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/joins/HashJoin/anonfun/existenceJoin/1/apply(org.apache.spark.sql.catalyst.InternalRow)#scala/Option/isEmpty()
org/apache/spark/sql/execution/joins/HashJoin/anonfun/existenceJoin/1/apply(org.apache.spark.sql.catalyst.InternalRow)#scala/runtime/VolatileByteRef/create(byte)
org/apache/spark/sql/execution/joins/HashJoin/anonfun/existenceJoin/1/apply(org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/sql/catalyst/expressions/GenericInternalRow/setBoolean(int,boolean)
org/apache/spark/sql/execution/joins/HashJoin/anonfun/existenceJoin/1/apply(org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/anyNull()
org/apache/spark/sql/execution/joins/HashJoin/anonfun/existenceJoin/1/apply(org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/sql/execution/joins/HashJoin$$anonfun$existenceJoin$1$$anonfun$6/6(org.apache.spark.sql.execution.joins.HashJoin$$anonfun$existenceJoin$1,org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/joins/HashJoin/anonfun/existenceJoin/1/apply(org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/sql/execution/joins/HashJoin$$anonfun$existenceJoin$1/buildIter$3(org.apache.spark.sql.catalyst.expressions.UnsafeRow,scala.runtime.ObjectRef,scala.runtime.VolatileByteRef)
org/apache/spark/sql/execution/joins/HashJoin/anonfun/existenceJoin/1/apply(org.apache.spark.sql.catalyst.InternalRow)#scala/runtime/ObjectRef/zero()
org/apache/spark/sql/execution/joins/HashJoin/anonfun/existenceJoin/1/apply(org.apache.spark.sql.catalyst.InternalRow)#scala/collection/Iterator/exists(scala.Function1)
org/apache/spark/sql/execution/streaming/MemorySink/anonfun/toDebugString/1/apply(org.apache.spark.sql.execution.streaming.MemorySink$AddedData)#scala/util/control/NonFatal$/unapply(java.lang.Throwable)
org/apache/spark/sql/execution/streaming/MemorySink/anonfun/toDebugString/1/apply(org.apache.spark.sql.execution.streaming.MemorySink$AddedData)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/MemorySink/anonfun/toDebugString/1/apply(org.apache.spark.sql.execution.streaming.MemorySink$AddedData)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/MemorySink/anonfun/toDebugString/1/apply(org.apache.spark.sql.execution.streaming.MemorySink$AddedData)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/streaming/MemorySink/anonfun/toDebugString/1/apply(org.apache.spark.sql.execution.streaming.MemorySink$AddedData)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/streaming/MemorySink/anonfun/toDebugString/1/apply(org.apache.spark.sql.execution.streaming.MemorySink$AddedData)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/MemorySink/anonfun/toDebugString/1/apply(org.apache.spark.sql.execution.streaming.MemorySink$AddedData)#scala/collection/mutable/ArrayOps/mkString(java.lang.String)
org/apache/spark/sql/execution/streaming/MemorySink/anonfun/toDebugString/1/apply(org.apache.spark.sql.execution.streaming.MemorySink$AddedData)#org/apache/spark/sql/execution/streaming/MemorySink$AddedData/batchId()
org/apache/spark/sql/execution/streaming/MemorySink/anonfun/toDebugString/1/apply(org.apache.spark.sql.execution.streaming.MemorySink$AddedData)#org/apache/spark/sql/execution/streaming/MemorySink$AddedData/data()
org/apache/spark/sql/execution/streaming/MemorySink/anonfun/toDebugString/1/apply(org.apache.spark.sql.execution.streaming.MemorySink$AddedData)#scala/Option/isEmpty()
org/apache/spark/sql/execution/streaming/MemorySink/anonfun/toDebugString/1/apply(org.apache.spark.sql.execution.streaming.MemorySink$AddedData)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/streaming/MemorySink/anonfun/toDebugString/1/apply(org.apache.spark.sql.execution.streaming.MemorySink$AddedData)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/MemoryStream/anonfun/getBatch/1/apply()#scala/collection/generic/TraversableForwarder/mkString(java.lang.String)
org/apache/spark/sql/execution/streaming/MemoryStream/anonfun/getBatch/1/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/MemoryStream/anonfun/getBatch/1/apply()#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/streaming/MemoryStream/anonfun/getBatch/1/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/MemoryStream/anonfun/getBatch/1/apply()#scala/collection/mutable/ListBuffer/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/streaming/MemoryStream/anonfun/getBatch/1/apply()#org/apache/spark/sql/execution/streaming/MemoryStream$$anonfun$getBatch$1$$anonfun$apply$1/1(org.apache.spark.sql.execution.streaming.MemoryStream$$anonfun$getBatch$1)
org/apache/spark/sql/execution/streaming/MemoryStream/anonfun/getBatch/1/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/MemoryStream/anonfun/getBatch/1/apply()#org/apache/spark/sql/execution/streaming/MemoryStream$$anonfun$getBatch$1/apply()
org/apache/spark/sql/execution/streaming/MemoryStream/anonfun/getBatch/1/apply()#scala/collection/mutable/ListBuffer$/canBuildFrom()
org/apache/spark/sql/execution/streaming/MemoryStream/anonfun/getBatch/1/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/org$apache$spark$sql$execution$datasources$InsertIntoHadoopFsRelationCommand$$refreshPartitionsCallback$1(scala.collection.Seq,org.apache.spark.sql.SparkSession,boolean,scala.runtime.ObjectRef)#scala/collection/Seq/toSet()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/org$apache$spark$sql$execution$datasources$InsertIntoHadoopFsRelationCommand$$refreshPartitionsCallback$1(scala.collection.Seq,org.apache.spark.sql.SparkSession,boolean,scala.runtime.ObjectRef)#scala/Option/get()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/org$apache$spark$sql$execution$datasources$InsertIntoHadoopFsRelationCommand$$refreshPartitionsCallback$1(scala.collection.Seq,org.apache.spark.sql.SparkSession,boolean,scala.runtime.ObjectRef)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/org$apache$spark$sql$execution$datasources$InsertIntoHadoopFsRelationCommand$$refreshPartitionsCallback$1(scala.collection.Seq,org.apache.spark.sql.SparkSession,boolean,scala.runtime.ObjectRef)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/org$apache$spark$sql$execution$datasources$InsertIntoHadoopFsRelationCommand$$refreshPartitionsCallback$1(scala.collection.Seq,org.apache.spark.sql.SparkSession,boolean,scala.runtime.ObjectRef)#scala/collection/immutable/Set/$minus$minus(scala.collection.GenTraversableOnce)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/org$apache$spark$sql$execution$datasources$InsertIntoHadoopFsRelationCommand$$refreshPartitionsCallback$1(scala.collection.Seq,org.apache.spark.sql.SparkSession,boolean,scala.runtime.ObjectRef)#scala/collection/immutable/Set/toSeq()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/org$apache$spark$sql$execution$datasources$InsertIntoHadoopFsRelationCommand$$refreshPartitionsCallback$1(scala.collection.Seq,org.apache.spark.sql.SparkSession,boolean,scala.runtime.ObjectRef)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/org$apache$spark$sql$execution$datasources$InsertIntoHadoopFsRelationCommand$$refreshPartitionsCallback$1(scala.collection.Seq,org.apache.spark.sql.SparkSession,boolean,scala.runtime.ObjectRef)#org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/org$apache$spark$sql$execution$datasources$InsertIntoHadoopFsRelationCommand$$refreshPartitionsCallback$1(scala.collection.Seq,org.apache.spark.sql.SparkSession,boolean,scala.runtime.ObjectRef)#scala/collection/immutable/Set/nonEmpty()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/org$apache$spark$sql$execution$datasources$InsertIntoHadoopFsRelationCommand$$refreshPartitionsCallback$1(scala.collection.Seq,org.apache.spark.sql.SparkSession,boolean,scala.runtime.ObjectRef)#org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand$$anonfun$org$apache$spark$sql$execution$datasources$InsertIntoHadoopFsRelationCommand$$refreshPartitionsCallback$1$1/1(org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/deleteMatchingPartitions(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,scala.collection.immutable.Map,org.apache.spark.internal.io.FileCommitProtocol)#org/apache/hadoop/fs/Path/suffix(java.lang.String)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/deleteMatchingPartitions(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,scala.collection.immutable.Map,org.apache.spark.internal.io.FileCommitProtocol)#java/io/IOException/IOException(java.lang.String)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/deleteMatchingPartitions(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,scala.collection.immutable.Map,org.apache.spark.internal.io.FileCommitProtocol)#org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand$$anonfun$deleteMatchingPartitions$2/2(org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand,org.apache.hadoop.fs.FileSystem,org.apache.spark.internal.io.FileCommitProtocol)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/deleteMatchingPartitions(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,scala.collection.immutable.Map,org.apache.spark.internal.io.FileCommitProtocol)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/deleteMatchingPartitions(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,scala.collection.immutable.Map,org.apache.spark.internal.io.FileCommitProtocol)#org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand$$anonfun$deleteMatchingPartitions$1/1(org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/deleteMatchingPartitions(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,scala.collection.immutable.Map,org.apache.spark.internal.io.FileCommitProtocol)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/deleteMatchingPartitions(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,scala.collection.immutable.Map,org.apache.spark.internal.io.FileCommitProtocol)#scala/collection/immutable/Map/withFilter(scala.Function1)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/deleteMatchingPartitions(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,scala.collection.immutable.Map,org.apache.spark.internal.io.FileCommitProtocol)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/deleteMatchingPartitions(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,scala.collection.immutable.Map,org.apache.spark.internal.io.FileCommitProtocol)#scala/collection/immutable/Map/nonEmpty()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/deleteMatchingPartitions(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,scala.collection.immutable.Map,org.apache.spark.internal.io.FileCommitProtocol)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/deleteMatchingPartitions(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,scala.collection.immutable.Map,org.apache.spark.internal.io.FileCommitProtocol)#org/apache/hadoop/fs/FileSystem/exists(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/deleteMatchingPartitions(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,scala.collection.immutable.Map,org.apache.spark.internal.io.FileCommitProtocol)#org/apache/spark/internal/io/FileCommitProtocol/deleteWithJob(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/deleteMatchingPartitions(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,scala.collection.immutable.Map,org.apache.spark.internal.io.FileCommitProtocol)#scala/collection/TraversableOnce/mkString(java.lang.String)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/deleteMatchingPartitions(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,scala.collection.immutable.Map,org.apache.spark.internal.io.FileCommitProtocol)#scala/collection/generic/FilterMonadic/foreach(scala.Function1)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/deleteMatchingPartitions(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,scala.collection.immutable.Map,org.apache.spark.internal.io.FileCommitProtocol)#scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/deleteMatchingPartitions(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,scala.collection.immutable.Map,org.apache.spark.internal.io.FileCommitProtocol)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/deleteMatchingPartitions(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,scala.collection.immutable.Map,org.apache.spark.internal.io.FileCommitProtocol)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/deleteMatchingPartitions(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,scala.collection.immutable.Map,org.apache.spark.internal.io.FileCommitProtocol)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/deleteMatchingPartitions(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,scala.collection.immutable.Map,org.apache.spark.internal.io.FileCommitProtocol)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/deleteMatchingPartitions(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,scala.collection.immutable.Map,org.apache.spark.internal.io.FileCommitProtocol)#org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand$$anonfun$3/3(org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/internal/SQLConf/manageFilesourcePartitions()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/types/StructType/fieldNames()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/TraversableOnce/mkString(java.lang.String)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/Dataset$/ofRows(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/datasources/FileFormatWriter$/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/foreach(scala.Function1)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/hadoop/fs/Path/toString()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/Map()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand$$anonfun$run$1/1(org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#java/lang/IllegalStateException/IllegalStateException(java.lang.String)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/internal/SQLConf/fileCommitProtocolClass()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand$$anonfun$run$3/3(org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/tracksPartitionsInCatalog()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/hadoop/fs/FileSystem/getUri()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/hadoop/fs/FileSystem/exists(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/empty()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand$$anonfun$run$4/4(org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/ArrayOps/groupBy(scala.Function1)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/isDefined()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#scala/Tuple2/_1()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand$$anonfun$run$2/2(org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand,org.apache.spark.sql.SparkSession,boolean,scala.runtime.ObjectRef)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#scala/Tuple2/_2$mcZ$sp()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/listPartitions(org.apache.spark.sql.catalyst.TableIdentifier,scala.Option)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/internal/io/FileCommitProtocol$/instantiate(java.lang.String,java.lang.String,java.lang.String,boolean)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#java/util/UUID/randomUUID()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/schema()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#scala/runtime/BoxesRunTime/boxToBoolean(boolean)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/datasources/FileFormatWriter$OutputSpec/OutputSpec(java.lang.String,scala.collection.immutable.Map)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/logInfo(scala.Function0)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/immutable/Iterable$/canBuildFrom()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/immutable/Map$/empty()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/hadoop/fs/Path/makeQualified(java.net.URI,org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/hadoop/fs/FileSystem/getWorkingDirectory()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/ArrayOps/distinct()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand$$anonfun$1/1(org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#java/util/UUID/toString()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand$$anonfun$2/2(org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#scala/runtime/ObjectRef/create(java.lang.Object)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/immutable/Map/collect(scala.PartialFunction,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/get()
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/genCodeToSetAggBuffers/1/1/apply(scala.Tuple2)#scala/collection/Seq/apply(int)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/genCodeToSetAggBuffers/1/1/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/genCodeToSetAggBuffers/1/1/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/genCodeToSetAggBuffers/1/1/apply(scala.Tuple2)#org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/groupingKeys()
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/genCodeToSetAggBuffers/1/1/apply(scala.Tuple2)#scala/collection/Seq/length()
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/genCodeToSetAggBuffers/1/1/apply(scala.Tuple2)#scala/Tuple2/_2$mcI$sp()
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/genCodeToSetAggBuffers/1/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/updateColumn(java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType,int,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode,boolean)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/genCodeToSetAggBuffers/1/1/apply(scala.Tuple2)#org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/buffVars()
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/genCodeToSetAggBuffers/1/1/apply(scala.Tuple2)#org/apache/spark/sql/execution/aggregate/HashMapGenerator$Buffer/dataType()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/anonfun/org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateLocationSize/2/anonfun/apply/2/apply()#org/apache/spark/sql/execution/command/AnalyzeTableCommand$$anonfun$org$apache$spark$sql$execution$command$AnalyzeTableCommand$$calculateLocationSize$2$$anonfun$apply$2/apply()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/anonfun/org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateLocationSize/2/anonfun/apply/2/apply()#org/apache/spark/sql/catalyst/TableIdentifier/table()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/anonfun/org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateLocationSize/2/anonfun/apply/2/apply()#java/lang/Throwable/toString()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/anonfun/org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateLocationSize/2/anonfun/apply/2/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/AnalyzeTableCommand/anonfun/org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateLocationSize/2/anonfun/apply/2/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/anonfun/org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateLocationSize/2/anonfun/apply/2/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/anonfun/org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateLocationSize/2/anonfun/apply/2/apply()#org/apache/spark/sql/catalyst/TableIdentifier/database()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/anonfun/org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateLocationSize/2/anonfun/apply/2/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/anonfun/org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateLocationSize/2/anonfun/apply/2/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/anonfun/org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateLocationSize/2/anonfun/apply/2/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/anonfun/org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateLocationSize/2/anonfun/apply/2/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/BaseLimitExec/class/doConsume(org.apache.spark.sql.execution.BaseLimitExec,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/BaseLimitExec/class/doConsume(org.apache.spark.sql.execution.BaseLimitExec,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/execution/BaseLimitExec/consume$default$3()
org/apache/spark/sql/execution/BaseLimitExec/class/doConsume(org.apache.spark.sql.execution.BaseLimitExec,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/BaseLimitExec/class/doConsume(org.apache.spark.sql.execution.BaseLimitExec,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addNewFunction(java.lang.String,java.lang.String)
org/apache/spark/sql/execution/BaseLimitExec/class/doConsume(org.apache.spark.sql.execution.BaseLimitExec,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/BaseLimitExec/class/doConsume(org.apache.spark.sql.execution.BaseLimitExec,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/BaseLimitExec/class/doConsume(org.apache.spark.sql.execution.BaseLimitExec,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/freshName(java.lang.String)
org/apache/spark/sql/execution/BaseLimitExec/class/doConsume(org.apache.spark.sql.execution.BaseLimitExec,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/BaseLimitExec/class/doConsume(org.apache.spark.sql.execution.BaseLimitExec,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/execution/BaseLimitExec/consume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,java.lang.String)
org/apache/spark/sql/execution/BaseLimitExec/class/doConsume(org.apache.spark.sql.execution.BaseLimitExec,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/BaseLimitExec/class/doConsume(org.apache.spark.sql.execution.BaseLimitExec,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/BaseLimitExec/class/doConsume(org.apache.spark.sql.execution.BaseLimitExec,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addMutableState(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/BaseLimitExec/class/doConsume(org.apache.spark.sql.execution.BaseLimitExec,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/datasources/FileSourceStrategy$/logInfo(scala.Function0)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/datasources/FileSourceStrategy$$anonfun$apply$4/4(org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/expressions/ExpressionSet/$minus$minus(scala.collection.GenTraversableOnce)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/expressions/ExpressionSet$/apply(scala.collection.TraversableOnce)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/expressions/AttributeSet$/apply(scala.collection.Iterable)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/datasources/FileSourceStrategy$$anonfun$3/3(org.apache.spark.sql.catalyst.expressions.AttributeSet)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/expressions/AttributeSet/toSeq()
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/expressions/ExpressionSet/filter(scala.Function1)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/datasources/FileSourceStrategy$$anonfun$4/4()
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Option/get()
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/analysis/Analyzer/resolver()
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/datasources/FileSourceStrategy$$anonfun$7/7()
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/Seq/reduceOption(scala.Function2)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/datasources/FileSourceStrategy$$anonfun$apply$2/2(org.apache.spark.sql.catalyst.expressions.ExpressionSet)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/Seq/filter(scala.Function1)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Tuple3/_1()
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Tuple3/_2()
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/datasources/FileSourceStrategy$$anonfun$9/9(org.apache.spark.sql.execution.FileSourceScanExec)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/QueryPlan/output()
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Tuple3/_3()
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/datasources/FileSourceStrategy$$anonfun$6/6(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Option/isEmpty()
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/datasources/FileSourceStrategy$$anonfun$8/8(org.apache.spark.sql.execution.FileSourceScanExec)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/datasources/FileSourceStrategy$$anonfun$1/1(org.apache.spark.sql.execution.datasources.LogicalRelation)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/planning/PhysicalOperation$/unapply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/TraversableLike/filterNot(scala.Function1)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/datasources/FileSourceStrategy$$anonfun$2/2(org.apache.spark.sql.catalyst.expressions.AttributeSet)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/expressions/package$AttributeSeq/toStructType()
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/expressions/package$/AttributeSeq(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/datasources/FileSourceStrategy$$anonfun$apply$3/3(scala.collection.immutable.Set)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/datasources/FileSourceStrategy$/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/datasources/FileSourceStrategy$$anonfun$5/5(org.apache.spark.sql.catalyst.expressions.AttributeSet)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/expressions/ExpressionSet/toSeq()
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/datasources/LogicalRelation/resolve(org.apache.spark.sql.types.StructType,scala.Function2)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/immutable/Set/toSeq()
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/CodegenSupport/anonfun/evaluateRequiredVariables/1/apply(scala.Tuple2)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/CodegenSupport/anonfun/evaluateRequiredVariables/1/apply(scala.Tuple2)#java/lang/String/trim()
org/apache/spark/sql/execution/CodegenSupport/anonfun/evaluateRequiredVariables/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/code()
org/apache/spark/sql/execution/CodegenSupport/anonfun/evaluateRequiredVariables/1/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/CodegenSupport/anonfun/evaluateRequiredVariables/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/AttributeSet/contains(org.apache.spark.sql.catalyst.expressions.NamedExpression)
org/apache/spark/sql/execution/CodegenSupport/anonfun/evaluateRequiredVariables/1/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/CodegenSupport/anonfun/evaluateRequiredVariables/1/apply(scala.Tuple2)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/CodegenSupport/anonfun/evaluateRequiredVariables/1/apply(scala.Tuple2)#scala/Tuple2/_2$mcI$sp()
org/apache/spark/sql/execution/CodegenSupport/anonfun/evaluateRequiredVariables/1/apply(scala.Tuple2)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/CodegenSupport/anonfun/evaluateRequiredVariables/1/apply(scala.Tuple2)#scala/collection/mutable/StringBuilder/append(java.lang.String)
org/apache/spark/sql/execution/CodegenSupport/anonfun/evaluateRequiredVariables/1/apply(scala.Tuple2)#scala/collection/Seq/apply(int)
org/apache/spark/sql/execution/CodegenSupport/anonfun/evaluateRequiredVariables/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/code_$eq(java.lang.String)
org/apache/spark/sql/execution/CodegenSupport/anonfun/evaluateRequiredVariables/1/apply(scala.Tuple2)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenInner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenInner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/broadcast/Broadcast/value()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenInner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenInner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenInner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#java/lang/Class/getName()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenInner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenInner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/freshName(java.lang.String)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenInner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenInner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenInner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenInner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenInner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenInner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/code()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenInner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Tuple3/_1()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenInner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/copyResult_$eq(boolean)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenInner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Tuple2/_1()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenInner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Tuple3/_2()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenInner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/value()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenInner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenInner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Tuple2/_2()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenInner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Tuple3/_3()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenInner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenInner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Tuple3/Tuple3(java.lang.Object,java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenInner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/genStreamSideJoinKey(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/currentVars_$eq(scala.collection.Seq)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/genStreamSideJoinKey(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/genStreamSideJoinKey(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/Expression/genCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/genStreamSideJoinKey(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/isNull()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/genStreamSideJoinKey(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/genStreamSideJoinKey(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/GenerateUnsafeProjection$/createCode$default$3()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/genStreamSideJoinKey(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/Seq/head()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/genStreamSideJoinKey(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/genStreamSideJoinKey(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/Expression/dataType()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/genStreamSideJoinKey(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/genStreamSideJoinKey(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/genStreamSideJoinKey(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/value()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/genStreamSideJoinKey(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/genStreamSideJoinKey(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/GenerateUnsafeProjection$/createCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,boolean)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/genStreamSideJoinKey(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/Seq/length()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenAnti(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenAnti(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/broadcast/Broadcast/value()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenAnti(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenAnti(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#java/lang/Class/getName()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenAnti(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenAnti(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/freshName(java.lang.String)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenAnti(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenAnti(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenAnti(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenAnti(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/code()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenAnti(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Tuple3/_1()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenAnti(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Tuple2/_1()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenAnti(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Tuple3/_2()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenAnti(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/value()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenAnti(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenAnti(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Tuple2/_2()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenAnti(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenAnti(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/prepareBroadcast(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/prepareBroadcast(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/broadcast/Broadcast/value()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/prepareBroadcast(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/prepareBroadcast(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#java/lang/Class/getName()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/prepareBroadcast(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/prepareBroadcast(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/freshName(java.lang.String)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/prepareBroadcast(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/prepareBroadcast(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/prepareBroadcast(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addMutableState(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/prepareBroadcast(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/prepareBroadcast(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#java/lang/Object/getClass()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/prepareBroadcast(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/prepareBroadcast(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addReferenceObj(java.lang.String,java.lang.Object,java.lang.String)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/prepareBroadcast(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/prepareBroadcast(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addReferenceObj$default$3()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenExistence(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/ExprCode(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenExistence(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenExistence(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Option/isDefined()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenExistence(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/isNull()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenExistence(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenExistence(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#java/lang/Class/getName()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenExistence(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenExistence(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/BindReferences$/bindReference(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.package$AttributeSeq,boolean)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenExistence(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenExistence(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenExistence(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenExistence(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/currentVars_$eq(scala.collection.Seq)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenExistence(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/BindReferences$/bindReference$default$3()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenExistence(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenExistence(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/broadcast/Broadcast/value()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenExistence(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/code()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenExistence(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/Expression/genCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenExistence(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Tuple2/_1()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenExistence(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/value()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenExistence(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenExistence(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Tuple2/_2()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenExistence(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenExistence(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenExistence(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenExistence(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/package$/AttributeSeq(scala.collection.Seq)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenExistence(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/freshName(java.lang.String)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenExistence(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenExistence(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/Expression/references()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenExistence(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenExistence(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Option/get()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenSemi(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenSemi(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/broadcast/Broadcast/value()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenSemi(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenSemi(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#java/lang/Class/getName()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenSemi(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenSemi(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/freshName(java.lang.String)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenSemi(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenSemi(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenSemi(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenSemi(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/code()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenSemi(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Tuple3/_1()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenSemi(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Tuple2/_1()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenSemi(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Tuple3/_2()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenSemi(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/value()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenSemi(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenSemi(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Tuple2/_2()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenSemi(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenSemi(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenOuter(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/isNull()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenOuter(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenOuter(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenOuter(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#java/lang/Class/getName()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenOuter(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/BindReferences$/bindReference(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.package$AttributeSeq,boolean)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenOuter(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenOuter(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenOuter(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenOuter(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/currentVars_$eq(scala.collection.Seq)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenOuter(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenOuter(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenOuter(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/BindReferences$/bindReference$default$3()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenOuter(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenOuter(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/broadcast/Broadcast/value()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenOuter(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#java/lang/String/trim()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenOuter(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Option/isDefined()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenOuter(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/code()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenOuter(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/Expression/genCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenOuter(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/Seq/filter(scala.Function1)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenOuter(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/copyResult_$eq(boolean)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenOuter(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Tuple2/_1()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenOuter(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/value()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenOuter(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenOuter(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Tuple2/_2()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenOuter(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/TraversableOnce/mkString(java.lang.String)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenOuter(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/execution/joins/BroadcastHashJoinExec$$anonfun$codegenOuter$1/1(org.apache.spark.sql.execution.joins.BroadcastHashJoinExec)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenOuter(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenOuter(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenOuter(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenOuter(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/execution/joins/BroadcastHashJoinExec$$anonfun$codegenOuter$2/2(org.apache.spark.sql.execution.joins.BroadcastHashJoinExec)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenOuter(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/package$/AttributeSeq(scala.collection.Seq)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenOuter(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/freshName(java.lang.String)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenOuter(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenOuter(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/Expression/references()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenOuter(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/codegenOuter(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Option/get()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/getJoinCondition(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,boolean)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/getJoinCondition(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,boolean)#scala/Option/isDefined()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/getJoinCondition(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,boolean)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/isNull()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/getJoinCondition(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,boolean)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/getJoinCondition(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,boolean)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/getJoinCondition(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,boolean)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/getJoinCondition(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,boolean)#scala/Tuple3/Tuple3(java.lang.Object,java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/getJoinCondition(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,boolean)#org/apache/spark/sql/catalyst/expressions/BindReferences$/bindReference(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.package$AttributeSeq,boolean)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/getJoinCondition(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,boolean)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/getJoinCondition(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,boolean)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/getJoinCondition(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,boolean)#scala/Option/get()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/getJoinCondition(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,boolean)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/getJoinCondition(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,boolean)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/currentVars_$eq(scala.collection.Seq)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/getJoinCondition(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,boolean)#org/apache/spark/sql/catalyst/expressions/BindReferences$/bindReference$default$3()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/getJoinCondition(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,boolean)#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/getJoinCondition(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,boolean)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/code()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/getJoinCondition(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,boolean)#org/apache/spark/sql/catalyst/expressions/Expression/genCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/getJoinCondition(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,boolean)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/value()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/getJoinCondition(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,boolean)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/getJoinCondition(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,boolean)#org/apache/spark/sql/catalyst/expressions/package$/AttributeSeq(scala.collection.Seq)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/getJoinCondition(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,boolean)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/freshName(java.lang.String)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/getJoinCondition(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,boolean)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/getJoinCondition(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,boolean)#org/apache/spark/sql/catalyst/expressions/Expression/references()
org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec/anonfun/2/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#org/apache/spark/sql/types/MetadataBuilder/putLong(java.lang.String,long)
org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec/anonfun/2/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#org/apache/spark/sql/types/MetadataBuilder/build()
org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec/anonfun/2/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#org/apache/spark/sql/types/MetadataBuilder/withMetadata(org.apache.spark.sql.types.Metadata)
org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec/anonfun/2/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#org/apache/spark/sql/types/MetadataBuilder/MetadataBuilder()
org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec/anonfun/2/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#org/apache/spark/sql/types/Metadata/contains(java.lang.String)
org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec/anonfun/2/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#org/apache/spark/sql/catalyst/plans/logical/EventTimeWatermark$/delayKey()
org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec/anonfun/2/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#org/apache/spark/sql/catalyst/expressions/Attribute/withMetadata(org.apache.spark.sql.types.Metadata)
org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec/anonfun/2/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#org/apache/spark/sql/catalyst/expressions/Attribute/semanticEquals(org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec/anonfun/2/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#org/apache/spark/sql/catalyst/expressions/Attribute/metadata()
org/apache/spark/sql/execution/streaming/EventTimeWatermarkExec/anonfun/2/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#org/apache/spark/sql/types/MetadataBuilder/remove(java.lang.String)
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/types/StructType/fieldNames()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/TableIdentifier/copy(java.lang.String,scala.Option)
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/createTable(org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/TableIdentifier/unquotedString()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$2()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/get()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/properties()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/tableExists(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/datasources/DataSource$/apply$default$3()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/provider()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq/isEmpty()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#scala/Option$/option2Iterable(scala.Option)
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/internal/SQLConf/manageFilesourcePartitions()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/empty()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/assert(boolean)
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/isDefined()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/tableType()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/CreateDataSourceTableCommand$$anonfun$2/2(org.apache.spark.sql.execution.command.CreateDataSourceTableCommand,org.apache.spark.sql.internal.SessionState)
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$1()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$3()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$10()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/schema()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$4()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/CreateDataSourceTableCommand$$anonfun$1/1(org.apache.spark.sql.execution.command.CreateDataSourceTableCommand)
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$11()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$5()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$12()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$6()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$13()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/bucketSpec()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$7()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$15()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$8()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/TableIdentifier/database()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$9()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/ArrayOps/toSeq()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$17()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/types/StructType/nonEmpty()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/immutable/Map/$plus$plus(scala.collection.GenTraversableOnce)
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTableType$/VIEW()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/locationUri()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/types/StructType/isEmpty()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/storage()
org/apache/spark/sql/execution/command/CreateDataSourceTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/TableIdentifier/copy$default$1()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anon/4/addBinary(org.apache.parquet.io.api.Binary)#org/apache/parquet/io/api/Binary/toByteBuffer()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anon/4/addBinary(org.apache.parquet.io.api.Binary)#org/apache/parquet/io/api/Binary/length()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anon/4/addBinary(org.apache.parquet.io.api.Binary)#scala/Predef$/assert(boolean,scala.Function0)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anon/4/addBinary(org.apache.parquet.io.api.Binary)#java/nio/ByteBuffer/order(java.nio.ByteOrder)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anon/4/addBinary(org.apache.parquet.io.api.Binary)#org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter$$anon$4/updater()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anon/4/addBinary(org.apache.parquet.io.api.Binary)#java/nio/ByteBuffer/getLong()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anon/4/addBinary(org.apache.parquet.io.api.Binary)#org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter$$anon$4$$anonfun$addBinary$1/1(org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$$anon$4,org.apache.parquet.io.api.Binary)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anon/4/addBinary(org.apache.parquet.io.api.Binary)#java/nio/ByteBuffer/getInt()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anon/4/addBinary(org.apache.parquet.io.api.Binary)#org/apache/spark/sql/catalyst/util/DateTimeUtils$/fromJulianDay(int,long)
org/apache/spark/sql/jdbc/PostgresDialect/getJDBCType(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/DecimalType/scale()
org/apache/spark/sql/jdbc/PostgresDialect/getJDBCType(org.apache.spark.sql.types.DataType)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/jdbc/PostgresDialect/getJDBCType(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/jdbc/PostgresDialect$$anonfun$getJDBCType$1/1()
org/apache/spark/sql/jdbc/PostgresDialect/getJDBCType(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/jdbc/PostgresDialect$$anonfun$getJDBCType$2/2(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/jdbc/PostgresDialect/getJDBCType(org.apache.spark.sql.types.DataType)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/jdbc/PostgresDialect/getJDBCType(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/jdbc/PostgresDialect$$anonfun$getJDBCType$3/3()
org/apache/spark/sql/jdbc/PostgresDialect/getJDBCType(org.apache.spark.sql.types.DataType)#scala/Option/orElse(scala.Function0)
org/apache/spark/sql/jdbc/PostgresDialect/getJDBCType(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/jdbc/PostgresDialect$/getJDBCType(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/jdbc/PostgresDialect/getJDBCType(org.apache.spark.sql.types.DataType)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/jdbc/PostgresDialect/getJDBCType(org.apache.spark.sql.types.DataType)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/jdbc/PostgresDialect/getJDBCType(org.apache.spark.sql.types.DataType)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/jdbc/PostgresDialect/getJDBCType(org.apache.spark.sql.types.DataType)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/jdbc/PostgresDialect/getJDBCType(org.apache.spark.sql.types.DataType)#scala/Option/map(scala.Function1)
org/apache/spark/sql/jdbc/PostgresDialect/getJDBCType(org.apache.spark.sql.types.DataType)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/jdbc/PostgresDialect/getJDBCType(org.apache.spark.sql.types.DataType)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/jdbc/PostgresDialect/getJDBCType(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/ArrayType/elementType()
org/apache/spark/sql/jdbc/PostgresDialect/getJDBCType(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/DecimalType/precision()
org/apache/spark/sql/jdbc/PostgresDialect/getCatalystType(int,java.lang.String,int,org.apache.spark.sql.types.MetadataBuilder)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/jdbc/PostgresDialect/getCatalystType(int,java.lang.String,int,org.apache.spark.sql.types.MetadataBuilder)#org/apache/spark/sql/types/Metadata/getLong(java.lang.String)
org/apache/spark/sql/jdbc/PostgresDialect/getCatalystType(int,java.lang.String,int,org.apache.spark.sql.types.MetadataBuilder)#scala/collection/immutable/StringOps/drop(int)
org/apache/spark/sql/jdbc/PostgresDialect/getCatalystType(int,java.lang.String,int,org.apache.spark.sql.types.MetadataBuilder)#org/apache/spark/sql/jdbc/PostgresDialect$/toCatalystType(java.lang.String,int,int)
org/apache/spark/sql/jdbc/PostgresDialect/getCatalystType(int,java.lang.String,int,org.apache.spark.sql.types.MetadataBuilder)#scala/Option/map(scala.Function1)
org/apache/spark/sql/jdbc/PostgresDialect/getCatalystType(int,java.lang.String,int,org.apache.spark.sql.types.MetadataBuilder)#java/lang/String/equals(java.lang.Object)
org/apache/spark/sql/jdbc/PostgresDialect/getCatalystType(int,java.lang.String,int,org.apache.spark.sql.types.MetadataBuilder)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/jdbc/PostgresDialect/getCatalystType(int,java.lang.String,int,org.apache.spark.sql.types.MetadataBuilder)#org/apache/spark/sql/jdbc/PostgresDialect$/getCatalystType(int,java.lang.String,int,org.apache.spark.sql.types.MetadataBuilder)
org/apache/spark/sql/jdbc/PostgresDialect/getCatalystType(int,java.lang.String,int,org.apache.spark.sql.types.MetadataBuilder)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/jdbc/PostgresDialect/getCatalystType(int,java.lang.String,int,org.apache.spark.sql.types.MetadataBuilder)#org/apache/spark/sql/types/MetadataBuilder/build()
org/apache/spark/sql/jdbc/PostgresDialect/getCatalystType(int,java.lang.String,int,org.apache.spark.sql.types.MetadataBuilder)#org/apache/spark/sql/jdbc/PostgresDialect$$anonfun$getCatalystType$1/1()
org/apache/spark/sql/execution/FilterExec/anonfun/13/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/FilterExec/anonfun/13/apply(org.apache.spark.sql.catalyst.expressions.Expression)#java/lang/String/trim()
org/apache/spark/sql/execution/FilterExec/anonfun/13/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/FilterExec/anonfun/13/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/AttributeSet/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/FilterExec/anonfun/13/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/collection/TraversableOnce/mkString(java.lang.String)
org/apache/spark/sql/execution/FilterExec/anonfun/13/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/FilterExec/anonfun/13/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/collection/Traversable$/canBuildFrom()
org/apache/spark/sql/execution/FilterExec/anonfun/13/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/FilterExec/anonfun/13/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/execution/FilterExec$$anonfun$13$$anonfun$14/14(org.apache.spark.sql.execution.FilterExec$$anonfun$13)
org/apache/spark/sql/execution/FilterExec/anonfun/13/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/FilterExec/anonfun/13/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/FilterExec/anonfun/13/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/FilterExec/anonfun/13/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/Expression/references()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/gatherPartitionStats/2/apply(scala.Tuple2)#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/gatherPartitionStats/2/apply(scala.Tuple2)#org/apache/hadoop/fs/Path/toString()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/gatherPartitionStats/2/apply(scala.Tuple2)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/gatherPartitionStats/2/apply(scala.Tuple2)#scala/Tuple2/_2()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/gatherPartitionStats/2/apply(scala.Tuple2)#scala/collection/mutable/ArrayOps/sum(scala.math.Numeric)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/gatherPartitionStats/2/apply(scala.Tuple2)#org/apache/hadoop/fs/FileSystem/listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/gatherPartitionStats/2/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/gatherPartitionStats/2/apply(scala.Tuple2)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/gatherPartitionStats/2/apply(scala.Tuple2)#scala/reflect/ClassTag$/Long()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/gatherPartitionStats/2/apply(scala.Tuple2)#scala/Predef$/longArrayOps(long[])
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/gatherPartitionStats/2/apply(scala.Tuple2)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/gatherPartitionStats/2/apply(scala.Tuple2)#org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand$$anonfun$gatherPartitionStats$2$$anonfun$apply$7/7(org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsCommand$$anonfun$gatherPartitionStats$2)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/gatherPartitionStats/2/apply(scala.Tuple2)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/ui/SQLHistoryListener/anonfun/onTaskEnd/2/apply(org.apache.spark.scheduler.AccumulableInfo)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/ui/SQLHistoryListener/anonfun/onTaskEnd/2/apply(org.apache.spark.scheduler.AccumulableInfo)#scala/Option$/option2Iterable(scala.Option)
org/apache/spark/sql/execution/ui/SQLHistoryListener/anonfun/onTaskEnd/2/apply(org.apache.spark.scheduler.AccumulableInfo)#org/apache/spark/sql/execution/ui/SQLHistoryListener$$anonfun$onTaskEnd$2$$anonfun$7/7(org.apache.spark.sql.execution.ui.SQLHistoryListener$$anonfun$onTaskEnd$2)
org/apache/spark/sql/execution/ui/SQLHistoryListener/anonfun/onTaskEnd/2/apply(org.apache.spark.scheduler.AccumulableInfo)#org/apache/spark/scheduler/AccumulableInfo/copy$default$1()
org/apache/spark/sql/execution/ui/SQLHistoryListener/anonfun/onTaskEnd/2/apply(org.apache.spark.scheduler.AccumulableInfo)#org/apache/spark/scheduler/AccumulableInfo/copy$default$2()
org/apache/spark/sql/execution/ui/SQLHistoryListener/anonfun/onTaskEnd/2/apply(org.apache.spark.scheduler.AccumulableInfo)#org/apache/spark/scheduler/AccumulableInfo/copy$default$4()
org/apache/spark/sql/execution/ui/SQLHistoryListener/anonfun/onTaskEnd/2/apply(org.apache.spark.scheduler.AccumulableInfo)#org/apache/spark/scheduler/AccumulableInfo/copy$default$5()
org/apache/spark/sql/execution/ui/SQLHistoryListener/anonfun/onTaskEnd/2/apply(org.apache.spark.scheduler.AccumulableInfo)#org/apache/spark/scheduler/AccumulableInfo/copy$default$7()
org/apache/spark/sql/execution/ui/SQLHistoryListener/anonfun/onTaskEnd/2/apply(org.apache.spark.scheduler.AccumulableInfo)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/ui/SQLHistoryListener/anonfun/onTaskEnd/2/apply(org.apache.spark.scheduler.AccumulableInfo)#org/apache/spark/scheduler/AccumulableInfo/update()
org/apache/spark/sql/execution/ui/SQLHistoryListener/anonfun/onTaskEnd/2/apply(org.apache.spark.scheduler.AccumulableInfo)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/ui/SQLHistoryListener/anonfun/onTaskEnd/2/apply(org.apache.spark.scheduler.AccumulableInfo)#org/apache/spark/scheduler/AccumulableInfo/metadata()
org/apache/spark/sql/execution/ui/SQLHistoryListener/anonfun/onTaskEnd/2/apply(org.apache.spark.scheduler.AccumulableInfo)#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/ui/SQLHistoryListener/anonfun/onTaskEnd/2/apply(org.apache.spark.scheduler.AccumulableInfo)#org/apache/spark/scheduler/AccumulableInfo/copy(long,scala.Option,scala.Option,scala.Option,boolean,boolean,scala.Option)
org/apache/spark/sql/execution/ui/SQLHistoryListener/anonfun/onTaskEnd/2/apply(org.apache.spark.scheduler.AccumulableInfo)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/ui/SQLHistoryListener/anonfun/onTaskEnd/2/apply(org.apache.spark.scheduler.AccumulableInfo)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/ui/SQLHistoryListener/anonfun/onTaskEnd/2/apply(org.apache.spark.scheduler.AccumulableInfo)#org/apache/spark/scheduler/AccumulableInfo/copy$default$6()
org/apache/spark/sql/execution/ui/SQLHistoryListener/anonfun/onTaskEnd/2/apply(org.apache.spark.scheduler.AccumulableInfo)#org/apache/spark/util/AccumulatorContext$/SQL_ACCUM_IDENTIFIER()
org/apache/spark/sql/execution/ui/SQLHistoryListener/anonfun/onTaskEnd/2/apply(org.apache.spark.scheduler.AccumulableInfo)#org/apache/spark/sql/execution/ui/SQLHistoryListener$$anonfun$onTaskEnd$2$$anonfun$1/1(org.apache.spark.sql.execution.ui.SQLHistoryListener$$anonfun$onTaskEnd$2)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAnalyze/1/apply()#org/apache/spark/sql/execution/command/AnalyzeTableCommand$/apply$default$2()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAnalyze/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$AnalyzeContext/identifier()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAnalyze/1/apply()#org/apache/spark/sql/catalyst/parser/ParseException/ParseException(java.lang.String,org.antlr.v4.runtime.ParserRuleContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAnalyze/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitAnalyze$1$$anonfun$apply$2/2(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitAnalyze$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAnalyze/1/apply()#java/lang/String/toLowerCase(java.util.Locale)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAnalyze/1/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAnalyze/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder/logWarning(scala.Function0)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAnalyze/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitAnalyze$1/apply()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAnalyze/1/apply()#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAnalyze/1/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAnalyze/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$IdentifierContext/getText()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAnalyze/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$AnalyzeContext/identifierSeq()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAnalyze/1/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAnalyze/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$AnalyzeContext/tableIdentifier()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAnalyze/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder/visitTableIdentifier(org.apache.spark.sql.catalyst.parser.SqlBaseParser$TableIdentifierContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAnalyze/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder/visitIdentifierSeq(org.apache.spark.sql.catalyst.parser.SqlBaseParser$IdentifierSeqContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAnalyze/1/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAnalyze/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$AnalyzeContext/partitionSpec()
org/apache/spark/sql/execution/command/DescribeFunctionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/expressions/ExpressionInfo/getExtended()
org/apache/spark/sql/execution/command/DescribeFunctionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/expressions/ExpressionInfo/getDb()
org/apache/spark/sql/execution/command/DescribeFunctionCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/DescribeFunctionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/expressions/ExpressionInfo/getUsage()
org/apache/spark/sql/execution/command/DescribeFunctionCommand/run(org.apache.spark.sql.SparkSession)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/DescribeFunctionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/expressions/ExpressionInfo/getName()
org/apache/spark/sql/execution/command/DescribeFunctionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/lookupFunctionInfo(org.apache.spark.sql.catalyst.FunctionIdentifier)
org/apache/spark/sql/execution/command/DescribeFunctionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/Row$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/command/DescribeFunctionCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/immutable/List$/canBuildFrom()
org/apache/spark/sql/execution/command/DescribeFunctionCommand/run(org.apache.spark.sql.SparkSession)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/command/DescribeFunctionCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)
org/apache/spark/sql/execution/command/DescribeFunctionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/FunctionIdentifier/funcName()
org/apache/spark/sql/execution/command/DescribeFunctionCommand/run(org.apache.spark.sql.SparkSession)#java/lang/String/toLowerCase(java.util.Locale)
org/apache/spark/sql/execution/command/DescribeFunctionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/expressions/ExpressionInfo/getClassName()
org/apache/spark/sql/execution/command/DescribeFunctionCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/command/DescribeFunctionCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/command/DescribeFunctionCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/immutable/List/$colon$plus(java.lang.Object,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/DescribeFunctionCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/DescribeFunctionCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/command/DescribeFunctionCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/immutable/List/$colon$colon(java.lang.Object)
org/apache/spark/sql/execution/command/DescribeFunctionCommand/run(org.apache.spark.sql.SparkSession)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/DescribeFunctionCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/GenerateExec/anonfun/10/apply(scala.Tuple2)#org/apache/spark/sql/types/StructField/nullable()
org/apache/spark/sql/execution/GenerateExec/anonfun/10/apply(scala.Tuple2)#org/apache/spark/sql/types/StructField/dataType()
org/apache/spark/sql/execution/GenerateExec/anonfun/10/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/GenerateExec/anonfun/10/apply(scala.Tuple2)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/GenerateExec/anonfun/10/apply(scala.Tuple2)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/GenerateExec/anonfun/10/apply(scala.Tuple2)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/GenerateExec/anonfun/10/apply(scala.Tuple2)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/GenerateExec/anonfun/10/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/GenerateExec/anonfun/10/apply(scala.Tuple2)#scala/Tuple2/_2$mcI$sp()
org/apache/spark/sql/execution/GenerateExec/anonfun/10/apply(scala.Tuple2)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#scala/Predef$/assert(boolean)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/DataSourceAnalysis$$anonfun$convertStaticPartitions$4/4(org.apache.spark.sql.execution.datasources.DataSourceAnalysis)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/DataSourceAnalysis$$anonfun$convertStaticPartitions$3/3(org.apache.spark.sql.execution.datasources.DataSourceAnalysis,org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/DataSourceAnalysis$$anonfun$3/3(org.apache.spark.sql.execution.datasources.DataSourceAnalysis,scala.collection.immutable.Map)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#scala/collection/mutable/ArrayOps/take(int)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#scala/collection/Seq/takeRight(int)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#scala/collection/mutable/ArrayOps/size()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#scala/collection/immutable/Map/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#scala/collection/immutable/Map/size()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/DataSourceAnalysis$$anonfun$convertStaticPartitions$5/5(org.apache.spark.sql.execution.datasources.DataSourceAnalysis)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/DataSourceAnalysis$$anonfun$convertStaticPartitions$2/2(org.apache.spark.sql.execution.datasources.DataSourceAnalysis)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/DataSourceAnalysis$$anonfun$5/5(org.apache.spark.sql.execution.datasources.DataSourceAnalysis)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#scala/collection/mutable/ArrayOps/collectFirst(scala.PartialFunction)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/DataSourceAnalysis$$anonfun$convertStaticPartitions$1/1(org.apache.spark.sql.execution.datasources.DataSourceAnalysis,org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/DataSourceAnalysis$$anonfun$2/2(org.apache.spark.sql.execution.datasources.DataSourceAnalysis)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#scala/collection/immutable/Map$/canBuildFrom()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#scala/collection/immutable/Map/exists(scala.Function1)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#scala/collection/Seq/size()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#scala/collection/immutable/Map/foreach(scala.Function1)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#scala/collection/mutable/ArrayOps/dropWhile(scala.Function1)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#scala/collection/Seq/take(int)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#scala/collection/mutable/ArrayOps/forall(scala.Function1)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/convertStaticPartitions(scala.collection.Seq,scala.collection.immutable.Map,scala.collection.Seq,org.apache.spark.sql.types.StructType)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/csv/CSVOptions/getChar(java.lang.String,char)#scala/Some/x()
org/apache/spark/sql/execution/datasources/csv/CSVOptions/getChar(java.lang.String,char)#java/lang/String/charAt(int)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/getChar(java.lang.String,char)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/csv/CSVOptions/getChar(java.lang.String,char)#java/lang/String/length()
org/apache/spark/sql/execution/datasources/csv/CSVOptions/getChar(java.lang.String,char)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/getChar(java.lang.String,char)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/getChar(java.lang.String,char)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/getChar(java.lang.String,char)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/getChar(java.lang.String,char)#org/apache/spark/sql/catalyst/util/CaseInsensitiveMap/get(java.lang.String)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/getChar(java.lang.String,char)#java/lang/RuntimeException/RuntimeException(java.lang.String)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/getBool(java.lang.String,boolean)#org/apache/spark/sql/catalyst/util/CaseInsensitiveMap/getOrElse(java.lang.Object,scala.Function0)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/getBool(java.lang.String,boolean)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/csv/CSVOptions/getBool(java.lang.String,boolean)#java/lang/String/toLowerCase(java.util.Locale)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/getBool(java.lang.String,boolean)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/getBool(java.lang.String,boolean)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/getBool(java.lang.String,boolean)#org/apache/spark/sql/execution/datasources/csv/CSVOptions$$anonfun$2/2(org.apache.spark.sql.execution.datasources.csv.CSVOptions,boolean)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/getBool(java.lang.String,boolean)#java/lang/Exception/Exception(java.lang.String)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/getBool(java.lang.String,boolean)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/getBool(java.lang.String,boolean)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/asWriterSettings()#com/univocity/parsers/csv/CsvFormat/setComment(char)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/asWriterSettings()#com/univocity/parsers/csv/CsvWriterSettings/setQuoteAllFields(boolean)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/asWriterSettings()#com/univocity/parsers/csv/CsvWriterSettings/getFormat()
org/apache/spark/sql/execution/datasources/csv/CSVOptions/asWriterSettings()#com/univocity/parsers/csv/CsvWriterSettings/setSkipEmptyLines(boolean)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/asWriterSettings()#com/univocity/parsers/csv/CsvFormat/setDelimiter(char)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/asWriterSettings()#com/univocity/parsers/csv/CsvWriterSettings/setQuoteEscapingEnabled(boolean)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/asWriterSettings()#com/univocity/parsers/csv/CsvFormat/setQuote(char)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/asWriterSettings()#com/univocity/parsers/csv/CsvWriterSettings/setEmptyValue(java.lang.String)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/asWriterSettings()#com/univocity/parsers/csv/CsvWriterSettings/setNullValue(java.lang.String)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/asWriterSettings()#com/univocity/parsers/csv/CsvWriterSettings/setIgnoreLeadingWhitespaces(boolean)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/asWriterSettings()#com/univocity/parsers/csv/CsvFormat/setQuoteEscape(char)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/asWriterSettings()#com/univocity/parsers/csv/CsvWriterSettings/setIgnoreTrailingWhitespaces(boolean)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/asWriterSettings()#com/univocity/parsers/csv/CsvWriterSettings/CsvWriterSettings()
org/apache/spark/sql/execution/datasources/csv/CSVOptions/getInt(java.lang.String,int)#scala/Some/x()
org/apache/spark/sql/execution/datasources/csv/CSVOptions/getInt(java.lang.String,int)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/getInt(java.lang.String,int)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/getInt(java.lang.String,int)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/getInt(java.lang.String,int)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/getInt(java.lang.String,int)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/getInt(java.lang.String,int)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/getInt(java.lang.String,int)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/getInt(java.lang.String,int)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/csv/CSVOptions/getInt(java.lang.String,int)#scala/collection/immutable/StringOps/toInt()
org/apache/spark/sql/execution/datasources/csv/CSVOptions/getInt(java.lang.String,int)#org/apache/spark/sql/catalyst/util/CaseInsensitiveMap/get(java.lang.String)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/getInt(java.lang.String,int)#java/lang/RuntimeException/RuntimeException(java.lang.String)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/asParserSettings()#com/univocity/parsers/csv/CsvParserSettings/setUnescapedQuoteHandling(com.univocity.parsers.csv.UnescapedQuoteHandling)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/asParserSettings()#com/univocity/parsers/csv/CsvParserSettings/setIgnoreTrailingWhitespaces(boolean)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/asParserSettings()#com/univocity/parsers/csv/CsvParserSettings/setMaxColumns(int)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/asParserSettings()#com/univocity/parsers/csv/CsvParserSettings/getFormat()
org/apache/spark/sql/execution/datasources/csv/CSVOptions/asParserSettings()#com/univocity/parsers/csv/CsvParserSettings/CsvParserSettings()
org/apache/spark/sql/execution/datasources/csv/CSVOptions/asParserSettings()#com/univocity/parsers/csv/CsvFormat/setQuote(char)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/asParserSettings()#com/univocity/parsers/csv/CsvFormat/setQuoteEscape(char)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/asParserSettings()#com/univocity/parsers/csv/CsvFormat/setComment(char)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/asParserSettings()#com/univocity/parsers/csv/CsvParserSettings/setInputBufferSize(int)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/asParserSettings()#com/univocity/parsers/csv/CsvFormat/setDelimiter(char)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/asParserSettings()#com/univocity/parsers/csv/CsvParserSettings/setNullValue(java.lang.String)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/asParserSettings()#com/univocity/parsers/csv/CsvParserSettings/setMaxCharsPerColumn(int)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/asParserSettings()#com/univocity/parsers/csv/CsvParserSettings/setIgnoreLeadingWhitespaces(boolean)
org/apache/spark/sql/execution/datasources/csv/CSVOptions/asParserSettings()#com/univocity/parsers/csv/CsvParserSettings/setReadInputOnSeparateThread(boolean)
org/apache/spark/sql/Dataset/anonfun/withWatermark/1/apply()#org/apache/spark/sql/catalyst/plans/logical/EventTimeWatermark/EventTimeWatermark(org.apache.spark.sql.catalyst.expressions.Attribute,org.apache.spark.unsafe.types.CalendarInterval,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/Dataset/anonfun/withWatermark/1/apply()#org/apache/spark/unsafe/types/CalendarInterval/milliseconds()
org/apache/spark/sql/Dataset/anonfun/withWatermark/1/apply()#org/apache/spark/sql/catalyst/analysis/EliminateEventTimeWatermark$/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/Dataset/anonfun/withWatermark/1/apply()#org/apache/spark/sql/Dataset$$anonfun$withWatermark$1/apply()
org/apache/spark/sql/Dataset/anonfun/withWatermark/1/apply()#org/apache/spark/unsafe/types/CalendarInterval/fromString(java.lang.String)
org/apache/spark/sql/Dataset/anonfun/withWatermark/1/apply()#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/Dataset/anonfun/withWatermark/1/apply()#scala/Predef$/require(boolean,scala.Function0)
org/apache/spark/sql/Dataset/anonfun/withWatermark/1/apply()#org/apache/spark/sql/Dataset$$anonfun$withWatermark$1$$anonfun$13/13(org.apache.spark.sql.Dataset$$anonfun$withWatermark$1)
org/apache/spark/sql/Dataset/anonfun/withWatermark/1/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/Dataset/anonfun/withWatermark/1/apply()#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/Dataset/anonfun/withWatermark/1/apply()#org/apache/spark/sql/Dataset$$anonfun$withWatermark$1$$anonfun$apply$7/7(org.apache.spark.sql.Dataset$$anonfun$withWatermark$1)
org/apache/spark/sql/Dataset/anonfun/withWatermark/1/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/Dataset/anonfun/withWatermark/1/apply()#org/apache/spark/sql/catalyst/analysis/UnresolvedAttribute$/apply(java.lang.String)
org/apache/spark/sql/Dataset/anonfun/withWatermark/1/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/copy()#org/apache/spark/sql/types/DecimalType/scale()
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/copy()#org/apache/spark/sql/execution/vectorized/ColumnarBatch$Row/getInt(int)
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/copy()#org/apache/spark/sql/catalyst/expressions/GenericInternalRow/setInt(int,int)
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/copy()#org/apache/spark/sql/execution/vectorized/ColumnarBatch$Row/getUTF8String(int)
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/copy()#org/apache/spark/sql/execution/vectorized/ColumnarBatch$Row/getBinary(int)
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/copy()#java/lang/StringBuilder/append(java.lang.String)
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/copy()#org/apache/spark/sql/execution/vectorized/ColumnarBatch$Row/getLong(int)
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/copy()#org/apache/spark/sql/catalyst/expressions/GenericInternalRow/setBoolean(int,boolean)
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/copy()#org/apache/spark/sql/execution/vectorized/ColumnarBatch$Row/getDouble(int)
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/copy()#org/apache/spark/sql/catalyst/expressions/GenericInternalRow/update(int,java.lang.Object)
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/copy()#org/apache/spark/sql/catalyst/expressions/GenericInternalRow/setDecimal(int,org.apache.spark.sql.types.Decimal,int)
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/copy()#org/apache/spark/sql/catalyst/expressions/GenericInternalRow/setNullAt(int)
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/copy()#org/apache/spark/sql/catalyst/expressions/GenericInternalRow/setShort(int,short)
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/copy()#org/apache/spark/sql/execution/vectorized/ColumnarBatch$Row/getBoolean(int)
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/copy()#org/apache/spark/sql/catalyst/expressions/GenericInternalRow/setFloat(int,float)
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/copy()#java/lang/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/copy()#org/apache/spark/sql/catalyst/expressions/GenericInternalRow/GenericInternalRow(int)
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/copy()#org/apache/spark/sql/execution/vectorized/ColumnarBatch$Row/getShort(int)
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/copy()#org/apache/spark/sql/types/DecimalType/precision()
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/copy()#org/apache/spark/sql/execution/vectorized/ColumnarBatch$Row/getFloat(int)
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/copy()#org/apache/spark/sql/catalyst/expressions/GenericInternalRow/setDouble(int,double)
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/copy()#org/apache/spark/sql/execution/vectorized/ColumnarBatch$Row/isNullAt(int)
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/copy()#org/apache/spark/sql/catalyst/expressions/GenericInternalRow/setByte(int,byte)
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/copy()#org/apache/spark/sql/catalyst/expressions/GenericInternalRow/setLong(int,long)
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/copy()#org/apache/spark/sql/execution/vectorized/ColumnarBatch$Row/numFields()
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/copy()#java/lang/RuntimeException/RuntimeException(java.lang.String)
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/copy()#java/lang/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/copy()#org/apache/spark/sql/execution/vectorized/ColumnarBatch$Row/getDecimal(int,int,int)
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/copy()#java/lang/StringBuilder/toString()
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/copy()#org/apache/spark/sql/execution/vectorized/ColumnarBatch$Row/getByte(int)
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/update(int,java.lang.Object)#java/lang/StringBuilder/append(java.lang.String)
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/update(int,java.lang.Object)#org/apache/spark/sql/execution/vectorized/ColumnarBatch$Row/setInt(int,int)
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/update(int,java.lang.Object)#java/lang/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/update(int,java.lang.Object)#org/apache/spark/sql/execution/vectorized/ColumnarBatch$Row/setBoolean(int,boolean)
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/update(int,java.lang.Object)#java/lang/Boolean/booleanValue()
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/update(int,java.lang.Object)#org/apache/spark/sql/types/DecimalType/scale()
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/update(int,java.lang.Object)#org/apache/spark/sql/execution/vectorized/ColumnarBatch$Row/setNullAt(int)
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/update(int,java.lang.Object)#java/lang/Integer/intValue()
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/update(int,java.lang.Object)#java/lang/Long/longValue()
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/update(int,java.lang.Object)#java/lang/Double/doubleValue()
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/update(int,java.lang.Object)#org/apache/spark/sql/execution/vectorized/ColumnarBatch$Row/setShort(int,short)
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/update(int,java.lang.Object)#org/apache/spark/sql/execution/vectorized/ColumnarBatch$Row/setFloat(int,float)
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/update(int,java.lang.Object)#org/apache/spark/sql/types/DecimalType/precision()
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/update(int,java.lang.Object)#java/lang/UnsupportedOperationException/UnsupportedOperationException(java.lang.String)
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/update(int,java.lang.Object)#org/apache/spark/sql/execution/vectorized/ColumnarBatch$Row/setDecimal(int,org.apache.spark.sql.types.Decimal,int)
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/update(int,java.lang.Object)#java/lang/Short/shortValue()
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/update(int,java.lang.Object)#java/lang/Float/floatValue()
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/update(int,java.lang.Object)#java/lang/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/update(int,java.lang.Object)#org/apache/spark/sql/types/Decimal/apply(java.math.BigDecimal,int,int)
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/update(int,java.lang.Object)#java/lang/StringBuilder/toString()
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/update(int,java.lang.Object)#org/apache/spark/sql/execution/vectorized/ColumnarBatch$Row/setLong(int,long)
org/apache/spark/sql/execution/vectorized/ColumnarBatch/Row/update(int,java.lang.Object)#org/apache/spark/sql/execution/vectorized/ColumnarBatch$Row/setDouble(int,double)
org/apache/spark/sql/execution/MapElementsExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/currentVars_$eq(scala.collection.Seq)
org/apache/spark/sql/execution/MapElementsExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)
org/apache/spark/sql/execution/MapElementsExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/MapElementsExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/codegen/package$ExpressionCanonicalizer$/execute(org.apache.spark.sql.catalyst.trees.TreeNode)
org/apache/spark/sql/execution/MapElementsExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/MapElementsExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/Attribute/dataType()
org/apache/spark/sql/execution/MapElementsExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/package$/AttributeSeq(scala.collection.Seq)
org/apache/spark/sql/execution/MapElementsExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/objects/Invoke$/apply$default$5()
org/apache/spark/sql/execution/MapElementsExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/objects/Invoke$/apply$default$6()
org/apache/spark/sql/execution/MapElementsExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/BindReferences$/bindReference(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.package$AttributeSeq,boolean)
org/apache/spark/sql/execution/MapElementsExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/BindReferences$/bindReference$default$3()
org/apache/spark/sql/execution/MapElementsExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/objects/Invoke/Invoke(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String,org.apache.spark.sql.types.DataType,scala.collection.Seq,boolean,boolean)
org/apache/spark/sql/execution/MapElementsExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/Seq/apply(int)
org/apache/spark/sql/execution/MapElementsExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/plans/logical/FunctionUtils$/getFunctionOneName(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/MapElementsExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/Expression/genCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)
org/apache/spark/sql/execution/MapElementsExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/Predef$/ArrowAssoc(java.lang.Object)
org/apache/spark/sql/execution/MapElementsExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/Tuple2/_1()
org/apache/spark/sql/execution/MapElementsExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/Literal$/create(java.lang.Object,org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/MapElementsExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/Tuple2/_2()
org/apache/spark/sql/execution/MapElementsExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/MapElementsExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/Expression/dataType()
org/apache/spark/sql/execution/MapElementsExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/types/ObjectType/ObjectType(java.lang.Class)
org/apache/spark/sql/execution/MapElementsExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anonfun/1/apply()#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anonfun/1/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anonfun/1/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anonfun/1/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anonfun/1/apply()#org/apache/spark/sql/types/StructType/prettyJson()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anonfun/1/apply()#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anonfun/1/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anonfun/1/apply()#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anonfun/1/apply()#org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter$$anonfun$1/apply()
org/apache/spark/sql/SparkSession/createDataFrame(org.apache.spark.rdd.RDD,org.apache.spark.sql.types.StructType,boolean)#org/apache/spark/sql/execution/LogicalRDD$/apply$default$4()
org/apache/spark/sql/SparkSession/createDataFrame(org.apache.spark.rdd.RDD,org.apache.spark.sql.types.StructType,boolean)#org/apache/spark/sql/catalyst/encoders/RowEncoder$/apply(org.apache.spark.sql.types.StructType)
org/apache/spark/sql/SparkSession/createDataFrame(org.apache.spark.rdd.RDD,org.apache.spark.sql.types.StructType,boolean)#org/apache/spark/sql/Dataset$/ofRows(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/SparkSession/createDataFrame(org.apache.spark.rdd.RDD,org.apache.spark.sql.types.StructType,boolean)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/SparkSession/createDataFrame(org.apache.spark.rdd.RDD,org.apache.spark.sql.types.StructType,boolean)#org/apache/spark/sql/execution/LogicalRDD$/apply$default$3()
org/apache/spark/sql/SparkSession/createDataFrame(org.apache.spark.rdd.RDD,org.apache.spark.sql.types.StructType,boolean)#org/apache/spark/sql/SparkSession$$anonfun$4/4(org.apache.spark.sql.SparkSession)
org/apache/spark/sql/SparkSession/createDataFrame(org.apache.spark.rdd.RDD,org.apache.spark.sql.types.StructType,boolean)#org/apache/spark/sql/types/StructType/toAttributes()
org/apache/spark/sql/SparkSession/createDataFrame(org.apache.spark.rdd.RDD,org.apache.spark.sql.types.StructType,boolean)#org/apache/spark/sql/SparkSession$$anonfun$3/3(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.encoders.ExpressionEncoder)
org/apache/spark/sql/SparkSession/createDataFrame(org.apache.spark.rdd.RDD,org.apache.spark.sql.types.StructType,boolean)#org/apache/spark/rdd/RDD/map(scala.Function1,scala.reflect.ClassTag)
org/apache/spark/sql/SparkSession/getSchema(java.lang.Class)#org/apache/spark/sql/SparkSession$$anonfun$getSchema$1/1(org.apache.spark.sql.SparkSession)
org/apache/spark/sql/SparkSession/getSchema(java.lang.Class)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/SparkSession/getSchema(java.lang.Class)#scala/Array$/fallbackCanBuildFrom(scala.Predef$DummyImplicit)
org/apache/spark/sql/SparkSession/getSchema(java.lang.Class)#org/apache/spark/sql/catalyst/JavaTypeInference$/inferDataType(java.lang.Class)
org/apache/spark/sql/SparkSession/getSchema(java.lang.Class)#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/SparkSession/getSchema(java.lang.Class)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/SparkSession/getSchema(java.lang.Class)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/SparkSession/getSchema(java.lang.Class)#scala/Tuple2/_1()
org/apache/spark/sql/SparkSession/getSchema(java.lang.Class)#scala/Predef$DummyImplicit$/dummyImplicit()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/doProduceWithoutKeys/1/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/ExprCode(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/doProduceWithoutKeys/1/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/doProduceWithoutKeys/1/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/isNull()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/doProduceWithoutKeys/1/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/doProduceWithoutKeys/1/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/Expression/dataType()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/doProduceWithoutKeys/1/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/doProduceWithoutKeys/1/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/doProduceWithoutKeys/1/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/doProduceWithoutKeys/1/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/code()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/doProduceWithoutKeys/1/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/Expression/genCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/doProduceWithoutKeys/1/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/value()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/doProduceWithoutKeys/1/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addMutableState(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/doProduceWithoutKeys/1/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/doProduceWithoutKeys/1/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/doProduceWithoutKeys/1/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/doProduceWithoutKeys/1/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/freshName(java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/doProduceWithoutKeys/1/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/javaType(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/doProduceWithoutKeys/1/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/doProduceWithoutKeys/1/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/CollectLimitExec/doExecute()#org/apache/spark/rdd/RDD/mapPartitionsInternal$default$2()
org/apache/spark/sql/execution/CollectLimitExec/doExecute()#org/apache/spark/sql/execution/CollectLimitExec$$anonfun$doExecute$1/1(org.apache.spark.sql.execution.CollectLimitExec)
org/apache/spark/sql/execution/CollectLimitExec/doExecute()#org/apache/spark/sql/execution/CollectLimitExec$$anonfun$1/1(org.apache.spark.sql.execution.CollectLimitExec)
org/apache/spark/sql/execution/CollectLimitExec/doExecute()#org/apache/spark/sql/execution/ShuffledRowRDD$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/CollectLimitExec/doExecute()#org/apache/spark/sql/execution/ShuffledRowRDD/mapPartitionsInternal$default$2()
org/apache/spark/sql/execution/CollectLimitExec/doExecute()#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/CollectLimitExec/doExecute()#org/apache/spark/rdd/RDD/mapPartitionsInternal(scala.Function1,boolean,scala.reflect.ClassTag)
org/apache/spark/sql/execution/CollectLimitExec/doExecute()#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/CollectLimitExec/doExecute()#org/apache/spark/sql/execution/exchange/ShuffleExchange$/prepareShuffleDependency(org.apache.spark.rdd.RDD,scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning,org.apache.spark.serializer.Serializer)
org/apache/spark/sql/execution/CollectLimitExec/doExecute()#org/apache/spark/sql/execution/ShuffledRowRDD/mapPartitionsInternal(scala.Function1,boolean,scala.reflect.ClassTag)
org/apache/spark/sql/execution/command/AlterViewAsCommand/alterPermanentView(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/command/ViewHelper$/generateViewProperties(scala.collection.immutable.Map,org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/command/AlterViewAsCommand/alterPermanentView(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/command/AlterViewAsCommand/alterPermanentView(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/CatalogTable/properties()
org/apache/spark/sql/execution/command/AlterViewAsCommand/alterPermanentView(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/command/AlterViewAsCommand/alterPermanentView(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$2()
org/apache/spark/sql/execution/command/AlterViewAsCommand/alterPermanentView(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$17()
org/apache/spark/sql/execution/command/AlterViewAsCommand/alterPermanentView(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/alterTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)
org/apache/spark/sql/execution/command/AlterViewAsCommand/alterPermanentView(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/AlterViewAsCommand/alterPermanentView(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/CatalogTable/tableType()
org/apache/spark/sql/execution/command/AlterViewAsCommand/alterPermanentView(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/schema()
org/apache/spark/sql/execution/command/AlterViewAsCommand/alterPermanentView(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/AlterViewAsCommand/alterPermanentView(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/command/AlterViewAsCommand/alterPermanentView(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/AlterViewAsCommand/alterPermanentView(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$1()
org/apache/spark/sql/execution/command/AlterViewAsCommand/alterPermanentView(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$3()
org/apache/spark/sql/execution/command/AlterViewAsCommand/alterPermanentView(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$10()
org/apache/spark/sql/execution/command/AlterViewAsCommand/alterPermanentView(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/command/AlterViewAsCommand/alterPermanentView(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/AlterViewAsCommand/alterPermanentView(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$5()
org/apache/spark/sql/execution/command/AlterViewAsCommand/alterPermanentView(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)
org/apache/spark/sql/execution/command/AlterViewAsCommand/alterPermanentView(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/command/AlterViewAsCommand/alterPermanentView(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$12()
org/apache/spark/sql/execution/command/AlterViewAsCommand/alterPermanentView(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/command/AlterViewAsCommand/alterPermanentView(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$6()
org/apache/spark/sql/execution/command/AlterViewAsCommand/alterPermanentView(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/command/AlterViewAsCommand/alterPermanentView(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$7()
org/apache/spark/sql/execution/command/AlterViewAsCommand/alterPermanentView(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()
org/apache/spark/sql/execution/command/AlterViewAsCommand/alterPermanentView(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/command/AlterViewAsCommand/alterPermanentView(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$15()
org/apache/spark/sql/execution/command/AlterViewAsCommand/alterPermanentView(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$8()
org/apache/spark/sql/execution/command/AlterViewAsCommand/alterPermanentView(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$16()
org/apache/spark/sql/execution/command/AlterViewAsCommand/alterPermanentView(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$9()
org/apache/spark/sql/execution/command/AlterViewAsCommand/alterPermanentView(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/CatalogTableType$/VIEW()
org/apache/spark/sql/execution/command/AlterViewAsCommand/alterPermanentView(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()
org/apache/spark/sql/execution/command/AlterViewAsCommand/alterPermanentView(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/AlterViewAsCommand/alterPermanentView(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/command/ViewHelper$/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/streaming/DataStreamReader/load()#org/apache/spark/sql/execution/datasources/DataSource$/apply$default$3()
org/apache/spark/sql/streaming/DataStreamReader/load()#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/streaming/DataStreamReader/load()#org/apache/spark/sql/execution/datasources/DataSource$/apply$default$5()
org/apache/spark/sql/streaming/DataStreamReader/load()#org/apache/spark/sql/execution/datasources/DataSource$/apply$default$8()
org/apache/spark/sql/streaming/DataStreamReader/load()#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/streaming/DataStreamReader/load()#java/lang/String/toLowerCase(java.util.Locale)
org/apache/spark/sql/streaming/DataStreamReader/load()#org/apache/spark/sql/Dataset$/ofRows(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/streaming/DataStreamReader/load()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/streaming/DataStreamReader/load()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/streaming/DataStreamReader/load()#scala/collection/mutable/HashMap/toMap(scala.Predef$$less$colon$less)
org/apache/spark/sql/streaming/DataStreamReader/load()#org/apache/spark/sql/execution/command/DDLUtils$/HIVE_PROVIDER()
org/apache/spark/sql/streaming/DataStreamReader/load()#org/apache/spark/sql/execution/datasources/DataSource$/apply$default$6()
org/apache/spark/sql/streaming/DataStreamReader/load()#org/apache/spark/sql/execution/streaming/StreamingRelation$/apply(org.apache.spark.sql.execution.datasources.DataSource)
org/apache/spark/sql/streaming/DataStreamReader/load()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/streaming/DataStreamReader/load()#scala/Predef$/$conforms()
org/apache/spark/sql/streaming/DataStreamReader/load()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/ResolveSQLOnFile/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#java/lang/Class/isAssignableFrom(java.lang.Class)
org/apache/spark/sql/execution/datasources/ResolveSQLOnFile/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/datasources/LogicalRelation$/apply(org.apache.spark.sql.sources.BaseRelation)
org/apache/spark/sql/execution/datasources/ResolveSQLOnFile/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/datasources/DataSource$/apply$default$7()
org/apache/spark/sql/execution/datasources/ResolveSQLOnFile/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/datasources/DataSource$/apply$default$8()
org/apache/spark/sql/execution/datasources/ResolveSQLOnFile/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/analysis/package$/AnalysisErrorAt(org.apache.spark.sql.catalyst.trees.TreeNode)
org/apache/spark/sql/execution/datasources/ResolveSQLOnFile/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/ResolveSQLOnFile/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/ResolveSQLOnFile/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#java/lang/String/toLowerCase(java.util.Locale)
org/apache/spark/sql/execution/datasources/ResolveSQLOnFile/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/ResolveSQLOnFile/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/ResolveSQLOnFile/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/TableIdentifier/database()
org/apache/spark/sql/execution/datasources/ResolveSQLOnFile/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/ResolveSQLOnFile/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)
org/apache/spark/sql/execution/datasources/ResolveSQLOnFile/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/datasources/DataSource$/apply$default$4()
org/apache/spark/sql/execution/datasources/ResolveSQLOnFile/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/analysis/package$AnalysisErrorAt/failAnalysis(java.lang.String)
org/apache/spark/sql/execution/datasources/ResolveSQLOnFile/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/datasources/DataSource$/apply$default$5()
org/apache/spark/sql/execution/datasources/ResolveSQLOnFile/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/TableIdentifier/table()
org/apache/spark/sql/execution/datasources/ResolveSQLOnFile/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/command/DDLUtils$/HIVE_PROVIDER()
org/apache/spark/sql/execution/datasources/ResolveSQLOnFile/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/datasources/DataSource$/apply$default$6()
org/apache/spark/sql/execution/datasources/ResolveSQLOnFile/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/ResolveSQLOnFile/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/ResolveSQLOnFile/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/ResolveSQLOnFile/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/ResolveSQLOnFile/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/ResolveSQLOnFile/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/analysis/UnresolvedRelation/tableIdentifier()
org/apache/spark/sql/execution/datasources/ResolveSQLOnFile/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/ResolveSQLOnFile/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/execution/datasources/ResolveSQLOnFile/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/ResolveSQLOnFile/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/ResolveSQLOnFile/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#java/lang/Exception/getMessage()
org/apache/spark/sql/execution/datasources/ResolveSQLOnFile/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Option/get()
org/apache/spark/sql/execution/CodegenSupport/anonfun/produce/1/apply()#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/CodegenSupport/anonfun/produce/1/apply()#org/apache/spark/sql/execution/CodegenSupport$$anonfun$produce$1/apply()
org/apache/spark/sql/execution/CodegenSupport/anonfun/produce/1/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/CodegenSupport/anonfun/produce/1/apply()#org/apache/spark/sql/execution/CodegenSupport$$anonfun$produce$1$$anonfun$apply$1/1(org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1)
org/apache/spark/sql/execution/CodegenSupport/anonfun/produce/1/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/CodegenSupport/anonfun/produce/1/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/CodegenSupport/anonfun/produce/1/apply()#org/apache/spark/sql/execution/CodegenSupport$class/org$apache$spark$sql$execution$CodegenSupport$$variablePrefix(org.apache.spark.sql.execution.CodegenSupport)
org/apache/spark/sql/execution/CodegenSupport/anonfun/produce/1/apply()#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/CodegenSupport/anonfun/produce/1/apply()#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/freshNamePrefix_$eq(java.lang.String)
org/apache/spark/sql/execution/CodegenSupport/anonfun/produce/1/apply()#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/CodegenSupport/anonfun/produce/1/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/CodegenSupport/anonfun/produce/1/apply()#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/registerComment(scala.Function0)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateTotalSize(org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateTotalSize(org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateTotalSize(org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/collection/Seq/isEmpty()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateTotalSize(org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/listPartitions(org.apache.spark.sql.catalyst.TableIdentifier,scala.Option)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateTotalSize(org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/collection/TraversableOnce/sum(scala.math.Numeric)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateTotalSize(org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/execution/command/AnalyzeTableCommand$$anonfun$calculateTotalSize$1/1(org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.catalog.CatalogTable)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateTotalSize(org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/listPartitions$default$2()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateTotalSize(org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateTotalSize(org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/execution/command/AnalyzeTableCommand$/org$apache$spark$sql$execution$command$AnalyzeTableCommand$$calculateLocationSize(org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.TableIdentifier,scala.Option)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateTotalSize(org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateTotalSize(org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateTotalSize(org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/execution/command/AnalyzeTableCommand$/calculateTotalSize(org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.catalog.CatalogTable)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateTotalSize(org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/locationUri()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateTotalSize(org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/storage()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/empty()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/AnalyzeTableCommand$$anonfun$run$1/1(org.apache.spark.sql.execution.command.AnalyzeTableCommand,long)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogStatistics$/apply$default$2()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$1()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTableType$/VIEW()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/AnalyzeTableCommand$$anonfun$4/4(org.apache.spark.sql.execution.command.AnalyzeTableCommand)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/AnalyzeTableCommand$/calculateTotalSize(org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.catalog.CatalogTable)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/AnalyzeTableCommand$$anonfun$5/5(org.apache.spark.sql.execution.command.AnalyzeTableCommand)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/stats()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/TableIdentifier/table()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/alterTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/refreshTable(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/isDefined()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/AnalyzeTableCommand$$anonfun$3/3(org.apache.spark.sql.execution.command.AnalyzeTableCommand,org.apache.spark.sql.internal.SessionState)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/flatMap(scala.Function1)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#scala/math/BigInt$/long2bigInt(long)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/tableType()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogStatistics$/apply$default$3()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogStatistics/CatalogStatistics(scala.math.BigInt,scala.Option,scala.collection.immutable.Map)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$2()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#scala/package$/BigInt()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$3()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#scala/math/BigInt$/apply(long)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$10()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$4()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$11()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$5()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$6()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/AnalyzeTableCommand$$anonfun$1/1(org.apache.spark.sql.execution.command.AnalyzeTableCommand)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$13()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$7()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$15()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/AnalyzeTableCommand$$anonfun$2/2(org.apache.spark.sql.execution.command.AnalyzeTableCommand)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$8()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$16()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/TableIdentifier/database()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$9()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$17()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/TableIdentifier/TableIdentifier(java.lang.String,scala.Option)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/ui/SparkPlanGraphNode/makeDotNode(scala.collection.immutable.Map)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/ui/SparkPlanGraphNode/makeDotNode(scala.collection.immutable.Map)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/ui/SparkPlanGraphNode/makeDotNode(scala.collection.immutable.Map)#scala/collection/mutable/StringBuilder/$plus$plus$eq(java.lang.String)
org/apache/spark/sql/execution/ui/SparkPlanGraphNode/makeDotNode(scala.collection.immutable.Map)#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/execution/ui/SparkPlanGraphNode/makeDotNode(scala.collection.immutable.Map)#org/apache/spark/sql/execution/ui/SparkPlanGraphNode$$anonfun$4/4(org.apache.spark.sql.execution.ui.SparkPlanGraphNode,scala.collection.immutable.Map)
org/apache/spark/sql/execution/ui/SparkPlanGraphNode/makeDotNode(scala.collection.immutable.Map)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/ui/SparkPlanGraphNode/makeDotNode(scala.collection.immutable.Map)#scala/collection/mutable/StringBuilder/StringBuilder(java.lang.String)
org/apache/spark/sql/execution/ui/SparkPlanGraphNode/makeDotNode(scala.collection.immutable.Map)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/ui/SparkPlanGraphNode/makeDotNode(scala.collection.immutable.Map)#org/apache/commons/lang3/StringEscapeUtils/escapeJava(java.lang.String)
org/apache/spark/sql/execution/ui/SparkPlanGraphNode/makeDotNode(scala.collection.immutable.Map)#scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/ui/SparkPlanGraphNode/makeDotNode(scala.collection.immutable.Map)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/ui/SparkPlanGraphNode/makeDotNode(scala.collection.immutable.Map)#scala/collection/Seq/mkString(java.lang.String)
org/apache/spark/sql/execution/ui/SparkPlanGraphNode/makeDotNode(scala.collection.immutable.Map)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/ui/SparkPlanGraphNode/makeDotNode(scala.collection.immutable.Map)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/SQLContext/beansToRows(scala.collection.Iterator,java.lang.Class,scala.collection.Seq)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/SQLContext/beansToRows(scala.collection.Iterator,java.lang.Class,scala.collection.Seq)#scala/collection/mutable/ArrayOps/zip(scala.collection.GenIterable,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/SQLContext/beansToRows(scala.collection.Iterator,java.lang.Class,scala.collection.Seq)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/SQLContext/beansToRows(scala.collection.Iterator,java.lang.Class,scala.collection.Seq)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/SQLContext/beansToRows(scala.collection.Iterator,java.lang.Class,scala.collection.Seq)#scala/collection/Iterator/map(scala.Function1)
org/apache/spark/sql/SQLContext/beansToRows(scala.collection.Iterator,java.lang.Class,scala.collection.Seq)#org/apache/spark/sql/SQLContext$$anonfun$1/1()
org/apache/spark/sql/SQLContext/beansToRows(scala.collection.Iterator,java.lang.Class,scala.collection.Seq)#org/apache/spark/sql/SQLContext$$anonfun$2/2()
org/apache/spark/sql/SQLContext/beansToRows(scala.collection.Iterator,java.lang.Class,scala.collection.Seq)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/SQLContext/beansToRows(scala.collection.Iterator,java.lang.Class,scala.collection.Seq)#org/apache/spark/sql/SQLContext$$anonfun$beansToRows$1/1(scala.Tuple2[])
org/apache/spark/sql/SQLContext/beansToRows(scala.collection.Iterator,java.lang.Class,scala.collection.Seq)#org/apache/spark/sql/catalyst/JavaTypeInference$/getJavaBeanReadableProperties(java.lang.Class)
org/apache/spark/sql/execution/datasources/PreWriteCheck/anonfun/apply/13/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/Seq/contains(java.lang.Object)
org/apache/spark/sql/execution/datasources/PreWriteCheck/anonfun/apply/13/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/datasources/PreWriteCheck$/failAnalysis(java.lang.String)
org/apache/spark/sql/execution/datasources/PreWriteCheck/anonfun/apply/13/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/PreWriteCheck/anonfun/apply/13/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PreWriteCheck/anonfun/apply/13/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/query()
org/apache/spark/sql/execution/datasources/PreWriteCheck/anonfun/apply/13/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/partition()
org/apache/spark/sql/execution/datasources/PreWriteCheck/anonfun/apply/13/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/PreWriteCheck/anonfun/apply/13/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/table()
org/apache/spark/sql/execution/datasources/PreWriteCheck/anonfun/apply/13/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/datasources/PreWriteCheck$$anonfun$apply$13$$anonfun$2/2(org.apache.spark.sql.execution.datasources.PreWriteCheck$$anonfun$apply$13)
org/apache/spark/sql/execution/datasources/PreWriteCheck/anonfun/apply/13/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PreWriteCheck/anonfun/apply/13/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/PreWriteCheck/anonfun/apply/13/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/immutable/Map/nonEmpty()
org/apache/spark/sql/execution/datasources/PreWriteCheck/anonfun/apply/13/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/collect(scala.PartialFunction)
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#org/apache/spark/SparkContext/getCallSite()
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#java/lang/Object/toString()
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#scala/Function0/apply()
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#org/apache/spark/SparkContext/setLocalProperty(java.lang.String,java.lang.String)
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#org/apache/spark/SparkContext/getLocalProperty(java.lang.String)
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#org/apache/spark/sql/execution/SQLExecution$/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#java/lang/System/currentTimeMillis()
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#org/apache/spark/scheduler/LiveListenerBus/post(org.apache.spark.scheduler.SparkListenerEvent)
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#java/util/concurrent/ConcurrentHashMap/remove(java.lang.Object)
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#org/apache/spark/util/CallSite/longForm()
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#java/util/concurrent/ConcurrentHashMap/put(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#org/apache/spark/sql/execution/SQLExecution$/executionIdToQueryExecution()
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#org/apache/spark/sql/execution/SQLExecution$/nextExecutionId()
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#org/apache/spark/sql/execution/SparkPlanInfo$/fromSparkPlan(org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#org/apache/spark/sql/execution/SQLExecution$/EXECUTION_ID_KEY()
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#org/apache/spark/SparkContext/listenerBus()
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#org/apache/spark/util/CallSite/shortForm()
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getCommonJDBCType(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/DecimalType/scale()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getCommonJDBCType(org.apache.spark.sql.types.DataType)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getCommonJDBCType(org.apache.spark.sql.types.DataType)#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getCommonJDBCType(org.apache.spark.sql.types.DataType)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getCommonJDBCType(org.apache.spark.sql.types.DataType)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getCommonJDBCType(org.apache.spark.sql.types.DataType)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getCommonJDBCType(org.apache.spark.sql.types.DataType)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getCommonJDBCType(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/getCommonJDBCType(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getCommonJDBCType(org.apache.spark.sql.types.DataType)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getCommonJDBCType(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/DecimalType/precision()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#java/sql/Connection/rollback()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#java/sql/DatabaseMetaData/supportsTransactionIsolationLevel(int)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#java/sql/SQLException/initCause(java.lang.Throwable)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#java/sql/SQLException/addSuppressed(java.lang.Throwable)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$savePartition$4/4()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#scala/collection/Iterator/hasNext()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#java/sql/Connection/close()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#scala/package$/Iterator()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$19/19(org.apache.spark.sql.jdbc.JdbcDialect,java.sql.Connection)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#java/sql/PreparedStatement/setNull(int,int)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#java/sql/Connection/getMetaData()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#scala/Option/get()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#java/sql/Connection/prepareStatement(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#java/sql/SQLException/getNextException()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#java/sql/PreparedStatement/addBatch()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#scala/reflect/ClassTag$/Int()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#scala/util/control/NonFatal$/unapply(java.lang.Throwable)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#scala/collection/Iterator/next()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#scala/collection/Iterator$/empty()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#java/sql/PreparedStatement/close()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#java/sql/DatabaseMetaData/supportsTransactions()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#scala/Function0/apply()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#java/sql/Connection/setTransactionIsolation(int)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#scala/Option/isEmpty()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#java/sql/DatabaseMetaData/getDefaultTransactionIsolation()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#java/sql/PreparedStatement/executeBatch()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#java/sql/SQLException/getCause()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$savePartition$3/3()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/logWarning(scala.Function0,java.lang.Throwable)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#java/sql/Connection/commit()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/logWarning(scala.Function0)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$20/20(org.apache.spark.sql.jdbc.JdbcDialect)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#org/apache/spark/sql/Row/isNullAt(int)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$savePartition$1/1(int,int)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$savePartition$2/2(int)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#scala/Function3/apply(java.lang.Object,java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#java/sql/Connection/setAutoCommit(boolean)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/createTable(java.sql.Connection,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#java/sql/Statement/close()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/createTable(java.sql.Connection,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/createTable(java.sql.Connection,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/schemaString(org.apache.spark.sql.Dataset,java.lang.String,scala.Option)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/createTable(java.sql.Connection,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/createTable(java.sql.Connection,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/createTable(java.sql.Connection,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#java/sql/Statement/executeUpdate(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/createTable(java.sql.Connection,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/createTable(java.sql.Connection,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/createTable(java.sql.Connection,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/createTable(java.sql.Connection,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#java/sql/Connection/createStatement()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#scala/Some/x()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions$/JDBC_NUM_PARTITIONS()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#org/apache/spark/sql/jdbc/JdbcDialects$/get(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$saveTable$1/1(java.lang.String,org.apache.spark.sql.jdbc.JdbcDialect,org.apache.spark.sql.types.StructType,scala.Function0,int,int,java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#org/apache/spark/rdd/RDD/getNumPartitions()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/createConnectionFactory(org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/types/MetadataBuilder/putString(java.lang.String,java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#java/sql/ResultSetMetaData/getColumnCount()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/types/StructField/StructField(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.types.Metadata)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#java/sql/ResultSetMetaData/isSigned(int)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#java/lang/Class/getName()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#java/lang/Object/getClass()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#java/sql/ResultSetMetaData/getScale(int)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#java/sql/ResultSetMetaData/getColumnType(int)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/types/MetadataBuilder/putLong(java.lang.String,long)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/types/MetadataBuilder/build()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/types/StructType/StructType(org.apache.spark.sql.types.StructField[])
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#java/sql/ResultSetMetaData/isNullable(int)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#java/sql/ResultSet/getMetaData()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#java/sql/ResultSetMetaData/getPrecision(int)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#java/sql/ResultSetMetaData/getColumnLabel(int)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/types/MetadataBuilder/MetadataBuilder()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#java/sql/SQLException/getMessage()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$8/8(int,int,int,boolean)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#java/sql/ResultSetMetaData/getColumnTypeName(int)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/catalyst/analysis/package$/caseSensitiveResolution()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)#scala/Option/get()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/types/StructType/fieldNames()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)#scala/collection/mutable/ArrayOps/mkString(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)#scala/Option/isEmpty()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$7/7()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$3/3(org.apache.spark.sql.jdbc.JdbcDialect)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$4/4(scala.Option,org.apache.spark.sql.jdbc.JdbcDialect,scala.Function2,java.lang.String[])
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/catalyst/analysis/package$/caseInsensitiveResolution()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/schemaString(org.apache.spark.sql.Dataset,java.lang.String,scala.Option)#scala/collection/mutable/StringBuilder/substring(int)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/schemaString(org.apache.spark.sql.Dataset,java.lang.String,scala.Option)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$21/21(org.apache.spark.sql.Dataset)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/schemaString(org.apache.spark.sql.Dataset,java.lang.String,scala.Option)#org/apache/spark/sql/jdbc/JdbcDialects$/get(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/schemaString(org.apache.spark.sql.Dataset,java.lang.String,scala.Option)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/schemaString(org.apache.spark.sql.Dataset,java.lang.String,scala.Option)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/schemaString(org.apache.spark.sql.Dataset,java.lang.String,scala.Option)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/schemaString(org.apache.spark.sql.Dataset,java.lang.String,scala.Option)#scala/collection/mutable/StringBuilder/length()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/schemaString(org.apache.spark.sql.Dataset,java.lang.String,scala.Option)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/schemaString(org.apache.spark.sql.Dataset,java.lang.String,scala.Option)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$22/22()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/schemaString(org.apache.spark.sql.Dataset,java.lang.String,scala.Option)#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/schemaString(org.apache.spark.sql.Dataset,java.lang.String,scala.Option)#scala/collection/mutable/ArrayOps/foreach(scala.Function1)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/schemaString(org.apache.spark.sql.Dataset,java.lang.String,scala.Option)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$schemaString$1/1(scala.collection.mutable.StringBuilder,org.apache.spark.sql.jdbc.JdbcDialect,scala.collection.immutable.Map)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/schemaString(org.apache.spark.sql.Dataset,java.lang.String,scala.Option)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/schemaString(org.apache.spark.sql.Dataset,java.lang.String,scala.Option)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/resultSetToRows(java.sql.ResultSet,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/catalyst/encoders/ExpressionEncoder/resolveAndBind$default$1()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/resultSetToRows(java.sql.ResultSet,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/resultSetToSparkInternalRows(java.sql.ResultSet,org.apache.spark.sql.types.StructType,org.apache.spark.executor.InputMetrics)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/resultSetToRows(java.sql.ResultSet,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/catalyst/encoders/RowEncoder$/apply(org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/resultSetToRows(java.sql.ResultSet,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/resultSetToRows(java.sql.ResultSet,org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/resultSetToRows(java.sql.ResultSet,org.apache.spark.sql.types.StructType)#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/resultSetToRows(java.sql.ResultSet,org.apache.spark.sql.types.StructType)#scala/collection/Iterator/map(scala.Function1)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/resultSetToRows(java.sql.ResultSet,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$resultSetToRows$1/1(org.apache.spark.sql.catalyst.encoders.ExpressionEncoder)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/resultSetToRows(java.sql.ResultSet,org.apache.spark.sql.types.StructType)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/resultSetToRows(java.sql.ResultSet,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$10/10()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/resultSetToRows(java.sql.ResultSet,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/catalyst/encoders/ExpressionEncoder/resolveAndBind$default$2()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/resultSetToRows(java.sql.ResultSet,org.apache.spark.sql.types.StructType)#org/apache/spark/TaskContext$/get()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/resultSetToRows(java.sql.ResultSet,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/catalyst/encoders/ExpressionEncoder/resolveAndBind(scala.collection.Seq,org.apache.spark.sql.catalyst.analysis.Analyzer)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/resultSetToRows(java.sql.ResultSet,org.apache.spark.sql.types.StructType)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/resultSetToRows(java.sql.ResultSet,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$9/9()
org/apache/spark/sql/execution/command/ShowTablesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/ShowTablesCommand$$anonfun$run$3/3(org.apache.spark.sql.execution.command.ShowTablesCommand,org.apache.spark.sql.catalyst.catalog.SessionCatalog)
org/apache/spark/sql/execution/command/ShowTablesCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/command/ShowTablesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/TableIdentifier/table()
org/apache/spark/sql/execution/command/ShowTablesCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/ShowTablesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/getPartition(org.apache.spark.sql.catalyst.TableIdentifier,scala.collection.immutable.Map)
org/apache/spark/sql/execution/command/ShowTablesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/ShowTablesCommand$$anonfun$13/13(org.apache.spark.sql.execution.command.ShowTablesCommand)
org/apache/spark/sql/execution/command/ShowTablesCommand/run(org.apache.spark.sql.SparkSession)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/command/ShowTablesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/Row$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/command/ShowTablesCommand/run(org.apache.spark.sql.SparkSession)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/ShowTablesCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/isEmpty()
org/apache/spark/sql/execution/command/ShowTablesCommand/run(org.apache.spark.sql.SparkSession)#scala/runtime/BoxesRunTime/boxToBoolean(boolean)
org/apache/spark/sql/execution/command/ShowTablesCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/command/ShowTablesCommand/run(org.apache.spark.sql.SparkSession)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/ShowTablesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/isTemporaryTable(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/ShowTablesCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/command/ShowTablesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/ShowTablesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/ShowTablesCommand$$anonfun$10/10(org.apache.spark.sql.execution.command.ShowTablesCommand,org.apache.spark.sql.catalyst.catalog.SessionCatalog,java.lang.String)
org/apache/spark/sql/execution/command/ShowTablesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/ShowTablesCommand$$anonfun$11/11(org.apache.spark.sql.execution.command.ShowTablesCommand,org.apache.spark.sql.catalyst.catalog.SessionCatalog,java.lang.String)
org/apache/spark/sql/execution/command/ShowTablesCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/ShowTablesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/TableIdentifier/database()
org/apache/spark/sql/execution/command/ShowTablesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTablePartition/simpleString()
org/apache/spark/sql/execution/command/ShowTablesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/ShowTablesCommand$$anonfun$9/9(org.apache.spark.sql.execution.command.ShowTablesCommand,org.apache.spark.sql.catalyst.catalog.SessionCatalog)
org/apache/spark/sql/execution/command/ShowTablesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/TableIdentifier/TableIdentifier(java.lang.String,scala.Option)
org/apache/spark/sql/execution/command/ShowTablesCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/command/ShowTablesCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/ShowTablesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()
org/apache/spark/sql/execution/command/ShowTablesCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/get()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/anonfun/showDataSourceTableNonDataColumns/1/apply(org.apache.spark.sql.catalyst.catalog.BucketSpec)#org/apache/spark/sql/catalyst/catalog/BucketSpec/numBuckets()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/anonfun/showDataSourceTableNonDataColumns/1/apply(org.apache.spark.sql.catalyst.catalog.BucketSpec)#org/apache/spark/sql/catalyst/catalog/BucketSpec/sortColumnNames()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/anonfun/showDataSourceTableNonDataColumns/1/apply(org.apache.spark.sql.catalyst.catalog.BucketSpec)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/ShowCreateTableCommand/anonfun/showDataSourceTableNonDataColumns/1/apply(org.apache.spark.sql.catalyst.catalog.BucketSpec)#scala/collection/mutable/StringBuilder/$plus$plus$eq(java.lang.String)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/anonfun/showDataSourceTableNonDataColumns/1/apply(org.apache.spark.sql.catalyst.catalog.BucketSpec)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/anonfun/showDataSourceTableNonDataColumns/1/apply(org.apache.spark.sql.catalyst.catalog.BucketSpec)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/anonfun/showDataSourceTableNonDataColumns/1/apply(org.apache.spark.sql.catalyst.catalog.BucketSpec)#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/anonfun/showDataSourceTableNonDataColumns/1/apply(org.apache.spark.sql.catalyst.catalog.BucketSpec)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/anonfun/showDataSourceTableNonDataColumns/1/apply(org.apache.spark.sql.catalyst.catalog.BucketSpec)#org/apache/spark/sql/catalyst/catalog/BucketSpec/bucketColumnNames()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/anonfun/showDataSourceTableNonDataColumns/1/apply(org.apache.spark.sql.catalyst.catalog.BucketSpec)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/anonfun/showDataSourceTableNonDataColumns/1/apply(org.apache.spark.sql.catalyst.catalog.BucketSpec)#scala/collection/Seq/mkString(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/stat/FrequentItems/anonfun/7/apply(scala.Tuple2)#org/apache/spark/sql/types/StructField$/apply$default$3()
org/apache/spark/sql/execution/stat/FrequentItems/anonfun/7/apply(scala.Tuple2)#org/apache/spark/sql/types/StructField$/apply$default$4()
org/apache/spark/sql/execution/stat/FrequentItems/anonfun/7/apply(scala.Tuple2)#org/apache/spark/sql/types/StructField/StructField(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.types.Metadata)
org/apache/spark/sql/execution/stat/FrequentItems/anonfun/7/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/stat/FrequentItems/anonfun/7/apply(scala.Tuple2)#scala/Tuple2/_2()
org/apache/spark/sql/execution/stat/FrequentItems/anonfun/7/apply(scala.Tuple2)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/stat/FrequentItems/anonfun/7/apply(scala.Tuple2)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/stat/FrequentItems/anonfun/7/apply(scala.Tuple2)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/stat/FrequentItems/anonfun/7/apply(scala.Tuple2)#org/apache/spark/sql/types/ArrayType/ArrayType(org.apache.spark.sql.types.DataType,boolean)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/1/applyOrElse(scala.Option,scala.Function1)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/1/applyOrElse(scala.Option,scala.Function1)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/1/applyOrElse(scala.Option,scala.Function1)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/1/applyOrElse(scala.Option,scala.Function1)#scala/collection/mutable/ArrayOps/mkString(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/1/applyOrElse(scala.Option,scala.Function1)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/1/applyOrElse(scala.Option,scala.Function1)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/1/applyOrElse(scala.Option,scala.Function1)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/1/applyOrElse(scala.Option,scala.Function1)#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/1/applyOrElse(scala.Option,scala.Function1)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/1/applyOrElse(scala.Option,scala.Function1)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/1/applyOrElse(scala.Option,scala.Function1)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/1/applyOrElse(scala.Option,scala.Function1)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/1/applyOrElse(scala.Option,scala.Function1)#org/apache/spark/sql/execution/datasources/DataSourceAnalysis$$anonfun$convertStaticPartitions$1$$anonfun$applyOrElse$1/1(org.apache.spark.sql.execution.datasources.DataSourceAnalysis$$anonfun$convertStaticPartitions$1)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/1/applyOrElse(scala.Option,scala.Function1)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/1/applyOrElse(scala.Option,scala.Function1)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/1/applyOrElse(scala.Option,scala.Function1)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/1/applyOrElse(scala.Option,scala.Function1)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/1/applyOrElse(scala.Option,scala.Function1)#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/1/applyOrElse(scala.Option,scala.Function1)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/1/applyOrElse(scala.Option,scala.Function1)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/8/apply(scala.Tuple2)#scala/collection/Seq/find(scala.Function1)
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/8/apply(scala.Tuple2)#scala/Predef$/ArrowAssoc(java.lang.Object)
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/8/apply(scala.Tuple2)#scala/Tuple2/_2()
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/8/apply(scala.Tuple2)#scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/8/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/8/apply(scala.Tuple2)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/8/apply(scala.Tuple2)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$8$$anonfun$9/9(org.apache.spark.sql.execution.datasources.PartitioningUtils$$anonfun$8,java.lang.String)
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/8/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/8/apply(scala.Tuple2)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$8$$anonfun$10/10(org.apache.spark.sql.execution.datasources.PartitioningUtils$$anonfun$8,java.lang.String)
org/apache/spark/sql/internal/SharedState/globalTempViewManager$lzycompute()#org/apache/spark/SparkException/SparkException(java.lang.String)
org/apache/spark/sql/internal/SharedState/globalTempViewManager$lzycompute()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/internal/SharedState/globalTempViewManager$lzycompute()#org/apache/spark/sql/catalyst/catalog/GlobalTempViewManager/GlobalTempViewManager(java.lang.String)
org/apache/spark/sql/internal/SharedState/globalTempViewManager$lzycompute()#org/apache/spark/SparkContext/conf()
org/apache/spark/sql/internal/SharedState/globalTempViewManager$lzycompute()#org/apache/spark/internal/config/ConfigEntry/key()
org/apache/spark/sql/internal/SharedState/globalTempViewManager$lzycompute()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/internal/SharedState/globalTempViewManager$lzycompute()#org/apache/spark/SparkConf/get(org.apache.spark.internal.config.ConfigEntry)
org/apache/spark/sql/internal/SharedState/globalTempViewManager$lzycompute()#java/lang/String/toLowerCase(java.util.Locale)
org/apache/spark/sql/internal/SharedState/globalTempViewManager$lzycompute()#org/apache/spark/sql/internal/StaticSQLConf$/GLOBAL_TEMP_DATABASE()
org/apache/spark/sql/internal/SharedState/globalTempViewManager$lzycompute()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/internal/SharedState/globalTempViewManager$lzycompute()#org/apache/spark/sql/catalyst/catalog/ExternalCatalog/databaseExists(java.lang.String)
org/apache/spark/sql/internal/SharedState/globalTempViewManager$lzycompute()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/internal/SharedState/globalTempViewManager$lzycompute()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/internal/SharedState/globalTempViewManager$lzycompute()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/internal/SharedState/globalTempViewManager$lzycompute()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/internal/SharedState/externalCatalog$lzycompute()#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/internal/SharedState/externalCatalog$lzycompute()#scala/collection/immutable/Map$/apply(scala.collection.Seq)
org/apache/spark/sql/internal/SharedState/externalCatalog$lzycompute()#scala/Predef$/Map()
org/apache/spark/sql/internal/SharedState/externalCatalog$lzycompute()#org/apache/spark/sql/internal/SharedState$/org$apache$spark$sql$internal$SharedState$$reflect(java.lang.String,java.lang.Object,java.lang.Object,scala.reflect.ClassTag,scala.reflect.ClassTag)
org/apache/spark/sql/internal/SharedState/externalCatalog$lzycompute()#org/apache/spark/sql/catalyst/catalog/ExternalCatalog/databaseExists(java.lang.String)
org/apache/spark/sql/internal/SharedState/externalCatalog$lzycompute()#org/apache/spark/SparkContext/conf()
org/apache/spark/sql/internal/SharedState/externalCatalog$lzycompute()#org/apache/spark/sql/catalyst/catalog/CatalogDatabase/CatalogDatabase(java.lang.String,java.lang.String,java.net.URI,scala.collection.immutable.Map)
org/apache/spark/sql/internal/SharedState/externalCatalog$lzycompute()#org/apache/spark/sql/catalyst/catalog/ExternalCatalog/addListener(java.lang.Object)
org/apache/spark/sql/internal/SharedState/externalCatalog$lzycompute()#org/apache/spark/sql/internal/SharedState$/org$apache$spark$sql$internal$SharedState$$externalCatalogClassName(org.apache.spark.SparkConf)
org/apache/spark/sql/internal/SharedState/externalCatalog$lzycompute()#org/apache/spark/sql/catalyst/catalog/ExternalCatalog/createDatabase(org.apache.spark.sql.catalyst.catalog.CatalogDatabase,boolean)
org/apache/spark/sql/internal/SharedState/externalCatalog$lzycompute()#org/apache/spark/sql/catalyst/catalog/SessionCatalog$/DEFAULT_DATABASE()
org/apache/spark/sql/internal/SharedState/externalCatalog$lzycompute()#org/apache/spark/sql/catalyst/catalog/CatalogUtils$/stringToURI(java.lang.String)
org/apache/spark/sql/internal/SharedState/externalCatalog$lzycompute()#org/apache/spark/sql/internal/SharedState$$anon$1/1(org.apache.spark.sql.internal.SharedState)
org/apache/spark/sql/internal/SharedState/externalCatalog$lzycompute()#org/apache/spark/SparkContext/hadoopConfiguration()
org/apache/spark/sql/execution/command/SetCommand/anonfun/7/anonfun/apply/6/apply()#org/apache/spark/sql/internal/SQLConf$Deprecated$/MAPRED_REDUCE_TASKS()
org/apache/spark/sql/execution/command/SetCommand/anonfun/7/anonfun/apply/6/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/SetCommand/anonfun/7/anonfun/apply/6/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/SetCommand/anonfun/7/anonfun/apply/6/apply()#org/apache/spark/sql/execution/command/SetCommand$$anonfun$7$$anonfun$apply$6/apply()
org/apache/spark/sql/execution/command/SetCommand/anonfun/7/anonfun/apply/6/apply()#org/apache/spark/internal/config/ConfigEntry/key()
org/apache/spark/sql/execution/command/SetCommand/anonfun/7/anonfun/apply/6/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/command/SetCommand/anonfun/7/anonfun/apply/6/apply()#org/apache/spark/sql/internal/SQLConf$/SHUFFLE_PARTITIONS()
org/apache/spark/sql/execution/command/SetCommand/anonfun/7/anonfun/apply/6/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/command/SetCommand/anonfun/7/anonfun/apply/6/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/SetCommand/anonfun/7/anonfun/apply/6/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/SetCommand/anonfun/7/anonfun/apply/6/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateDatabase/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateDatabaseContext/identifier()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateDatabase/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateDatabaseContext/locationSpec()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateDatabase/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateDatabase$1/apply()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateDatabase/1/apply()#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateDatabase/1/apply()#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateDatabase/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateDatabaseContext/EXISTS()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateDatabase/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateDatabase$1$$anonfun$apply$17/17(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateDatabase$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateDatabase/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateDatabase$1$$anonfun$apply$18/18(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateDatabase$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateDatabase/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateDatabase$1$$anonfun$apply$19/19(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateDatabase$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateDatabase/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateDatabase$1$$anonfun$apply$20/20(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateDatabase$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateDatabase/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$IdentifierContext/getText()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateDatabase/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateDatabaseContext/tablePropertyList()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateDatabase/1/apply()#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anonfun/2/apply()#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anonfun/2/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anonfun/2/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anonfun/2/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anonfun/2/apply()#org/apache/spark/sql/types/StructType/prettyJson()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anonfun/2/apply()#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anonfun/2/apply()#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anonfun/2/apply()#org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter$$anonfun$2/apply()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anonfun/2/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$/makeLt()
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$/makeGt()
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$/makeGtEq()
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$createFilter$8/8(java.lang.String,java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$createFilter$1/1(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$createFilter$3/3(java.lang.String,java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$/makeNotEq()
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$/getFieldMap(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#scala/PartialFunction/lift()
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$createFilter$10/10(java.lang.String,java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$/makeEq()
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#scala/Option/flatMap(scala.Function1)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$/canMakeFilterOn$1(java.lang.String,scala.collection.immutable.Map)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$createFilter$12/12(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$createFilter$11/11(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$/makeLtEq()
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#scala/collection/immutable/Map/apply(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$createFilter$9/9(java.lang.String,java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$createFilter$7/7(java.lang.String,java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$createFilter$13/13()
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$createFilter$6/6(java.lang.String,java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$createFilter$2/2(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$createFilter$5/5(java.lang.String,java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$createFilter$4/4(java.lang.String,java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/anonfun/lookupDataSource/1/apply()#scala/collection/immutable/List/mkString(java.lang.String)
org/apache/spark/sql/execution/datasources/DataSource/anonfun/lookupDataSource/1/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/DataSource/anonfun/lookupDataSource/1/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSource/anonfun/lookupDataSource/1/apply()#java/lang/Class/getName()
org/apache/spark/sql/execution/datasources/DataSource/anonfun/lookupDataSource/1/apply()#scala/collection/immutable/List/head()
org/apache/spark/sql/execution/datasources/DataSource/anonfun/lookupDataSource/1/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/anonfun/lookupDataSource/1/apply()#java/lang/Object/getClass()
org/apache/spark/sql/execution/datasources/DataSource/anonfun/lookupDataSource/1/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/DataSource/anonfun/lookupDataSource/1/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/anonfun/lookupDataSource/1/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSource/anonfun/lookupDataSource/1/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/DataSource/anonfun/lookupDataSource/1/apply()#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$lookupDataSource$1/apply()
org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource/readFile(org.apache.hadoop.conf.Configuration,org.apache.spark.sql.execution.datasources.PartitionedFile,org.apache.spark.sql.catalyst.json.JacksonParser,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/catalyst/json/JacksonParser/options()
org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource/readFile(org.apache.hadoop.conf.Configuration,org.apache.spark.sql.execution.datasources.PartitionedFile,org.apache.spark.sql.catalyst.json.JacksonParser,org.apache.spark.sql.types.StructType)#org/apache/spark/TaskContext$/get()
org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource/readFile(org.apache.hadoop.conf.Configuration,org.apache.spark.sql.execution.datasources.PartitionedFile,org.apache.spark.sql.catalyst.json.JacksonParser,org.apache.spark.sql.types.StructType)#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource/readFile(org.apache.hadoop.conf.Configuration,org.apache.spark.sql.execution.datasources.PartitionedFile,org.apache.spark.sql.catalyst.json.JacksonParser,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource$$anonfun$readFile$1/1(org.apache.spark.sql.execution.datasources.HadoopFileLinesReader)
org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource/readFile(org.apache.hadoop.conf.Configuration,org.apache.spark.sql.execution.datasources.PartitionedFile,org.apache.spark.sql.catalyst.json.JacksonParser,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/catalyst/json/JSONOptions/parseMode()
org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource/readFile(org.apache.hadoop.conf.Configuration,org.apache.spark.sql.execution.datasources.PartitionedFile,org.apache.spark.sql.catalyst.json.JacksonParser,org.apache.spark.sql.types.StructType)#scala/Option/foreach(scala.Function1)
org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource/readFile(org.apache.hadoop.conf.Configuration,org.apache.spark.sql.execution.datasources.PartitionedFile,org.apache.spark.sql.catalyst.json.JacksonParser,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource$$anonfun$readFile$2/2(org.apache.spark.sql.execution.datasources.FailureSafeParser)
org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource/readFile(org.apache.hadoop.conf.Configuration,org.apache.spark.sql.execution.datasources.PartitionedFile,org.apache.spark.sql.catalyst.json.JacksonParser,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource$/readFile(org.apache.hadoop.conf.Configuration,org.apache.spark.sql.execution.datasources.PartitionedFile,org.apache.spark.sql.catalyst.json.JacksonParser,org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource/readFile(org.apache.hadoop.conf.Configuration,org.apache.spark.sql.execution.datasources.PartitionedFile,org.apache.spark.sql.catalyst.json.JacksonParser,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/catalyst/json/JSONOptions/columnNameOfCorruptRecord()
org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource/readFile(org.apache.hadoop.conf.Configuration,org.apache.spark.sql.execution.datasources.PartitionedFile,org.apache.spark.sql.catalyst.json.JacksonParser,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource$$anonfun$5/5(org.apache.spark.sql.catalyst.json.JacksonParser)
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator/anonfun/receiveAndReply/1/anonfun/applyOrElse/4/apply()#org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$4/apply()
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator/anonfun/receiveAndReply/1/anonfun/applyOrElse/4/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator/anonfun/receiveAndReply/1/anonfun/applyOrElse/4/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator/anonfun/receiveAndReply/1/anonfun/applyOrElse/4/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator/anonfun/receiveAndReply/1/anonfun/applyOrElse/4/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator/anonfun/receiveAndReply/1/anonfun/applyOrElse/4/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator/anonfun/receiveAndReply/1/anonfun/applyOrElse/4/apply()#scala/collection/Seq/mkString(java.lang.String)
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator/anonfun/receiveAndReply/1/anonfun/applyOrElse/4/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinator/anonfun/receiveAndReply/1/anonfun/applyOrElse/4/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/FlatMapGroupsInRExec/anonfun/13/apply(scala.collection.Iterator)#org/apache/spark/sql/types/StructType/fieldNames()
org/apache/spark/sql/execution/FlatMapGroupsInRExec/anonfun/13/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/FlatMapGroupsInRExec$$anonfun$13$$anonfun$14/14(org.apache.spark.sql.execution.FlatMapGroupsInRExec$$anonfun$13,scala.Function1,scala.Function1)
org/apache/spark/sql/execution/FlatMapGroupsInRExec/anonfun/13/apply(scala.collection.Iterator)#scala/collection/Iterator/map(scala.Function1)
org/apache/spark/sql/execution/FlatMapGroupsInRExec/anonfun/13/apply(scala.collection.Iterator)#org/apache/spark/api/r/SerializationFormats$/ROW()
org/apache/spark/sql/execution/FlatMapGroupsInRExec/anonfun/13/apply(scala.collection.Iterator)#org/apache/spark/api/r/RRunner/compute(scala.collection.Iterator,int)
org/apache/spark/sql/execution/FlatMapGroupsInRExec/anonfun/13/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/ObjectOperator$/wrapObjectToRow(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/FlatMapGroupsInRExec/anonfun/13/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/FlatMapGroupsInRExec/anonfun/13/apply(scala.collection.Iterator)#org/apache/spark/api/r/RRunner/RRunner(byte[],java.lang.String,java.lang.String,byte[],org.apache.spark.broadcast.Broadcast[],int,boolean,java.lang.String[],int)
org/apache/spark/sql/execution/FlatMapGroupsInRExec/anonfun/13/apply(scala.collection.Iterator)#org/apache/spark/sql/catalyst/expressions/Attribute/dataType()
org/apache/spark/sql/execution/FlatMapGroupsInRExec/anonfun/13/apply(scala.collection.Iterator)#org/apache/spark/api/r/RRunnerModes$/DATAFRAME_GAPPLY()
org/apache/spark/sql/execution/FlatMapGroupsInRExec/anonfun/13/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/FlatMapGroupsInRExec$$anonfun$13$$anonfun$16/16(org.apache.spark.sql.execution.FlatMapGroupsInRExec$$anonfun$13)
org/apache/spark/sql/execution/FlatMapGroupsInRExec/anonfun/13/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/FlatMapGroupsInRExec$$anonfun$13$$anonfun$17/17(org.apache.spark.sql.execution.FlatMapGroupsInRExec$$anonfun$13)
org/apache/spark/sql/execution/FlatMapGroupsInRExec/anonfun/13/apply(scala.collection.Iterator)#org/apache/spark/api/r/RRunner$/$lessinit$greater$default$6()
org/apache/spark/sql/execution/FlatMapGroupsInRExec/anonfun/13/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/ObjectOperator$/deserializeRowToObject(org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq)
org/apache/spark/sql/execution/FlatMapGroupsInRExec/anonfun/13/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/GroupedIterator$/apply(scala.collection.Iterator,scala.collection.Seq,scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#org/apache/spark/SparkContext/parallelize(scala.collection.Seq,int,scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#org/apache/spark/rdd/RDD/mapPartitions$default$2()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#java/lang/Math/max(int,int)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#org/apache/spark/SparkContext/defaultParallelism()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$mergeSchemasInParallel$1/1(scala.runtime.ObjectRef)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#scala/collection/mutable/ArrayOps/tail()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#scala/collection/mutable/ArrayOps/foreach(scala.Function1)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#scala/collection/Seq/size()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#scala/collection/mutable/ArrayOps/isEmpty()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#java/lang/Math/min(int,int)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#scala/collection/mutable/ArrayOps/head()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$11/11(boolean,boolean,boolean,boolean,org.apache.spark.util.SerializableConfiguration,boolean)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#org/apache/spark/rdd/RDD/collect()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#org/apache/spark/rdd/RDD/mapPartitions(scala.Function1,boolean,scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#org/apache/spark/sql/internal/SQLConf/writeLegacyParquetFormat()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#org/apache/spark/sql/internal/SQLConf/isParquetBinaryAsString()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$10/10()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#org/apache/spark/sql/internal/SQLConf/isParquetINT96AsTimestamp()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#org/apache/spark/sql/internal/SQLConf/ignoreCorruptFiles()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#org/apache/spark/sql/internal/SQLConf/isParquetINT64AsTimestampMillis()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#scala/runtime/ObjectRef/create(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#org/apache/spark/util/SerializableConfiguration/SerializableConfiguration(org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/readParquetFootersInParallel(org.apache.hadoop.conf.Configuration,scala.collection.Seq,boolean)#scala/collection/Seq/par()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/readParquetFootersInParallel(org.apache.hadoop.conf.Configuration,scala.collection.Seq,boolean)#scala/concurrent/forkjoin/ForkJoinPool/shutdown()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/readParquetFootersInParallel(org.apache.hadoop.conf.Configuration,scala.collection.Seq,boolean)#scala/collection/parallel/ParSeq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/readParquetFootersInParallel(org.apache.hadoop.conf.Configuration,scala.collection.Seq,boolean)#org/apache/spark/util/ThreadUtils$/newForkJoinPool(java.lang.String,int)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/readParquetFootersInParallel(org.apache.hadoop.conf.Configuration,scala.collection.Seq,boolean)#scala/collection/parallel/ParSeq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/readParquetFootersInParallel(org.apache.hadoop.conf.Configuration,scala.collection.Seq,boolean)#scala/collection/parallel/ForkJoinTaskSupport/ForkJoinTaskSupport(scala.concurrent.forkjoin.ForkJoinPool)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/readParquetFootersInParallel(org.apache.hadoop.conf.Configuration,scala.collection.Seq,boolean)#scala/collection/parallel/ParIterableLike/seq()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/readParquetFootersInParallel(org.apache.hadoop.conf.Configuration,scala.collection.Seq,boolean)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$readParquetFootersInParallel$1/1(org.apache.hadoop.conf.Configuration,boolean)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/readParquetFootersInParallel(org.apache.hadoop.conf.Configuration,scala.collection.Seq,boolean)#scala/collection/parallel/ParSeq/tasksupport_$eq(scala.collection.parallel.TaskSupport)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/readSchemaFromFooter(org.apache.parquet.hadoop.Footer,org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$/readSchemaFromFooter(org.apache.parquet.hadoop.Footer,org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/readSchemaFromFooter(org.apache.parquet.hadoop.Footer,org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$readSchemaFromFooter$2/2(org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter,org.apache.parquet.hadoop.metadata.FileMetaData)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/readSchemaFromFooter(org.apache.parquet.hadoop.Footer,org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter)#org/apache/parquet/hadoop/Footer/getParquetMetadata()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/readSchemaFromFooter(org.apache.parquet.hadoop.Footer,org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter)#scala/Option/flatMap(scala.Function1)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/readSchemaFromFooter(org.apache.parquet.hadoop.Footer,org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter)#scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/readSchemaFromFooter(org.apache.parquet.hadoop.Footer,org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter)#scala/collection/convert/Decorators$AsScala/asScala()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/readSchemaFromFooter(org.apache.parquet.hadoop.Footer,org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter)#org/apache/parquet/hadoop/metadata/FileMetaData/getKeyValueMetaData()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/readSchemaFromFooter(org.apache.parquet.hadoop.Footer,org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter)#scala/Predef$/$conforms()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/readSchemaFromFooter(org.apache.parquet.hadoop.Footer,org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter)#org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport$/SPARK_METADATA_KEY()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/readSchemaFromFooter(org.apache.parquet.hadoop.Footer,org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/readSchemaFromFooter(org.apache.parquet.hadoop.Footer,org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$readSchemaFromFooter$1/1()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/readSchemaFromFooter(org.apache.parquet.hadoop.Footer,org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter)#org/apache/parquet/hadoop/metadata/ParquetMetadata/getFileMetaData()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/readSchemaFromFooter(org.apache.parquet.hadoop.Footer,org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter)#scala/collection/immutable/Map/get(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/readSchemaFromFooter(org.apache.parquet.hadoop.Footer,org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter)#scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$close$1(scala.runtime.BooleanRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$close$1$5/5(org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$close$1(scala.runtime.BooleanRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef)#scala/util/control/NonFatal$/unapply(java.lang.Throwable)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$close$1(scala.runtime.BooleanRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$close$1$4/4(org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$close$1(scala.runtime.BooleanRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$close$1$3/3(org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$close$1(scala.runtime.BooleanRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef)#java/sql/PreparedStatement/close()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$close$1(scala.runtime.BooleanRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$close$1$2/2(org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$close$1(scala.runtime.BooleanRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$close$1$1/1(org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$close$1(scala.runtime.BooleanRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef)#scala/Option/isEmpty()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$close$1(scala.runtime.BooleanRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef)#java/sql/Connection/close()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$close$1(scala.runtime.BooleanRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef)#java/sql/Connection/commit()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$close$1(scala.runtime.BooleanRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef)#java/sql/Connection/getAutoCommit()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$close$1(scala.runtime.BooleanRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef)#java/sql/ResultSet/close()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$close$1(scala.runtime.BooleanRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef)#scala/Option/get()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$close$1(scala.runtime.BooleanRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef)#java/sql/Connection/isClosed()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$$anonfun$compileFilter$3/3()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$$anonfun$3/3(org.apache.spark.sql.jdbc.JdbcDialect)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#scala/Predef$/genericArrayOps(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/sources/In/values()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$$anonfun$compileFilter$1/1()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$$anonfun$compileFilter$2/2()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$$anonfun$compileFilter$4/4()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#scala/collection/TraversableOnce/mkString(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#scala/collection/TraversableLike/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$$anonfun$2/2(org.apache.spark.sql.jdbc.JdbcDialect)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#scala/collection/Seq/size()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$/org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$compileValue(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#scala/collection/mutable/ArrayOps/isEmpty()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$/quote$1(java.lang.String,org.apache.spark.sql.jdbc.JdbcDialect)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/scanTable(org.apache.spark.SparkContext,org.apache.spark.sql.types.StructType,java.lang.String[],org.apache.spark.sql.sources.Filter[],org.apache.spark.Partition[],org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/scanTable(org.apache.spark.SparkContext,org.apache.spark.sql.types.StructType,java.lang.String[],org.apache.spark.sql.sources.Filter[],org.apache.spark.Partition[],org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/scanTable(org.apache.spark.SparkContext,org.apache.spark.sql.types.StructType,java.lang.String[],org.apache.spark.sql.sources.Filter[],org.apache.spark.Partition[],org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/scanTable(org.apache.spark.SparkContext,org.apache.spark.sql.types.StructType,java.lang.String[],org.apache.spark.sql.sources.Filter[],org.apache.spark.Partition[],org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$/pruneSchema(org.apache.spark.sql.types.StructType,java.lang.String[])
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/scanTable(org.apache.spark.SparkContext,org.apache.spark.sql.types.StructType,java.lang.String[],org.apache.spark.sql.sources.Filter[],org.apache.spark.Partition[],org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#org/apache/spark/sql/jdbc/JdbcDialects$/get(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/scanTable(org.apache.spark.SparkContext,org.apache.spark.sql.types.StructType,java.lang.String[],org.apache.spark.sql.sources.Filter[],org.apache.spark.Partition[],org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$$anonfun$4/4(org.apache.spark.sql.jdbc.JdbcDialect)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/scanTable(org.apache.spark.SparkContext,org.apache.spark.sql.types.StructType,java.lang.String[],org.apache.spark.sql.sources.Filter[],org.apache.spark.Partition[],org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$/scanTable(org.apache.spark.SparkContext,org.apache.spark.sql.types.StructType,java.lang.String[],org.apache.spark.sql.sources.Filter[],org.apache.spark.Partition[],org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/scanTable(org.apache.spark.SparkContext,org.apache.spark.sql.types.StructType,java.lang.String[],org.apache.spark.sql.sources.Filter[],org.apache.spark.Partition[],org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/scanTable(org.apache.spark.SparkContext,org.apache.spark.sql.types.StructType,java.lang.String[],org.apache.spark.sql.sources.Filter[],org.apache.spark.Partition[],org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/createConnectionFactory(org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/resolveTable(org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#java/sql/Connection/prepareStatement(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/resolveTable(org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#java/sql/PreparedStatement/close()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/resolveTable(org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#scala/Function0/apply()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/resolveTable(org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#org/apache/spark/sql/jdbc/JdbcDialects$/get(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/resolveTable(org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#java/sql/Connection/close()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/resolveTable(org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/resolveTable(org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/createConnectionFactory(org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/resolveTable(org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$/resolveTable(org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/resolveTable(org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#java/sql/ResultSet/close()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/resolveTable(org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#java/sql/PreparedStatement/executeQuery()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compute(org.apache.spark.Partition,org.apache.spark.TaskContext)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$$anonfun$compute$1/1(org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD,scala.runtime.BooleanRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compute(org.apache.spark.Partition,org.apache.spark.TaskContext)#org/apache/spark/InterruptibleIterator/InterruptibleIterator(org.apache.spark.TaskContext,scala.collection.Iterator)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compute(org.apache.spark.Partition,org.apache.spark.TaskContext)#scala/collection/JavaConverters$/propertiesAsScalaMapConverter(java.util.Properties)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compute(org.apache.spark.Partition,org.apache.spark.TaskContext)#scala/Function0/apply()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compute(org.apache.spark.Partition,org.apache.spark.TaskContext)#scala/collection/convert/Decorators$AsScala/asScala()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compute(org.apache.spark.Partition,org.apache.spark.TaskContext)#org/apache/spark/sql/jdbc/JdbcDialects$/get(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compute(org.apache.spark.Partition,org.apache.spark.TaskContext)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compute(org.apache.spark.Partition,org.apache.spark.TaskContext)#org/apache/spark/TaskContext/addTaskCompletionListener(scala.Function1)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compute(org.apache.spark.Partition,org.apache.spark.TaskContext)#scala/runtime/BooleanRef/create(boolean)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compute(org.apache.spark.Partition,org.apache.spark.TaskContext)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$$anonfun$compute$2/2(org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD,scala.runtime.BooleanRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef,scala.runtime.ObjectRef)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compute(org.apache.spark.Partition,org.apache.spark.TaskContext)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/resultSetToSparkInternalRows(java.sql.ResultSet,org.apache.spark.sql.types.StructType,org.apache.spark.executor.InputMetrics)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compute(org.apache.spark.Partition,org.apache.spark.TaskContext)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compute(org.apache.spark.Partition,org.apache.spark.TaskContext)#org/apache/spark/TaskContext/taskMetrics()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compute(org.apache.spark.Partition,org.apache.spark.TaskContext)#scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compute(org.apache.spark.Partition,org.apache.spark.TaskContext)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compute(org.apache.spark.Partition,org.apache.spark.TaskContext)#scala/Predef$/$conforms()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compute(org.apache.spark.Partition,org.apache.spark.TaskContext)#java/sql/PreparedStatement/setFetchSize(int)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compute(org.apache.spark.Partition,org.apache.spark.TaskContext)#org/apache/spark/executor/TaskMetrics/inputMetrics()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compute(org.apache.spark.Partition,org.apache.spark.TaskContext)#java/sql/Connection/prepareStatement(java.lang.String,int,int)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compute(org.apache.spark.Partition,org.apache.spark.TaskContext)#java/sql/PreparedStatement/executeQuery()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compute(org.apache.spark.Partition,org.apache.spark.TaskContext)#org/apache/spark/util/CompletionIterator$/apply(scala.collection.Iterator,scala.Function0)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compute(org.apache.spark.Partition,org.apache.spark.TaskContext)#scala/runtime/ObjectRef/create(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compute(org.apache.spark.Partition,org.apache.spark.TaskContext)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/AggregationIterator/anonfun/1/applyOrElse(scala.Tuple2,scala.Function1)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/aggregate/AggregationIterator/anonfun/1/applyOrElse(scala.Tuple2,scala.Function1)#org/apache/spark/sql/catalyst/expressions/aggregate/AggregateExpression/mode()
org/apache/spark/sql/execution/aggregate/AggregationIterator/anonfun/1/applyOrElse(scala.Tuple2,scala.Function1)#scala/collection/Seq/apply(int)
org/apache/spark/sql/execution/aggregate/AggregationIterator/anonfun/1/applyOrElse(scala.Tuple2,scala.Function1)#scala/Tuple2/_1()
org/apache/spark/sql/execution/aggregate/AggregationIterator/anonfun/1/applyOrElse(scala.Tuple2,scala.Function1)#scala/Tuple2/_2$mcI$sp()
org/apache/spark/sql/execution/aggregate/AggregationIterator/anonfun/1/applyOrElse(scala.Tuple2,scala.Function1)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/aggregate/AggregationIterator/anonfun/1/applyOrElse(scala.Tuple2,scala.Function1)#org/apache/spark/sql/execution/aggregate/AggregationIterator$$anonfun$1$$anonfun$applyOrElse$2/2(org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$1,org.apache.spark.sql.catalyst.expressions.aggregate.ImperativeAggregate)
org/apache/spark/sql/execution/aggregate/AggregationIterator/anonfun/1/applyOrElse(scala.Tuple2,scala.Function1)#org/apache/spark/sql/execution/aggregate/AggregationIterator$$anonfun$1$$anonfun$applyOrElse$1/1(org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$1,org.apache.spark.sql.catalyst.expressions.aggregate.ImperativeAggregate)
org/apache/spark/sql/execution/aggregate/AggregationIterator/anonfun/1/applyOrElse(scala.Tuple2,scala.Function1)#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/execution/command/CreateViewCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/empty()
org/apache/spark/sql/execution/command/CreateViewCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/createTable(org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)
org/apache/spark/sql/execution/command/CreateViewCommand/run(org.apache.spark.sql.SparkSession)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/command/CreateViewCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/execution/command/CreateViewCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/command/CreateViewCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/command/CreateViewCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/command/CreateViewCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq/length()
org/apache/spark/sql/execution/command/CreateViewCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/tableExists(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/CreateViewCommand/run(org.apache.spark.sql.SparkSession)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/command/CreateViewCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/TableIdentifier/table()
org/apache/spark/sql/execution/command/CreateViewCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/command/CreateViewCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/createTempView(java.lang.String,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,boolean)
org/apache/spark/sql/execution/command/CreateViewCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/CreateViewCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/tableType()
org/apache/spark/sql/execution/command/CreateViewCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/createGlobalTempView(java.lang.String,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,boolean)
org/apache/spark/sql/execution/command/CreateViewCommand/run(org.apache.spark.sql.SparkSession)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/CreateViewCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/output()
org/apache/spark/sql/execution/command/CreateViewCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/command/CreateViewCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/CreateViewCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/command/CreateViewCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/command/CreateViewCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/CreateViewCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/dropTable(org.apache.spark.sql.catalyst.TableIdentifier,boolean,boolean)
org/apache/spark/sql/execution/command/CreateViewCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/command/CreateViewCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTableType$/VIEW()
org/apache/spark/sql/execution/command/CreateViewCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()
org/apache/spark/sql/execution/command/CreateViewCommand/run(org.apache.spark.sql.SparkSession)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/CreateViewCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/command/CreateViewCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/ViewHelper$/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/CreateViewCommand/prepareTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/command/ViewHelper$/generateViewProperties(scala.collection.immutable.Map,org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/command/CreateViewCommand/prepareTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat$/empty()
org/apache/spark/sql/execution/command/CreateViewCommand/prepareTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/command/CreateViewCommand/prepareTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Option/isEmpty()
org/apache/spark/sql/execution/command/CreateViewCommand/prepareTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$9()
org/apache/spark/sql/execution/command/CreateViewCommand/prepareTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$15()
org/apache/spark/sql/execution/command/CreateViewCommand/prepareTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$16()
org/apache/spark/sql/execution/command/CreateViewCommand/prepareTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/CatalogTable/CatalogTable(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)
org/apache/spark/sql/execution/command/CreateViewCommand/prepareTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/schema()
org/apache/spark/sql/execution/command/CreateViewCommand/prepareTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$5()
org/apache/spark/sql/execution/command/CreateViewCommand/prepareTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$10()
org/apache/spark/sql/execution/command/CreateViewCommand/prepareTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/command/CreateViewCommand/prepareTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$6()
org/apache/spark/sql/execution/command/CreateViewCommand/prepareTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/command/CreateViewCommand/prepareTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$7()
org/apache/spark/sql/execution/command/CreateViewCommand/prepareTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$12()
org/apache/spark/sql/execution/command/CreateViewCommand/prepareTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/command/CreateViewCommand/prepareTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$8()
org/apache/spark/sql/execution/command/CreateViewCommand/prepareTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/command/CreateViewCommand/prepareTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/CatalogTableType$/VIEW()
org/apache/spark/sql/execution/command/CreateViewCommand/prepareTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$17()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/3/anon/1/findNextMatch()#org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec$$anonfun$3$$anon$1/foundMatch()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/3/anon/1/findNextMatch()#scala/collection/Iterator/next()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/3/anon/1/findNextMatch()#org/apache/spark/sql/catalyst/expressions/JoinedRow/apply(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/3/anon/1/findNextMatch()#org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec$$anonfun$3$$anon$1/streamRow()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/3/anon/1/findNextMatch()#org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec$$anonfun$3$$anon$1/resultRow_$eq(org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/3/anon/1/findNextMatch()#org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec$$anonfun$3$$anon$1/streamRow_$eq(org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/3/anon/1/findNextMatch()#org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec$$anonfun$3$$anon$1/nextIndex()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/3/anon/1/findNextMatch()#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/3/anon/1/findNextMatch()#org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec$$anonfun$3/org$apache$spark$sql$execution$joins$BroadcastNestedLoopJoinExec$$anonfun$$$outer()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/3/anon/1/findNextMatch()#org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec$$anonfun$3$$anon$1/foundMatch_$eq(boolean)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/3/anon/1/findNextMatch()#scala/collection/Iterator/hasNext()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/3/anon/1/findNextMatch()#org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec$$anonfun$3$$anon$1/nextIndex_$eq(int)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/3/anon/1/findNextMatch()#scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/3/anon/1/findNextMatch()#org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec$$anonfun$3$$anon$1/resultRow()
org/apache/spark/sql/execution/streaming/FileStreamSource/fetchAllFiles()#scala/collection/Seq/sortBy(scala.Function1,scala.math.Ordering)
org/apache/spark/sql/execution/streaming/FileStreamSource/fetchAllFiles()#java/lang/System/nanoTime()
org/apache/spark/sql/execution/streaming/FileStreamSource/fetchAllFiles()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/FileStreamSource/fetchAllFiles()#org/apache/spark/sql/execution/streaming/FileStreamSource$$anonfun$fetchAllFiles$3/3(org.apache.spark.sql.execution.streaming.FileStreamSource,scala.collection.Seq)
org/apache/spark/sql/execution/streaming/FileStreamSource/fetchAllFiles()#org/apache/spark/sql/execution/streaming/FileStreamSource$$anonfun$fetchAllFiles$2/2(org.apache.spark.sql.execution.streaming.FileStreamSource,scala.collection.Seq,double)
org/apache/spark/sql/execution/streaming/FileStreamSource/fetchAllFiles()#org/apache/spark/sql/execution/streaming/FileStreamSource$$anonfun$fetchAllFiles$1/1(org.apache.spark.sql.execution.streaming.FileStreamSource,scala.collection.Seq,double)
org/apache/spark/sql/execution/streaming/FileStreamSource/fetchAllFiles()#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/FileStreamSource/fetchAllFiles()#scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)
org/apache/spark/sql/execution/streaming/FileStreamSource/fetchAllFiles()#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/streaming/FileStreamSource/fetchAllFiles()#scala/Some/x()
org/apache/spark/sql/execution/streaming/FileStreamSource/fetchAllFiles()#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/streaming/FileStreamSource/fetchAllFiles()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/streaming/FileStreamSource/fetchAllFiles()#scala/collection/Seq/isEmpty()
org/apache/spark/sql/execution/streaming/FileStreamSource/fetchAllFiles()#scala/runtime/BoxesRunTime/boxToBoolean(boolean)
org/apache/spark/sql/execution/streaming/FileStreamSource/fetchAllFiles()#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/streaming/FileStreamSource/fetchAllFiles()#org/apache/spark/sql/execution/streaming/FileStreamSink$/hasMetadata(scala.collection.Seq,org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/streaming/FileStreamSource/fetchAllFiles()#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/streaming/FileStreamSource/fetchAllFiles()#org/apache/spark/sql/execution/streaming/FileStreamSource$$anonfun$13/13(org.apache.spark.sql.execution.streaming.FileStreamSource)
org/apache/spark/sql/execution/streaming/FileStreamSource/fetchAllFiles()#org/apache/spark/sql/execution/streaming/FileStreamSource$$anonfun$12/12(org.apache.spark.sql.execution.streaming.FileStreamSource)
org/apache/spark/sql/execution/streaming/FileStreamSource/fetchMaxOffset()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/streaming/FileStreamSource/fetchMaxOffset()#org/apache/spark/sql/execution/streaming/FileStreamSource$$anonfun$fetchMaxOffset$2/2(org.apache.spark.sql.execution.streaming.FileStreamSource,scala.collection.Seq,scala.collection.Seq,int)
org/apache/spark/sql/execution/streaming/FileStreamSource/fetchMaxOffset()#org/apache/spark/sql/execution/streaming/FileStreamSource$$anonfun$fetchMaxOffset$3/3(org.apache.spark.sql.execution.streaming.FileStreamSource)
org/apache/spark/sql/execution/streaming/FileStreamSource/fetchMaxOffset()#org/apache/spark/sql/execution/streaming/FileStreamSource$SeenFilesMap/purge()
org/apache/spark/sql/execution/streaming/FileStreamSource/fetchMaxOffset()#scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)
org/apache/spark/sql/execution/streaming/FileStreamSource/fetchMaxOffset()#scala/Option/get()
org/apache/spark/sql/execution/streaming/FileStreamSource/fetchMaxOffset()#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/streaming/FileStreamSource/fetchMaxOffset()#org/apache/spark/sql/execution/streaming/FileStreamSource$$anonfun$fetchMaxOffset$4/4(org.apache.spark.sql.execution.streaming.FileStreamSource,scala.collection.Seq)
org/apache/spark/sql/execution/streaming/FileStreamSource/fetchMaxOffset()#org/apache/spark/sql/execution/streaming/FileStreamSource$$anonfun$8/8(org.apache.spark.sql.execution.streaming.FileStreamSource)
org/apache/spark/sql/execution/streaming/FileStreamSource/fetchMaxOffset()#scala/collection/Seq/filter(scala.Function1)
org/apache/spark/sql/execution/streaming/FileStreamSource/fetchMaxOffset()#scala/collection/Seq/foreach(scala.Function1)
org/apache/spark/sql/execution/streaming/FileStreamSource/fetchMaxOffset()#scala/Option/nonEmpty()
org/apache/spark/sql/execution/streaming/FileStreamSource/fetchMaxOffset()#org/apache/spark/sql/execution/streaming/FileStreamSource$$anonfun$fetchMaxOffset$1/1(org.apache.spark.sql.execution.streaming.FileStreamSource)
org/apache/spark/sql/execution/streaming/FileStreamSource/fetchMaxOffset()#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/execution/streaming/FileStreamSource/fetchMaxOffset()#scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/streaming/FileStreamSource/fetchMaxOffset()#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/streaming/FileStreamSource/fetchMaxOffset()#scala/collection/Seq/take(int)
org/apache/spark/sql/execution/streaming/FileStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/streaming/FileStreamSource$$anonfun$9/9(org.apache.spark.sql.execution.streaming.FileStreamSource)
org/apache/spark/sql/execution/streaming/FileStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/streaming/FileStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/Predef$DummyImplicit$/dummyImplicit()
org/apache/spark/sql/execution/streaming/FileStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/Array$/fallbackCanBuildFrom(scala.Predef$DummyImplicit)
org/apache/spark/sql/execution/streaming/FileStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/streaming/FileStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/streaming/FileStreamSource$$anonfun$getBatch$2/2(org.apache.spark.sql.execution.streaming.FileStreamSource,org.apache.spark.sql.execution.streaming.FileStreamSource$FileEntry[])
org/apache/spark/sql/execution/streaming/FileStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/streaming/FileStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/collection/mutable/ArrayOps/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/streaming/FileStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/streaming/FileStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/streaming/FileStreamSource$$anonfun$getBatch$1/1(org.apache.spark.sql.execution.streaming.FileStreamSource,long,long,org.apache.spark.sql.execution.streaming.FileStreamSource$FileEntry[])
org/apache/spark/sql/execution/streaming/FileStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/datasources/LogicalRelation$/apply(org.apache.spark.sql.sources.BaseRelation)
org/apache/spark/sql/execution/streaming/FileStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/streaming/FileStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/streaming/FileStreamSourceOffset$/apply(org.apache.spark.sql.execution.streaming.Offset)
org/apache/spark/sql/execution/streaming/FileStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/datasources/DataSource$/apply$default$6()
org/apache/spark/sql/execution/streaming/FileStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/datasources/DataSource$/apply$default$8()
org/apache/spark/sql/execution/streaming/FileStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/Predef$/assert(boolean)
org/apache/spark/sql/execution/streaming/FileStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/streaming/FileStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/streaming/FileStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/streaming/FileStreamSource$$anonfun$2/2(org.apache.spark.sql.execution.streaming.FileStreamSource)
org/apache/spark/sql/execution/streaming/FileStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/Dataset$/ofRows(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/streaming/FileStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/streaming/FileStreamSource$$anonfun$11/11(org.apache.spark.sql.execution.streaming.FileStreamSource)
org/apache/spark/sql/execution/streaming/FileStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/streaming/FileStreamSource$$anonfun$10/10(org.apache.spark.sql.execution.streaming.FileStreamSource)
org/apache/spark/sql/execution/streaming/FileStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/streaming/FileStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/command/AlterTableAddPartitionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/AlterTableAddPartitionCommand$$anonfun$8/8(org.apache.spark.sql.execution.command.AlterTableAddPartitionCommand,org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable)
org/apache/spark/sql/execution/command/AlterTableAddPartitionCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/command/AlterTableAddPartitionCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/empty()
org/apache/spark/sql/execution/command/AlterTableAddPartitionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/createPartitions(org.apache.spark.sql.catalyst.TableIdentifier,scala.collection.Seq,boolean)
org/apache/spark/sql/execution/command/AlterTableAddPartitionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/DDLUtils$/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)
org/apache/spark/sql/execution/command/AlterTableAddPartitionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/AlterTableAddPartitionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/DDLUtils$/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)
org/apache/spark/sql/execution/command/AlterTableAddPartitionCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/AlterTableAddPartitionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDescribeTable/1/anonfun/6/apply(scala.Tuple2)#scala/Some/x()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDescribeTable/1/anonfun/6/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/parser/ParseException/ParseException(java.lang.String,org.antlr.v4.runtime.ParserRuleContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDescribeTable/1/anonfun/6/apply(scala.Tuple2)#scala/Predef$/ArrowAssoc(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDescribeTable/1/anonfun/6/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDescribeTable/1/anonfun/6/apply(scala.Tuple2)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDescribeTable/1/anonfun/6/apply(scala.Tuple2)#scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDescribeTable/1/anonfun/6/apply(scala.Tuple2)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDescribeTable/1/anonfun/6/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDescribeTable/1/anonfun/6/apply(scala.Tuple2)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDescribeTable/1/anonfun/6/apply(scala.Tuple2)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDescribeTable/1/anonfun/6/apply(scala.Tuple2)#scala/Tuple2/_2()
org/apache/spark/sql/execution/streaming/ProgressReporter/class/reportTimeTaken(org.apache.spark.sql.execution.streaming.ProgressReporter,java.lang.String,scala.Function0)#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/reportTimeTaken(org.apache.spark.sql.execution.streaming.ProgressReporter,java.lang.String,scala.Function0)#scala/math/package$/max(long,long)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/reportTimeTaken(org.apache.spark.sql.execution.streaming.ProgressReporter,java.lang.String,scala.Function0)#org/apache/spark/sql/execution/streaming/ProgressReporter$$anonfun$4/4(org.apache.spark.sql.execution.streaming.ProgressReporter)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/reportTimeTaken(org.apache.spark.sql.execution.streaming.ProgressReporter,java.lang.String,scala.Function0)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/reportTimeTaken(org.apache.spark.sql.execution.streaming.ProgressReporter,java.lang.String,scala.Function0)#org/apache/spark/sql/execution/streaming/ProgressReporter/logDebug(scala.Function0)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/reportTimeTaken(org.apache.spark.sql.execution.streaming.ProgressReporter,java.lang.String,scala.Function0)#org/apache/spark/util/Clock/getTimeMillis()
org/apache/spark/sql/execution/streaming/ProgressReporter/class/reportTimeTaken(org.apache.spark.sql.execution.streaming.ProgressReporter,java.lang.String,scala.Function0)#scala/collection/mutable/HashMap/getOrElse(java.lang.Object,scala.Function0)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/reportTimeTaken(org.apache.spark.sql.execution.streaming.ProgressReporter,java.lang.String,scala.Function0)#scala/collection/mutable/HashMap/put(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/reportTimeTaken(org.apache.spark.sql.execution.streaming.ProgressReporter,java.lang.String,scala.Function0)#org/apache/spark/sql/execution/streaming/ProgressReporter$$anonfun$reportTimeTaken$1/1(org.apache.spark.sql.execution.streaming.ProgressReporter,long,java.lang.String)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/reportTimeTaken(org.apache.spark.sql.execution.streaming.ProgressReporter,java.lang.String,scala.Function0)#scala/Function0/apply()
org/apache/spark/sql/execution/streaming/ProgressReporter/class/extractExecutionStats(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#scala/collection/immutable/Map$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/extractExecutionStats(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#scala/Predef$/Map()
org/apache/spark/sql/execution/streaming/ProgressReporter/class/extractExecutionStats(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#org/apache/spark/sql/execution/streaming/ProgressReporter$$anonfun$12/12(org.apache.spark.sql.execution.streaming.ProgressReporter,scala.collection.immutable.Map)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/extractExecutionStats(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#org/apache/spark/sql/execution/streaming/ProgressReporter$class/extractStateOperatorMetrics(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/extractExecutionStats(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#scala/collection/Seq/zip(scala.collection.GenIterable,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/extractExecutionStats(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#scala/collection/immutable/MapLike/$plus$plus(scala.collection.GenTraversableOnce)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/extractExecutionStats(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#scala/collection/Seq/size()
org/apache/spark/sql/execution/streaming/ProgressReporter/class/extractExecutionStats(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/extractExecutionStats(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#scala/collection/Seq/groupBy(scala.Function1)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/extractExecutionStats(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/streaming/ProgressReporter/class/extractExecutionStats(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/collect(scala.PartialFunction)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/extractExecutionStats(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#scala/collection/immutable/Map/mapValues(scala.Function1)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/extractExecutionStats(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#scala/collection/immutable/Map/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/extractExecutionStats(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#org/apache/spark/sql/execution/streaming/ProgressReporter$$anonfun$7/7(org.apache.spark.sql.execution.streaming.ProgressReporter)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/extractExecutionStats(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#scala/collection/Seq/headOption()
org/apache/spark/sql/execution/streaming/ProgressReporter/class/extractExecutionStats(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#scala/Predef$/ArrowAssoc(java.lang.Object)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/extractExecutionStats(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/ProgressReporter/class/extractExecutionStats(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#org/apache/spark/sql/execution/streaming/ProgressReporter$$anonfun$6/6(org.apache.spark.sql.execution.streaming.ProgressReporter)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/extractExecutionStats(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/collectLeaves()
org/apache/spark/sql/execution/streaming/ProgressReporter/class/extractExecutionStats(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#scala/collection/TraversableLike/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/extractExecutionStats(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#org/apache/spark/sql/execution/streaming/ProgressReporter/logWarning(scala.Function0)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/extractExecutionStats(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/extractExecutionStats(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#scala/collection/immutable/Map$/canBuildFrom()
org/apache/spark/sql/execution/streaming/ProgressReporter/class/extractExecutionStats(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/execution/streaming/ProgressReporter/class/extractExecutionStats(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#org/apache/spark/sql/execution/SparkPlan/collectLeaves()
org/apache/spark/sql/execution/streaming/ProgressReporter/class/extractExecutionStats(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#scala/collection/immutable/Map$/empty()
org/apache/spark/sql/execution/streaming/ProgressReporter/class/extractExecutionStats(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#org/apache/spark/sql/execution/streaming/ProgressReporter$$anonfun$18/18(org.apache.spark.sql.execution.streaming.ProgressReporter)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/extractExecutionStats(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#org/apache/spark/sql/execution/streaming/ProgressReporter$$anonfun$16/16(org.apache.spark.sql.execution.streaming.ProgressReporter)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/extractExecutionStats(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#org/apache/spark/sql/execution/streaming/ProgressReporter$$anonfun$15/15(org.apache.spark.sql.execution.streaming.ProgressReporter)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/extractExecutionStats(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#org/apache/spark/sql/execution/streaming/ProgressReporter$$anonfun$17/17(org.apache.spark.sql.execution.streaming.ProgressReporter,scala.collection.Seq,scala.collection.Seq)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/extractExecutionStats(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#org/apache/spark/sql/execution/streaming/ProgressReporter$$anonfun$13/13(org.apache.spark.sql.execution.streaming.ProgressReporter)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/extractExecutionStats(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#org/apache/spark/sql/execution/streaming/ProgressReporter$$anonfun$11/11(org.apache.spark.sql.execution.streaming.ProgressReporter)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/extractExecutionStats(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#org/apache/spark/sql/execution/streaming/ProgressReporter$class/org$apache$spark$sql$execution$streaming$ProgressReporter$$formatTimestamp(org.apache.spark.sql.execution.streaming.ProgressReporter,long)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/extractExecutionStats(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#org/apache/spark/sql/execution/SparkPlan/collect(scala.PartialFunction)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/extractExecutionStats(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#org/apache/spark/sql/execution/streaming/ProgressReporter$ExecutionStats/ExecutionStats(org.apache.spark.sql.execution.streaming.ProgressReporter,scala.collection.immutable.Map,scala.collection.Seq,scala.collection.immutable.Map)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/extractExecutionStats(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/finishTrigger(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#java/util/HashMap/HashMap(java.util.Map)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/finishTrigger(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#org/apache/spark/sql/execution/streaming/ProgressReporter$class/extractExecutionStats(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/finishTrigger(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#scala/collection/JavaConverters$/mapAsJavaMapConverter(scala.collection.Map)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/finishTrigger(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/finishTrigger(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#scala/collection/mutable/HashMap/toMap(scala.Predef$$less$colon$less)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/finishTrigger(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#org/apache/spark/util/Clock/getTimeMillis()
org/apache/spark/sql/execution/streaming/ProgressReporter/class/finishTrigger(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/finishTrigger(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#org/apache/spark/sql/execution/streaming/ProgressReporter$$anonfun$finishTrigger$1/1(org.apache.spark.sql.execution.streaming.ProgressReporter,org.apache.spark.sql.execution.streaming.ProgressReporter$ExecutionStats)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/finishTrigger(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#scala/collection/convert/Decorators$AsJava/asJava()
org/apache/spark/sql/execution/streaming/ProgressReporter/class/finishTrigger(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/streaming/ProgressReporter/class/finishTrigger(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#java/lang/Object/toString()
org/apache/spark/sql/execution/streaming/ProgressReporter/class/finishTrigger(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#scala/collection/immutable/Map/mapValues(scala.Function1)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/finishTrigger(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#scala/collection/Seq/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/finishTrigger(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#org/apache/spark/sql/execution/streaming/ProgressReporter$$anonfun$9/9(org.apache.spark.sql.execution.streaming.ProgressReporter)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/finishTrigger(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#org/apache/spark/sql/execution/streaming/ProgressReporter$class/updateProgress(org.apache.spark.sql.execution.streaming.ProgressReporter,org.apache.spark.sql.streaming.StreamingQueryProgress)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/finishTrigger(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#org/apache/spark/sql/execution/streaming/ProgressReporter$$anonfun$8/8(org.apache.spark.sql.execution.streaming.ProgressReporter,org.apache.spark.sql.execution.streaming.ProgressReporter$ExecutionStats,double,double)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/finishTrigger(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#org/apache/spark/sql/execution/streaming/ProgressReporter$ExecutionStats/eventTimeStats()
org/apache/spark/sql/execution/streaming/ProgressReporter/class/finishTrigger(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#scala/Predef$/$conforms()
org/apache/spark/sql/execution/streaming/ProgressReporter/class/finishTrigger(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#org/apache/spark/sql/execution/streaming/ProgressReporter/logDebug(scala.Function0)
org/apache/spark/sql/execution/streaming/ProgressReporter/class/finishTrigger(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#org/apache/spark/sql/execution/streaming/ProgressReporter$ExecutionStats/stateOperators()
org/apache/spark/sql/execution/streaming/ProgressReporter/class/finishTrigger(org.apache.spark.sql.execution.streaming.ProgressReporter,boolean)#org/apache/spark/sql/execution/streaming/ProgressReporter$class/org$apache$spark$sql$execution$streaming$ProgressReporter$$formatTimestamp(org.apache.spark.sql.execution.streaming.ProgressReporter,long)
org/apache/spark/sql/execution/SparkPlan/executeTake(int)#java/lang/Math/min(long,long)
org/apache/spark/sql/execution/SparkPlan/executeTake(int)#org/apache/spark/rdd/RDD/partitions()
org/apache/spark/sql/execution/SparkPlan/executeTake(int)#scala/Predef$DummyImplicit$/dummyImplicit()
org/apache/spark/sql/execution/SparkPlan/executeTake(int)#scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)
org/apache/spark/sql/execution/SparkPlan/executeTake(int)#scala/collection/mutable/ArrayBuffer/take(int)
org/apache/spark/sql/execution/SparkPlan/executeTake(int)#scala/collection/mutable/ArrayBuffer/ArrayBuffer()
org/apache/spark/sql/execution/SparkPlan/executeTake(int)#scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/SparkPlan/executeTake(int)#org/apache/spark/sql/internal/SQLConf/limitScaleUpFactor()
org/apache/spark/sql/execution/SparkPlan/executeTake(int)#org/apache/spark/sql/execution/SparkPlan$$anonfun$3/3(org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/SparkPlan/executeTake(int)#scala/math/package$/min(long,long)
org/apache/spark/sql/execution/SparkPlan/executeTake(int)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/SparkPlan/executeTake(int)#scala/collection/mutable/ArrayOps/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/SparkPlan/executeTake(int)#scala/runtime/RichInt$/until$extension0(int,int)
org/apache/spark/sql/execution/SparkPlan/executeTake(int)#scala/Predef$/intWrapper(int)
org/apache/spark/sql/execution/SparkPlan/executeTake(int)#scala/collection/mutable/ArrayBuffer/size()
org/apache/spark/sql/execution/SparkPlan/executeTake(int)#scala/collection/immutable/Range/size()
org/apache/spark/sql/execution/SparkPlan/executeTake(int)#java/lang/Math/max(int,int)
org/apache/spark/sql/execution/SparkPlan/executeTake(int)#org/apache/spark/SparkContext/runJob(org.apache.spark.rdd.RDD,scala.Function1,scala.collection.Seq,scala.reflect.ClassTag)
org/apache/spark/sql/execution/SparkPlan/executeTake(int)#scala/Array$/fallbackCanBuildFrom(scala.Predef$DummyImplicit)
org/apache/spark/sql/execution/SparkPlan/executeTake(int)#org/apache/spark/sql/execution/SparkPlan$$anonfun$executeTake$1/1(org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/SparkPlan/executeTake(int)#scala/collection/mutable/ArrayBuffer/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/SparkPlan/executeTake(int)#scala/collection/mutable/ArrayBuffer/isEmpty()
org/apache/spark/sql/execution/SparkPlan/executeTake(int)#scala/collection/mutable/ArrayBuffer/$plus$plus$eq(scala.collection.TraversableOnce)
org/apache/spark/sql/execution/SparkPlan/executeTake(int)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/SparkPlan/org$apache$spark$sql$execution$SparkPlan$$decodeUnsafeRows(byte[])#org/apache/spark/io/CompressionCodec$/createCodec(org.apache.spark.SparkConf)
org/apache/spark/sql/execution/SparkPlan/org$apache$spark$sql$execution$SparkPlan$$decodeUnsafeRows(byte[])#org/apache/spark/sql/types/StructType/length()
org/apache/spark/sql/execution/SparkPlan/org$apache$spark$sql$execution$SparkPlan$$decodeUnsafeRows(byte[])#org/apache/spark/sql/execution/SparkPlan/schema()
org/apache/spark/sql/execution/SparkPlan/org$apache$spark$sql$execution$SparkPlan$$decodeUnsafeRows(byte[])#java/io/ByteArrayInputStream/ByteArrayInputStream(byte[])
org/apache/spark/sql/execution/SparkPlan/org$apache$spark$sql$execution$SparkPlan$$decodeUnsafeRows(byte[])#org/apache/spark/SparkEnv$/get()
org/apache/spark/sql/execution/SparkPlan/org$apache$spark$sql$execution$SparkPlan$$decodeUnsafeRows(byte[])#java/io/DataInputStream/DataInputStream(java.io.InputStream)
org/apache/spark/sql/execution/SparkPlan/org$apache$spark$sql$execution$SparkPlan$$decodeUnsafeRows(byte[])#org/apache/spark/io/CompressionCodec/compressedInputStream(java.io.InputStream)
org/apache/spark/sql/execution/SparkPlan/org$apache$spark$sql$execution$SparkPlan$$decodeUnsafeRows(byte[])#org/apache/spark/SparkEnv/conf()
org/apache/spark/sql/execution/SparkPlan/org$apache$spark$sql$execution$SparkPlan$$decodeUnsafeRows(byte[])#org/apache/spark/sql/execution/SparkPlan$$anon$1/1(org.apache.spark.sql.execution.SparkPlan,int,java.io.DataInputStream)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Some/x()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/internal/SQLConf/resolver()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/datasources/PreprocessTableCreation$$anonfun$apply$2$$anonfun$applyOrElse$2/2(org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/TableIdentifier/copy(java.lang.String,scala.Option)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$1()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$2()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/types/StructType/length()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/tableExists(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/types/StructType/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/CatalogTable/provider()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Predef$/assert(boolean,scala.Function0)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#java/lang/Class/getSimpleName()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/datasources/PreprocessTableCreation$$anonfun$apply$2$$anonfun$9/9(org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/types/StructType/catalogString()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/datasources/PreprocessTableCreation$$anonfun$apply$2$$anonfun$8/8(org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/datasources/DataSource$/lookupDataSource(java.lang.String)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/resolved()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Predef$/assert(boolean)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/plans/logical/Project/Project(scala.collection.Seq,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Option/isDefined()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/CatalogTable/tableType()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/datasources/PreprocessTableCreation$$anonfun$apply$2$$anonfun$4/4(org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/datasources/PreprocessTableCreation$$anonfun$apply$2$$anonfun$applyOrElse$1/1(org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/output()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Option/forall(scala.Function1)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/schema()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/datasources/PreprocessTableCreation$$anonfun$apply$2$$anonfun$3/3(org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/TableIdentifier/unquotedString()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$3()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$10()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/CatalogTable/schema()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$11()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$5()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$12()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$6()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/datasources/PreprocessTableCreation$$anonfun$apply$2$$anonfun$14/14(org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2,org.apache.spark.sql.catalyst.catalog.CatalogTable)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$13()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/datasources/PreprocessTableCreation$$anonfun$apply$2$$anonfun$12/12(org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2,scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/datasources/PreprocessTableCreation$$anonfun$apply$2$$anonfun$15/15(org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2,scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/datasources/PreprocessTableCreation$$anonfun$apply$2$$anonfun$5/5(org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function2)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$7()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/datasources/PreprocessTableCreation$$anonfun$apply$2$$anonfun$13/13(org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2,scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/CatalogTable/bucketSpec()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/Seq/filterNot(scala.Function1)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/types/StructType$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$15()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/datasources/PreprocessTableCreation$$anonfun$apply$2$$anonfun$11/11(org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/datasources/PreprocessTableCreation$$anonfun$apply$2$$anonfun$10/10(org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$8()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$16()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/TableIdentifier/database()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$9()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$17()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/CatalogTableType$/VIEW()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/datasources/PreprocessTableCreation$$anonfun$apply$2$$anonfun$7/7(org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2,java.lang.String,scala.Function2,scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/Seq/mkString(java.lang.String)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/types/StructType/isEmpty()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/CatalogUtils$/normalizePartCols(java.lang.String,scala.collection.Seq,scala.collection.Seq,scala.Function2)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/TableIdentifier/copy$default$1()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Option/get()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/types/StructType/filterNot(scala.Function1)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/isDefinedAt(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Some/x()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/isDefinedAt(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/tableExists(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/isDefinedAt(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Option/forall(scala.Function1)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/isDefinedAt(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/CatalogTable/schema()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/isDefinedAt(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/isDefinedAt(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/types/StructType/isEmpty()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/isDefinedAt(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/isDefinedAt(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/datasources/PreprocessTableCreation$$anonfun$apply$2$$anonfun$isDefinedAt$1/1(org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/isDefinedAt(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/resolved()
org/apache/spark/sql/execution/command/DDLUtils/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/command/DDLUtils/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/command/DDLUtils/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/DDLUtils/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/command/DDLUtils/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/command/DDLUtils/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/command/DDLUtils/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/command/DDLUtils/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/DDLUtils/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/DDLUtils/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)#org/apache/spark/sql/catalyst/catalog/CatalogTable/tableType()
org/apache/spark/sql/execution/command/DDLUtils/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/isTemporaryTable(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/DDLUtils/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)#org/apache/spark/sql/execution/command/DDLUtils$/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)
org/apache/spark/sql/execution/command/DDLUtils/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)#org/apache/spark/sql/catalyst/catalog/CatalogTableType$/VIEW()
org/apache/spark/sql/execution/command/DDLUtils/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)#org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()
org/apache/spark/sql/execution/command/DDLUtils/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)#org/apache/spark/sql/catalyst/catalog/CatalogTable/tracksPartitionsInCatalog()
org/apache/spark/sql/execution/command/DDLUtils/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/command/DDLUtils/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/DDLUtils/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)#org/apache/spark/sql/execution/command/DDLUtils$/isDatasourceTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)
org/apache/spark/sql/execution/command/DDLUtils/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/DDLUtils/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/command/DDLUtils/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/command/DDLUtils/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/command/DDLUtils/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)#org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()
org/apache/spark/sql/execution/command/DDLUtils/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)#org/apache/spark/sql/internal/SQLConf/manageFilesourcePartitions()
org/apache/spark/sql/execution/command/DDLUtils/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)#org/apache/spark/sql/catalyst/TableIdentifier/table()
org/apache/spark/sql/execution/command/DDLUtils/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/command/DDLUtils/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/command/DDLUtils/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/DDLUtils/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/command/DDLUtils/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)#org/apache/spark/sql/execution/command/DDLUtils$/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)
org/apache/spark/sql/execution/command/DDLUtils/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/DDLUtils/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTempViewUsing/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$QualifiedNameContext/getText()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTempViewUsing/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTempViewUsingContext/GLOBAL()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTempViewUsing/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTempViewUsing$1/apply()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTempViewUsing/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTempViewUsingContext/REPLACE()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTempViewUsing/1/apply()#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTempViewUsing/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTempViewUsingContext/tableProvider()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTempViewUsing/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTempViewUsingContext/tablePropertyList()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTempViewUsing/1/apply()#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTempViewUsing/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder/visitTableIdentifier(org.apache.spark.sql.catalyst.parser.SqlBaseParser$TableIdentifierContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTempViewUsing/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$TableProviderContext/qualifiedName()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTempViewUsing/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTempViewUsingContext/tableIdentifier()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTempViewUsing/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTempViewUsing$1$$anonfun$apply$14/14(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTempViewUsing$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTempViewUsing/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTempViewUsing$1$$anonfun$apply$13/13(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTempViewUsing$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTempViewUsing/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTempViewUsingContext/colTypeList()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTempViewUsing/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTempViewUsing$1$$anonfun$apply$12/12(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTempViewUsing$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTempViewUsing/1/apply()#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/columnPartition(org.apache.spark.sql.execution.datasources.jdbc.JDBCPartitioningInfo)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/columnPartition(org.apache.spark.sql.execution.datasources.jdbc.JDBCPartitioningInfo)#scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/columnPartition(org.apache.spark.sql.execution.datasources.jdbc.JDBCPartitioningInfo)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/columnPartition(org.apache.spark.sql.execution.datasources.jdbc.JDBCPartitioningInfo)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/columnPartition(org.apache.spark.sql.execution.datasources.jdbc.JDBCPartitioningInfo)#scala/collection/mutable/ArrayBuffer/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/columnPartition(org.apache.spark.sql.execution.datasources.jdbc.JDBCPartitioningInfo)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation$$anonfun$columnPartition$1/1(long,long)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/columnPartition(org.apache.spark.sql.execution.datasources.jdbc.JDBCPartitioningInfo)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/columnPartition(org.apache.spark.sql.execution.datasources.jdbc.JDBCPartitioningInfo)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation$/columnPartition(org.apache.spark.sql.execution.datasources.jdbc.JDBCPartitioningInfo)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/columnPartition(org.apache.spark.sql.execution.datasources.jdbc.JDBCPartitioningInfo)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation$$anonfun$1/1(org.apache.spark.sql.execution.datasources.jdbc.JDBCPartitioningInfo,long,long)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/columnPartition(org.apache.spark.sql.execution.datasources.jdbc.JDBCPartitioningInfo)#scala/Predef$/require(boolean,scala.Function0)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/columnPartition(org.apache.spark.sql.execution.datasources.jdbc.JDBCPartitioningInfo)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation$/logWarning(scala.Function0)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/columnPartition(org.apache.spark.sql.execution.datasources.jdbc.JDBCPartitioningInfo)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/columnPartition(org.apache.spark.sql.execution.datasources.jdbc.JDBCPartitioningInfo)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/columnPartition(org.apache.spark.sql.execution.datasources.jdbc.JDBCPartitioningInfo)#scala/collection/mutable/ArrayBuffer/ArrayBuffer()
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/collectStatisticalData/3/anonfun/apply/5/apply()#org/apache/spark/sql/execution/stat/StatFunctions$$anonfun$collectStatisticalData$3$$anonfun$apply$5/apply()
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/collectStatisticalData/3/anonfun/apply/5/apply()#org/apache/spark/sql/types/StructField/dataType()
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/collectStatisticalData/3/anonfun/apply/5/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/collectStatisticalData/3/anonfun/apply/5/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/collectStatisticalData/3/anonfun/apply/5/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/collectStatisticalData/3/anonfun/apply/5/apply()#scala/Option/get()
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/collectStatisticalData/3/anonfun/apply/5/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/collectStatisticalData/3/anonfun/apply/5/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/collectStatisticalData/3/anonfun/apply/5/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/collectStatisticalData/3/anonfun/apply/5/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/constructNextBatch/2/anonfun/apply/mcV/sp/3/apply()#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch$2$$anonfun$apply$mcV$sp$3/apply()
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/constructNextBatch/2/anonfun/apply/mcV/sp/3/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/constructNextBatch/2/anonfun/apply/mcV/sp/3/apply()#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch$2/org$apache$spark$sql$execution$streaming$StreamExecution$$anonfun$$$outer()
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/constructNextBatch/2/anonfun/apply/mcV/sp/3/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/constructNextBatch/2/anonfun/apply/mcV/sp/3/apply()#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/constructNextBatch/2/anonfun/apply/mcV/sp/3/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/constructNextBatch/2/anonfun/apply/mcV/sp/3/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/constructNextBatch/2/anonfun/apply/mcV/sp/3/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/constructNextBatch/2/anonfun/apply/mcV/sp/3/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/constructNextBatch/2/anonfun/apply/mcV/sp/3/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/FileScanRDD/anon/1/nextIterator()#org/apache/spark/rdd/InputFileBlockHolder$/unset()
org/apache/spark/sql/execution/datasources/FileScanRDD/anon/1/nextIterator()#org/apache/spark/sql/execution/datasources/FileScanRDD/logInfo(scala.Function0)
org/apache/spark/sql/execution/datasources/FileScanRDD/anon/1/nextIterator()#scala/collection/Iterator/next()
org/apache/spark/sql/execution/datasources/FileScanRDD/anon/1/nextIterator()#org/apache/spark/sql/execution/datasources/FileScanRDD$$anon$1/hasNext()
org/apache/spark/sql/execution/datasources/FileScanRDD/anon/1/nextIterator()#org/apache/spark/sql/execution/datasources/FileScanRDD$$anon$1/org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile()
org/apache/spark/sql/execution/datasources/FileScanRDD/anon/1/nextIterator()#scala/collection/Iterator/hasNext()
org/apache/spark/sql/execution/datasources/FileScanRDD/anon/1/nextIterator()#org/apache/spark/rdd/InputFileBlockHolder$/set(java.lang.String,long,long)
org/apache/spark/sql/execution/datasources/FileScanRDD/anon/1/nextIterator()#org/apache/spark/sql/execution/datasources/FileScanRDD$$anon$1/updateBytesReadWithFileSize()
org/apache/spark/sql/execution/datasources/FileScanRDD/anon/1/nextIterator()#org/apache/spark/sql/execution/datasources/FileScanRDD$$anon$1$$anon$2/2(org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1)
org/apache/spark/sql/execution/datasources/FileScanRDD/anon/1/nextIterator()#org/apache/spark/sql/execution/datasources/FileScanRDD$$anon$1$$anonfun$nextIterator$1/1(org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1)
org/apache/spark/sql/execution/FileSourceScanExec/createBucketedReadRDD(org.apache.spark.sql.catalyst.catalog.BucketSpec,scala.Function1,scala.collection.Seq,org.apache.spark.sql.execution.datasources.HadoopFsRelation)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/FileSourceScanExec/createBucketedReadRDD(org.apache.spark.sql.catalyst.catalog.BucketSpec,scala.Function1,scala.collection.Seq,org.apache.spark.sql.execution.datasources.HadoopFsRelation)#org/apache/spark/sql/catalyst/catalog/BucketSpec/numBuckets()
org/apache/spark/sql/execution/FileSourceScanExec/createBucketedReadRDD(org.apache.spark.sql.catalyst.catalog.BucketSpec,scala.Function1,scala.collection.Seq,org.apache.spark.sql.execution.datasources.HadoopFsRelation)#org/apache/spark/sql/execution/FileSourceScanExec$$anonfun$26/26(org.apache.spark.sql.execution.FileSourceScanExec,scala.collection.immutable.Map)
org/apache/spark/sql/execution/FileSourceScanExec/createBucketedReadRDD(org.apache.spark.sql.catalyst.catalog.BucketSpec,scala.Function1,scala.collection.Seq,org.apache.spark.sql.execution.datasources.HadoopFsRelation)#scala/collection/Seq$/tabulate(int,scala.Function1)
org/apache/spark/sql/execution/FileSourceScanExec/createBucketedReadRDD(org.apache.spark.sql.catalyst.catalog.BucketSpec,scala.Function1,scala.collection.Seq,org.apache.spark.sql.execution.datasources.HadoopFsRelation)#org/apache/spark/sql/execution/FileSourceScanExec/logInfo(scala.Function0)
org/apache/spark/sql/execution/FileSourceScanExec/createBucketedReadRDD(org.apache.spark.sql.catalyst.catalog.BucketSpec,scala.Function1,scala.collection.Seq,org.apache.spark.sql.execution.datasources.HadoopFsRelation)#scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/FileSourceScanExec/createBucketedReadRDD(org.apache.spark.sql.catalyst.catalog.BucketSpec,scala.Function1,scala.collection.Seq,org.apache.spark.sql.execution.datasources.HadoopFsRelation)#org/apache/spark/sql/execution/FileSourceScanExec$$anonfun$createBucketedReadRDD$1/1(org.apache.spark.sql.execution.FileSourceScanExec,org.apache.spark.sql.catalyst.catalog.BucketSpec)
org/apache/spark/sql/execution/FileSourceScanExec/createBucketedReadRDD(org.apache.spark.sql.catalyst.catalog.BucketSpec,scala.Function1,scala.collection.Seq,org.apache.spark.sql.execution.datasources.HadoopFsRelation)#org/apache/spark/sql/execution/FileSourceScanExec$$anonfun$25/25(org.apache.spark.sql.execution.FileSourceScanExec)
org/apache/spark/sql/execution/FileSourceScanExec/createBucketedReadRDD(org.apache.spark.sql.catalyst.catalog.BucketSpec,scala.Function1,scala.collection.Seq,org.apache.spark.sql.execution.datasources.HadoopFsRelation)#org/apache/spark/sql/execution/FileSourceScanExec$$anonfun$24/24(org.apache.spark.sql.execution.FileSourceScanExec)
org/apache/spark/sql/execution/FileSourceScanExec/createBucketedReadRDD(org.apache.spark.sql.catalyst.catalog.BucketSpec,scala.Function1,scala.collection.Seq,org.apache.spark.sql.execution.datasources.HadoopFsRelation)#scala/collection/TraversableLike/groupBy(scala.Function1)
org/apache/spark/sql/execution/FileSourceScanExec/org$apache$spark$sql$execution$FileSourceScanExec$$selectedPartitions$lzycompute()#java/lang/System/nanoTime()
org/apache/spark/sql/execution/FileSourceScanExec/org$apache$spark$sql$execution$FileSourceScanExec$$selectedPartitions$lzycompute()#org/apache/spark/sql/execution/metric/SQLMetrics$/postDriverMetricUpdates(org.apache.spark.SparkContext,java.lang.String,scala.collection.Seq)
org/apache/spark/sql/execution/FileSourceScanExec/org$apache$spark$sql$execution$FileSourceScanExec$$selectedPartitions$lzycompute()#org/apache/spark/sql/execution/FileSourceScanExec$$anonfun$org$apache$spark$sql$execution$FileSourceScanExec$$selectedPartitions$1/1(org.apache.spark.sql.execution.FileSourceScanExec)
org/apache/spark/sql/execution/FileSourceScanExec/org$apache$spark$sql$execution$FileSourceScanExec$$selectedPartitions$lzycompute()#org/apache/spark/sql/execution/SQLExecution$/EXECUTION_ID_KEY()
org/apache/spark/sql/execution/FileSourceScanExec/org$apache$spark$sql$execution$FileSourceScanExec$$selectedPartitions$lzycompute()#org/apache/spark/sql/execution/FileSourceScanExec/sparkContext()
org/apache/spark/sql/execution/FileSourceScanExec/org$apache$spark$sql$execution$FileSourceScanExec$$selectedPartitions$lzycompute()#scala/collection/immutable/List/$colon$colon(java.lang.Object)
org/apache/spark/sql/execution/FileSourceScanExec/org$apache$spark$sql$execution$FileSourceScanExec$$selectedPartitions$lzycompute()#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/FileSourceScanExec/org$apache$spark$sql$execution$FileSourceScanExec$$selectedPartitions$lzycompute()#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/FileSourceScanExec/org$apache$spark$sql$execution$FileSourceScanExec$$selectedPartitions$lzycompute()#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/FileSourceScanExec/org$apache$spark$sql$execution$FileSourceScanExec$$selectedPartitions$lzycompute()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/FileSourceScanExec/org$apache$spark$sql$execution$FileSourceScanExec$$selectedPartitions$lzycompute()#scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)
org/apache/spark/sql/execution/FileSourceScanExec/org$apache$spark$sql$execution$FileSourceScanExec$$selectedPartitions$lzycompute()#scala/collection/TraversableOnce/sum(scala.math.Numeric)
org/apache/spark/sql/execution/FileSourceScanExec/org$apache$spark$sql$execution$FileSourceScanExec$$selectedPartitions$lzycompute()#scala/collection/immutable/Map/apply(java.lang.Object)
org/apache/spark/sql/execution/FileSourceScanExec/org$apache$spark$sql$execution$FileSourceScanExec$$selectedPartitions$lzycompute()#org/apache/spark/SparkContext/getLocalProperty(java.lang.String)
org/apache/spark/sql/execution/FileSourceScanExec/org$apache$spark$sql$execution$FileSourceScanExec$$selectedPartitions$lzycompute()#org/apache/spark/sql/execution/FileSourceScanExec$$anonfun$1/1(org.apache.spark.sql.execution.FileSourceScanExec)
org/apache/spark/sql/execution/FileSourceScanExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/Seq/zipWithIndex(scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/FileSourceScanExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/FileSourceScanExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/FileSourceScanExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/FileSourceScanExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/FileSourceScanExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/FileSourceScanExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/FileSourceScanExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/FileSourceScanExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/FileSourceScanExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/currentVars_$eq(scala.collection.Seq)
org/apache/spark/sql/execution/FileSourceScanExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#java/lang/String/trim()
org/apache/spark/sql/execution/FileSourceScanExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addMutableState(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/FileSourceScanExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/FileSourceScanExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/execution/ColumnarBatchScan$class/doProduce(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)
org/apache/spark/sql/execution/FileSourceScanExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/INPUT_ROW_$eq(java.lang.String)
org/apache/spark/sql/execution/FileSourceScanExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/FileSourceScanExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/freshName(java.lang.String)
org/apache/spark/sql/execution/FileSourceScanExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/execution/FileSourceScanExec$$anonfun$22/22(org.apache.spark.sql.execution.FileSourceScanExec)
org/apache/spark/sql/execution/FileSourceScanExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/execution/FileSourceScanExec$$anonfun$23/23(org.apache.spark.sql.execution.FileSourceScanExec,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)
org/apache/spark/sql/execution/FileSourceScanExec/createNonBucketedReadRDD(scala.Function1,scala.collection.Seq,org.apache.spark.sql.execution.datasources.HadoopFsRelation)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/FileSourceScanExec/createNonBucketedReadRDD(scala.Function1,scala.collection.Seq,org.apache.spark.sql.execution.datasources.HadoopFsRelation)#org/apache/spark/sql/execution/FileSourceScanExec$$anonfun$createNonBucketedReadRDD$1/1(org.apache.spark.sql.execution.FileSourceScanExec,long,long)
org/apache/spark/sql/execution/FileSourceScanExec/createNonBucketedReadRDD(scala.Function1,scala.collection.Seq,org.apache.spark.sql.execution.datasources.HadoopFsRelation)#org/apache/spark/SparkContext/defaultParallelism()
org/apache/spark/sql/execution/FileSourceScanExec/createNonBucketedReadRDD(scala.Function1,scala.collection.Seq,org.apache.spark.sql.execution.datasources.HadoopFsRelation)#scala/collection/TraversableOnce/sum(scala.math.Numeric)
org/apache/spark/sql/execution/FileSourceScanExec/createNonBucketedReadRDD(scala.Function1,scala.collection.Seq,org.apache.spark.sql.execution.datasources.HadoopFsRelation)#org/apache/spark/sql/internal/SQLConf/filesMaxPartitionBytes()
org/apache/spark/sql/execution/FileSourceScanExec/createNonBucketedReadRDD(scala.Function1,scala.collection.Seq,org.apache.spark.sql.execution.datasources.HadoopFsRelation)#scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/FileSourceScanExec/createNonBucketedReadRDD(scala.Function1,scala.collection.Seq,org.apache.spark.sql.execution.datasources.HadoopFsRelation)#scala/collection/mutable/ArrayOps/sortBy(scala.Function1,scala.math.Ordering)
org/apache/spark/sql/execution/FileSourceScanExec/createNonBucketedReadRDD(scala.Function1,scala.collection.Seq,org.apache.spark.sql.execution.datasources.HadoopFsRelation)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/FileSourceScanExec/createNonBucketedReadRDD(scala.Function1,scala.collection.Seq,org.apache.spark.sql.execution.datasources.HadoopFsRelation)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/FileSourceScanExec/createNonBucketedReadRDD(scala.Function1,scala.collection.Seq,org.apache.spark.sql.execution.datasources.HadoopFsRelation)#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/FileSourceScanExec/createNonBucketedReadRDD(scala.Function1,scala.collection.Seq,org.apache.spark.sql.execution.datasources.HadoopFsRelation)#scala/math/Ordering/reverse()
org/apache/spark/sql/execution/FileSourceScanExec/createNonBucketedReadRDD(scala.Function1,scala.collection.Seq,org.apache.spark.sql.execution.datasources.HadoopFsRelation)#java/lang/Math/min(long,long)
org/apache/spark/sql/execution/FileSourceScanExec/createNonBucketedReadRDD(scala.Function1,scala.collection.Seq,org.apache.spark.sql.execution.datasources.HadoopFsRelation)#scala/Predef$/implicitly(java.lang.Object)
org/apache/spark/sql/execution/FileSourceScanExec/createNonBucketedReadRDD(scala.Function1,scala.collection.Seq,org.apache.spark.sql.execution.datasources.HadoopFsRelation)#java/lang/Math/max(long,long)
org/apache/spark/sql/execution/FileSourceScanExec/createNonBucketedReadRDD(scala.Function1,scala.collection.Seq,org.apache.spark.sql.execution.datasources.HadoopFsRelation)#org/apache/spark/sql/execution/FileSourceScanExec/logInfo(scala.Function0)
org/apache/spark/sql/execution/FileSourceScanExec/createNonBucketedReadRDD(scala.Function1,scala.collection.Seq,org.apache.spark.sql.execution.datasources.HadoopFsRelation)#scala/runtime/LongRef/create(long)
org/apache/spark/sql/execution/FileSourceScanExec/createNonBucketedReadRDD(scala.Function1,scala.collection.Seq,org.apache.spark.sql.execution.datasources.HadoopFsRelation)#scala/collection/mutable/ArrayBuffer/ArrayBuffer()
org/apache/spark/sql/execution/FileSourceScanExec/createNonBucketedReadRDD(scala.Function1,scala.collection.Seq,org.apache.spark.sql.execution.datasources.HadoopFsRelation)#org/apache/spark/sql/execution/FileSourceScanExec$$anonfun$28/28(org.apache.spark.sql.execution.FileSourceScanExec,org.apache.spark.sql.execution.datasources.HadoopFsRelation,long)
org/apache/spark/sql/execution/FileSourceScanExec/createNonBucketedReadRDD(scala.Function1,scala.collection.Seq,org.apache.spark.sql.execution.datasources.HadoopFsRelation)#scala/collection/mutable/ArrayOps/foreach(scala.Function1)
org/apache/spark/sql/execution/FileSourceScanExec/createNonBucketedReadRDD(scala.Function1,scala.collection.Seq,org.apache.spark.sql.execution.datasources.HadoopFsRelation)#scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/FileSourceScanExec/createNonBucketedReadRDD(scala.Function1,scala.collection.Seq,org.apache.spark.sql.execution.datasources.HadoopFsRelation)#org/apache/spark/sql/execution/FileSourceScanExec$$anonfun$27/27(org.apache.spark.sql.execution.FileSourceScanExec,long)
org/apache/spark/sql/execution/FileSourceScanExec/createNonBucketedReadRDD(scala.Function1,scala.collection.Seq,org.apache.spark.sql.execution.datasources.HadoopFsRelation)#org/apache/spark/sql/internal/SQLConf/filesOpenCostInBytes()
org/apache/spark/sql/execution/FileSourceScanExec/createNonBucketedReadRDD(scala.Function1,scala.collection.Seq,org.apache.spark.sql.execution.datasources.HadoopFsRelation)#org/apache/spark/sql/execution/FileSourceScanExec$$anonfun$29/29(org.apache.spark.sql.execution.FileSourceScanExec)
org/apache/spark/sql/execution/FileSourceScanExec/createNonBucketedReadRDD(scala.Function1,scala.collection.Seq,org.apache.spark.sql.execution.datasources.HadoopFsRelation)#org/apache/spark/sql/execution/FileSourceScanExec$$anonfun$createNonBucketedReadRDD$2/2(org.apache.spark.sql.execution.FileSourceScanExec,long,long,scala.collection.mutable.ArrayBuffer,scala.collection.mutable.ArrayBuffer,scala.runtime.LongRef)
org/apache/spark/sql/execution/FileSourceScanExec/org$apache$spark$sql$execution$FileSourceScanExec$$getBlockHosts(org.apache.hadoop.fs.BlockLocation[],long,long)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/FileSourceScanExec/org$apache$spark$sql$execution$FileSourceScanExec$$getBlockHosts(org.apache.hadoop.fs.BlockLocation[],long,long)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/FileSourceScanExec/org$apache$spark$sql$execution$FileSourceScanExec$$getBlockHosts(org.apache.hadoop.fs.BlockLocation[],long,long)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/FileSourceScanExec/org$apache$spark$sql$execution$FileSourceScanExec$$getBlockHosts(org.apache.hadoop.fs.BlockLocation[],long,long)#scala/Tuple2/_1()
org/apache/spark/sql/execution/FileSourceScanExec/org$apache$spark$sql$execution$FileSourceScanExec$$getBlockHosts(org.apache.hadoop.fs.BlockLocation[],long,long)#scala/Array$/empty(scala.reflect.ClassTag)
org/apache/spark/sql/execution/FileSourceScanExec/org$apache$spark$sql$execution$FileSourceScanExec$$getBlockHosts(org.apache.hadoop.fs.BlockLocation[],long,long)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/FileSourceScanExec/org$apache$spark$sql$execution$FileSourceScanExec$$getBlockHosts(org.apache.hadoop.fs.BlockLocation[],long,long)#scala/collection/mutable/ArrayOps/isEmpty()
org/apache/spark/sql/execution/FileSourceScanExec/org$apache$spark$sql$execution$FileSourceScanExec$$getBlockHosts(org.apache.hadoop.fs.BlockLocation[],long,long)#scala/collection/mutable/ArrayOps/filter(scala.Function1)
org/apache/spark/sql/execution/FileSourceScanExec/org$apache$spark$sql$execution$FileSourceScanExec$$getBlockHosts(org.apache.hadoop.fs.BlockLocation[],long,long)#org/apache/spark/sql/execution/FileSourceScanExec$$anonfun$30/30(org.apache.spark.sql.execution.FileSourceScanExec,long,long)
org/apache/spark/sql/execution/FileSourceScanExec/org$apache$spark$sql$execution$FileSourceScanExec$$getBlockHosts(org.apache.hadoop.fs.BlockLocation[],long,long)#org/apache/spark/sql/execution/FileSourceScanExec$$anonfun$32/32(org.apache.spark.sql.execution.FileSourceScanExec)
org/apache/spark/sql/execution/FileSourceScanExec/org$apache$spark$sql$execution$FileSourceScanExec$$getBlockHosts(org.apache.hadoop.fs.BlockLocation[],long,long)#org/apache/spark/sql/execution/FileSourceScanExec$$anonfun$31/31(org.apache.spark.sql.execution.FileSourceScanExec)
org/apache/spark/sql/execution/FileSourceScanExec/org$apache$spark$sql$execution$FileSourceScanExec$$getBlockHosts(org.apache.hadoop.fs.BlockLocation[],long,long)#scala/collection/mutable/ArrayOps/maxBy(scala.Function1,scala.math.Ordering)
org/apache/spark/sql/execution/FileSourceScanExec/org$apache$spark$sql$execution$FileSourceScanExec$$getBlockHosts(org.apache.hadoop.fs.BlockLocation[],long,long)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/columnar/NullableColumnBuilder/class/build(org.apache.spark.sql.execution.columnar.NullableColumnBuilder)#java/nio/ByteOrder/nativeOrder()
org/apache/spark/sql/execution/columnar/NullableColumnBuilder/class/build(org.apache.spark.sql.execution.columnar.NullableColumnBuilder)#java/nio/ByteBuffer/allocate(int)
org/apache/spark/sql/execution/columnar/NullableColumnBuilder/class/build(org.apache.spark.sql.execution.columnar.NullableColumnBuilder)#java/nio/ByteBuffer/position()
org/apache/spark/sql/execution/columnar/NullableColumnBuilder/class/build(org.apache.spark.sql.execution.columnar.NullableColumnBuilder)#java/nio/ByteBuffer/order(java.nio.ByteOrder)
org/apache/spark/sql/execution/columnar/NullableColumnBuilder/class/build(org.apache.spark.sql.execution.columnar.NullableColumnBuilder)#java/nio/ByteBuffer/remaining()
org/apache/spark/sql/execution/columnar/NullableColumnBuilder/class/build(org.apache.spark.sql.execution.columnar.NullableColumnBuilder)#java/nio/ByteBuffer/rewind()
org/apache/spark/sql/execution/columnar/NullableColumnBuilder/class/build(org.apache.spark.sql.execution.columnar.NullableColumnBuilder)#java/nio/ByteBuffer/putInt(int)
org/apache/spark/sql/execution/columnar/NullableColumnBuilder/class/build(org.apache.spark.sql.execution.columnar.NullableColumnBuilder)#java/nio/ByteBuffer/limit(int)
org/apache/spark/sql/execution/columnar/NullableColumnBuilder/class/build(org.apache.spark.sql.execution.columnar.NullableColumnBuilder)#java/nio/ByteBuffer/put(java.nio.ByteBuffer)
org/apache/spark/sql/execution/datasources/FileFormatWriter/SingleDirectoryWriteTask/execute(scala.collection.Iterator)#scala/Predef$/assert(boolean,scala.Function0)
org/apache/spark/sql/execution/datasources/FileFormatWriter/SingleDirectoryWriteTask/execute(scala.collection.Iterator)#scala/collection/Iterator/next()
org/apache/spark/sql/execution/datasources/FileFormatWriter/SingleDirectoryWriteTask/execute(scala.collection.Iterator)#scala/Predef$/Set()
org/apache/spark/sql/execution/datasources/FileFormatWriter/SingleDirectoryWriteTask/execute(scala.collection.Iterator)#org/apache/spark/sql/execution/datasources/FileFormatWriter$WriteJobDescription/maxRecordsPerFile()
org/apache/spark/sql/execution/datasources/FileFormatWriter/SingleDirectoryWriteTask/execute(scala.collection.Iterator)#org/apache/spark/sql/execution/datasources/FileFormatWriter$SingleDirectoryWriteTask$$anonfun$execute$1/1(org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask,scala.runtime.IntRef)
org/apache/spark/sql/execution/datasources/FileFormatWriter/SingleDirectoryWriteTask/execute(scala.collection.Iterator)#org/apache/spark/sql/execution/datasources/FileFormatWriter$SingleDirectoryWriteTask/releaseResources()
org/apache/spark/sql/execution/datasources/FileFormatWriter/SingleDirectoryWriteTask/execute(scala.collection.Iterator)#scala/collection/Iterator/hasNext()
org/apache/spark/sql/execution/datasources/FileFormatWriter/SingleDirectoryWriteTask/execute(scala.collection.Iterator)#scala/collection/immutable/Set$/empty()
org/apache/spark/sql/execution/datasources/FileFormatWriter/SingleDirectoryWriteTask/execute(scala.collection.Iterator)#scala/runtime/IntRef/create(int)
org/apache/spark/sql/execution/datasources/FileFormatWriter/SingleDirectoryWriteTask/execute(scala.collection.Iterator)#org/apache/spark/sql/execution/datasources/FileFormatWriter$/org$apache$spark$sql$execution$datasources$FileFormatWriter$$MAX_FILE_COUNTER()
org/apache/spark/sql/execution/datasources/FileFormatWriter/SingleDirectoryWriteTask/execute(scala.collection.Iterator)#org/apache/spark/sql/execution/datasources/FileFormatWriter$SingleDirectoryWriteTask/newOutputWriter(int)
org/apache/spark/sql/execution/datasources/FileFormatWriter/SingleDirectoryWriteTask/newOutputWriter(int)#org/apache/spark/sql/execution/datasources/FileFormatWriter$WriteJobDescription/dataColumns()
org/apache/spark/sql/execution/datasources/FileFormatWriter/SingleDirectoryWriteTask/newOutputWriter(int)#org/apache/spark/sql/execution/datasources/FileFormatWriter$WriteJobDescription/outputWriterFactory()
org/apache/spark/sql/execution/datasources/FileFormatWriter/SingleDirectoryWriteTask/newOutputWriter(int)#org/apache/spark/sql/catalyst/expressions/package$/AttributeSeq(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/FileFormatWriter/SingleDirectoryWriteTask/newOutputWriter(int)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/FileFormatWriter/SingleDirectoryWriteTask/newOutputWriter(int)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/datasources/FileFormatWriter/SingleDirectoryWriteTask/newOutputWriter(int)#scala/collection/immutable/StringOps/format(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/FileFormatWriter/SingleDirectoryWriteTask/newOutputWriter(int)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/datasources/FileFormatWriter/SingleDirectoryWriteTask/newOutputWriter(int)#org/apache/spark/sql/catalyst/expressions/package$AttributeSeq/toStructType()
org/apache/spark/sql/execution/datasources/FileFormatWriter/SingleDirectoryWriteTask/newOutputWriter(int)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/FileFormatWriter/SingleDirectoryWriteTask/newOutputWriter(int)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/FileFormatWriter/SingleDirectoryWriteTask/newOutputWriter(int)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/FileFormatWriter/SingleDirectoryWriteTask/newOutputWriter(int)#org/apache/spark/internal/io/FileCommitProtocol/newTaskTempFile(org.apache.hadoop.mapreduce.TaskAttemptContext,scala.Option,java.lang.String)
org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding/Encoder/compress(java.nio.ByteBuffer,java.nio.ByteBuffer)#org/apache/spark/sql/execution/columnar/NativeColumnType/append(java.lang.Object,java.nio.ByteBuffer)
org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding/Encoder/compress(java.nio.ByteBuffer,java.nio.ByteBuffer)#java/nio/ByteBuffer/putShort(short)
org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding/Encoder/compress(java.nio.ByteBuffer,java.nio.ByteBuffer)#scala/runtime/BoxesRunTime/unboxToShort(java.lang.Object)
org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding/Encoder/compress(java.nio.ByteBuffer,java.nio.ByteBuffer)#scala/collection/mutable/ArrayBuffer/length()
org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding/Encoder/compress(java.nio.ByteBuffer,java.nio.ByteBuffer)#java/nio/ByteBuffer/hasRemaining()
org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding/Encoder/compress(java.nio.ByteBuffer,java.nio.ByteBuffer)#scala/collection/mutable/ArrayBuffer/apply(int)
org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding/Encoder/compress(java.nio.ByteBuffer,java.nio.ByteBuffer)#org/apache/spark/sql/execution/columnar/NativeColumnType/extract(java.nio.ByteBuffer)
org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding/Encoder/compress(java.nio.ByteBuffer,java.nio.ByteBuffer)#java/nio/ByteBuffer/putInt(int)
org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding/Encoder/compress(java.nio.ByteBuffer,java.nio.ByteBuffer)#java/lang/IllegalStateException/IllegalStateException(java.lang.String)
org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding/Encoder/compress(java.nio.ByteBuffer,java.nio.ByteBuffer)#scala/collection/mutable/HashMap/apply(java.lang.Object)
org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding/Encoder/compress(java.nio.ByteBuffer,java.nio.ByteBuffer)#scala/collection/mutable/HashMap/size()
org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding/Encoder/compress(java.nio.ByteBuffer,java.nio.ByteBuffer)#org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding$Encoder/dictionary()
org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding/Encoder/compress(java.nio.ByteBuffer,java.nio.ByteBuffer)#java/nio/ByteBuffer/rewind()
org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding/Encoder/compress(java.nio.ByteBuffer,java.nio.ByteBuffer)#org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding$/typeId()
org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding/Encoder/compress(java.nio.ByteBuffer,java.nio.ByteBuffer)#org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding$Encoder/overflow()
org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding/Encoder/compress(java.nio.ByteBuffer,java.nio.ByteBuffer)#org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding$Encoder/values()
org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding/Encoder/gatherCompressibilityStats(org.apache.spark.sql.catalyst.InternalRow,int)#scala/collection/mutable/ArrayBuffer/clear()
org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding/Encoder/gatherCompressibilityStats(org.apache.spark.sql.catalyst.InternalRow,int)#scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object)
org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding/Encoder/gatherCompressibilityStats(org.apache.spark.sql.catalyst.InternalRow,int)#scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding/Encoder/gatherCompressibilityStats(org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/execution/columnar/NativeColumnType/clone(java.lang.Object)
org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding/Encoder/gatherCompressibilityStats(org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding$/MAX_DICT_SIZE()
org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding/Encoder/gatherCompressibilityStats(org.apache.spark.sql.catalyst.InternalRow,int)#scala/runtime/BoxesRunTime/boxToShort(short)
org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding/Encoder/gatherCompressibilityStats(org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/execution/columnar/NativeColumnType/actualSize(org.apache.spark.sql.catalyst.InternalRow,int)
org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding/Encoder/gatherCompressibilityStats(org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding$Encoder/count()
org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding/Encoder/gatherCompressibilityStats(org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding$Encoder/dictionarySize()
org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding/Encoder/gatherCompressibilityStats(org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding$Encoder/overflow_$eq(boolean)
org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding/Encoder/gatherCompressibilityStats(org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/execution/columnar/NativeColumnType/getField(org.apache.spark.sql.catalyst.InternalRow,int)
org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding/Encoder/gatherCompressibilityStats(org.apache.spark.sql.catalyst.InternalRow,int)#scala/collection/mutable/HashMap/clear()
org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding/Encoder/gatherCompressibilityStats(org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding$Encoder/dictionarySize_$eq(int)
org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding/Encoder/gatherCompressibilityStats(org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding$Encoder/_uncompressedSize()
org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding/Encoder/gatherCompressibilityStats(org.apache.spark.sql.catalyst.InternalRow,int)#scala/collection/mutable/HashMap/size()
org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding/Encoder/gatherCompressibilityStats(org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding$Encoder/dictionary()
org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding/Encoder/gatherCompressibilityStats(org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding$Encoder/_uncompressedSize_$eq(int)
org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding/Encoder/gatherCompressibilityStats(org.apache.spark.sql.catalyst.InternalRow,int)#scala/collection/mutable/HashMap/contains(java.lang.Object)
org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding/Encoder/gatherCompressibilityStats(org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding$Encoder/count_$eq(int)
org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding/Encoder/gatherCompressibilityStats(org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding$Encoder/overflow()
org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding/Encoder/gatherCompressibilityStats(org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/execution/columnar/compression/DictionaryEncoding$Encoder/values()
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/createBaseRdd(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/createBaseRdd(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/createBaseRdd(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/collection/Seq/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/createBaseRdd(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/SparkContext/defaultMinPartitions()
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/createBaseRdd(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/rdd/BinaryFileRDD/BinaryFileRDD(org.apache.spark.SparkContext,java.lang.Class,java.lang.Class,java.lang.Class,org.apache.hadoop.conf.Configuration,int)
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/createBaseRdd(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/createBaseRdd(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/hadoop/mapreduce/Job/getConfiguration()
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/createBaseRdd(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/createBaseRdd(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/createBaseRdd(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/rdd/RDD$/rddToPairRDDFunctions(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag,scala.math.Ordering)
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/createBaseRdd(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/rdd/PairRDDFunctions/values()
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/createBaseRdd(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/rdd/BinaryFileRDD/setName(java.lang.String)
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/createBaseRdd(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/hadoop/mapreduce/Job/getInstance(org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/createBaseRdd(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/createBaseRdd(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource$$anonfun$10/10()
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/createBaseRdd(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/hadoop/mapreduce/lib/input/FileInputFormat/setInputPaths(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.fs.Path[])
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/createBaseRdd(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/collection/Seq/mkString(java.lang.String)
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/createBaseRdd(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/infer(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource$/infer(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/infer(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource$/makeSafeHeader(java.lang.String[],boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/infer(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/infer(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/rdd/RDD/flatMap(scala.Function1,scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/infer(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVInferSchema$/infer(org.apache.spark.rdd.RDD,java.lang.String[],org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/infer(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource$$anonfun$8/8(org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/infer(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/rdd/RDD/take(int)
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/infer(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/collection/mutable/ArrayOps/headOption()
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/infer(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/infer(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/infer(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/infer(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/Some/x()
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/infer(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource$$anonfun$9/9(org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/infer(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/infer(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/internal/SQLConf/caseSensitiveAnalysis()
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/infer(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/types/StructType$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/infer(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource$/createBaseRdd(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/parseUserSpecifiedCreateTableColumnTypes/2/anonfun/apply/9/apply()#org/apache/spark/sql/types/StructType/catalogString()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/parseUserSpecifiedCreateTableColumnTypes/2/anonfun/apply/9/apply()#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/parseUserSpecifiedCreateTableColumnTypes/2/anonfun/apply/9/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/parseUserSpecifiedCreateTableColumnTypes/2/anonfun/apply/9/apply()#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$parseUserSpecifiedCreateTableColumnTypes$2$$anonfun$apply$9/apply()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/parseUserSpecifiedCreateTableColumnTypes/2/anonfun/apply/9/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/parseUserSpecifiedCreateTableColumnTypes/2/anonfun/apply/9/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/parseUserSpecifiedCreateTableColumnTypes/2/anonfun/apply/9/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/parseUserSpecifiedCreateTableColumnTypes/2/anonfun/apply/9/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/parseUserSpecifiedCreateTableColumnTypes/2/anonfun/apply/9/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/parseUserSpecifiedCreateTableColumnTypes/2/anonfun/apply/9/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/parseUserSpecifiedCreateTableColumnTypes/2/anonfun/apply/9/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/parseUserSpecifiedCreateTableColumnTypes/2/anonfun/apply/9/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/parseUserSpecifiedCreateTableColumnTypes/2/anonfun/apply/9/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/parseUserSpecifiedCreateTableColumnTypes/2/anonfun/apply/9/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/8/anonfun/10/apply()#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/8/anonfun/10/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/8/anonfun/10/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/8/anonfun/10/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/8/anonfun/10/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/8/anonfun/10/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/8/anonfun/10/apply()#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$8$$anonfun$10/apply()
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/8/anonfun/10/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/8/anonfun/10/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/8/anonfun/10/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/3/apply(scala.Tuple2)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/3/apply(scala.Tuple2)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/3/apply(scala.Tuple2)#scala/collection/mutable/ArrayOps/mkString(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/3/apply(scala.Tuple2)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/3/apply(scala.Tuple2)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/3/apply(scala.Tuple2)#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/3/apply(scala.Tuple2)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/3/apply(scala.Tuple2)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/3/apply(scala.Tuple2)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/3/apply(scala.Tuple2)#scala/collection/mutable/ArrayOps/exists(scala.Function1)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/3/apply(scala.Tuple2)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/3/apply(scala.Tuple2)#org/apache/spark/sql/execution/datasources/DataSourceAnalysis$$anonfun$convertStaticPartitions$3$$anonfun$apply$5/5(org.apache.spark.sql.execution.datasources.DataSourceAnalysis$$anonfun$convertStaticPartitions$3)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/3/apply(scala.Tuple2)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/3/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/3/apply(scala.Tuple2)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/3/apply(scala.Tuple2)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/3/apply(scala.Tuple2)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/3/apply(scala.Tuple2)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/3/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/3/apply(scala.Tuple2)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/3/apply(scala.Tuple2)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/3/apply(scala.Tuple2)#org/apache/spark/sql/execution/datasources/DataSourceAnalysis$$anonfun$convertStaticPartitions$3$$anonfun$apply$4/4(org.apache.spark.sql.execution.datasources.DataSourceAnalysis$$anonfun$convertStaticPartitions$3,java.lang.String)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/convertStaticPartitions/3/apply(scala.Tuple2)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/DataSource/anonfun/13/apply()#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/DataSource/anonfun/13/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/DataSource/anonfun/13/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSource/anonfun/13/apply()#org/apache/spark/sql/execution/streaming/MetadataLogFileIndex/allFiles()
org/apache/spark/sql/execution/datasources/DataSource/anonfun/13/apply()#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$13/apply()
org/apache/spark/sql/execution/datasources/DataSource/anonfun/13/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/DataSource/anonfun/13/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/DataSource/anonfun/13/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/DataSource/anonfun/13/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/anonfun/13/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/DataSource/anonfun/13/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/DataSource/anonfun/13/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/anonfun/13/apply()#scala/collection/Seq/mkString(java.lang.String)
org/apache/spark/sql/execution/datasources/DataSource/anonfun/13/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSource/anonfun/13/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/streaming/FileStreamSource/anonfun/fetchMaxOffset/2/apply()#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/streaming/FileStreamSource/anonfun/fetchMaxOffset/2/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/FileStreamSource/anonfun/fetchMaxOffset/2/apply()#org/apache/spark/sql/execution/streaming/FileStreamSource$SeenFilesMap/size()
org/apache/spark/sql/execution/streaming/FileStreamSource/anonfun/fetchMaxOffset/2/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/FileStreamSource/anonfun/fetchMaxOffset/2/apply()#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/streaming/FileStreamSource/anonfun/fetchMaxOffset/2/apply()#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/streaming/FileStreamSource/anonfun/fetchMaxOffset/2/apply()#org/apache/spark/sql/execution/streaming/FileStreamSource$$anonfun$fetchMaxOffset$2/apply()
org/apache/spark/sql/execution/streaming/FileStreamSource/anonfun/fetchMaxOffset/2/apply()#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/streaming/FileStreamSource/anonfun/fetchMaxOffset/2/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/FileStreamSource/anonfun/fetchMaxOffset/2/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/FileStreamSource/anonfun/fetchMaxOffset/2/apply()#scala/collection/Seq/size()
org/apache/spark/sql/execution/joins/LongToUnsafeRowMap/ensureAcquireMemory(long)#org/apache/spark/SparkException/SparkException(java.lang.String)
org/apache/spark/sql/execution/joins/LongToUnsafeRowMap/ensureAcquireMemory(long)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/joins/LongToUnsafeRowMap/ensureAcquireMemory(long)#org/apache/spark/sql/execution/joins/LongToUnsafeRowMap/acquireMemory(long)
org/apache/spark/sql/execution/joins/LongToUnsafeRowMap/ensureAcquireMemory(long)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/joins/LongToUnsafeRowMap/ensureAcquireMemory(long)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/joins/LongToUnsafeRowMap/ensureAcquireMemory(long)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/joins/LongToUnsafeRowMap/ensureAcquireMemory(long)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/joins/LongToUnsafeRowMap/ensureAcquireMemory(long)#org/apache/spark/sql/execution/joins/LongToUnsafeRowMap/freeMemory(long)
org/apache/spark/sql/execution/joins/LongToUnsafeRowMap/ensureAcquireMemory(long)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/joins/LongToUnsafeRowMap/ensureAcquireMemory(long)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/joins/LongToUnsafeRowMap/ensureAcquireMemory(long)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anonfun/4/apply()#org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter$$anonfun$4/apply()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anonfun/4/apply()#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anonfun/4/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anonfun/4/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anonfun/4/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anonfun/4/apply()#org/apache/spark/sql/types/StructType/prettyJson()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anonfun/4/apply()#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anonfun/4/apply()#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anonfun/4/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/UDFRegistration/registerPython(java.lang.String,org.apache.spark.sql.execution.python.UserDefinedPythonFunction)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/UDFRegistration/registerPython(java.lang.String,org.apache.spark.sql.execution.python.UserDefinedPythonFunction)#scala/Predef$/byteArrayOps(byte[])
org/apache/spark/sql/UDFRegistration/registerPython(java.lang.String,org.apache.spark.sql.execution.python.UserDefinedPythonFunction)#org/apache/spark/api/python/PythonFunction/pythonIncludes()
org/apache/spark/sql/UDFRegistration/registerPython(java.lang.String,org.apache.spark.sql.execution.python.UserDefinedPythonFunction)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/UDFRegistration/registerPython(java.lang.String,org.apache.spark.sql.execution.python.UserDefinedPythonFunction)#org/apache/spark/sql/UDFRegistration$$anonfun$registerPython$1/1(org.apache.spark.sql.UDFRegistration,org.apache.spark.sql.execution.python.UserDefinedPythonFunction)
org/apache/spark/sql/UDFRegistration/registerPython(java.lang.String,org.apache.spark.sql.execution.python.UserDefinedPythonFunction)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/UDFRegistration/registerPython(java.lang.String,org.apache.spark.sql.execution.python.UserDefinedPythonFunction)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/UDFRegistration/registerPython(java.lang.String,org.apache.spark.sql.execution.python.UserDefinedPythonFunction)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/UDFRegistration/registerPython(java.lang.String,org.apache.spark.sql.execution.python.UserDefinedPythonFunction)#scala/collection/mutable/ArrayOps/toSeq()
org/apache/spark/sql/UDFRegistration/registerPython(java.lang.String,org.apache.spark.sql.execution.python.UserDefinedPythonFunction)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/UDFRegistration/registerPython(java.lang.String,org.apache.spark.sql.execution.python.UserDefinedPythonFunction)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/UDFRegistration/registerPython(java.lang.String,org.apache.spark.sql.execution.python.UserDefinedPythonFunction)#org/apache/spark/sql/catalyst/analysis/FunctionRegistry/registerFunction(java.lang.String,scala.Function1)
org/apache/spark/sql/UDFRegistration/registerPython(java.lang.String,org.apache.spark.sql.execution.python.UserDefinedPythonFunction)#org/slf4j/Logger/debug(java.lang.String)
org/apache/spark/sql/UDFRegistration/registerPython(java.lang.String,org.apache.spark.sql.execution.python.UserDefinedPythonFunction)#org/apache/spark/api/python/PythonFunction/command()
org/apache/spark/sql/UDFRegistration/registerPython(java.lang.String,org.apache.spark.sql.execution.python.UserDefinedPythonFunction)#org/apache/spark/api/python/PythonFunction/pythonExec()
org/apache/spark/sql/UDFRegistration/registerPython(java.lang.String,org.apache.spark.sql.execution.python.UserDefinedPythonFunction)#org/apache/spark/api/python/PythonFunction/envVars()
org/apache/spark/sql/UDFRegistration/registerJava(java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/UDFRegistration$$anonfun$registerJava$1/1(org.apache.spark.sql.UDFRegistration,int)
org/apache/spark/sql/UDFRegistration/registerJava(java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/UDFRegistration/registerJava(java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType)#java/io/IOException/IOException(java.lang.String)
org/apache/spark/sql/UDFRegistration/registerJava(java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType)#java/lang/Class/newInstance()
org/apache/spark/sql/UDFRegistration/registerJava(java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/UDFRegistration/registerJava(java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/UDFRegistration/registerJava(java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/UDFRegistration$$anonfun$registerJava$3/3(org.apache.spark.sql.UDFRegistration,java.lang.String)
org/apache/spark/sql/UDFRegistration/registerJava(java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/UDFRegistration/registerJava(java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType)#java/lang/Class/getGenericInterfaces()
org/apache/spark/sql/UDFRegistration/registerJava(java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType)#scala/Tuple2/_1()
org/apache/spark/sql/UDFRegistration/registerJava(java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType)#org/apache/spark/util/Utils$/classForName(java.lang.String)
org/apache/spark/sql/UDFRegistration/registerJava(java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/UDFRegistration/registerJava(java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/UDFRegistration$$anonfun$24/24(org.apache.spark.sql.UDFRegistration)
org/apache/spark/sql/UDFRegistration/registerJava(java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/UDFRegistration$$anonfun$25/25(org.apache.spark.sql.UDFRegistration)
org/apache/spark/sql/UDFRegistration/registerJava(java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/UDFRegistration$$anonfun$26/26(org.apache.spark.sql.UDFRegistration)
org/apache/spark/sql/UDFRegistration/registerJava(java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/catalyst/JavaTypeInference$/inferDataType(java.lang.reflect.Type)
org/apache/spark/sql/UDFRegistration/registerJava(java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType)#java/lang/reflect/ParameterizedType/getActualTypeArguments()
org/apache/spark/sql/UDFRegistration/registerJava(java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType)#scala/collection/mutable/ArrayOps/last()
org/apache/spark/sql/UDFRegistration/registerJava(java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/UDFRegistration/registerJava(java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/UDFRegistration$$anonfun$registerJava$2/2(org.apache.spark.sql.UDFRegistration,java.lang.String)
org/apache/spark/sql/UDFRegistration/registerJava(java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/UDFRegistration/registerJava(java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/UDFRegistration/registerJava(java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType)#scala/collection/mutable/ArrayOps/filter(scala.Function1)
org/apache/spark/sql/execution/streaming/OffsetSeqLog/deserialize(java.io.InputStream)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/streaming/OffsetSeqLog/deserialize(java.io.InputStream)#org/apache/spark/sql/execution/streaming/OffsetSeqLog/parseVersion(java.lang.String,int)
org/apache/spark/sql/execution/streaming/OffsetSeqLog/deserialize(java.io.InputStream)#org/apache/spark/sql/execution/streaming/OffsetSeqLog$/VERSION()
org/apache/spark/sql/execution/streaming/OffsetSeqLog/deserialize(java.io.InputStream)#java/lang/IllegalStateException/IllegalStateException(java.lang.String)
org/apache/spark/sql/execution/streaming/OffsetSeqLog/deserialize(java.io.InputStream)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/streaming/OffsetSeqLog/deserialize(java.io.InputStream)#java/lang/String/trim()
org/apache/spark/sql/execution/streaming/OffsetSeqLog/deserialize(java.io.InputStream)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/OffsetSeqLog/deserialize(java.io.InputStream)#scala/collection/Iterator/hasNext()
org/apache/spark/sql/execution/streaming/OffsetSeqLog/deserialize(java.io.InputStream)#scala/io/Source$/fromInputStream(java.io.InputStream,java.lang.String)
org/apache/spark/sql/execution/streaming/OffsetSeqLog/deserialize(java.io.InputStream)#java/nio/charset/Charset/name()
org/apache/spark/sql/execution/streaming/OffsetSeqLog/deserialize(java.io.InputStream)#scala/io/BufferedSource/getLines()
org/apache/spark/sql/execution/streaming/OffsetSeqLog/deserialize(java.io.InputStream)#org/apache/spark/sql/execution/streaming/OffsetSeq$/fill(scala.Option,scala.collection.Seq)
org/apache/spark/sql/execution/streaming/OffsetSeqLog/deserialize(java.io.InputStream)#org/apache/spark/sql/execution/streaming/OffsetSeqLog$$anonfun$deserialize$1/1(org.apache.spark.sql.execution.streaming.OffsetSeqLog)
org/apache/spark/sql/execution/streaming/OffsetSeqLog/deserialize(java.io.InputStream)#scala/collection/Iterator/next()
org/apache/spark/sql/execution/streaming/OffsetSeqLog/deserialize(java.io.InputStream)#scala/collection/Iterator/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/streaming/OffsetSeqLog/deserialize(java.io.InputStream)#scala/collection/Iterator/map(scala.Function1)
org/apache/spark/sql/execution/streaming/OffsetSeqLog/deserialize(java.io.InputStream)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/streaming/OffsetSeqLog/serialize(org.apache.spark.sql.execution.streaming.OffsetSeq,java.io.OutputStream)#org/apache/spark/sql/execution/streaming/OffsetSeqLog$/VERSION()
org/apache/spark/sql/execution/streaming/OffsetSeqLog/serialize(org.apache.spark.sql.execution.streaming.OffsetSeq,java.io.OutputStream)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/streaming/OffsetSeqLog/serialize(org.apache.spark.sql.execution.streaming.OffsetSeq,java.io.OutputStream)#scala/collection/IterableLike/foreach(scala.Function1)
org/apache/spark/sql/execution/streaming/OffsetSeqLog/serialize(org.apache.spark.sql.execution.streaming.OffsetSeq,java.io.OutputStream)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/streaming/OffsetSeqLog/serialize(org.apache.spark.sql.execution.streaming.OffsetSeq,java.io.OutputStream)#org/apache/spark/sql/execution/streaming/OffsetSeqLog$$anonfun$serialize$4/4(org.apache.spark.sql.execution.streaming.OffsetSeqLog,java.io.OutputStream)
org/apache/spark/sql/execution/streaming/OffsetSeqLog/serialize(org.apache.spark.sql.execution.streaming.OffsetSeq,java.io.OutputStream)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/streaming/OffsetSeqLog/serialize(org.apache.spark.sql.execution.streaming.OffsetSeq,java.io.OutputStream)#java/io/OutputStream/write(byte[])
org/apache/spark/sql/execution/streaming/OffsetSeqLog/serialize(org.apache.spark.sql.execution.streaming.OffsetSeq,java.io.OutputStream)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/streaming/OffsetSeqLog/serialize(org.apache.spark.sql.execution.streaming.OffsetSeq,java.io.OutputStream)#java/io/OutputStream/write(int)
org/apache/spark/sql/execution/streaming/OffsetSeqLog/serialize(org.apache.spark.sql.execution.streaming.OffsetSeq,java.io.OutputStream)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/streaming/OffsetSeqLog/serialize(org.apache.spark.sql.execution.streaming.OffsetSeq,java.io.OutputStream)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/streaming/OffsetSeqLog/serialize(org.apache.spark.sql.execution.streaming.OffsetSeq,java.io.OutputStream)#org/apache/spark/sql/execution/streaming/OffsetSeqLog$$anonfun$serialize$3/3(org.apache.spark.sql.execution.streaming.OffsetSeqLog)
org/apache/spark/sql/execution/streaming/OffsetSeqLog/serialize(org.apache.spark.sql.execution.streaming.OffsetSeq,java.io.OutputStream)#java/lang/String/getBytes(java.nio.charset.Charset)
org/apache/spark/sql/execution/streaming/OffsetSeqLog/serialize(org.apache.spark.sql.execution.streaming.OffsetSeq,java.io.OutputStream)#org/apache/spark/sql/execution/streaming/OffsetSeqLog$$anonfun$serialize$2/2(org.apache.spark.sql.execution.streaming.OffsetSeqLog)
org/apache/spark/sql/execution/streaming/OffsetSeqLog/serialize(org.apache.spark.sql.execution.streaming.OffsetSeq,java.io.OutputStream)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/streaming/OffsetSeqLog/serialize(org.apache.spark.sql.execution.streaming.OffsetSeq,java.io.OutputStream)#org/apache/spark/sql/execution/streaming/OffsetSeqLog$$anonfun$serialize$1/1(org.apache.spark.sql.execution.streaming.OffsetSeqLog)
org/apache/spark/sql/execution/streaming/OffsetSeqLog/serialize(org.apache.spark.sql.execution.streaming.OffsetSeq,java.io.OutputStream)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/InputAdapter/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/InputAdapter/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/InputAdapter/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/InputAdapter/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/freshName(java.lang.String)
org/apache/spark/sql/execution/InputAdapter/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/InputAdapter/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/InputAdapter/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/InputAdapter/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#java/lang/String/trim()
org/apache/spark/sql/execution/InputAdapter/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addMutableState(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/InputAdapter/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/sources/In/equals(java.lang.Object)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/sources/In/equals(java.lang.Object)#org/apache/spark/sql/sources/In/values()
org/apache/spark/sql/sources/In/equals(java.lang.Object)#org/apache/spark/sql/sources/In$$anonfun$equals$1/1(org.apache.spark.sql.sources.In)
org/apache/spark/sql/sources/In/equals(java.lang.Object)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/sources/In/equals(java.lang.Object)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/sources/In/equals(java.lang.Object)#scala/collection/mutable/ArrayOps/forall(scala.Function1)
org/apache/spark/sql/sources/In/equals(java.lang.Object)#scala/Predef$/genericArrayOps(java.lang.Object)
org/apache/spark/sql/sources/In/equals(java.lang.Object)#scala/collection/mutable/ArrayOps/zip(scala.collection.GenIterable,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/sources/In/equals(java.lang.Object)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/sources/In/equals(java.lang.Object)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/SparkStrategies/FlatMapGroupsWithStateStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsWithState/dataAttributes()
org/apache/spark/sql/execution/SparkStrategies/FlatMapGroupsWithStateStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)
org/apache/spark/sql/execution/SparkStrategies/FlatMapGroupsWithStateStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsWithState/keyDeserializer()
org/apache/spark/sql/execution/SparkStrategies/FlatMapGroupsWithStateStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsWithState/stateEncoder()
org/apache/spark/sql/execution/SparkStrategies/FlatMapGroupsWithStateStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsWithState/child()
org/apache/spark/sql/execution/SparkStrategies/FlatMapGroupsWithStateStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/SparkStrategies$FlatMapGroupsWithStateStrategy$/planLater(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/SparkStrategies/FlatMapGroupsWithStateStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsWithState/timeout()
org/apache/spark/sql/execution/SparkStrategies/FlatMapGroupsWithStateStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsWithState/valueDeserializer()
org/apache/spark/sql/execution/SparkStrategies/FlatMapGroupsWithStateStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsWithState/outputObjAttr()
org/apache/spark/sql/execution/SparkStrategies/FlatMapGroupsWithStateStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsWithState/func()
org/apache/spark/sql/execution/SparkStrategies/FlatMapGroupsWithStateStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsWithState/outputMode()
org/apache/spark/sql/execution/SparkStrategies/FlatMapGroupsWithStateStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsWithState/groupingAttributes()
org/apache/spark/sql/execution/debug/package/codegenString(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/zipWithIndex(scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/debug/package/codegenString(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/debug/package$$anonfun$codegenString$3/3(scala.collection.mutable.HashSet,scala.runtime.ObjectRef)
org/apache/spark/sql/execution/debug/package/codegenString(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/debug/package/codegenString(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/debug/package$$anonfun$codegenString$1/1(scala.collection.mutable.HashSet)
org/apache/spark/sql/execution/debug/package/codegenString(org.apache.spark.sql.execution.SparkPlan)#scala/collection/TraversableLike/withFilter(scala.Function1)
org/apache/spark/sql/execution/debug/package/codegenString(org.apache.spark.sql.execution.SparkPlan)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/debug/package/codegenString(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/debug/package$$anonfun$codegenString$2/2()
org/apache/spark/sql/execution/debug/package/codegenString(org.apache.spark.sql.execution.SparkPlan)#scala/collection/generic/FilterMonadic/foreach(scala.Function1)
org/apache/spark/sql/execution/debug/package/codegenString(org.apache.spark.sql.execution.SparkPlan)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/debug/package/codegenString(org.apache.spark.sql.execution.SparkPlan)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/debug/package/codegenString(org.apache.spark.sql.execution.SparkPlan)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/debug/package/codegenString(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/SparkPlan/transform(scala.PartialFunction)
org/apache/spark/sql/execution/debug/package/codegenString(org.apache.spark.sql.execution.SparkPlan)#scala/collection/mutable/HashSet/HashSet()
org/apache/spark/sql/execution/debug/package/codegenString(org.apache.spark.sql.execution.SparkPlan)#scala/collection/mutable/HashSet/size()
org/apache/spark/sql/execution/debug/package/codegenString(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/debug/package$/codegenString(org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/debug/package/codegenString(org.apache.spark.sql.execution.SparkPlan)#scala/collection/mutable/HashSet/toSeq()
org/apache/spark/sql/execution/debug/package/codegenString(org.apache.spark.sql.execution.SparkPlan)#scala/runtime/ObjectRef/create(java.lang.Object)
org/apache/spark/sql/execution/debug/package/codegenString(org.apache.spark.sql.execution.SparkPlan)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/python/EvaluatePython/RowPickler/pickle(java.lang.Object,java.io.OutputStream,net.razorvine.pickle.Pickler)#org/apache/spark/sql/catalyst/expressions/GenericRowWithSchema/schema()
org/apache/spark/sql/execution/python/EvaluatePython/RowPickler/pickle(java.lang.Object,java.io.OutputStream,net.razorvine.pickle.Pickler)#org/apache/spark/sql/catalyst/expressions/GenericRowWithSchema/values()
org/apache/spark/sql/execution/python/EvaluatePython/RowPickler/pickle(java.lang.Object,java.io.OutputStream,net.razorvine.pickle.Pickler)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/python/EvaluatePython/RowPickler/pickle(java.lang.Object,java.io.OutputStream,net.razorvine.pickle.Pickler)#net/razorvine/pickle/Pickler/save(java.lang.Object)
org/apache/spark/sql/execution/python/EvaluatePython/RowPickler/pickle(java.lang.Object,java.io.OutputStream,net.razorvine.pickle.Pickler)#java/io/OutputStream/write(int)
org/apache/spark/sql/execution/python/EvaluatePython/RowPickler/pickle(java.lang.Object,java.io.OutputStream,net.razorvine.pickle.Pickler)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/python/EvaluatePython/RowPickler/pickle(java.lang.Object,java.io.OutputStream,net.razorvine.pickle.Pickler)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/python/EvaluatePython/RowPickler/pickle(java.lang.Object,java.io.OutputStream,net.razorvine.pickle.Pickler)#java/io/OutputStream/write(byte[])
org/apache/spark/sql/execution/python/EvaluatePython/RowPickler/pickle(java.lang.Object,java.io.OutputStream,net.razorvine.pickle.Pickler)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/python/EvaluatePython/RowPickler/pickle(java.lang.Object,java.io.OutputStream,net.razorvine.pickle.Pickler)#org/apache/spark/sql/execution/python/EvaluatePython$/org$apache$spark$sql$execution$python$EvaluatePython$$module()
org/apache/spark/sql/execution/python/EvaluatePython/RowPickler/pickle(java.lang.Object,java.io.OutputStream,net.razorvine.pickle.Pickler)#java/lang/String/getBytes(java.nio.charset.Charset)
org/apache/spark/sql/internal/HiveSerDe/getDefaultStorage(org.apache.spark.sql.internal.SQLConf)#org/apache/spark/sql/internal/HiveSerDe$$anonfun$4/4()
org/apache/spark/sql/internal/HiveSerDe/getDefaultStorage(org.apache.spark.sql.internal.SQLConf)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat$/empty()
org/apache/spark/sql/internal/HiveSerDe/getDefaultStorage(org.apache.spark.sql.internal.SQLConf)#org/apache/spark/sql/internal/HiveSerDe$$anonfun$5/5()
org/apache/spark/sql/internal/HiveSerDe/getDefaultStorage(org.apache.spark.sql.internal.SQLConf)#org/apache/spark/sql/internal/HiveSerDe$$anonfun$6/6()
org/apache/spark/sql/internal/HiveSerDe/getDefaultStorage(org.apache.spark.sql.internal.SQLConf)#org/apache/spark/sql/internal/HiveSerDe$/getDefaultStorage(org.apache.spark.sql.internal.SQLConf)
org/apache/spark/sql/internal/HiveSerDe/getDefaultStorage(org.apache.spark.sql.internal.SQLConf)#scala/Option/flatMap(scala.Function1)
org/apache/spark/sql/internal/HiveSerDe/getDefaultStorage(org.apache.spark.sql.internal.SQLConf)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$1()
org/apache/spark/sql/internal/HiveSerDe/getDefaultStorage(org.apache.spark.sql.internal.SQLConf)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$5()
org/apache/spark/sql/internal/HiveSerDe/getDefaultStorage(org.apache.spark.sql.internal.SQLConf)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$6()
org/apache/spark/sql/internal/HiveSerDe/getDefaultStorage(org.apache.spark.sql.internal.SQLConf)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy(scala.Option,scala.Option,scala.Option,scala.Option,boolean,scala.collection.immutable.Map)
org/apache/spark/sql/internal/HiveSerDe/getDefaultStorage(org.apache.spark.sql.internal.SQLConf)#org/apache/spark/sql/internal/SQLConf/getConfString(java.lang.String,java.lang.String)
org/apache/spark/sql/internal/HiveSerDe/getDefaultStorage(org.apache.spark.sql.internal.SQLConf)#org/apache/spark/sql/internal/HiveSerDe$$anonfun$1/1()
org/apache/spark/sql/internal/HiveSerDe/getDefaultStorage(org.apache.spark.sql.internal.SQLConf)#org/apache/spark/sql/internal/HiveSerDe$$anonfun$2/2()
org/apache/spark/sql/internal/HiveSerDe/getDefaultStorage(org.apache.spark.sql.internal.SQLConf)#org/apache/spark/sql/internal/HiveSerDe$$anonfun$3/3()
org/apache/spark/sql/internal/HiveSerDe/getDefaultStorage(org.apache.spark.sql.internal.SQLConf)#org/apache/spark/sql/internal/HiveSerDe$/sourceToSerDe(java.lang.String)
org/apache/spark/sql/internal/HiveSerDe/getDefaultStorage(org.apache.spark.sql.internal.SQLConf)#scala/Option/orElse(scala.Function0)
org/apache/spark/sql/execution/command/CreateTableLikeCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/isEmpty()
org/apache/spark/sql/execution/command/CreateTableLikeCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$5()
org/apache/spark/sql/execution/command/CreateTableLikeCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$6()
org/apache/spark/sql/execution/command/CreateTableLikeCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/schema()
org/apache/spark/sql/execution/command/CreateTableLikeCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$10()
org/apache/spark/sql/execution/command/CreateTableLikeCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$12()
org/apache/spark/sql/execution/command/CreateTableLikeCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$15()
org/apache/spark/sql/execution/command/CreateTableLikeCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/CatalogTable(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)
org/apache/spark/sql/execution/command/CreateTableLikeCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTableType$/EXTERNAL()
org/apache/spark/sql/execution/command/CreateTableLikeCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/provider()
org/apache/spark/sql/execution/command/CreateTableLikeCommand/run(org.apache.spark.sql.SparkSession)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/command/CreateTableLikeCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/internal/SQLConf/defaultDataSourceName()
org/apache/spark/sql/execution/command/CreateTableLikeCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/empty()
org/apache/spark/sql/execution/command/CreateTableLikeCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTempViewOrPermanentTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/CreateTableLikeCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/tableType()
org/apache/spark/sql/execution/command/CreateTableLikeCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$2()
org/apache/spark/sql/execution/command/CreateTableLikeCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/createTable(org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)
org/apache/spark/sql/execution/command/CreateTableLikeCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/command/CreateTableLikeCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$3()
org/apache/spark/sql/execution/command/CreateTableLikeCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/CreateTableLikeCommand$$anonfun$3/3(org.apache.spark.sql.execution.command.CreateTableLikeCommand)
org/apache/spark/sql/execution/command/CreateTableLikeCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$4()
org/apache/spark/sql/execution/command/CreateTableLikeCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$11()
org/apache/spark/sql/execution/command/CreateTableLikeCommand/run(org.apache.spark.sql.SparkSession)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/command/CreateTableLikeCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$13()
org/apache/spark/sql/execution/command/CreateTableLikeCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy(scala.Option,scala.Option,scala.Option,scala.Option,boolean,scala.collection.immutable.Map)
org/apache/spark/sql/execution/command/CreateTableLikeCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/bucketSpec()
org/apache/spark/sql/execution/command/CreateTableLikeCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$8()
org/apache/spark/sql/execution/command/CreateTableLikeCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$9()
org/apache/spark/sql/execution/command/CreateTableLikeCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$14()
org/apache/spark/sql/execution/command/CreateTableLikeCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()
org/apache/spark/sql/execution/command/CreateTableLikeCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$16()
org/apache/spark/sql/execution/command/CreateTableLikeCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTableType$/VIEW()
org/apache/spark/sql/execution/command/CreateTableLikeCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$17()
org/apache/spark/sql/execution/command/CreateTableLikeCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTableType$/MANAGED()
org/apache/spark/sql/execution/command/CreateTableLikeCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/storage()
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport$$anonfun$org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter$4/4(org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/StructType/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport$$anonfun$org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter$6/6(org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport$$anonfun$org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter$7/7(org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport$$anonfun$org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter$8/8(org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport$$anonfun$org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter$12/12(org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport,scala.Function2[],org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport$$anonfun$2/2(org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter(org.apache.spark.sql.types.DataType)#scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter(org.apache.spark.sql.types.DataType)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/DecimalType$Fixed$/unapply(org.apache.spark.sql.types.DecimalType)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport$$anonfun$org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter$1/1(org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter(org.apache.spark.sql.types.DataType)#scala/Tuple2/_2$mcI$sp()
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter(org.apache.spark.sql.types.DataType)#scala/Option/get()
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter(org.apache.spark.sql.types.DataType)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport$$anonfun$org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter$3/3(org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter(org.apache.spark.sql.types.DataType)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter(org.apache.spark.sql.types.DataType)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport$$anonfun$org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter$5/5(org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter(org.apache.spark.sql.types.DataType)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter(org.apache.spark.sql.types.DataType)#scala/Option/isEmpty()
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/UserDefinedType/sqlType()
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport$$anonfun$org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter$9/9(org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport$$anonfun$1/1(org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter(org.apache.spark.sql.types.DataType)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter(org.apache.spark.sql.types.DataType)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter(org.apache.spark.sql.types.DataType)#scala/sys/package$/error(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport$$anonfun$org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter$10/10(org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport$$anonfun$org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter$11/11(org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter(org.apache.spark.sql.types.DataType)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport$$anonfun$org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter$2/2(org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetWriteSupport$$makeWriter(org.apache.spark.sql.types.DataType)#scala/Tuple2/_1$mcI$sp()
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/makeDecimalWriter(int,int)#scala/Predef$/assert(boolean,scala.Function0)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/makeDecimalWriter(int,int)#org/apache/spark/sql/types/Decimal$/MAX_LONG_DIGITS()
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/makeDecimalWriter(int,int)#org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport$$anonfun$3/3(org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport,int,int)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/makeDecimalWriter(int,int)#org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport$$anonfun$5/5(org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport,int,int,int)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/makeDecimalWriter(int,int)#org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport$$anonfun$4/4(org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport,int,int)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/makeDecimalWriter(int,int)#org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport$$anonfun$6/6(org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport,int,int,int)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/makeDecimalWriter(int,int)#org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter$/minBytesForPrecision()
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/makeDecimalWriter(int,int)#org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport$$anonfun$makeDecimalWriter$1/1(org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport,int)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/makeDecimalWriter(int,int)#org/apache/spark/sql/types/Decimal$/MAX_INT_DIGITS()
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/makeDecimalWriter(int,int)#org/apache/spark/sql/types/DecimalType$/MAX_PRECISION()
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/init(org.apache.hadoop.conf.Configuration)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/init(org.apache.hadoop.conf.Configuration)#scala/collection/immutable/Map$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/init(org.apache.hadoop.conf.Configuration)#scala/Predef$/Map()
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/init(org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/internal/SQLConf$/PARQUET_INT64_AS_TIMESTAMP_MILLIS()
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/init(org.apache.hadoop.conf.Configuration)#scala/Predef$/ArrowAssoc(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/init(org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport$$anonfun$init$1/1(org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/init(org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport$$anonfun$init$2/2(org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/init(org.apache.hadoop.conf.Configuration)#scala/collection/JavaConverters$/mapAsJavaMapConverter(scala.collection.Map)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/init(org.apache.hadoop.conf.Configuration)#org/apache/parquet/hadoop/api/WriteSupport$WriteContext/WriteContext(org.apache.parquet.schema.MessageType,java.util.Map)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/init(org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport$$anonfun$init$3/3(org.apache.spark.sql.execution.datasources.parquet.ParquetWriteSupport,org.apache.parquet.schema.MessageType)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/init(org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport$/SPARK_METADATA_KEY()
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/init(org.apache.hadoop.conf.Configuration)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/init(org.apache.hadoop.conf.Configuration)#org/apache/hadoop/conf/Configuration/get(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/init(org.apache.hadoop.conf.Configuration)#scala/collection/convert/Decorators$AsJava/asJava()
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/init(org.apache.hadoop.conf.Configuration)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/init(org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/types/StructType/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/init(org.apache.hadoop.conf.Configuration)#scala/collection/immutable/StringOps/toBoolean()
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/init(org.apache.hadoop.conf.Configuration)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/init(org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/internal/SQLConf$/PARQUET_WRITE_LEGACY_FORMAT()
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/init(org.apache.hadoop.conf.Configuration)#scala/Predef$/assert(boolean)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/init(org.apache.hadoop.conf.Configuration)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/init(org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/types/StructType$/fromString(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/init(org.apache.hadoop.conf.Configuration)#scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/init(org.apache.hadoop.conf.Configuration)#scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/init(org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport$/SPARK_ROW_SCHEMA()
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/init(org.apache.hadoop.conf.Configuration)#org/apache/spark/internal/config/ConfigEntry/key()
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/init(org.apache.hadoop.conf.Configuration)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/setSchema(org.apache.spark.sql.types.StructType,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport$/setSchema(org.apache.spark.sql.types.StructType,org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/setSchema(org.apache.spark.sql.types.StructType,org.apache.hadoop.conf.Configuration)#scala/collection/IterableLike/foreach(scala.Function1)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/setSchema(org.apache.spark.sql.types.StructType,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/conf/Configuration/set(java.lang.String,java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/setSchema(org.apache.spark.sql.types.StructType,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport$$anonfun$setSchema$1/1()
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/setSchema(org.apache.spark.sql.types.StructType,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport$$anonfun$setSchema$2/2()
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/setSchema(org.apache.spark.sql.types.StructType,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/types/StructType/json()
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/setSchema(org.apache.spark.sql.types.StructType,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/types/StructType/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/setSchema(org.apache.spark.sql.types.StructType,org.apache.hadoop.conf.Configuration)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/setSchema(org.apache.spark.sql.types.StructType,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/conf/Configuration/setIfUnset(java.lang.String,java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/setSchema(org.apache.spark.sql.types.StructType,org.apache.hadoop.conf.Configuration)#org/apache/parquet/column/ParquetProperties$WriterVersion/toString()
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/setSchema(org.apache.spark.sql.types.StructType,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport$/SPARK_ROW_SCHEMA()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/checkFieldName(java.lang.String)#org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter$$anonfun$checkFieldName$1/1(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/checkFieldName(java.lang.String)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/checkFieldName(java.lang.String)#java/lang/String/split(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/checkFieldName(java.lang.String)#org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter$/checkFieldName(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/checkFieldName(java.lang.String)#java/lang/String/trim()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/checkFieldName(java.lang.String)#scala/collection/mutable/ArrayOps/mkString(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/checkFieldName(java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/checkFieldName(java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/checkFieldName(java.lang.String)#org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter$/checkConversionRequirement(scala.Function0,java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/checkFieldName(java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/checkFieldName(java.lang.String)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/checkFieldName(java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/checkFieldName(java.lang.String)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/checkFieldName(java.lang.String)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/execution/datasources/FileFormatWriter$/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$9/9(org.apache.spark.sql.SparkSession)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$8/8()
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#scala/collection/Iterable/toSeq()
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/hadoop/mapreduce/Job/setOutputValueClass(java.lang.Class)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/expressions/package$AttributeSeq/toStructType()
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/expressions/AttributeSet$/apply(scala.collection.Iterable)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/util/CaseInsensitiveMap/get(java.lang.String)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#scala/Option$/option2Iterable(scala.Option)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#scala/collection/IterableLike/forall(scala.Function1)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$5/5(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/hadoop/mapreduce/Job/getConfiguration()
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#scala/collection/Seq/zip(scala.collection.GenIterable,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#java/util/UUID/randomUUID()
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$7/7(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/hadoop/mapreduce/Job/getInstance(org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/hadoop/mapreduce/lib/output/FileOutputFormat/setOutputPath(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/execution/datasources/FileFormatWriter$WriteJobDescription/WriteJobDescription(java.lang.String,org.apache.spark.util.SerializableConfiguration,org.apache.spark.sql.execution.datasources.OutputWriterFactory,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.Option,java.lang.String,scala.collection.immutable.Map,long,java.lang.String)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/util/DateTimeUtils$/TIMEZONE_OPTION()
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/execution/datasources/FileFormatWriter$OutputSpec/customPartitionLocations()
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/execution/SQLExecution$/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/expressions/package$/AttributeSeq(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/execution/datasources/FileFormatWriter$OutputSpec/outputPath()
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$10/10()
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#scala/collection/Seq/filterNot(scala.Function1)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$11/11()
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$1/1(org.apache.spark.sql.SparkSession)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#scala/collection/Seq/length()
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/util/CaseInsensitiveMap$/apply(scala.collection.immutable.Map)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$4/4(org.apache.spark.sql.catalyst.expressions.AttributeSet)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#java/util/UUID/toString()
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/hadoop/mapreduce/Job/setOutputKeyClass(java.lang.Class)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/hadoop/fs/Path/Path(java.lang.String)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$3/3(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.collection.Seq)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/util/SerializableConfiguration/SerializableConfiguration(org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$write$1/1(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.internal.io.FileCommitProtocol,scala.Function1,org.apache.hadoop.mapreduce.Job,org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteJobDescription,scala.collection.Seq,boolean)
org/apache/spark/sql/execution/datasources/FileFormatWriter/org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteJobDescription,int,int,int,org.apache.spark.internal.io.FileCommitProtocol,scala.collection.Iterator)#scala/Option/isEmpty()
org/apache/spark/sql/execution/datasources/FileFormatWriter/org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteJobDescription,int,int,int,org.apache.spark.internal.io.FileCommitProtocol,scala.collection.Iterator)#org/apache/hadoop/conf/Configuration/set(java.lang.String,java.lang.String)
org/apache/spark/sql/execution/datasources/FileFormatWriter/org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteJobDescription,int,int,int,org.apache.spark.internal.io.FileCommitProtocol,scala.collection.Iterator)#java/util/Date/Date()
org/apache/spark/sql/execution/datasources/FileFormatWriter/org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteJobDescription,int,int,int,org.apache.spark.internal.io.FileCommitProtocol,scala.collection.Iterator)#org/apache/hadoop/mapreduce/TaskID/toString()
org/apache/spark/sql/execution/datasources/FileFormatWriter/org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteJobDescription,int,int,int,org.apache.spark.internal.io.FileCommitProtocol,scala.collection.Iterator)#org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$2/2(scala.Function0)
org/apache/spark/sql/execution/datasources/FileFormatWriter/org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteJobDescription,int,int,int,org.apache.spark.internal.io.FileCommitProtocol,scala.collection.Iterator)#org/apache/spark/sql/execution/datasources/FileFormatWriter$WriteJobDescription/partitionColumns()
org/apache/spark/sql/execution/datasources/FileFormatWriter/org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteJobDescription,int,int,int,org.apache.spark.internal.io.FileCommitProtocol,scala.collection.Iterator)#org/apache/hadoop/mapreduce/TaskID/TaskID(org.apache.hadoop.mapreduce.JobID,org.apache.hadoop.mapreduce.TaskType,int)
org/apache/spark/sql/execution/datasources/FileFormatWriter/org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteJobDescription,int,int,int,org.apache.spark.internal.io.FileCommitProtocol,scala.collection.Iterator)#org/apache/spark/internal/io/SparkHadoopWriterUtils$/createJobID(java.util.Date,int)
org/apache/spark/sql/execution/datasources/FileFormatWriter/org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteJobDescription,int,int,int,org.apache.spark.internal.io.FileCommitProtocol,scala.collection.Iterator)#org/apache/spark/SparkException/SparkException(java.lang.String,java.lang.Throwable)
org/apache/spark/sql/execution/datasources/FileFormatWriter/org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteJobDescription,int,int,int,org.apache.spark.internal.io.FileCommitProtocol,scala.collection.Iterator)#org/apache/spark/sql/execution/datasources/FileFormatWriter$SingleDirectoryWriteTask/SingleDirectoryWriteTask(org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteJobDescription,org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.spark.internal.io.FileCommitProtocol)
org/apache/spark/sql/execution/datasources/FileFormatWriter/org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteJobDescription,int,int,int,org.apache.spark.internal.io.FileCommitProtocol,scala.collection.Iterator)#scala/collection/Seq/isEmpty()
org/apache/spark/sql/execution/datasources/FileFormatWriter/org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteJobDescription,int,int,int,org.apache.spark.internal.io.FileCommitProtocol,scala.collection.Iterator)#org/apache/spark/util/Utils$/tryWithSafeFinallyAndFailureCallbacks(scala.Function0,scala.Function0,scala.Function0)
org/apache/spark/sql/execution/datasources/FileFormatWriter/org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteJobDescription,int,int,int,org.apache.spark.internal.io.FileCommitProtocol,scala.collection.Iterator)#org/apache/hadoop/mapred/JobID/toString()
org/apache/spark/sql/execution/datasources/FileFormatWriter/org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteJobDescription,int,int,int,org.apache.spark.internal.io.FileCommitProtocol,scala.collection.Iterator)#org/apache/hadoop/mapreduce/TaskAttemptID/getTaskID()
org/apache/spark/sql/execution/datasources/FileFormatWriter/org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteJobDescription,int,int,int,org.apache.spark.internal.io.FileCommitProtocol,scala.collection.Iterator)#org/apache/spark/sql/execution/datasources/FileFormatWriter$WriteJobDescription/bucketIdExpression()
org/apache/spark/sql/execution/datasources/FileFormatWriter/org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteJobDescription,int,int,int,org.apache.spark.internal.io.FileCommitProtocol,scala.collection.Iterator)#org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$1/1(org.apache.spark.internal.io.FileCommitProtocol,org.apache.hadoop.mapred.JobID,org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.spark.sql.execution.datasources.FileFormatWriter$ExecuteWriteTask)
org/apache/spark/sql/execution/datasources/FileFormatWriter/org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteJobDescription,int,int,int,org.apache.spark.internal.io.FileCommitProtocol,scala.collection.Iterator)#org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3/3(org.apache.spark.internal.io.FileCommitProtocol,scala.collection.Iterator,org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.spark.sql.execution.datasources.FileFormatWriter$ExecuteWriteTask)
org/apache/spark/sql/execution/datasources/FileFormatWriter/org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteJobDescription,int,int,int,org.apache.spark.internal.io.FileCommitProtocol,scala.collection.Iterator)#org/apache/spark/sql/execution/datasources/FileFormatWriter$WriteJobDescription/serializableHadoopConf()
org/apache/spark/sql/execution/datasources/FileFormatWriter/org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteJobDescription,int,int,int,org.apache.spark.internal.io.FileCommitProtocol,scala.collection.Iterator)#org/apache/spark/util/SerializableConfiguration/value()
org/apache/spark/sql/execution/datasources/FileFormatWriter/org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteJobDescription,int,int,int,org.apache.spark.internal.io.FileCommitProtocol,scala.collection.Iterator)#org/apache/hadoop/mapreduce/task/TaskAttemptContextImpl/TaskAttemptContextImpl(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.TaskAttemptID)
org/apache/spark/sql/execution/datasources/FileFormatWriter/org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteJobDescription,int,int,int,org.apache.spark.internal.io.FileCommitProtocol,scala.collection.Iterator)#org/apache/hadoop/conf/Configuration/setBoolean(java.lang.String,boolean)
org/apache/spark/sql/execution/datasources/FileFormatWriter/org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteJobDescription,int,int,int,org.apache.spark.internal.io.FileCommitProtocol,scala.collection.Iterator)#org/apache/hadoop/conf/Configuration/setInt(java.lang.String,int)
org/apache/spark/sql/execution/datasources/FileFormatWriter/org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteJobDescription,int,int,int,org.apache.spark.internal.io.FileCommitProtocol,scala.collection.Iterator)#org/apache/spark/internal/io/FileCommitProtocol/setupTask(org.apache.hadoop.mapreduce.TaskAttemptContext)
org/apache/spark/sql/execution/datasources/FileFormatWriter/org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteJobDescription,int,int,int,org.apache.spark.internal.io.FileCommitProtocol,scala.collection.Iterator)#org/apache/hadoop/mapreduce/TaskAttemptID/toString()
org/apache/spark/sql/execution/datasources/FileFormatWriter/org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteJobDescription,int,int,int,org.apache.spark.internal.io.FileCommitProtocol,scala.collection.Iterator)#org/apache/hadoop/mapreduce/TaskAttemptID/TaskAttemptID(org.apache.hadoop.mapreduce.TaskID,int)
org/apache/spark/sql/execution/datasources/FileFormatWriter/org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteJobDescription,int,int,int,org.apache.spark.internal.io.FileCommitProtocol,scala.collection.Iterator)#org/apache/spark/sql/execution/datasources/FileFormatWriter$DynamicPartitionWriteTask/DynamicPartitionWriteTask(org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteJobDescription,org.apache.hadoop.mapreduce.TaskAttemptContext,org.apache.spark.internal.io.FileCommitProtocol)
org/apache/spark/sql/api/r/SQLUtils/anonfun/3/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/api/r/SQLUtils/anonfun/3/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/api/r/SQLUtils/anonfun/3/apply()#org/apache/spark/internal/config/ConfigEntry/key()
org/apache/spark/sql/api/r/SQLUtils/anonfun/3/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/api/r/SQLUtils/anonfun/3/apply()#org/apache/spark/sql/internal/StaticSQLConf$/CATALOG_IMPLEMENTATION()
org/apache/spark/sql/api/r/SQLUtils/anonfun/3/apply()#org/apache/spark/sql/api/r/SQLUtils$$anonfun$3/apply()
org/apache/spark/sql/api/r/SQLUtils/anonfun/3/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/api/r/SQLUtils/anonfun/3/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/api/r/SQLUtils/anonfun/3/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/api/r/SQLUtils/anonfun/3/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/SQLImplicits/typecreator2/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/selectType(scala.reflect.api.Symbols$SymbolApi,java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator2/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ModuleSymbolApi/asModule()
org/apache/spark/sql/SQLImplicits/typecreator2/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticPackage(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator2/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$InternalApi/reificationSupport()
org/apache/spark/sql/SQLImplicits/typecreator2/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/RootClass()
org/apache/spark/sql/SQLImplicits/typecreator2/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticModule(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator2/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ClassSymbolApi/asType()
org/apache/spark/sql/SQLImplicits/typecreator2/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ModuleSymbolApi/moduleClass()
org/apache/spark/sql/SQLImplicits/typecreator2/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/thisPrefix(scala.reflect.api.Symbols$SymbolApi)
org/apache/spark/sql/SQLImplicits/typecreator2/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/TypeRef(scala.reflect.api.Types$TypeApi,scala.reflect.api.Symbols$SymbolApi,scala.collection.immutable.List)
org/apache/spark/sql/SQLImplicits/typecreator2/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticClass(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator2/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/universe()
org/apache/spark/sql/SQLImplicits/typecreator2/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$TypeSymbolApi/toTypeConstructor()
org/apache/spark/sql/SQLImplicits/typecreator2/1/apply(scala.reflect.api.Mirror)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/SQLImplicits/typecreator2/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/SingleType(scala.reflect.api.Types$TypeApi,scala.reflect.api.Symbols$SymbolApi)
org/apache/spark/sql/SQLImplicits/typecreator2/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Universe/internal()
org/apache/spark/sql/SQLImplicits/typecreator2/1/apply(scala.reflect.api.Mirror)#scala/collection/immutable/List$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/StreamingDeduplicateExec/doExecute()#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/streaming/StreamingDeduplicateExec/doExecute()#org/apache/spark/sql/execution/streaming/state/package$StateStoreOps/mapPartitionsWithStateStore(java.lang.String,long,long,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.internal.SessionState,scala.Option,scala.Function2,scala.reflect.ClassTag)
org/apache/spark/sql/execution/streaming/StreamingDeduplicateExec/doExecute()#org/apache/spark/sql/execution/streaming/StreamingDeduplicateExec/sqlContext()
org/apache/spark/sql/execution/streaming/StreamingDeduplicateExec/doExecute()#org/apache/spark/sql/execution/streaming/StreamingDeduplicateExec$$anonfun$doExecute$4/4(org.apache.spark.sql.execution.streaming.StreamingDeduplicateExec)
org/apache/spark/sql/execution/streaming/StreamingDeduplicateExec/doExecute()#org/apache/spark/sql/catalyst/expressions/package$AttributeSeq/toStructType()
org/apache/spark/sql/execution/streaming/StreamingDeduplicateExec/doExecute()#org/apache/spark/sql/catalyst/expressions/package$/AttributeSeq(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/StreamingDeduplicateExec/doExecute()#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/streaming/StreamingDeduplicateExec/doExecute()#org/apache/spark/sql/execution/streaming/state/package$/StateStoreOps(org.apache.spark.rdd.RDD,scala.reflect.ClassTag)
org/apache/spark/sql/execution/streaming/StreamingDeduplicateExec/doExecute()#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/DecimalType/scale()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#java/lang/Class/getComponentType()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/Tuple2/_1()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/UserDefinedType/sqlType()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#java/lang/Object/getClass()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/reflect/ClassTag$/Any()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/collection/mutable/Buffer$/canBuildFrom()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#java/lang/Class/isArray()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/Predef$/genericArrayOps(java.lang.Object)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#java/lang/IllegalStateException/IllegalStateException(java.lang.String)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/collection/mutable/ArrayOps/zip(scala.collection.GenIterable,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#java/lang/Object/toString()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/unsafe/types/UTF8String/fromString(java.lang.String)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/runtime/BoxesRunTime/boxToShort(short)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/runtime/BoxesRunTime/boxToFloat(float)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/Tuple2/_2()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/MapType/valueType()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/collection/convert/Decorators$AsScala/asScala()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/catalyst/util/ArrayBasedMapData$/apply(java.util.Map,scala.Function1,scala.Function1)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/runtime/BoxesRunTime/boxToBoolean(boolean)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/runtime/ScalaRunTime$/array_length(java.lang.Object)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/runtime/BoxesRunTime/boxToDouble(double)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/runtime/BoxesRunTime/boxToByte(byte)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/python/EvaluatePython$/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#java/lang/Class/getName()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/runtime/BoxesRunTime/unboxToDouble(java.lang.Object)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/ArrayType/elementType()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/DecimalType/precision()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#java/lang/String/getBytes(java.nio.charset.Charset)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/python/EvaluatePython$$anonfun$fromJava$4/4(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/python/EvaluatePython$$anonfun$fromJava$3/3(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/catalyst/util/GenericArrayData/GenericArrayData(java.lang.Object[])
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/MapType/keyType()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/catalyst/expressions/GenericInternalRow/GenericInternalRow(java.lang.Object[])
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/Decimal$/apply(java.math.BigDecimal,int,int)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/python/EvaluatePython$$anonfun$fromJava$5/5()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/python/EvaluatePython$$anonfun$fromJava$2/2(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/python/EvaluatePython$$anonfun$fromJava$1/1(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/Decimal/toJavaBigDecimal()
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/catalyst/util/ArrayData/foreach(org.apache.spark.sql.types.DataType,scala.Function2)
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/StructField/dataType()
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/Tuple2/_1()
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/UserDefinedType/sqlType()
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/MapType/valueType()
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/catalyst/util/MapData/numElements()
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/catalyst/util/MapData/foreach(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType,scala.Function2)
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/python/EvaluatePython$$anonfun$toJava$1/1(java.util.ArrayList,org.apache.spark.sql.types.ArrayType)
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/MapType/keyType()
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/catalyst/InternalRow/numFields()
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/catalyst/expressions/GenericRowWithSchema/GenericRowWithSchema(java.lang.Object[],org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/unsafe/types/UTF8String/toString()
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#java/util/ArrayList/ArrayList(int)
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/python/EvaluatePython$/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/catalyst/util/ArrayData/numElements()
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/Tuple2/_2()
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/python/EvaluatePython$$anonfun$toJava$2/2(java.util.HashMap,org.apache.spark.sql.types.MapType)
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/catalyst/InternalRow/get(int,org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#java/util/HashMap/HashMap(int)
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/ArrayType/elementType()
org/apache/spark/sql/execution/datasources/DataSource/createSource(java.lang.String)#org/apache/spark/sql/catalyst/util/CaseInsensitiveMap/getOrElse(java.lang.Object,scala.Function0)
org/apache/spark/sql/execution/datasources/DataSource/createSource(java.lang.String)#org/apache/spark/sql/execution/datasources/DataSource$SourceInfo/schema()
org/apache/spark/sql/execution/datasources/DataSource/createSource(java.lang.String)#java/lang/Class/newInstance()
org/apache/spark/sql/execution/datasources/DataSource/createSource(java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/DataSource/createSource(java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSource/createSource(java.lang.String)#org/apache/spark/sql/execution/datasources/DataSource$SourceInfo/partitionColumns()
org/apache/spark/sql/execution/datasources/DataSource/createSource(java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSource/createSource(java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/createSource(java.lang.String)#java/lang/UnsupportedOperationException/UnsupportedOperationException(java.lang.String)
org/apache/spark/sql/execution/datasources/DataSource/createSource(java.lang.String)#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$10/10(org.apache.spark.sql.execution.datasources.DataSource)
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#scala/collection/Iterable$/canBuildFrom()
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$14/14(org.apache.spark.sql.execution.datasources.DataSource,boolean,org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#scala/Option/orElse(scala.Function0)
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#org/apache/spark/sql/types/StructType/asNullable()
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#org/apache/spark/sql/execution/datasources/FileStatusCache$/getOrCreate(org.apache.spark.sql.SparkSession)
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#scala/Some/x()
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#org/apache/spark/sql/catalyst/catalog/CatalogTable/stats()
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#org/apache/spark/sql/internal/SQLConf/defaultSizeInBytes()
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#scala/Option$/option2Iterable(scala.Option)
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#org/apache/spark/sql/internal/SQLConf/manageFilesourcePartitions()
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#org/apache/spark/sql/execution/streaming/MetadataLogFileIndex/partitionSchema()
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#org/apache/spark/sql/catalyst/catalog/CatalogTable/tracksPartitionsInCatalog()
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#java/lang/Class/newInstance()
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#scala/Option/isDefined()
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#scala/Tuple2/_1()
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#scala/Tuple2/_2()
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#scala/collection/Iterable/toSeq()
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#scala/collection/Iterable/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#org/apache/spark/sql/execution/streaming/FileStreamSink$/hasMetadata(scala.collection.Seq,org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#scala/collection/Iterable/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$12/12(org.apache.spark.sql.execution.datasources.DataSource,org.apache.spark.sql.execution.streaming.MetadataLogFileIndex,org.apache.spark.sql.execution.datasources.FileFormat)
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$1/1(org.apache.spark.sql.execution.datasources.DataSource,long)
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$13/13(org.apache.spark.sql.execution.datasources.DataSource,org.apache.spark.sql.execution.streaming.MetadataLogFileIndex,org.apache.spark.sql.execution.datasources.FileFormat)
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#scala/collection/IterableLike/head()
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$15/15(org.apache.spark.sql.execution.datasources.DataSource)
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#org/apache/spark/sql/catalyst/util/CaseInsensitiveMap/get(java.lang.String)
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#org/apache/hadoop/fs/Path/Path(java.lang.String)
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/resolveRelation(boolean)#scala/Option/get()
org/apache/spark/sql/execution/datasources/DataSource/writeInFileFormat(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#scala/Option$/option2Iterable(scala.Option)
org/apache/spark/sql/execution/datasources/DataSource/writeInFileFormat(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#org/apache/hadoop/fs/FileSystem/getUri()
org/apache/spark/sql/execution/datasources/DataSource/writeInFileFormat(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#org/apache/spark/sql/execution/datasources/PartitioningUtils$/validatePartitionColumn(org.apache.spark.sql.types.StructType,scala.collection.Seq,boolean)
org/apache/spark/sql/execution/datasources/DataSource/writeInFileFormat(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#scala/collection/immutable/Map$/empty()
org/apache/spark/sql/execution/datasources/DataSource/writeInFileFormat(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#scala/collection/Seq/length()
org/apache/spark/sql/execution/datasources/DataSource/writeInFileFormat(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/datasources/DataSource/writeInFileFormat(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#scala/Predef$/Map()
org/apache/spark/sql/execution/datasources/DataSource/writeInFileFormat(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/DataSource/writeInFileFormat(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/DataSource/writeInFileFormat(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/datasources/DataSource/writeInFileFormat(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSource/writeInFileFormat(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/DataSource/writeInFileFormat(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#scala/collection/Seq/head()
org/apache/spark/sql/execution/datasources/DataSource/writeInFileFormat(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/DataSource/writeInFileFormat(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/writeInFileFormat(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#org/apache/hadoop/fs/Path/makeQualified(java.net.URI,org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/datasources/DataSource/writeInFileFormat(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#org/apache/spark/sql/internal/SQLConf/caseSensitiveAnalysis()
org/apache/spark/sql/execution/datasources/DataSource/writeInFileFormat(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#org/apache/hadoop/fs/FileSystem/getWorkingDirectory()
org/apache/spark/sql/execution/datasources/DataSource/writeInFileFormat(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#org/apache/spark/sql/catalyst/util/CaseInsensitiveMap/get(java.lang.String)
org/apache/spark/sql/execution/datasources/DataSource/writeInFileFormat(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$17/17(org.apache.spark.sql.execution.datasources.DataSource)
org/apache/spark/sql/execution/datasources/DataSource/writeInFileFormat(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/writeInFileFormat(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$16/16(org.apache.spark.sql.execution.datasources.DataSource)
org/apache/spark/sql/execution/datasources/DataSource/writeInFileFormat(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/execution/datasources/DataSource/writeInFileFormat(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#org/apache/hadoop/fs/Path/Path(java.lang.String)
org/apache/spark/sql/execution/datasources/DataSource/writeInFileFormat(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#scala/collection/Seq/mkString(java.lang.String)
org/apache/spark/sql/execution/datasources/DataSource/writeInFileFormat(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSource/writeInFileFormat(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/DataSource/tempFileIndex$lzycompute$1(org.apache.spark.sql.execution.datasources.FileStatusCache,scala.runtime.ObjectRef,scala.runtime.VolatileByteRef)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/DataSource/tempFileIndex$lzycompute$1(org.apache.spark.sql.execution.datasources.FileStatusCache,scala.runtime.ObjectRef,scala.runtime.VolatileByteRef)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/DataSource/tempFileIndex$lzycompute$1(org.apache.spark.sql.execution.datasources.FileStatusCache,scala.runtime.ObjectRef,scala.runtime.VolatileByteRef)#scala/Option$/option2Iterable(scala.Option)
org/apache/spark/sql/execution/datasources/DataSource/tempFileIndex$lzycompute$1(org.apache.spark.sql.execution.datasources.FileStatusCache,scala.runtime.ObjectRef,scala.runtime.VolatileByteRef)#scala/collection/Iterable$/canBuildFrom()
org/apache/spark/sql/execution/datasources/DataSource/tempFileIndex$lzycompute$1(org.apache.spark.sql.execution.datasources.FileStatusCache,scala.runtime.ObjectRef,scala.runtime.VolatileByteRef)#scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/DataSource/tempFileIndex$lzycompute$1(org.apache.spark.sql.execution.datasources.FileStatusCache,scala.runtime.ObjectRef,scala.runtime.VolatileByteRef)#scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/DataSource/tempFileIndex$lzycompute$1(org.apache.spark.sql.execution.datasources.FileStatusCache,scala.runtime.ObjectRef,scala.runtime.VolatileByteRef)#org/apache/spark/sql/catalyst/util/CaseInsensitiveMap/get(java.lang.String)
org/apache/spark/sql/execution/datasources/DataSource/tempFileIndex$lzycompute$1(org.apache.spark.sql.execution.datasources.FileStatusCache,scala.runtime.ObjectRef,scala.runtime.VolatileByteRef)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/DataSource/tempFileIndex$lzycompute$1(org.apache.spark.sql.execution.datasources.FileStatusCache,scala.runtime.ObjectRef,scala.runtime.VolatileByteRef)#scala/collection/Iterable/toSeq()
org/apache/spark/sql/execution/datasources/DataSource/tempFileIndex$lzycompute$1(org.apache.spark.sql.execution.datasources.FileStatusCache,scala.runtime.ObjectRef,scala.runtime.VolatileByteRef)#scala/collection/Iterable/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/DataSource/tempFileIndex$lzycompute$1(org.apache.spark.sql.execution.datasources.FileStatusCache,scala.runtime.ObjectRef,scala.runtime.VolatileByteRef)#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$2/2(org.apache.spark.sql.execution.datasources.DataSource,org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/datasources/DataSource/getOrInferFileFormatSchema(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.execution.datasources.FileStatusCache)#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$getOrInferFileFormatSchema$1/1(org.apache.spark.sql.execution.datasources.DataSource)
org/apache/spark/sql/execution/datasources/DataSource/getOrInferFileFormatSchema(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.execution.datasources.FileStatusCache)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/DataSource/getOrInferFileFormatSchema(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.execution.datasources.FileStatusCache)#org/apache/spark/sql/types/StructType/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/DataSource/getOrInferFileFormatSchema(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.execution.datasources.FileStatusCache)#scala/Option/orElse(scala.Function0)
org/apache/spark/sql/execution/datasources/DataSource/getOrInferFileFormatSchema(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.execution.datasources.FileStatusCache)#scala/runtime/VolatileByteRef/create(byte)
org/apache/spark/sql/execution/datasources/DataSource/getOrInferFileFormatSchema(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.execution.datasources.FileStatusCache)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/datasources/DataSource/getOrInferFileFormatSchema(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.execution.datasources.FileStatusCache)#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$3/3(org.apache.spark.sql.execution.datasources.DataSource)
org/apache/spark/sql/execution/datasources/DataSource/getOrInferFileFormatSchema(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.execution.datasources.FileStatusCache)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/datasources/DataSource/getOrInferFileFormatSchema(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.execution.datasources.FileStatusCache)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/getOrInferFileFormatSchema(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.execution.datasources.FileStatusCache)#org/apache/spark/sql/types/StructType/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/DataSource/getOrInferFileFormatSchema(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.execution.datasources.FileStatusCache)#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$8/8(org.apache.spark.sql.execution.datasources.DataSource,org.apache.spark.sql.execution.datasources.FileFormat)
org/apache/spark/sql/execution/datasources/DataSource/getOrInferFileFormatSchema(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.execution.datasources.FileStatusCache)#scala/collection/Seq/isEmpty()
org/apache/spark/sql/execution/datasources/DataSource/getOrInferFileFormatSchema(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.execution.datasources.FileStatusCache)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex/partitionSchema()
org/apache/spark/sql/execution/datasources/DataSource/getOrInferFileFormatSchema(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.execution.datasources.FileStatusCache)#scala/Option/isEmpty()
org/apache/spark/sql/execution/datasources/DataSource/getOrInferFileFormatSchema(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.execution.datasources.FileStatusCache)#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$6/6(org.apache.spark.sql.execution.datasources.DataSource,org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/datasources/DataSource/getOrInferFileFormatSchema(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.execution.datasources.FileStatusCache)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/DataSource/getOrInferFileFormatSchema(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.execution.datasources.FileStatusCache)#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$4/4(org.apache.spark.sql.execution.datasources.DataSource,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.execution.datasources.FileStatusCache,scala.runtime.ObjectRef,scala.runtime.VolatileByteRef)
org/apache/spark/sql/execution/datasources/DataSource/getOrInferFileFormatSchema(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.execution.datasources.FileStatusCache)#org/apache/spark/sql/internal/SQLConf/caseSensitiveAnalysis()
org/apache/spark/sql/execution/datasources/DataSource/getOrInferFileFormatSchema(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.execution.datasources.FileStatusCache)#org/apache/spark/sql/util/SchemaUtils$/checkColumnNameDuplication(scala.collection.Seq,java.lang.String,boolean)
org/apache/spark/sql/execution/datasources/DataSource/getOrInferFileFormatSchema(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.execution.datasources.FileStatusCache)#org/apache/spark/sql/types/StructType$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSource/getOrInferFileFormatSchema(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.execution.datasources.FileStatusCache)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/DataSource/getOrInferFileFormatSchema(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.execution.datasources.FileStatusCache)#scala/runtime/ObjectRef/zero()
org/apache/spark/sql/execution/datasources/DataSource/getOrInferFileFormatSchema(org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.execution.datasources.FileStatusCache)#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$7/7(org.apache.spark.sql.execution.datasources.DataSource,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.sql.execution.datasources.FileStatusCache,scala.runtime.ObjectRef,scala.runtime.VolatileByteRef)
org/apache/spark/sql/execution/datasources/DataSource/buildStorageFormatFromOptions(scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat$/empty()
org/apache/spark/sql/execution/datasources/DataSource/buildStorageFormatFromOptions(scala.collection.immutable.Map)#scala/collection/immutable/Map/filterKeys(scala.Function1)
org/apache/spark/sql/execution/datasources/DataSource/buildStorageFormatFromOptions(scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$2()
org/apache/spark/sql/execution/datasources/DataSource/buildStorageFormatFromOptions(scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$5()
org/apache/spark/sql/execution/datasources/DataSource/buildStorageFormatFromOptions(scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy(scala.Option,scala.Option,scala.Option,scala.Option,boolean,scala.collection.immutable.Map)
org/apache/spark/sql/execution/datasources/DataSource/buildStorageFormatFromOptions(scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/util/CaseInsensitiveMap$/apply(scala.collection.immutable.Map)
org/apache/spark/sql/execution/datasources/DataSource/buildStorageFormatFromOptions(scala.collection.immutable.Map)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/datasources/DataSource/buildStorageFormatFromOptions(scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$3()
org/apache/spark/sql/execution/datasources/DataSource/buildStorageFormatFromOptions(scala.collection.immutable.Map)#org/apache/spark/sql/execution/datasources/DataSource$/buildStorageFormatFromOptions(scala.collection.immutable.Map)
org/apache/spark/sql/execution/datasources/DataSource/buildStorageFormatFromOptions(scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$4()
org/apache/spark/sql/execution/datasources/DataSource/buildStorageFormatFromOptions(scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/util/CaseInsensitiveMap/get(java.lang.String)
org/apache/spark/sql/execution/datasources/DataSource/buildStorageFormatFromOptions(scala.collection.immutable.Map)#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$24/24()
org/apache/spark/sql/execution/datasources/DataSource/buildStorageFormatFromOptions(scala.collection.immutable.Map)#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$25/25()
org/apache/spark/sql/execution/datasources/DataSource/writeAndRead(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#scala/collection/IterableLike/exists(scala.Function1)
org/apache/spark/sql/execution/datasources/DataSource/writeAndRead(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/DataSource/writeAndRead(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSource/writeAndRead(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$writeAndRead$2/2(org.apache.spark.sql.execution.datasources.DataSource)
org/apache/spark/sql/execution/datasources/DataSource/writeAndRead(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$writeAndRead$1/1(org.apache.spark.sql.execution.datasources.DataSource)
org/apache/spark/sql/execution/datasources/DataSource/writeAndRead(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSource/writeAndRead(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#org/apache/spark/sql/types/StructType/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/DataSource/writeAndRead(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/DataSource/writeAndRead(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#java/lang/Class/newInstance()
org/apache/spark/sql/execution/datasources/DataSource/writeAndRead(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/DataSource/writeAndRead(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#java/lang/Class/getCanonicalName()
org/apache/spark/sql/execution/datasources/DataSource/writeAndRead(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#org/apache/spark/sql/types/StructType/asNullable()
org/apache/spark/sql/execution/datasources/DataSource/writeAndRead(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/DataSource/writeAndRead(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/writeAndRead(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/DataSource/writeAndRead(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/writeAndRead(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#scala/sys/package$/error(java.lang.String)
org/apache/spark/sql/execution/datasources/DataSource/writeAndRead(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/DataSource/writeAndRead(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/DataSource/write(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#scala/collection/IterableLike/exists(scala.Function1)
org/apache/spark/sql/execution/datasources/DataSource/write(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#org/apache/spark/sql/types/StructType/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/DataSource/write(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/DataSource/write(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#java/lang/Class/newInstance()
org/apache/spark/sql/execution/datasources/DataSource/write(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/DataSource/write(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#java/lang/Class/getCanonicalName()
org/apache/spark/sql/execution/datasources/DataSource/write(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSource/write(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/DataSource/write(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$write$1/1(org.apache.spark.sql.execution.datasources.DataSource)
org/apache/spark/sql/execution/datasources/DataSource/write(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSource/write(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/DataSource/write(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/DataSource/write(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/write(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#scala/sys/package$/error(java.lang.String)
org/apache/spark/sql/execution/datasources/DataSource/write(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/DataSource/write(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/DataSource/write(org.apache.spark.sql.SaveMode,org.apache.spark.sql.Dataset)#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$write$2/2(org.apache.spark.sql.execution.datasources.DataSource)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/collection/immutable/List$/canBuildFrom()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/collection/immutable/$colon$colon/tl$1()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/util/Try/orElse(scala.Function0)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#org/apache/spark/sql/execution/datasources/DataSource$/spark2RemovedClasses()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#java/lang/String/toLowerCase(java.util.Locale)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#org/apache/spark/sql/execution/datasources/DataSource$/logWarning(scala.Function0)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#java/util/ServiceConfigurationError/getMessage()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#java/lang/Object/getClass()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/util/Success/value()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#org/apache/spark/sql/execution/datasources/DataSource$/backwardCompatibilityMap()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#org/apache/spark/util/Utils$/getContextOrSparkClassLoader()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/collection/immutable/List/mkString(java.lang.String)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/util/Failure/exception()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#java/lang/String/replaceAll(java.lang.String,java.lang.String)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#org/apache/spark/sql/execution/datasources/DataSource$/lookupDataSource(java.lang.String)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/collection/immutable/List/filter(scala.Function1)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$20/20(java.lang.String,java.lang.ClassLoader)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$21/21(java.lang.String,java.lang.ClassLoader)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/collection/JavaConverters$/iterableAsScalaIterableConverter(java.lang.Iterable)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/collection/immutable/Set/contains(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/collection/convert/Decorators$AsScala/asScala()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/collection/immutable/Map/getOrElse(java.lang.Object,scala.Function0)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#java/lang/Throwable/getMessage()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/collection/TraversableLike/filter(scala.Function1)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/util/Try$/apply(scala.Function0)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#java/util/ServiceLoader/load(java.lang.Class,java.lang.ClassLoader)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$18/18(java.lang.String)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$19/19(java.lang.String)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#java/lang/NoClassDefFoundError/getMessage()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#java/util/ServiceConfigurationError/getCause()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/collection/immutable/List/head()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/collection/immutable/List/size()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$lookupDataSource$1/1(java.lang.String,scala.collection.immutable.List,scala.collection.immutable.List)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#java/lang/String/startsWith(java.lang.String)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#java/lang/ClassNotFoundException/ClassNotFoundException(java.lang.String,java.lang.Throwable)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$22/22()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$23/23()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/collection/immutable/$colon$colon/head()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/collection/immutable/List/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/collection/TraversableOnce/toList()
org/apache/spark/sql/execution/datasources/DataSource/createSink(org.apache.spark.sql.streaming.OutputMode)#org/apache/spark/sql/catalyst/util/CaseInsensitiveMap/getOrElse(java.lang.Object,scala.Function0)
org/apache/spark/sql/execution/datasources/DataSource/createSink(org.apache.spark.sql.streaming.OutputMode)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/createSink(org.apache.spark.sql.streaming.OutputMode)#java/lang/Class/newInstance()
org/apache/spark/sql/execution/datasources/DataSource/createSink(org.apache.spark.sql.streaming.OutputMode)#org/apache/spark/sql/streaming/OutputMode/Append()
org/apache/spark/sql/execution/datasources/DataSource/createSink(org.apache.spark.sql.streaming.OutputMode)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/DataSource/createSink(org.apache.spark.sql.streaming.OutputMode)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/DataSource/createSink(org.apache.spark.sql.streaming.OutputMode)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/DataSource/createSink(org.apache.spark.sql.streaming.OutputMode)#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$11/11(org.apache.spark.sql.execution.datasources.DataSource)
org/apache/spark/sql/execution/datasources/DataSource/createSink(org.apache.spark.sql.streaming.OutputMode)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSource/createSink(org.apache.spark.sql.streaming.OutputMode)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/DataSource/createSink(org.apache.spark.sql.streaming.OutputMode)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSource/createSink(org.apache.spark.sql.streaming.OutputMode)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/DataSource/createSink(org.apache.spark.sql.streaming.OutputMode)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/createSink(org.apache.spark.sql.streaming.OutputMode)#java/lang/UnsupportedOperationException/UnsupportedOperationException(java.lang.String)
org/apache/spark/sql/execution/datasources/DataSource/createSink(org.apache.spark.sql.streaming.OutputMode)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/DataSource/sourceSchema()#org/apache/spark/deploy/SparkHadoopUtil$/get()
org/apache/spark/sql/execution/datasources/DataSource/sourceSchema()#org/apache/spark/sql/catalyst/util/CaseInsensitiveMap/getOrElse(java.lang.Object,scala.Function0)
org/apache/spark/sql/execution/datasources/DataSource/sourceSchema()#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$9/9(org.apache.spark.sql.execution.datasources.DataSource)
org/apache/spark/sql/execution/datasources/DataSource/sourceSchema()#java/lang/Class/newInstance()
org/apache/spark/sql/execution/datasources/DataSource/sourceSchema()#org/apache/spark/sql/types/StructType/fieldNames()
org/apache/spark/sql/execution/datasources/DataSource/sourceSchema()#scala/Option/isEmpty()
org/apache/spark/sql/execution/datasources/DataSource/sourceSchema()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSource/sourceSchema()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSource/sourceSchema()#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/sourceSchema()#org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/datasources/DataSource/sourceSchema()#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/sourceSchema()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/DataSource/sourceSchema()#org/apache/hadoop/fs/FileSystem/exists(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/datasources/DataSource/sourceSchema()#org/apache/spark/deploy/SparkHadoopUtil/isGlobPath(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/datasources/DataSource/sourceSchema()#org/apache/spark/sql/types/StructType/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/DataSource/sourceSchema()#scala/Tuple2/_1()
org/apache/spark/sql/execution/datasources/DataSource/sourceSchema()#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/DataSource/sourceSchema()#org/apache/spark/sql/execution/datasources/DataSource$SourceInfo/SourceInfo(org.apache.spark.sql.execution.datasources.DataSource,java.lang.String,org.apache.spark.sql.types.StructType,scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSource/sourceSchema()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/DataSource/sourceSchema()#scala/Tuple2/_2()
org/apache/spark/sql/execution/datasources/DataSource/sourceSchema()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/DataSource/sourceSchema()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/sourceSchema()#java/lang/UnsupportedOperationException/UnsupportedOperationException(java.lang.String)
org/apache/spark/sql/execution/datasources/DataSource/sourceSchema()#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/sourceSchema()#org/apache/spark/sql/internal/SQLConf/streamingSchemaInference()
org/apache/spark/sql/execution/datasources/DataSource/sourceSchema()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/DataSource/sourceSchema()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/DataSource/sourceSchema()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/DataSource/sourceSchema()#org/apache/spark/sql/types/StructType$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSource/sourceSchema()#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/execution/datasources/DataSource/sourceSchema()#org/apache/hadoop/fs/Path/Path(java.lang.String)
org/apache/spark/sql/execution/datasources/DataSource/anonfun/4/anonfun/apply/7/anonfun/apply/8/apply()#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/datasources/DataSource/anonfun/4/anonfun/apply/7/anonfun/apply/8/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/DataSource/anonfun/4/anonfun/apply/7/anonfun/apply/8/apply()#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$4$$anonfun$apply$7/org$apache$spark$sql$execution$datasources$DataSource$$anonfun$$anonfun$$$outer()
org/apache/spark/sql/execution/datasources/DataSource/anonfun/4/anonfun/apply/7/anonfun/apply/8/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/anonfun/4/anonfun/apply/7/anonfun/apply/8/apply()#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/datasources/DataSource/anonfun/4/anonfun/apply/7/anonfun/apply/8/apply()#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/datasources/DataSource/anonfun/4/anonfun/apply/7/anonfun/apply/8/apply()#scala/Option/orNull(scala.Predef$$less$colon$less)
org/apache/spark/sql/execution/datasources/DataSource/anonfun/4/anonfun/apply/7/anonfun/apply/8/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSource/anonfun/4/anonfun/apply/7/anonfun/apply/8/apply()#scala/Predef$/$conforms()
org/apache/spark/sql/execution/datasources/DataSource/anonfun/4/anonfun/apply/7/anonfun/apply/8/apply()#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$4$$anonfun$apply$7$$anonfun$apply$8/apply()
org/apache/spark/sql/execution/datasources/DataSource/anonfun/4/anonfun/apply/7/anonfun/apply/8/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSource/anonfun/4/anonfun/apply/7/anonfun/apply/8/apply()#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$4/org$apache$spark$sql$execution$datasources$DataSource$$anonfun$$$outer()
org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/anonfun/relationFuture/1/anonfun/apply/1/apply()#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/anonfun/relationFuture/1/anonfun/apply/1/apply()#scala/collection/immutable/Map/values()
org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/anonfun/relationFuture/1/anonfun/apply/1/apply()#java/lang/OutOfMemoryError/getCause()
org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/anonfun/relationFuture/1/anonfun/apply/1/apply()#java/lang/System/nanoTime()
org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/anonfun/relationFuture/1/anonfun/apply/1/apply()#org/apache/spark/SparkContext/broadcast(java.lang.Object,scala.reflect.ClassTag)
org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/anonfun/relationFuture/1/anonfun/apply/1/apply()#org/apache/spark/sql/execution/metric/SQLMetrics$/postDriverMetricUpdates(org.apache.spark.SparkContext,java.lang.String,scala.collection.Seq)
org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/anonfun/relationFuture/1/anonfun/apply/1/apply()#org/apache/spark/sql/execution/exchange/BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1$$anonfun$1/1(org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1)
org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/anonfun/relationFuture/1/anonfun/apply/1/apply()#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/anonfun/relationFuture/1/anonfun/apply/1/apply()#scala/reflect/ClassTag$/Any()
org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/anonfun/relationFuture/1/anonfun/apply/1/apply()#org/apache/spark/sql/catalyst/plans/physical/BroadcastMode/transform(org.apache.spark.sql.catalyst.InternalRow[])
org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/anonfun/relationFuture/1/anonfun/apply/1/apply()#java/lang/OutOfMemoryError/initCause(java.lang.Throwable)
org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/anonfun/relationFuture/1/anonfun/apply/1/apply()#org/apache/spark/SparkException/SparkException(java.lang.String)
org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/anonfun/relationFuture/1/anonfun/apply/1/apply()#org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/longMetric(java.lang.String)
org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/anonfun/relationFuture/1/anonfun/apply/1/apply()#scala/Predef$/longArrayOps(long[])
org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/anonfun/relationFuture/1/anonfun/apply/1/apply()#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/anonfun/relationFuture/1/anonfun/apply/1/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/anonfun/relationFuture/1/anonfun/apply/1/apply()#scala/collection/Iterable/toSeq()
org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/anonfun/relationFuture/1/anonfun/apply/1/apply()#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/anonfun/relationFuture/1/anonfun/apply/1/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/anonfun/relationFuture/1/anonfun/apply/1/apply()#org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/sparkContext()
org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/anonfun/relationFuture/1/anonfun/apply/1/apply()#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/anonfun/relationFuture/1/anonfun/apply/1/apply()#scala/collection/mutable/ArrayOps/sum(scala.math.Numeric)
org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/anonfun/relationFuture/1/anonfun/apply/1/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/anonfun/relationFuture/1/anonfun/apply/1/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/anonfun/relationFuture/1/anonfun/apply/1/apply()#org/apache/spark/internal/config/ConfigEntry/key()
org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/anonfun/relationFuture/1/anonfun/apply/1/apply()#org/apache/spark/sql/internal/SQLConf$/AUTO_BROADCASTJOIN_THRESHOLD()
org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/anonfun/relationFuture/1/anonfun/apply/1/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/anonfun/relationFuture/1/anonfun/apply/1/apply()#java/lang/OutOfMemoryError/OutOfMemoryError(java.lang.String)
org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/anonfun/relationFuture/1/anonfun/apply/1/apply()#org/apache/spark/sql/execution/exchange/BroadcastExchangeExec$$anonfun$relationFuture$1/org$apache$spark$sql$execution$exchange$BroadcastExchangeExec$$anonfun$$$outer()
org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/anonfun/relationFuture/1/anonfun/apply/1/apply()#org/apache/spark/sql/execution/exchange/BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1/apply()
org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/anonfun/relationFuture/1/anonfun/apply/1/apply()#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/anonfun/relationFuture/1/anonfun/apply/1/apply()#scala/reflect/ClassTag$/Long()
org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/anonfun/relationFuture/1/anonfun/apply/1/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/exchange/BroadcastExchangeExec/anonfun/relationFuture/1/anonfun/apply/1/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/GenerateExec/org$apache$spark$sql$execution$GenerateExec$$codeGenAccessor(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType,boolean,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/ExprCode(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/GenerateExec/org$apache$spark$sql$execution$GenerateExec$$codeGenAccessor(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType,boolean,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/getValue(java.lang.String,org.apache.spark.sql.types.DataType,java.lang.String)
org/apache/spark/sql/execution/GenerateExec/org$apache$spark$sql$execution$GenerateExec$$codeGenAccessor(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType,boolean,scala.collection.Seq)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/GenerateExec/org$apache$spark$sql$execution$GenerateExec$$codeGenAccessor(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType,boolean,scala.collection.Seq)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/GenerateExec/org$apache$spark$sql$execution$GenerateExec$$codeGenAccessor(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType,boolean,scala.collection.Seq)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/GenerateExec/org$apache$spark$sql$execution$GenerateExec$$codeGenAccessor(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType,boolean,scala.collection.Seq)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/GenerateExec/org$apache$spark$sql$execution$GenerateExec$$codeGenAccessor(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType,boolean,scala.collection.Seq)#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/execution/GenerateExec/org$apache$spark$sql$execution$GenerateExec$$codeGenAccessor(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType,boolean,scala.collection.Seq)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/GenerateExec/org$apache$spark$sql$execution$GenerateExec$$codeGenAccessor(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType,boolean,scala.collection.Seq)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/GenerateExec/org$apache$spark$sql$execution$GenerateExec$$codeGenAccessor(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType,boolean,scala.collection.Seq)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/GenerateExec/org$apache$spark$sql$execution$GenerateExec$$codeGenAccessor(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType,boolean,scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/GenerateExec/org$apache$spark$sql$execution$GenerateExec$$codeGenAccessor(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType,boolean,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/defaultValue(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/GenerateExec/org$apache$spark$sql$execution$GenerateExec$$codeGenAccessor(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType,boolean,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/freshName(java.lang.String)
org/apache/spark/sql/execution/GenerateExec/org$apache$spark$sql$execution$GenerateExec$$codeGenAccessor(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType,boolean,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/javaType(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/GenerateExec/org$apache$spark$sql$execution$GenerateExec$$codeGenAccessor(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType,boolean,scala.collection.Seq)#org/apache/spark/sql/execution/GenerateExec$$anonfun$11/11(org.apache.spark.sql.execution.GenerateExec,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/GenerateExec/org$apache$spark$sql$execution$GenerateExec$$codeGenAccessor(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType,boolean,scala.collection.Seq)#scala/collection/Seq/mkString(java.lang.String)
org/apache/spark/sql/execution/GenerateExec/org$apache$spark$sql$execution$GenerateExec$$codeGenAccessor(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType,boolean,scala.collection.Seq)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/GenerateExec/codeGenTraversableOnce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/Seq/zipWithIndex(scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/GenerateExec/codeGenTraversableOnce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/GenerateExec/codeGenTraversableOnce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/GenerateExec/codeGenTraversableOnce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/execution/GenerateExec$$anonfun$10/10(org.apache.spark.sql.execution.GenerateExec,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,scala.collection.Seq)
org/apache/spark/sql/execution/GenerateExec/codeGenTraversableOnce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/GenerateExec/codeGenTraversableOnce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/GenerateExec/codeGenTraversableOnce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/execution/GenerateExec/codeGenTraversableOnce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/GenerateExec/codeGenTraversableOnce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/freshName(java.lang.String)
org/apache/spark/sql/execution/GenerateExec/codeGenTraversableOnce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/GenerateExec/codeGenTraversableOnce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/GenerateExec/codeGenTraversableOnce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/execution/GenerateExec$$anonfun$9/9(org.apache.spark.sql.execution.GenerateExec,java.lang.String)
org/apache/spark/sql/execution/GenerateExec/codeGenTraversableOnce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/code()
org/apache/spark/sql/execution/GenerateExec/codeGenTraversableOnce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/Expression/genCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)
org/apache/spark/sql/execution/GenerateExec/codeGenTraversableOnce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/value()
org/apache/spark/sql/execution/GenerateExec/codeGenTraversableOnce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/GenerateExec/codeGenTraversableOnce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/GenerateExec/codeGenTraversableOnce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/types/ArrayType/elementType()
org/apache/spark/sql/execution/GenerateExec/codeGenTraversableOnce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/GenerateExec/codeGenTraversableOnce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/Expression/dataType()
org/apache/spark/sql/execution/GenerateExec/codeGenTraversableOnce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/mutable/ArrayOps/toSeq()
org/apache/spark/sql/execution/GenerateExec/codeGenTraversableOnce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/GenerateExec/codeGenTraversableOnce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/execution/GenerateExec$$anonfun$8/8(org.apache.spark.sql.execution.GenerateExec,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode,scala.collection.Seq)
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/Seq$/empty()
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/execution/GenerateExec$$anonfun$7/7(org.apache.spark.sql.execution.GenerateExec,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/freshName(java.lang.String)
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/ExprCode(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/Seq/zipWithIndex(scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/code()
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/Expression/genCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/Tuple3/_1()
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/Tuple3/_2()
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/value()
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/Tuple3/_3()
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/CollectionGenerator/collectionType()
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/types/MapType/valueType()
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/isNull()
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/CollectionGenerator/inline()
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/types/ArrayType/elementType()
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/Tuple3/Tuple3(java.lang.Object,java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/types/MapType/valueContainsNull()
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/execution/GenerateExec$$anonfun$6/6(org.apache.spark.sql.execution.GenerateExec,java.lang.String)
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/mutable/ArrayOps/toSeq()
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/types/MapType/keyType()
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/types/ArrayType/containsNull()
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/GenerateExec/codeGenCollection(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,org.apache.spark.sql.catalyst.expressions.CollectionGenerator,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/CollectionGenerator/position()
org/apache/spark/sql/execution/datasources/DataSource/anonfun/14/apply(java.lang.String)#org/apache/spark/deploy/SparkHadoopUtil$/get()
org/apache/spark/sql/execution/datasources/DataSource/anonfun/14/apply(java.lang.String)#org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/datasources/DataSource/anonfun/14/apply(java.lang.String)#scala/collection/Seq/isEmpty()
org/apache/spark/sql/execution/datasources/DataSource/anonfun/14/apply(java.lang.String)#org/apache/hadoop/fs/FileSystem/getUri()
org/apache/spark/sql/execution/datasources/DataSource/anonfun/14/apply(java.lang.String)#org/apache/hadoop/fs/FileSystem/exists(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/datasources/DataSource/anonfun/14/apply(java.lang.String)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/DataSource/anonfun/14/apply(java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/DataSource/anonfun/14/apply(java.lang.String)#org/apache/spark/deploy/SparkHadoopUtil/globPathIfNecessary(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/datasources/DataSource/anonfun/14/apply(java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSource/anonfun/14/apply(java.lang.String)#scala/collection/Seq/head()
org/apache/spark/sql/execution/datasources/DataSource/anonfun/14/apply(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/DataSource/anonfun/14/apply(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/DataSource/anonfun/14/apply(java.lang.String)#org/apache/hadoop/fs/Path/Path(java.lang.String)
org/apache/spark/sql/execution/datasources/DataSource/anonfun/14/apply(java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSource/anonfun/14/apply(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/DataSource/anonfun/14/apply(java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/anonfun/14/apply(java.lang.String)#org/apache/hadoop/fs/Path/makeQualified(java.net.URI,org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/datasources/DataSource/anonfun/14/apply(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/DataSource/anonfun/14/apply(java.lang.String)#org/apache/hadoop/fs/FileSystem/getWorkingDirectory()
org/apache/spark/sql/execution/window/WindowExec/anonfun/3/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/BoundReference/BoundReference(int,org.apache.spark.sql.types.DataType,boolean)
org/apache/spark/sql/execution/window/WindowExec/anonfun/3/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/window/WindowExec/anonfun/3/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/SortOrder$/apply(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.SortDirection,scala.collection.immutable.Set)
org/apache/spark/sql/execution/window/WindowExec/anonfun/3/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/window/WindowExec/anonfun/3/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/SortOrder$/apply$default$3()
org/apache/spark/sql/execution/window/WindowExec/anonfun/3/apply(scala.Tuple2)#scala/Tuple2/_2$mcI$sp()
org/apache/spark/sql/execution/window/WindowExec/anonfun/3/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/SortOrder/dataType()
org/apache/spark/sql/execution/window/WindowExec/anonfun/3/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/SortOrder/direction()
org/apache/spark/sql/execution/window/WindowExec/anonfun/3/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/SortOrder/nullable()
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/anonfun/4/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/anonfun/4/apply()#org/apache/spark/sql/internal/SQLConf$/FILE_SOURCE_LOG_COMPACT_INTERVAL()
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/anonfun/4/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/anonfun/4/apply()#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/anonfun/4/apply()#org/apache/spark/internal/config/ConfigEntry/key()
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/anonfun/4/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/anonfun/4/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/anonfun/4/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/anonfun/4/apply()#org/apache/spark/sql/execution/streaming/FileStreamSourceLog$$anonfun$4/apply()
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/anonfun/4/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/anonfun/4/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/r/MapPartitionsRWrapper/apply(scala.collection.Iterator)#scala/Tuple3/_2()
org/apache/spark/sql/execution/r/MapPartitionsRWrapper/apply(scala.collection.Iterator)#scala/Tuple3/_3()
org/apache/spark/sql/execution/r/MapPartitionsRWrapper/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/r/MapPartitionsRWrapper$$anonfun$1/1(org.apache.spark.sql.execution.r.MapPartitionsRWrapper)
org/apache/spark/sql/execution/r/MapPartitionsRWrapper/apply(scala.collection.Iterator)#org/apache/spark/api/r/RRunner/RRunner(byte[],java.lang.String,java.lang.String,byte[],org.apache.spark.broadcast.Broadcast[],int,boolean,java.lang.String[],int)
org/apache/spark/sql/execution/r/MapPartitionsRWrapper/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/r/MapPartitionsRWrapper$$anonfun$2/2(org.apache.spark.sql.execution.r.MapPartitionsRWrapper)
org/apache/spark/sql/execution/r/MapPartitionsRWrapper/apply(scala.collection.Iterator)#org/apache/spark/sql/api/r/SQLUtils$/SERIALIZED_R_DATA_SCHEMA()
org/apache/spark/sql/execution/r/MapPartitionsRWrapper/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/r/MapPartitionsRWrapper$$anonfun$apply$1/1(org.apache.spark.sql.execution.r.MapPartitionsRWrapper)
org/apache/spark/sql/execution/r/MapPartitionsRWrapper/apply(scala.collection.Iterator)#org/apache/spark/api/r/SerializationFormats$/ROW()
org/apache/spark/sql/execution/r/MapPartitionsRWrapper/apply(scala.collection.Iterator)#org/apache/spark/api/r/RRunner/compute(scala.collection.Iterator,int)
org/apache/spark/sql/execution/r/MapPartitionsRWrapper/apply(scala.collection.Iterator)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/r/MapPartitionsRWrapper/apply(scala.collection.Iterator)#org/apache/spark/sql/types/StructType/fieldNames()
org/apache/spark/sql/execution/r/MapPartitionsRWrapper/apply(scala.collection.Iterator)#scala/Tuple3/_1()
org/apache/spark/sql/execution/r/MapPartitionsRWrapper/apply(scala.collection.Iterator)#org/apache/spark/api/r/RRunnerModes$/DATAFRAME_DAPPLY()
org/apache/spark/sql/execution/r/MapPartitionsRWrapper/apply(scala.collection.Iterator)#org/apache/spark/api/r/SerializationFormats$/BYTE()
org/apache/spark/sql/execution/r/MapPartitionsRWrapper/apply(scala.collection.Iterator)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/r/MapPartitionsRWrapper/apply(scala.collection.Iterator)#scala/Tuple3/Tuple3(java.lang.Object,java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/r/MapPartitionsRWrapper/apply(scala.collection.Iterator)#scala/collection/Iterator/map(scala.Function1)
org/apache/spark/sql/execution/r/MapPartitionsRWrapper/apply(scala.collection.Iterator)#org/apache/spark/api/r/RRunner$/$lessinit$greater$default$6()
org/apache/spark/sql/execution/r/MapPartitionsRWrapper/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/r/MapPartitionsRWrapper$$anonfun$apply$2/2(org.apache.spark.sql.execution.r.MapPartitionsRWrapper)
org/apache/spark/sql/Dataset/anonfun/17/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/expressions/EqualTo/left()
org/apache/spark/sql/Dataset/anonfun/17/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/Dataset$$anonfun$17$$anonfun$apply$1$$anonfun$applyOrElse$2/2(org.apache.spark.sql.Dataset$$anonfun$17$$anonfun$apply$1)
org/apache/spark/sql/Dataset/anonfun/17/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/Dataset$$anonfun$17$$anonfun$apply$1$$anonfun$applyOrElse$1/1(org.apache.spark.sql.Dataset$$anonfun$17$$anonfun$apply$1)
org/apache/spark/sql/Dataset/anonfun/17/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/Dataset$$anonfun$17/org$apache$spark$sql$Dataset$$anonfun$$$outer()
org/apache/spark/sql/Dataset/anonfun/17/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/expressions/AttributeReference/name()
org/apache/spark/sql/Dataset/anonfun/17/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/expressions/EqualTo/EqualTo(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/Dataset/anonfun/17/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/expressions/EqualTo/right()
org/apache/spark/sql/Dataset/anonfun/17/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/expressions/AttributeReference/sameRef(org.apache.spark.sql.catalyst.expressions.AttributeReference)
org/apache/spark/sql/Dataset/anonfun/17/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/newTaskTempFile(org.apache.hadoop.mapreduce.TaskAttemptContext,scala.Option,java.lang.String)#scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object)
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/newTaskTempFile(org.apache.hadoop.mapreduce.TaskAttemptContext,scala.Option,java.lang.String)#org/apache/hadoop/mapreduce/TaskAttemptID/getTaskID()
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/newTaskTempFile(org.apache.hadoop.mapreduce.TaskAttemptContext,scala.Option,java.lang.String)#java/util/UUID/randomUUID()
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/newTaskTempFile(org.apache.hadoop.mapreduce.TaskAttemptContext,scala.Option,java.lang.String)#org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol$$anonfun$3/3(org.apache.spark.sql.execution.streaming.ManifestFileCommitProtocol,java.lang.String)
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/newTaskTempFile(org.apache.hadoop.mapreduce.TaskAttemptContext,scala.Option,java.lang.String)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/newTaskTempFile(org.apache.hadoop.mapreduce.TaskAttemptContext,scala.Option,java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/newTaskTempFile(org.apache.hadoop.mapreduce.TaskAttemptContext,scala.Option,java.lang.String)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/newTaskTempFile(org.apache.hadoop.mapreduce.TaskAttemptContext,scala.Option,java.lang.String)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/newTaskTempFile(org.apache.hadoop.mapreduce.TaskAttemptContext,scala.Option,java.lang.String)#java/util/UUID/toString()
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/newTaskTempFile(org.apache.hadoop.mapreduce.TaskAttemptContext,scala.Option,java.lang.String)#org/apache/hadoop/mapreduce/TaskID/getId()
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/newTaskTempFile(org.apache.hadoop.mapreduce.TaskAttemptContext,scala.Option,java.lang.String)#scala/collection/immutable/StringOps/format(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/newTaskTempFile(org.apache.hadoop.mapreduce.TaskAttemptContext,scala.Option,java.lang.String)#org/apache/hadoop/mapreduce/TaskAttemptContext/getTaskAttemptID()
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/newTaskTempFile(org.apache.hadoop.mapreduce.TaskAttemptContext,scala.Option,java.lang.String)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/newTaskTempFile(org.apache.hadoop.mapreduce.TaskAttemptContext,scala.Option,java.lang.String)#org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol$$anonfun$2/2(org.apache.spark.sql.execution.streaming.ManifestFileCommitProtocol,java.lang.String)
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext)#scala/collection/Seq$/empty()
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext)#scala/collection/mutable/ArrayBuffer/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext)#scala/collection/mutable/ArrayBuffer$/canBuildFrom()
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol$$anonfun$4/4(org.apache.spark.sql.execution.streaming.ManifestFileCommitProtocol,org.apache.hadoop.fs.FileSystem)
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext)#scala/collection/mutable/ArrayBuffer/head()
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext)#scala/collection/mutable/ArrayBuffer/nonEmpty()
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/hadoop/mapreduce/TaskAttemptContext/getConfiguration()
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/spark/internal/io/FileCommitProtocol$TaskCommitMessage/TaskCommitMessage(java.lang.Object)
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/commitTask(org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/hadoop/fs/Path/Path(java.lang.String)
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/commitJob(org.apache.hadoop.mapreduce.JobContext,scala.collection.Seq)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/commitJob(org.apache.hadoop.mapreduce.JobContext,scala.collection.Seq)#java/lang/IllegalStateException/IllegalStateException(java.lang.String)
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/commitJob(org.apache.hadoop.mapreduce.JobContext,scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/commitJob(org.apache.hadoop.mapreduce.JobContext,scala.collection.Seq)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/commitJob(org.apache.hadoop.mapreduce.JobContext,scala.collection.Seq)#org/apache/spark/sql/execution/streaming/FileStreamSinkLog/add(long,java.lang.Object[])
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/commitJob(org.apache.hadoop.mapreduce.JobContext,scala.collection.Seq)#scala/Predef$/require(boolean,scala.Function0)
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/commitJob(org.apache.hadoop.mapreduce.JobContext,scala.collection.Seq)#org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol$$anonfun$commitJob$2/2(org.apache.spark.sql.execution.streaming.ManifestFileCommitProtocol)
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/commitJob(org.apache.hadoop.mapreduce.JobContext,scala.collection.Seq)#org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol$$anonfun$commitJob$1/1(org.apache.spark.sql.execution.streaming.ManifestFileCommitProtocol)
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/commitJob(org.apache.hadoop.mapreduce.JobContext,scala.collection.Seq)#scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/commitJob(org.apache.hadoop.mapreduce.JobContext,scala.collection.Seq)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/commitJob(org.apache.hadoop.mapreduce.JobContext,scala.collection.Seq)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/commitJob(org.apache.hadoop.mapreduce.JobContext,scala.collection.Seq)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/commitJob(org.apache.hadoop.mapreduce.JobContext,scala.collection.Seq)#org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol$$anonfun$1/1(org.apache.spark.sql.execution.streaming.ManifestFileCommitProtocol)
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/commitJob(org.apache.hadoop.mapreduce.JobContext,scala.collection.Seq)#scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/streaming/ManifestFileCommitProtocol/commitJob(org.apache.hadoop.mapreduce.JobContext,scala.collection.Seq)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/setSchema(org.apache.spark.sql.types.StructType,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport$/setSchema(org.apache.spark.sql.types.StructType,org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/setSchema(org.apache.spark.sql.types.StructType,org.apache.hadoop.conf.Configuration)#scala/collection/IterableLike/foreach(scala.Function1)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/setSchema(org.apache.spark.sql.types.StructType,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/conf/Configuration/set(java.lang.String,java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/setSchema(org.apache.spark.sql.types.StructType,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport$$anonfun$setSchema$1/1()
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/setSchema(org.apache.spark.sql.types.StructType,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport$$anonfun$setSchema$2/2()
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/setSchema(org.apache.spark.sql.types.StructType,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/types/StructType/json()
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/setSchema(org.apache.spark.sql.types.StructType,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/types/StructType/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/setSchema(org.apache.spark.sql.types.StructType,org.apache.hadoop.conf.Configuration)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/setSchema(org.apache.spark.sql.types.StructType,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/conf/Configuration/setIfUnset(java.lang.String,java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/setSchema(org.apache.spark.sql.types.StructType,org.apache.hadoop.conf.Configuration)#org/apache/parquet/column/ParquetProperties$WriterVersion/toString()
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/setSchema(org.apache.spark.sql.types.StructType,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport$/SPARK_ROW_SCHEMA()
org/apache/spark/sql/execution/streaming/TextSocketSource/typecreator2/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/selectType(scala.reflect.api.Symbols$SymbolApi,java.lang.String)
org/apache/spark/sql/execution/streaming/TextSocketSource/typecreator2/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ModuleSymbolApi/asModule()
org/apache/spark/sql/execution/streaming/TextSocketSource/typecreator2/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticPackage(java.lang.String)
org/apache/spark/sql/execution/streaming/TextSocketSource/typecreator2/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$InternalApi/reificationSupport()
org/apache/spark/sql/execution/streaming/TextSocketSource/typecreator2/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ClassSymbolApi/asType()
org/apache/spark/sql/execution/streaming/TextSocketSource/typecreator2/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ModuleSymbolApi/moduleClass()
org/apache/spark/sql/execution/streaming/TextSocketSource/typecreator2/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/TypeRef(scala.reflect.api.Types$TypeApi,scala.reflect.api.Symbols$SymbolApi,scala.collection.immutable.List)
org/apache/spark/sql/execution/streaming/TextSocketSource/typecreator2/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticClass(java.lang.String)
org/apache/spark/sql/execution/streaming/TextSocketSource/typecreator2/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/universe()
org/apache/spark/sql/execution/streaming/TextSocketSource/typecreator2/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$TypeSymbolApi/toTypeConstructor()
org/apache/spark/sql/execution/streaming/TextSocketSource/typecreator2/1/apply(scala.reflect.api.Mirror)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/TextSocketSource/typecreator2/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/SingleType(scala.reflect.api.Types$TypeApi,scala.reflect.api.Symbols$SymbolApi)
org/apache/spark/sql/execution/streaming/TextSocketSource/typecreator2/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticModule(java.lang.String)
org/apache/spark/sql/execution/streaming/TextSocketSource/typecreator2/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/ThisType(scala.reflect.api.Symbols$SymbolApi)
org/apache/spark/sql/execution/streaming/TextSocketSource/typecreator2/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Universe/internal()
org/apache/spark/sql/execution/streaming/TextSocketSource/typecreator2/1/apply(scala.reflect.api.Mirror)#scala/collection/immutable/List$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/ui/SQLListener/onJobStart(org.apache.spark.scheduler.SparkListenerJobStart)#scala/Option/foreach(scala.Function1)
org/apache/spark/sql/execution/ui/SQLListener/onJobStart(org.apache.spark.scheduler.SparkListenerJobStart)#org/apache/spark/sql/execution/ui/SQLListener$$anonfun$onJobStart$1/1(org.apache.spark.sql.execution.ui.SQLListener,long,int,scala.collection.Seq)
org/apache/spark/sql/execution/ui/SQLListener/onJobStart(org.apache.spark.scheduler.SparkListenerJobStart)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/ui/SQLListener/onJobStart(org.apache.spark.scheduler.SparkListenerJobStart)#org/apache/spark/sql/execution/SQLExecution$/EXECUTION_ID_KEY()
org/apache/spark/sql/execution/ui/SQLListener/onJobStart(org.apache.spark.scheduler.SparkListenerJobStart)#scala/collection/immutable/StringOps/toLong()
org/apache/spark/sql/execution/ui/SQLListener/onJobStart(org.apache.spark.scheduler.SparkListenerJobStart)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/ui/SQLListener/onJobStart(org.apache.spark.scheduler.SparkListenerJobStart)#org/apache/spark/scheduler/SparkListenerJobStart/jobId()
org/apache/spark/sql/execution/ui/SQLListener/onJobStart(org.apache.spark.scheduler.SparkListenerJobStart)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/ui/SQLListener/onJobStart(org.apache.spark.scheduler.SparkListenerJobStart)#java/util/Properties/getProperty(java.lang.String)
org/apache/spark/sql/execution/ui/SQLListener/onJobStart(org.apache.spark.scheduler.SparkListenerJobStart)#org/apache/spark/scheduler/SparkListenerJobStart/properties()
org/apache/spark/sql/execution/ui/SQLListener/onJobStart(org.apache.spark.scheduler.SparkListenerJobStart)#org/apache/spark/scheduler/SparkListenerJobStart/stageIds()
org/apache/spark/sql/execution/ui/SQLListener/onJobStart(org.apache.spark.scheduler.SparkListenerJobStart)#scala/collection/mutable/HashMap/get(java.lang.Object)
org/apache/spark/sql/execution/ui/SQLListener/onOtherEvent(org.apache.spark.scheduler.SparkListenerEvent)#scala/Option/foreach(scala.Function1)
org/apache/spark/sql/execution/ui/SQLListener/onOtherEvent(org.apache.spark.scheduler.SparkListenerEvent)#org/apache/spark/sql/execution/ui/SparkPlanGraph$/apply(org.apache.spark.sql.execution.SparkPlanInfo)
org/apache/spark/sql/execution/ui/SQLListener/onOtherEvent(org.apache.spark.scheduler.SparkListenerEvent)#org/apache/spark/sql/execution/ui/SQLListener$$anonfun$4/4(org.apache.spark.sql.execution.ui.SQLListener)
org/apache/spark/sql/execution/ui/SQLListener/onOtherEvent(org.apache.spark.scheduler.SparkListenerEvent)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/ui/SQLListener/onOtherEvent(org.apache.spark.scheduler.SparkListenerEvent)#scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/ui/SQLListener/onOtherEvent(org.apache.spark.scheduler.SparkListenerEvent)#org/apache/spark/sql/execution/ui/SQLListener$$anonfun$onOtherEvent$1/1(org.apache.spark.sql.execution.ui.SQLListener,long,long)
org/apache/spark/sql/execution/ui/SQLListener/onOtherEvent(org.apache.spark.scheduler.SparkListenerEvent)#scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/ui/SQLListener/onOtherEvent(org.apache.spark.scheduler.SparkListenerEvent)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/ui/SQLListener/onOtherEvent(org.apache.spark.scheduler.SparkListenerEvent)#scala/collection/Seq/toMap(scala.Predef$$less$colon$less)
org/apache/spark/sql/execution/ui/SQLListener/onOtherEvent(org.apache.spark.scheduler.SparkListenerEvent)#scala/collection/mutable/HashMap/get(java.lang.Object)
org/apache/spark/sql/execution/ui/SQLListener/onOtherEvent(org.apache.spark.scheduler.SparkListenerEvent)#scala/Predef$/$conforms()
org/apache/spark/sql/execution/ui/SQLListener/onOtherEvent(org.apache.spark.scheduler.SparkListenerEvent)#org/apache/spark/sql/execution/ui/SQLListener$$anonfun$onOtherEvent$2/2(org.apache.spark.sql.execution.ui.SQLListener,scala.collection.Seq)
org/apache/spark/sql/execution/ui/SQLListener/getExecutionMetrics(long)#scala/Some/x()
org/apache/spark/sql/execution/ui/SQLListener/getExecutionMetrics(long)#scala/Predef$/Map()
org/apache/spark/sql/execution/ui/SQLListener/getExecutionMetrics(long)#scala/collection/mutable/Buffer/filter(scala.Function1)
org/apache/spark/sql/execution/ui/SQLListener/getExecutionMetrics(long)#scala/collection/mutable/HashMap/toSeq()
org/apache/spark/sql/execution/ui/SQLListener/getExecutionMetrics(long)#org/apache/spark/sql/execution/ui/SQLListener$$anonfun$getExecutionMetrics$1/1(org.apache.spark.sql.execution.ui.SQLListener,org.apache.spark.sql.execution.ui.SQLExecutionUIData)
org/apache/spark/sql/execution/ui/SQLListener/getExecutionMetrics(long)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/ui/SQLListener/getExecutionMetrics(long)#scala/collection/mutable/ArrayBuffer$/canBuildFrom()
org/apache/spark/sql/execution/ui/SQLListener/getExecutionMetrics(long)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/ui/SQLListener/getExecutionMetrics(long)#scala/collection/immutable/Map$/empty()
org/apache/spark/sql/execution/ui/SQLListener/getExecutionMetrics(long)#org/apache/spark/sql/execution/ui/SQLListener$$anonfun$6/6(org.apache.spark.sql.execution.ui.SQLListener,org.apache.spark.sql.execution.ui.SQLExecutionUIData)
org/apache/spark/sql/execution/ui/SQLListener/getExecutionMetrics(long)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/ui/SQLListener/getExecutionMetrics(long)#scala/collection/mutable/ArrayBuffer/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/ui/SQLListener/getExecutionMetrics(long)#org/apache/spark/sql/execution/ui/SQLListener$$anonfun$5/5(org.apache.spark.sql.execution.ui.SQLListener)
org/apache/spark/sql/execution/ui/SQLListener/getExecutionMetrics(long)#scala/collection/mutable/HashMap/get(java.lang.Object)
org/apache/spark/sql/execution/ui/SQLListener/getExecutionMetrics(long)#scala/collection/mutable/ArrayBuffer/$plus$plus(scala.collection.GenTraversableOnce)
org/apache/spark/sql/execution/ui/SQLListener/updateTaskAccumulatorValues(long,int,int,scala.collection.Seq,boolean)#scala/Some/x()
org/apache/spark/sql/execution/ui/SQLListener/updateTaskAccumulatorValues(long,int,int,scala.collection.Seq,boolean)#scala/collection/mutable/HashMap/update(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/ui/SQLListener/updateTaskAccumulatorValues(long,int,int,scala.collection.Seq,boolean)#scala/collection/Seq/filter(scala.Function1)
org/apache/spark/sql/execution/ui/SQLListener/updateTaskAccumulatorValues(long,int,int,scala.collection.Seq,boolean)#scala/collection/mutable/HashMap/get(java.lang.Object)
org/apache/spark/sql/execution/ui/SQLListener/updateTaskAccumulatorValues(long,int,int,scala.collection.Seq,boolean)#org/apache/spark/sql/execution/ui/SQLListener$$anonfun$3/3(org.apache.spark.sql.execution.ui.SQLListener)
org/apache/spark/sql/execution/ui/SQLListener/updateTaskAccumulatorValues(long,int,int,scala.collection.Seq,boolean)#org/apache/spark/sql/execution/ui/SQLListener$$anonfun$2/2(org.apache.spark.sql.execution.ui.SQLListener)
org/apache/spark/sql/execution/ui/SQLListener/updateTaskAccumulatorValues(long,int,int,scala.collection.Seq,boolean)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/ui/SQLListener/updateTaskAccumulatorValues(long,int,int,scala.collection.Seq,boolean)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/ui/SQLListener/updateTaskAccumulatorValues(long,int,int,scala.collection.Seq,boolean)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/ui/SQLListener/updateTaskAccumulatorValues(long,int,int,scala.collection.Seq,boolean)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/ui/SQLListener/updateTaskAccumulatorValues(long,int,int,scala.collection.Seq,boolean)#org/apache/spark/sql/execution/ui/SQLListener$$anonfun$updateTaskAccumulatorValues$1/1(org.apache.spark.sql.execution.ui.SQLListener,int,org.apache.spark.sql.execution.ui.SQLStageMetrics)
org/apache/spark/sql/execution/ui/SQLListener/updateTaskAccumulatorValues(long,int,int,scala.collection.Seq,boolean)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/ui/SQLListener/onTaskEnd(org.apache.spark.scheduler.SparkListenerTaskEnd)#org/apache/spark/sql/execution/ui/SQLListener$$anonfun$onTaskEnd$1/1(org.apache.spark.sql.execution.ui.SQLListener)
org/apache/spark/sql/execution/ui/SQLListener/onTaskEnd(org.apache.spark.scheduler.SparkListenerTaskEnd)#org/apache/spark/executor/TaskMetrics/externalAccums()
org/apache/spark/sql/execution/ui/SQLListener/onTaskEnd(org.apache.spark.scheduler.SparkListenerTaskEnd)#org/apache/spark/scheduler/SparkListenerTaskEnd/stageId()
org/apache/spark/sql/execution/ui/SQLListener/onTaskEnd(org.apache.spark.scheduler.SparkListenerTaskEnd)#scala/collection/mutable/ArrayBuffer/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/ui/SQLListener/onTaskEnd(org.apache.spark.scheduler.SparkListenerTaskEnd)#org/apache/spark/scheduler/SparkListenerTaskEnd/taskMetrics()
org/apache/spark/sql/execution/ui/SQLListener/onTaskEnd(org.apache.spark.scheduler.SparkListenerTaskEnd)#org/apache/spark/scheduler/SparkListenerTaskEnd/stageAttemptId()
org/apache/spark/sql/execution/ui/SQLListener/onTaskEnd(org.apache.spark.scheduler.SparkListenerTaskEnd)#scala/collection/mutable/ArrayBuffer$/canBuildFrom()
org/apache/spark/sql/execution/ui/SQLListener/onTaskEnd(org.apache.spark.scheduler.SparkListenerTaskEnd)#org/apache/spark/scheduler/TaskInfo/taskId()
org/apache/spark/sql/execution/ui/SQLListener/onTaskEnd(org.apache.spark.scheduler.SparkListenerTaskEnd)#org/apache/spark/scheduler/SparkListenerTaskEnd/taskInfo()
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/alterTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/types/StructField/dataType()
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand$$anonfun$2/2(org.apache.spark.sql.execution.command.AlterTableChangeColumnCommand,org.apache.spark.sql.types.StructField)
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$1()
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$2()
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$17()
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/internal/SQLConf/resolver()
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/types/StructType/StructType(org.apache.spark.sql.types.StructField[])
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/empty()
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/types/StructField/name()
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/DDLUtils$/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$3()
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$10()
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/schema()
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$11()
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$5()
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$12()
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$6()
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$13()
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$7()
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$15()
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$8()
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$16()
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$9()
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/joins/HashJoin/class/org$apache$spark$sql$execution$joins$HashJoin$$x$8(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/joins/HashJoin$$anonfun$3/3(org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/joins/HashJoin/class/org$apache$spark$sql$execution$joins$HashJoin$$x$8(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/joins/HashJoin$$anonfun$4/4(org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/joins/HashJoin/class/org$apache$spark$sql$execution$joins$HashJoin$$x$8(org.apache.spark.sql.execution.SparkPlan)#scala/Tuple2/_1()
org/apache/spark/sql/execution/joins/HashJoin/class/org$apache$spark$sql$execution$joins$HashJoin$$x$8(org.apache.spark.sql.execution.SparkPlan)#scala/Tuple2/_2()
org/apache/spark/sql/execution/joins/HashJoin/class/org$apache$spark$sql$execution$joins$HashJoin$$x$8(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/joins/HashJoin$$anonfun$2/2(org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/joins/HashJoin/class/org$apache$spark$sql$execution$joins$HashJoin$$x$8(org.apache.spark.sql.execution.SparkPlan)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/joins/HashJoin/class/org$apache$spark$sql$execution$joins$HashJoin$$x$8(org.apache.spark.sql.execution.SparkPlan)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/joins/HashJoin/class/org$apache$spark$sql$execution$joins$HashJoin$$x$8(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/joins/HashJoin$$anonfun$5/5(org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/joins/HashJoin/class/org$apache$spark$sql$execution$joins$HashJoin$$x$8(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/joins/HashJoin/class/org$apache$spark$sql$execution$joins$HashJoin$$x$8(org.apache.spark.sql.execution.SparkPlan)#scala/Predef$/require(boolean,scala.Function0)
org/apache/spark/sql/execution/joins/HashJoin/class/org$apache$spark$sql$execution$joins$HashJoin$$x$8(org.apache.spark.sql.execution.SparkPlan)#scala/runtime/BoxesRunTime/equals(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/joins/HashJoin/class/org$apache$spark$sql$execution$joins$HashJoin$$x$8(org.apache.spark.sql.execution.SparkPlan)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/joins/HashJoin/class/org$apache$spark$sql$execution$joins$HashJoin$$x$8(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/joins/HashJoin$/rewriteKeyExpr(scala.collection.Seq)
org/apache/spark/sql/execution/joins/HashJoin/class/org$apache$spark$sql$execution$joins$HashJoin$$x$8(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/joins/HashJoin$$anonfun$1/1(org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/joins/HashJoin/class/org$apache$spark$sql$execution$joins$HashJoin$$x$8(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/joins/HashJoin/class/join(org.apache.spark.sql.execution.SparkPlan,scala.collection.Iterator,org.apache.spark.sql.execution.joins.HashedRelation,org.apache.spark.sql.execution.metric.SQLMetric)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/joins/HashJoin/class/join(org.apache.spark.sql.execution.SparkPlan,scala.collection.Iterator,org.apache.spark.sql.execution.joins.HashedRelation,org.apache.spark.sql.execution.metric.SQLMetric)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/joins/HashJoin/class/join(org.apache.spark.sql.execution.SparkPlan,scala.collection.Iterator,org.apache.spark.sql.execution.joins.HashedRelation,org.apache.spark.sql.execution.metric.SQLMetric)#org/apache/spark/sql/execution/joins/HashJoin$$anonfun$join$1/1(org.apache.spark.sql.execution.SparkPlan,scala.Function1,org.apache.spark.sql.execution.metric.SQLMetric)
org/apache/spark/sql/execution/joins/HashJoin/class/join(org.apache.spark.sql.execution.SparkPlan,scala.collection.Iterator,org.apache.spark.sql.execution.joins.HashedRelation,org.apache.spark.sql.execution.metric.SQLMetric)#org/apache/spark/sql/execution/joins/HashJoin$class/innerJoin(org.apache.spark.sql.execution.SparkPlan,scala.collection.Iterator,org.apache.spark.sql.execution.joins.HashedRelation)
org/apache/spark/sql/execution/joins/HashJoin/class/join(org.apache.spark.sql.execution.SparkPlan,scala.collection.Iterator,org.apache.spark.sql.execution.joins.HashedRelation,org.apache.spark.sql.execution.metric.SQLMetric)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/joins/HashJoin/class/join(org.apache.spark.sql.execution.SparkPlan,scala.collection.Iterator,org.apache.spark.sql.execution.joins.HashedRelation,org.apache.spark.sql.execution.metric.SQLMetric)#org/apache/spark/sql/execution/joins/HashJoin$class/antiJoin(org.apache.spark.sql.execution.SparkPlan,scala.collection.Iterator,org.apache.spark.sql.execution.joins.HashedRelation)
org/apache/spark/sql/execution/joins/HashJoin/class/join(org.apache.spark.sql.execution.SparkPlan,scala.collection.Iterator,org.apache.spark.sql.execution.joins.HashedRelation,org.apache.spark.sql.execution.metric.SQLMetric)#org/apache/spark/sql/execution/joins/HashJoin$class/outerJoin(org.apache.spark.sql.execution.SparkPlan,scala.collection.Iterator,org.apache.spark.sql.execution.joins.HashedRelation)
org/apache/spark/sql/execution/joins/HashJoin/class/join(org.apache.spark.sql.execution.SparkPlan,scala.collection.Iterator,org.apache.spark.sql.execution.joins.HashedRelation,org.apache.spark.sql.execution.metric.SQLMetric)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/joins/HashJoin/class/join(org.apache.spark.sql.execution.SparkPlan,scala.collection.Iterator,org.apache.spark.sql.execution.joins.HashedRelation,org.apache.spark.sql.execution.metric.SQLMetric)#scala/collection/Iterator/map(scala.Function1)
org/apache/spark/sql/execution/joins/HashJoin/class/join(org.apache.spark.sql.execution.SparkPlan,scala.collection.Iterator,org.apache.spark.sql.execution.joins.HashedRelation,org.apache.spark.sql.execution.metric.SQLMetric)#org/apache/spark/sql/execution/joins/HashJoin$class/semiJoin(org.apache.spark.sql.execution.SparkPlan,scala.collection.Iterator,org.apache.spark.sql.execution.joins.HashedRelation)
org/apache/spark/sql/execution/joins/HashJoin/class/join(org.apache.spark.sql.execution.SparkPlan,scala.collection.Iterator,org.apache.spark.sql.execution.joins.HashedRelation,org.apache.spark.sql.execution.metric.SQLMetric)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/joins/HashJoin/class/join(org.apache.spark.sql.execution.SparkPlan,scala.collection.Iterator,org.apache.spark.sql.execution.joins.HashedRelation,org.apache.spark.sql.execution.metric.SQLMetric)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/execution/joins/HashJoin/class/join(org.apache.spark.sql.execution.SparkPlan,scala.collection.Iterator,org.apache.spark.sql.execution.joins.HashedRelation,org.apache.spark.sql.execution.metric.SQLMetric)#org/apache/spark/sql/execution/joins/HashJoin$class/existenceJoin(org.apache.spark.sql.execution.SparkPlan,scala.collection.Iterator,org.apache.spark.sql.execution.joins.HashedRelation)
org/apache/spark/sql/execution/joins/HashJoin/class/output(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/catalyst/plans/LeftExistence$/unapply(org.apache.spark.sql.catalyst.plans.JoinType)
org/apache/spark/sql/execution/joins/HashJoin/class/output(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/joins/HashJoin/class/output(org.apache.spark.sql.execution.SparkPlan)#scala/Option/isEmpty()
org/apache/spark/sql/execution/joins/HashJoin/class/output(org.apache.spark.sql.execution.SparkPlan)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/joins/HashJoin/class/output(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/joins/HashJoin/class/output(org.apache.spark.sql.execution.SparkPlan)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/joins/HashJoin/class/output(org.apache.spark.sql.execution.SparkPlan)#scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/joins/HashJoin/class/output(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/$colon$plus(java.lang.Object,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/joins/HashJoin/class/output(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/joins/HashJoin$$anonfun$output$2/2(org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/joins/HashJoin/class/output(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/joins/HashJoin/class/output(org.apache.spark.sql.execution.SparkPlan)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/joins/HashJoin/class/output(org.apache.spark.sql.execution.SparkPlan)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/joins/HashJoin/class/output(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/joins/HashJoin/class/output(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/catalyst/plans/ExistenceJoin/exists()
org/apache/spark/sql/execution/joins/HashJoin/class/output(org.apache.spark.sql.execution.SparkPlan)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/joins/HashJoin/class/output(org.apache.spark.sql.execution.SparkPlan)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/execution/joins/HashJoin/class/output(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/joins/HashJoin$$anonfun$output$1/1(org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/columnar/BooleanColumnStats/gatherStats(org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/catalyst/InternalRow/getBoolean(int)
org/apache/spark/sql/execution/columnar/BooleanColumnStats/gatherStats(org.apache.spark.sql.catalyst.InternalRow,int)#scala/runtime/BoxesRunTime/boxToBoolean(boolean)
org/apache/spark/sql/execution/columnar/BooleanColumnStats/gatherStats(org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/execution/columnar/BOOLEAN$/defaultSize()
org/apache/spark/sql/execution/columnar/BooleanColumnStats/gatherStats(org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/catalyst/InternalRow/isNullAt(int)
org/apache/spark/sql/execution/columnar/BooleanColumnStats/gatherStats(org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/execution/columnar/ColumnStats$class/gatherStats(org.apache.spark.sql.execution.columnar.ColumnStats,org.apache.spark.sql.catalyst.InternalRow,int)
org/apache/spark/sql/execution/columnar/BooleanColumnStats/gatherStats(org.apache.spark.sql.catalyst.InternalRow,int)#scala/runtime/RichBoolean/$greater(java.lang.Object)
org/apache/spark/sql/execution/columnar/BooleanColumnStats/gatherStats(org.apache.spark.sql.catalyst.InternalRow,int)#scala/runtime/RichBoolean/RichBoolean(boolean)
org/apache/spark/sql/execution/columnar/BooleanColumnStats/gatherStats(org.apache.spark.sql.catalyst.InternalRow,int)#scala/runtime/RichBoolean/$less(java.lang.Object)
org/apache/spark/sql/execution/columnar/BooleanColumnStats/gatherStats(org.apache.spark.sql.catalyst.InternalRow,int)#scala/Predef$/booleanWrapper(boolean)
org/apache/spark/sql/SQLImplicits/typecreator3/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/selectType(scala.reflect.api.Symbols$SymbolApi,java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator3/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ModuleSymbolApi/asModule()
org/apache/spark/sql/SQLImplicits/typecreator3/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticPackage(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator3/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$InternalApi/reificationSupport()
org/apache/spark/sql/SQLImplicits/typecreator3/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/RootClass()
org/apache/spark/sql/SQLImplicits/typecreator3/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticModule(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator3/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ClassSymbolApi/asType()
org/apache/spark/sql/SQLImplicits/typecreator3/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ModuleSymbolApi/moduleClass()
org/apache/spark/sql/SQLImplicits/typecreator3/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/thisPrefix(scala.reflect.api.Symbols$SymbolApi)
org/apache/spark/sql/SQLImplicits/typecreator3/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/TypeRef(scala.reflect.api.Types$TypeApi,scala.reflect.api.Symbols$SymbolApi,scala.collection.immutable.List)
org/apache/spark/sql/SQLImplicits/typecreator3/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticClass(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator3/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/universe()
org/apache/spark/sql/SQLImplicits/typecreator3/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$TypeSymbolApi/toTypeConstructor()
org/apache/spark/sql/SQLImplicits/typecreator3/1/apply(scala.reflect.api.Mirror)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/SQLImplicits/typecreator3/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/SingleType(scala.reflect.api.Types$TypeApi,scala.reflect.api.Symbols$SymbolApi)
org/apache/spark/sql/SQLImplicits/typecreator3/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Universe/internal()
org/apache/spark/sql/SQLImplicits/typecreator3/1/apply(scala.reflect.api.Mirror)#scala/collection/immutable/List$/apply(scala.collection.Seq)
org/apache/spark/sql/api/r/SQLUtils/anonfun/4/apply(java.lang.String)#org/apache/spark/sql/api/r/SQLUtils$/createStructField(java.lang.String,java.lang.String,boolean)
org/apache/spark/sql/api/r/SQLUtils/anonfun/4/apply(java.lang.String)#org/apache/spark/sql/api/r/SQLUtils$/org$apache$spark$sql$api$r$SQLUtils$$RegexContext(scala.StringContext)
org/apache/spark/sql/api/r/SQLUtils/anonfun/4/apply(java.lang.String)#scala/collection/LinearSeqOptimized/apply(int)
org/apache/spark/sql/api/r/SQLUtils/anonfun/4/apply(java.lang.String)#scala/Option/isEmpty()
org/apache/spark/sql/api/r/SQLUtils/anonfun/4/apply(java.lang.String)#scala/util/matching/Regex/unapplySeq(java.lang.CharSequence)
org/apache/spark/sql/api/r/SQLUtils/anonfun/4/apply(java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/api/r/SQLUtils/anonfun/4/apply(java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/api/r/SQLUtils/anonfun/4/apply(java.lang.String)#scala/collection/LinearSeqOptimized/lengthCompare(int)
org/apache/spark/sql/api/r/SQLUtils/anonfun/4/apply(java.lang.String)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/api/r/SQLUtils/anonfun/4/apply(java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/api/r/SQLUtils/anonfun/4/apply(java.lang.String)#scala/Option/get()
org/apache/spark/sql/api/r/SQLUtils/anonfun/4/apply(java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/api/r/SQLUtils/anonfun/4/apply(java.lang.String)#org/apache/spark/sql/api/r/SQLUtils$RegexContext/r()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/savePartition/1/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/savePartition/1/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/savePartition/1/apply()#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/savePartition/1/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/savePartition/1/apply()#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$savePartition$1/apply()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/savePartition/1/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/savePartition/1/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/savePartition/1/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/savePartition/1/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#scala/util/control/NonFatal$/unapply(java.lang.Throwable)
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/empty()
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/TruncateTableCommand$$anonfun$run$2/2(org.apache.spark.sql.execution.command.TruncateTableCommand,java.lang.String,org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/get()
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTableType$/EXTERNAL()
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq/isEmpty()
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#org/slf4j/Logger/warn(java.lang.String,java.lang.Throwable)
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/isDefined()
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/TruncateTableCommand$$anonfun$7/7(org.apache.spark.sql.execution.command.TruncateTableCommand,scala.collection.Seq,scala.collection.Seq)
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/TruncateTableCommand$$anonfun$8/8(org.apache.spark.sql.execution.command.TruncateTableCommand,org.apache.spark.sql.catalyst.catalog.CatalogTable)
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq/foreach(scala.Function1)
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/withFilter(scala.Function1)
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/TruncateTableCommand$$anonfun$6/6(org.apache.spark.sql.execution.command.TruncateTableCommand)
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/tableType()
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/isEmpty()
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/listPartitions(org.apache.spark.sql.catalyst.TableIdentifier,scala.Option)
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/TableIdentifier/unquotedString()
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/DDLUtils$/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/TableIdentifier/quotedString()
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/TruncateTableCommand/log()
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#scala/Option$WithFilter/foreach(scala.Function1)
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/TruncateTableCommand$$anonfun$5/5(org.apache.spark.sql.execution.command.TruncateTableCommand,org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.Seq)
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTableType$/VIEW()
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/locationUri()
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/command/TruncateTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/storage()
org/apache/spark/sql/execution/streaming/state/StateStore/anonfun/org/apache/spark/sql/execution/streaming/state/StateStore/doMaintenance/2/apply(scala.Tuple2)#scala/util/control/NonFatal$/unapply(java.lang.Throwable)
org/apache/spark/sql/execution/streaming/state/StateStore/anonfun/org/apache/spark/sql/execution/streaming/state/StateStore/doMaintenance/2/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/streaming/state/StateStore/anonfun/org/apache/spark/sql/execution/streaming/state/StateStore/doMaintenance/2/apply(scala.Tuple2)#scala/Tuple2/_2()
org/apache/spark/sql/execution/streaming/state/StateStore/anonfun/org/apache/spark/sql/execution/streaming/state/StateStore/doMaintenance/2/apply(scala.Tuple2)#scala/Option/isEmpty()
org/apache/spark/sql/execution/streaming/state/StateStore/anonfun/org/apache/spark/sql/execution/streaming/state/StateStore/doMaintenance/2/apply(scala.Tuple2)#org/apache/spark/sql/execution/streaming/state/StateStore$$anonfun$org$apache$spark$sql$execution$streaming$state$StateStore$$doMaintenance$2$$anonfun$apply$2/2(org.apache.spark.sql.execution.streaming.state.StateStore$$anonfun$org$apache$spark$sql$execution$streaming$state$StateStore$$doMaintenance$2,org.apache.spark.sql.execution.streaming.state.StateStoreProvider)
org/apache/spark/sql/execution/streaming/state/StateStore/anonfun/org/apache/spark/sql/execution/streaming/state/StateStore/doMaintenance/2/apply(scala.Tuple2)#org/apache/spark/sql/execution/streaming/state/StateStore$/logInfo(scala.Function0)
org/apache/spark/sql/execution/streaming/state/StateStore/anonfun/org/apache/spark/sql/execution/streaming/state/StateStore/doMaintenance/2/apply(scala.Tuple2)#org/apache/spark/sql/execution/streaming/state/StateStore$/unload(org.apache.spark.sql.execution.streaming.state.StateStoreId)
org/apache/spark/sql/execution/streaming/state/StateStore/anonfun/org/apache/spark/sql/execution/streaming/state/StateStore/doMaintenance/2/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/StateStore/anonfun/org/apache/spark/sql/execution/streaming/state/StateStore/doMaintenance/2/apply(scala.Tuple2)#org/apache/spark/sql/execution/streaming/state/StateStore$/logWarning(scala.Function0)
org/apache/spark/sql/execution/streaming/state/StateStore/anonfun/org/apache/spark/sql/execution/streaming/state/StateStore/doMaintenance/2/apply(scala.Tuple2)#scala/Option/get()
org/apache/spark/sql/execution/streaming/state/StateStore/anonfun/org/apache/spark/sql/execution/streaming/state/StateStore/doMaintenance/2/apply(scala.Tuple2)#org/apache/spark/sql/execution/streaming/state/StateStore$/org$apache$spark$sql$execution$streaming$state$StateStore$$verifyIfStoreInstanceActive(org.apache.spark.sql.execution.streaming.state.StateStoreId)
org/apache/spark/sql/execution/streaming/state/StateStore/anonfun/org/apache/spark/sql/execution/streaming/state/StateStore/doMaintenance/2/apply(scala.Tuple2)#org/apache/spark/sql/execution/streaming/state/StateStore$$anonfun$org$apache$spark$sql$execution$streaming$state$StateStore$$doMaintenance$2$$anonfun$apply$1/1(org.apache.spark.sql.execution.streaming.state.StateStore$$anonfun$org$apache$spark$sql$execution$streaming$state$StateStore$$doMaintenance$2,org.apache.spark.sql.execution.streaming.state.StateStoreProvider)
org/apache/spark/sql/execution/command/ShowTablesCommand/anonfun/run/3/apply(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/TableIdentifier/table()
org/apache/spark/sql/execution/command/ShowTablesCommand/anonfun/run/3/apply(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/isTemporaryTable(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/ShowTablesCommand/anonfun/run/3/apply(org.apache.spark.sql.catalyst.TableIdentifier)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/ShowTablesCommand/anonfun/run/3/apply(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/catalog/CatalogTable/simpleString()
org/apache/spark/sql/execution/command/ShowTablesCommand/anonfun/run/3/apply(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/Row$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/command/ShowTablesCommand/anonfun/run/3/apply(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/execution/command/ShowTablesCommand$$anonfun$run$3$$anonfun$12/12(org.apache.spark.sql.execution.command.ShowTablesCommand$$anonfun$run$3)
org/apache/spark/sql/execution/command/ShowTablesCommand/anonfun/run/3/apply(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/TableIdentifier/database()
org/apache/spark/sql/execution/command/ShowTablesCommand/anonfun/run/3/apply(org.apache.spark.sql.catalyst.TableIdentifier)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/command/ShowTablesCommand/anonfun/run/3/apply(org.apache.spark.sql.catalyst.TableIdentifier)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/ShowTablesCommand/anonfun/run/3/apply(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTempViewOrPermanentTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/ShowTablesCommand/anonfun/run/3/apply(org.apache.spark.sql.catalyst.TableIdentifier)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/ShowTablesCommand/anonfun/run/3/apply(org.apache.spark.sql.catalyst.TableIdentifier)#scala/runtime/BoxesRunTime/boxToBoolean(boolean)
org/apache/spark/sql/execution/command/ShowTablesCommand/anonfun/run/3/apply(org.apache.spark.sql.catalyst.TableIdentifier)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSource/buildStorageFormatFromOptions(scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat$/empty()
org/apache/spark/sql/execution/datasources/DataSource/buildStorageFormatFromOptions(scala.collection.immutable.Map)#scala/collection/immutable/Map/filterKeys(scala.Function1)
org/apache/spark/sql/execution/datasources/DataSource/buildStorageFormatFromOptions(scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$2()
org/apache/spark/sql/execution/datasources/DataSource/buildStorageFormatFromOptions(scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$5()
org/apache/spark/sql/execution/datasources/DataSource/buildStorageFormatFromOptions(scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy(scala.Option,scala.Option,scala.Option,scala.Option,boolean,scala.collection.immutable.Map)
org/apache/spark/sql/execution/datasources/DataSource/buildStorageFormatFromOptions(scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/util/CaseInsensitiveMap$/apply(scala.collection.immutable.Map)
org/apache/spark/sql/execution/datasources/DataSource/buildStorageFormatFromOptions(scala.collection.immutable.Map)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/datasources/DataSource/buildStorageFormatFromOptions(scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$3()
org/apache/spark/sql/execution/datasources/DataSource/buildStorageFormatFromOptions(scala.collection.immutable.Map)#org/apache/spark/sql/execution/datasources/DataSource$/buildStorageFormatFromOptions(scala.collection.immutable.Map)
org/apache/spark/sql/execution/datasources/DataSource/buildStorageFormatFromOptions(scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$4()
org/apache/spark/sql/execution/datasources/DataSource/buildStorageFormatFromOptions(scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/util/CaseInsensitiveMap/get(java.lang.String)
org/apache/spark/sql/execution/datasources/DataSource/buildStorageFormatFromOptions(scala.collection.immutable.Map)#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$24/24()
org/apache/spark/sql/execution/datasources/DataSource/buildStorageFormatFromOptions(scala.collection.immutable.Map)#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$25/25()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/collection/immutable/List$/canBuildFrom()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/collection/immutable/$colon$colon/tl$1()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/util/Try/orElse(scala.Function0)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#org/apache/spark/sql/execution/datasources/DataSource$/spark2RemovedClasses()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#java/lang/String/toLowerCase(java.util.Locale)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#org/apache/spark/sql/execution/datasources/DataSource$/logWarning(scala.Function0)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#java/util/ServiceConfigurationError/getMessage()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#java/lang/Object/getClass()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/util/Success/value()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#org/apache/spark/sql/execution/datasources/DataSource$/backwardCompatibilityMap()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#org/apache/spark/util/Utils$/getContextOrSparkClassLoader()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/collection/immutable/List/mkString(java.lang.String)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/util/Failure/exception()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#java/lang/String/replaceAll(java.lang.String,java.lang.String)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#org/apache/spark/sql/execution/datasources/DataSource$/lookupDataSource(java.lang.String)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/collection/immutable/List/filter(scala.Function1)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$20/20(java.lang.String,java.lang.ClassLoader)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$21/21(java.lang.String,java.lang.ClassLoader)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/collection/JavaConverters$/iterableAsScalaIterableConverter(java.lang.Iterable)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/collection/immutable/Set/contains(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/collection/convert/Decorators$AsScala/asScala()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/collection/immutable/Map/getOrElse(java.lang.Object,scala.Function0)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#java/lang/Throwable/getMessage()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/collection/TraversableLike/filter(scala.Function1)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/util/Try$/apply(scala.Function0)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#java/util/ServiceLoader/load(java.lang.Class,java.lang.ClassLoader)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$18/18(java.lang.String)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$19/19(java.lang.String)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#java/lang/NoClassDefFoundError/getMessage()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#java/util/ServiceConfigurationError/getCause()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/collection/immutable/List/head()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/collection/immutable/List/size()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$lookupDataSource$1/1(java.lang.String,scala.collection.immutable.List,scala.collection.immutable.List)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#java/lang/String/startsWith(java.lang.String)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#java/lang/ClassNotFoundException/ClassNotFoundException(java.lang.String,java.lang.Throwable)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$22/22()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$23/23()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/collection/immutable/$colon$colon/head()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/collection/immutable/List/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/DataSource/lookupDataSource(java.lang.String)#scala/collection/TraversableOnce/toList()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/DecimalType/scale()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#java/lang/Class/getComponentType()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/Tuple2/_1()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/UserDefinedType/sqlType()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#java/lang/Object/getClass()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/reflect/ClassTag$/Any()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/collection/mutable/Buffer$/canBuildFrom()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#java/lang/Class/isArray()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/Predef$/genericArrayOps(java.lang.Object)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#java/lang/IllegalStateException/IllegalStateException(java.lang.String)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/collection/mutable/ArrayOps/zip(scala.collection.GenIterable,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#java/lang/Object/toString()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/unsafe/types/UTF8String/fromString(java.lang.String)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/runtime/BoxesRunTime/boxToShort(short)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/runtime/BoxesRunTime/boxToFloat(float)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/Tuple2/_2()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/MapType/valueType()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/collection/convert/Decorators$AsScala/asScala()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/catalyst/util/ArrayBasedMapData$/apply(java.util.Map,scala.Function1,scala.Function1)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/runtime/BoxesRunTime/boxToBoolean(boolean)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/runtime/ScalaRunTime$/array_length(java.lang.Object)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/runtime/BoxesRunTime/boxToDouble(double)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/runtime/BoxesRunTime/boxToByte(byte)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/python/EvaluatePython$/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#java/lang/Class/getName()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/runtime/BoxesRunTime/unboxToDouble(java.lang.Object)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/ArrayType/elementType()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/DecimalType/precision()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#java/lang/String/getBytes(java.nio.charset.Charset)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/python/EvaluatePython$$anonfun$fromJava$4/4(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/python/EvaluatePython$$anonfun$fromJava$3/3(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/catalyst/util/GenericArrayData/GenericArrayData(java.lang.Object[])
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/MapType/keyType()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/catalyst/expressions/GenericInternalRow/GenericInternalRow(java.lang.Object[])
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/Decimal$/apply(java.math.BigDecimal,int,int)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/python/EvaluatePython$$anonfun$fromJava$5/5()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/python/EvaluatePython$$anonfun$fromJava$2/2(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/python/EvaluatePython$$anonfun$fromJava$1/1(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/python/EvaluatePython/fromJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/Decimal/toJavaBigDecimal()
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/catalyst/util/ArrayData/foreach(org.apache.spark.sql.types.DataType,scala.Function2)
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/StructField/dataType()
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/Tuple2/_1()
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/UserDefinedType/sqlType()
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/MapType/valueType()
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/catalyst/util/MapData/numElements()
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/catalyst/util/MapData/foreach(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType,scala.Function2)
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/python/EvaluatePython$$anonfun$toJava$1/1(java.util.ArrayList,org.apache.spark.sql.types.ArrayType)
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/MapType/keyType()
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/catalyst/InternalRow/numFields()
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/catalyst/expressions/GenericRowWithSchema/GenericRowWithSchema(java.lang.Object[],org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/unsafe/types/UTF8String/toString()
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#java/util/ArrayList/ArrayList(int)
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/python/EvaluatePython$/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/catalyst/util/ArrayData/numElements()
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#scala/Tuple2/_2()
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/python/EvaluatePython$$anonfun$toJava$2/2(java.util.HashMap,org.apache.spark.sql.types.MapType)
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/catalyst/InternalRow/get(int,org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#java/util/HashMap/HashMap(int)
org/apache/spark/sql/execution/python/EvaluatePython/toJava(java.lang.Object,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/ArrayType/elementType()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateFunction/1/anonfun/20/apply(org.apache.spark.sql.catalyst.parser.SqlBaseParser$ResourceContext)#org/apache/spark/sql/catalyst/parser/ParserUtils$/operationNotAllowed(java.lang.String,org.antlr.v4.runtime.ParserRuleContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateFunction/1/anonfun/20/apply(org.apache.spark.sql.catalyst.parser.SqlBaseParser$ResourceContext)#java/lang/String/toLowerCase(java.util.Locale)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateFunction/1/anonfun/20/apply(org.apache.spark.sql.catalyst.parser.SqlBaseParser$ResourceContext)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateFunction/1/anonfun/20/apply(org.apache.spark.sql.catalyst.parser.SqlBaseParser$ResourceContext)#org/apache/spark/sql/catalyst/parser/ParserUtils$/string(org.antlr.v4.runtime.tree.TerminalNode)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateFunction/1/anonfun/20/apply(org.apache.spark.sql.catalyst.parser.SqlBaseParser$ResourceContext)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateFunction/1/anonfun/20/apply(org.apache.spark.sql.catalyst.parser.SqlBaseParser$ResourceContext)#org/apache/spark/sql/catalyst/catalog/FunctionResource/FunctionResource(org.apache.spark.sql.catalyst.catalog.FunctionResourceType,java.lang.String)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateFunction/1/anonfun/20/apply(org.apache.spark.sql.catalyst.parser.SqlBaseParser$ResourceContext)#org/apache/spark/sql/catalyst/parser/SqlBaseParser$ResourceContext/identifier()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateFunction/1/anonfun/20/apply(org.apache.spark.sql.catalyst.parser.SqlBaseParser$ResourceContext)#org/apache/spark/sql/catalyst/parser/SqlBaseParser$ResourceContext/STRING()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateFunction/1/anonfun/20/apply(org.apache.spark.sql.catalyst.parser.SqlBaseParser$ResourceContext)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateFunction/1/anonfun/20/apply(org.apache.spark.sql.catalyst.parser.SqlBaseParser$ResourceContext)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateFunction/1/anonfun/20/apply(org.apache.spark.sql.catalyst.parser.SqlBaseParser$ResourceContext)#org/apache/spark/sql/catalyst/parser/SqlBaseParser$IdentifierContext/getText()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateFunction/1/anonfun/20/apply(org.apache.spark.sql.catalyst.parser.SqlBaseParser$ResourceContext)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateFunction/1/anonfun/20/apply(org.apache.spark.sql.catalyst.parser.SqlBaseParser$ResourceContext)#org/apache/spark/sql/catalyst/catalog/FunctionResourceType$/fromString(java.lang.String)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/ifPartitionNotExists()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/output()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/datasources/DataSourceAnalysis$$anonfun$apply$1$$anonfun$1/1(org.apache.spark.sql.execution.datasources.DataSourceAnalysis$$anonfun$apply$1)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/query()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Some/x()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/Seq/contains(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/types/StructType/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/collect(scala.PartialFunction)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/resolved()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/plans/logical/Project/Project(scala.collection.Seq,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/command/DDLUtils$/isDatasourceTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/Seq/flatten(scala.Function1)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/immutable/Map$/canBuildFrom()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/partition()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/Seq/head()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/immutable/Map/exists(scala.Function1)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/overwrite()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/Seq/size()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/immutable/Map/isEmpty()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Predef$/$conforms()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/table()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/immutable/Map/filter(scala.Function1)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/datasources/DataSourceAnalysis$$anonfun$apply$1$$anonfun$applyOrElse$2/2(org.apache.spark.sql.execution.datasources.DataSourceAnalysis$$anonfun$apply$1)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/datasources/DataSourceAnalysis$$anonfun$apply$1$$anonfun$6/6(org.apache.spark.sql.execution.datasources.DataSourceAnalysis$$anonfun$apply$1)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/datasources/DataSourceAnalysis$$anonfun$apply$1$$anonfun$7/7(org.apache.spark.sql.execution.datasources.DataSourceAnalysis$$anonfun$apply$1)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/datasources/DataSourceAnalysis$$anonfun$apply$1$$anonfun$8/8(org.apache.spark.sql.execution.datasources.DataSourceAnalysis$$anonfun$apply$1)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/execution/datasources/FileFormatWriter$/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$9/9(org.apache.spark.sql.SparkSession)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$8/8()
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#scala/collection/Iterable/toSeq()
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/hadoop/mapreduce/Job/setOutputValueClass(java.lang.Class)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/expressions/package$AttributeSeq/toStructType()
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/expressions/AttributeSet$/apply(scala.collection.Iterable)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/util/CaseInsensitiveMap/get(java.lang.String)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#scala/Option$/option2Iterable(scala.Option)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#scala/collection/IterableLike/forall(scala.Function1)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$5/5(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/hadoop/mapreduce/Job/getConfiguration()
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#scala/collection/Seq/zip(scala.collection.GenIterable,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#java/util/UUID/randomUUID()
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$7/7(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/hadoop/mapreduce/Job/getInstance(org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/hadoop/mapreduce/lib/output/FileOutputFormat/setOutputPath(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/execution/datasources/FileFormatWriter$WriteJobDescription/WriteJobDescription(java.lang.String,org.apache.spark.util.SerializableConfiguration,org.apache.spark.sql.execution.datasources.OutputWriterFactory,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.Option,java.lang.String,scala.collection.immutable.Map,long,java.lang.String)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/util/DateTimeUtils$/TIMEZONE_OPTION()
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/execution/datasources/FileFormatWriter$OutputSpec/customPartitionLocations()
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/execution/SQLExecution$/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/expressions/package$/AttributeSeq(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/execution/datasources/FileFormatWriter$OutputSpec/outputPath()
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$10/10()
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#scala/collection/Seq/filterNot(scala.Function1)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$11/11()
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$1/1(org.apache.spark.sql.SparkSession)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#scala/collection/Seq/length()
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/util/CaseInsensitiveMap$/apply(scala.collection.immutable.Map)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$4/4(org.apache.spark.sql.catalyst.expressions.AttributeSet)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#java/util/UUID/toString()
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/hadoop/mapreduce/Job/setOutputKeyClass(java.lang.Class)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/hadoop/fs/Path/Path(java.lang.String)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$3/3(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.collection.Seq)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/util/SerializableConfiguration/SerializableConfiguration(org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/datasources/FileFormatWriter/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)#org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$write$1/1(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.internal.io.FileCommitProtocol,scala.Function1,org.apache.hadoop.mapreduce.Job,org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteJobDescription,scala.collection.Seq,boolean)
org/apache/spark/sql/execution/SparkStrategies/StatefulAggregationStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/EventTimeWatermark/delay()
org/apache/spark/sql/execution/SparkStrategies/StatefulAggregationStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)
org/apache/spark/sql/execution/SparkStrategies/StatefulAggregationStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/EventTimeWatermark/eventTime()
org/apache/spark/sql/execution/SparkStrategies/StatefulAggregationStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Tuple4/_1()
org/apache/spark/sql/execution/SparkStrategies/StatefulAggregationStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Tuple4/_2()
org/apache/spark/sql/execution/SparkStrategies/StatefulAggregationStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Tuple4/_4()
org/apache/spark/sql/execution/SparkStrategies/StatefulAggregationStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Option/isEmpty()
org/apache/spark/sql/execution/SparkStrategies/StatefulAggregationStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/planning/PhysicalAggregation$/unapply(java.lang.Object)
org/apache/spark/sql/execution/SparkStrategies/StatefulAggregationStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/SparkStrategies/StatefulAggregationStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/EventTimeWatermark/child()
org/apache/spark/sql/execution/SparkStrategies/StatefulAggregationStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Option/get()
org/apache/spark/sql/execution/SparkStrategies/StatefulAggregationStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/SparkStrategies$StatefulAggregationStrategy$/planLater(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/SparkStrategies/StatefulAggregationStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Tuple4/_3()
org/apache/spark/sql/execution/datasources/json/JsonFileFormat/anonfun/buildReader/1/apply$mcVI$sp(int)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/json/JsonFileFormat/anonfun/buildReader/1/apply$mcVI$sp(int)#org/apache/spark/sql/types/StructField/nullable()
org/apache/spark/sql/execution/datasources/json/JsonFileFormat/anonfun/buildReader/1/apply$mcVI$sp(int)#org/apache/spark/sql/types/StructField/dataType()
org/apache/spark/sql/execution/datasources/json/JsonFileFormat/anonfun/buildReader/1/apply$mcVI$sp(int)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/json/JsonFileFormat/anonfun/buildReader/1/apply$mcVI$sp(int)#org/apache/spark/sql/types/StructType/apply(int)
org/apache/spark/sql/execution/datasources/json/JsonFileFormat/anonfun/buildReader/1/apply$mcVI$sp(int)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/json/JsonFileFormat/anonfun/buildReader/1/apply$mcVI$sp(int)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/json/JsonFileFormat/anonfun/buildReader/1/apply$mcVI$sp(int)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/json/JsonFileFormat/anonfun/buildReader/1/apply$mcVI$sp(int)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/python/ExtractPythonUDFs/org$apache$spark$sql$execution$python$ExtractPythonUDFs$$extract(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/isEmpty()
org/apache/spark/sql/execution/python/ExtractPythonUDFs/org$apache$spark$sql$execution$python$ExtractPythonUDFs$$extract(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/python/ExtractPythonUDFs/org$apache$spark$sql$execution$python$ExtractPythonUDFs$$extract(org.apache.spark.sql.execution.SparkPlan)#scala/collection/mutable/HashMap$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/org$apache$spark$sql$execution$python$ExtractPythonUDFs$$extract(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/SparkPlan/expressions()
org/apache/spark/sql/execution/python/ExtractPythonUDFs/org$apache$spark$sql$execution$python$ExtractPythonUDFs$$extract(org.apache.spark.sql.execution.SparkPlan)#scala/collection/IterableLike/foreach(scala.Function1)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/org$apache$spark$sql$execution$python$ExtractPythonUDFs$$extract(org.apache.spark.sql.execution.SparkPlan)#scala/collection/TraversableLike/filter(scala.Function1)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/org$apache$spark$sql$execution$python$ExtractPythonUDFs$$extract(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/SparkPlan/withNewChildren(scala.collection.Seq)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/org$apache$spark$sql$execution$python$ExtractPythonUDFs$$extract(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/filterNot(scala.Function1)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/org$apache$spark$sql$execution$python$ExtractPythonUDFs$$extract(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/python/ExtractPythonUDFs$$anonfun$4/4(org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/org$apache$spark$sql$execution$python$ExtractPythonUDFs$$extract(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/python/ExtractPythonUDFs$/trySplitFilter(org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/org$apache$spark$sql$execution$python$ExtractPythonUDFs$$extract(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/python/ExtractPythonUDFs$$anonfun$2/2(scala.collection.mutable.HashMap)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/org$apache$spark$sql$execution$python$ExtractPythonUDFs$$extract(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/python/ExtractPythonUDFs$$anonfun$3/3()
org/apache/spark/sql/execution/python/ExtractPythonUDFs/org$apache$spark$sql$execution$python$ExtractPythonUDFs$$extract(org.apache.spark.sql.execution.SparkPlan)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/org$apache$spark$sql$execution$python$ExtractPythonUDFs$$extract(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/python/ExtractPythonUDFs/org$apache$spark$sql$execution$python$ExtractPythonUDFs$$extract(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/SparkPlan/children()
org/apache/spark/sql/execution/python/ExtractPythonUDFs/org$apache$spark$sql$execution$python$ExtractPythonUDFs$$extract(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/org$apache$spark$sql$execution$python$ExtractPythonUDFs$$extract(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/python/ExtractPythonUDFs$/org$apache$spark$sql$execution$python$ExtractPythonUDFs$$extract(org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/org$apache$spark$sql$execution$python$ExtractPythonUDFs$$extract(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/org$apache$spark$sql$execution$python$ExtractPythonUDFs$$extract(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/python/ExtractPythonUDFs$$anonfun$5/5(scala.collection.Seq,scala.collection.mutable.HashMap)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/org$apache$spark$sql$execution$python$ExtractPythonUDFs$$extract(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/python/ExtractPythonUDFs$$anonfun$org$apache$spark$sql$execution$python$ExtractPythonUDFs$$extract$1/1(scala.collection.mutable.HashMap)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/org$apache$spark$sql$execution$python$ExtractPythonUDFs$$extract(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/catalyst/plans/QueryPlan/transformExpressions(scala.PartialFunction)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/org$apache$spark$sql$execution$python$ExtractPythonUDFs$$extract(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/python/ExtractPythonUDFs$$anonfun$org$apache$spark$sql$execution$python$ExtractPythonUDFs$$extract$2/2()
org/apache/spark/sql/execution/python/ExtractPythonUDFs/trySplitFilter(org.apache.spark.sql.execution.SparkPlan)#scala/collection/TraversableOnce/reduceLeft(scala.Function2)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/trySplitFilter(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/python/ExtractPythonUDFs/trySplitFilter(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/partition(scala.Function1)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/trySplitFilter(org.apache.spark.sql.execution.SparkPlan)#scala/Tuple2/_1()
org/apache/spark/sql/execution/python/ExtractPythonUDFs/trySplitFilter(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/python/ExtractPythonUDFs$$anonfun$9/9()
org/apache/spark/sql/execution/python/ExtractPythonUDFs/trySplitFilter(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/reduceLeft(scala.Function2)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/trySplitFilter(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/execution/python/ExtractPythonUDFs/trySplitFilter(org.apache.spark.sql.execution.SparkPlan)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/trySplitFilter(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/span(scala.Function1)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/trySplitFilter(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/python/ExtractPythonUDFs$/splitConjunctivePredicates(org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/trySplitFilter(org.apache.spark.sql.execution.SparkPlan)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/trySplitFilter(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/python/ExtractPythonUDFs$$anonfun$8/8()
org/apache/spark/sql/execution/python/ExtractPythonUDFs/trySplitFilter(org.apache.spark.sql.execution.SparkPlan)#scala/Tuple2/_2()
org/apache/spark/sql/execution/python/ExtractPythonUDFs/trySplitFilter(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/csv/UnivocityParser/org$apache$spark$sql$execution$datasources$csv$UnivocityParser$$convert(java.lang.String[])#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/csv/UnivocityParser/org$apache$spark$sql$execution$datasources$csv$UnivocityParser$$convert(java.lang.String[])#org/apache/spark/sql/execution/datasources/csv/UnivocityParser$$anonfun$org$apache$spark$sql$execution$datasources$csv$UnivocityParser$$convert$2/2(org.apache.spark.sql.execution.datasources.csv.UnivocityParser,java.lang.String[])
org/apache/spark/sql/execution/datasources/csv/UnivocityParser/org$apache$spark$sql$execution$datasources$csv$UnivocityParser$$convert(java.lang.String[])#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/csv/UnivocityParser/org$apache$spark$sql$execution$datasources$csv$UnivocityParser$$convert(java.lang.String[])#scala/collection/mutable/ArrayOps/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/csv/UnivocityParser/org$apache$spark$sql$execution$datasources$csv$UnivocityParser$$convert(java.lang.String[])#scala/Option/isEmpty()
org/apache/spark/sql/execution/datasources/csv/UnivocityParser/org$apache$spark$sql$execution$datasources$csv$UnivocityParser$$convert(java.lang.String[])#org/apache/spark/sql/catalyst/util/BadRecordException/BadRecordException(scala.Function0,scala.Function0,java.lang.Throwable)
org/apache/spark/sql/execution/datasources/csv/UnivocityParser/org$apache$spark$sql$execution$datasources$csv$UnivocityParser$$convert(java.lang.String[])#scala/collection/mutable/ArrayOps/take(int)
org/apache/spark/sql/execution/datasources/csv/UnivocityParser/org$apache$spark$sql$execution$datasources$csv$UnivocityParser$$convert(java.lang.String[])#java/lang/RuntimeException/RuntimeException(java.lang.String)
org/apache/spark/sql/execution/datasources/csv/UnivocityParser/org$apache$spark$sql$execution$datasources$csv$UnivocityParser$$convert(java.lang.String[])#scala/Option/get()
org/apache/spark/sql/execution/datasources/csv/UnivocityParser/org$apache$spark$sql$execution$datasources$csv$UnivocityParser$$convert(java.lang.String[])#org/apache/spark/sql/execution/datasources/csv/UnivocityParser$$anonfun$org$apache$spark$sql$execution$datasources$csv$UnivocityParser$$convert$3/3(org.apache.spark.sql.execution.datasources.csv.UnivocityParser)
org/apache/spark/sql/execution/datasources/csv/UnivocityParser/org$apache$spark$sql$execution$datasources$csv$UnivocityParser$$convert(java.lang.String[])#org/apache/spark/sql/types/StructType/length()
org/apache/spark/sql/execution/datasources/csv/UnivocityParser/org$apache$spark$sql$execution$datasources$csv$UnivocityParser$$convert(java.lang.String[])#org/apache/spark/sql/execution/datasources/csv/UnivocityParser$$anonfun$org$apache$spark$sql$execution$datasources$csv$UnivocityParser$$convert$4/4(org.apache.spark.sql.execution.datasources.csv.UnivocityParser)
org/apache/spark/sql/execution/datasources/csv/UnivocityParser/org$apache$spark$sql$execution$datasources$csv$UnivocityParser$$convert(java.lang.String[])#scala/util/control/NonFatal$/unapply(java.lang.Throwable)
org/apache/spark/sql/execution/datasources/csv/UnivocityParser/org$apache$spark$sql$execution$datasources$csv$UnivocityParser$$convert(java.lang.String[])#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/UnivocityParser/org$apache$spark$sql$execution$datasources$csv$UnivocityParser$$convert(java.lang.String[])#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/csv/UnivocityParser/org$apache$spark$sql$execution$datasources$csv$UnivocityParser$$convert(java.lang.String[])#org/apache/spark/sql/catalyst/expressions/GenericInternalRow/update(int,java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/UnivocityParser/org$apache$spark$sql$execution$datasources$csv$UnivocityParser$$convert(java.lang.String[])#org/apache/spark/sql/execution/datasources/csv/UnivocityParser$$anonfun$org$apache$spark$sql$execution$datasources$csv$UnivocityParser$$convert$1/1(org.apache.spark.sql.execution.datasources.csv.UnivocityParser)
org/apache/spark/sql/execution/datasources/csv/UnivocityParser/makeConverter(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/UnivocityParser$$anonfun$makeConverter$9/9(org.apache.spark.sql.execution.datasources.csv.UnivocityParser,java.lang.String,boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/UnivocityParser/makeConverter(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/UnivocityParser$$anonfun$makeConverter$8/8(org.apache.spark.sql.execution.datasources.csv.UnivocityParser,java.lang.String,boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions,org.apache.spark.sql.types.DecimalType)
org/apache/spark/sql/execution/datasources/csv/UnivocityParser/makeConverter(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/UnivocityParser$$anonfun$makeConverter$5/5(org.apache.spark.sql.execution.datasources.csv.UnivocityParser,java.lang.String,boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/UnivocityParser/makeConverter(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/UnivocityParser$$anonfun$makeConverter$4/4(org.apache.spark.sql.execution.datasources.csv.UnivocityParser,java.lang.String,boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/UnivocityParser/makeConverter(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/UnivocityParser/makeConverter(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/UnivocityParser$$anonfun$makeConverter$3/3(org.apache.spark.sql.execution.datasources.csv.UnivocityParser,java.lang.String,boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/UnivocityParser/makeConverter(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/UnivocityParser$$anonfun$makeConverter$2/2(org.apache.spark.sql.execution.datasources.csv.UnivocityParser,java.lang.String,boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/UnivocityParser/makeConverter(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#java/lang/RuntimeException/RuntimeException(java.lang.String)
org/apache/spark/sql/execution/datasources/csv/UnivocityParser/makeConverter(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/UnivocityParser$$anonfun$makeConverter$1/1(org.apache.spark.sql.execution.datasources.csv.UnivocityParser,java.lang.String,boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/UnivocityParser/makeConverter(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/types/DataType/typeName()
org/apache/spark/sql/execution/datasources/csv/UnivocityParser/makeConverter(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/csv/UnivocityParser/makeConverter(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/UnivocityParser$$anonfun$makeConverter$12/12(org.apache.spark.sql.execution.datasources.csv.UnivocityParser,java.lang.String,boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions,org.apache.spark.sql.types.UserDefinedType)
org/apache/spark/sql/execution/datasources/csv/UnivocityParser/makeConverter(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/UnivocityParser$$anonfun$makeConverter$7/7(org.apache.spark.sql.execution.datasources.csv.UnivocityParser,java.lang.String,boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/UnivocityParser/makeConverter(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/csv/UnivocityParser/makeConverter(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/UnivocityParser$$anonfun$makeConverter$6/6(org.apache.spark.sql.execution.datasources.csv.UnivocityParser,java.lang.String,boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/UnivocityParser/makeConverter(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/UnivocityParser$$anonfun$makeConverter$10/10(org.apache.spark.sql.execution.datasources.csv.UnivocityParser,java.lang.String,boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/UnivocityParser/makeConverter(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/csv/UnivocityParser/makeConverter(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/UnivocityParser$$anonfun$makeConverter$11/11(org.apache.spark.sql.execution.datasources.csv.UnivocityParser,java.lang.String,boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convert(org.apache.spark.sql.types.StructType)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convert(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/types/StructType/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convert(org.apache.spark.sql.types.StructType)#org/apache/parquet/schema/Types/buildMessage()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convert(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter$$anonfun$convert$1/1(org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convert(org.apache.spark.sql.types.StructType)#org/apache/parquet/schema/Types$MessageTypeBuilder/addFields(org.apache.parquet.schema.Type[])
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convert(org.apache.spark.sql.types.StructType)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convert(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter$/SPARK_PARQUET_SCHEMA_NAME()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convert(org.apache.spark.sql.types.StructType)#org/apache/parquet/schema/Types$BaseGroupBuilder/named(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convert(org.apache.spark.sql.types.StructType)#scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/checkFieldName(java.lang.String)#org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter$$anonfun$checkFieldName$1/1(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/checkFieldName(java.lang.String)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/checkFieldName(java.lang.String)#java/lang/String/split(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/checkFieldName(java.lang.String)#org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter$/checkFieldName(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/checkFieldName(java.lang.String)#java/lang/String/trim()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/checkFieldName(java.lang.String)#scala/collection/mutable/ArrayOps/mkString(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/checkFieldName(java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/checkFieldName(java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/checkFieldName(java.lang.String)#org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter$/checkConversionRequirement(scala.Function0,java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/checkFieldName(java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/checkFieldName(java.lang.String)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/checkFieldName(java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/checkFieldName(java.lang.String)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/checkFieldName(java.lang.String)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/makeDecimalType$1(int,org.apache.parquet.schema.PrimitiveType,org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName)#org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter$$anonfun$makeDecimalType$1$1/1(org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter,int,int)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/makeDecimalType$1(int,org.apache.parquet.schema.PrimitiveType,org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/makeDecimalType$1(int,org.apache.parquet.schema.PrimitiveType,org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName)#org/apache/parquet/schema/DecimalMetadata/getPrecision()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/makeDecimalType$1(int,org.apache.parquet.schema.PrimitiveType,org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName)#org/apache/parquet/schema/PrimitiveType/getDecimalMetadata()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/makeDecimalType$1(int,org.apache.parquet.schema.PrimitiveType,org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/makeDecimalType$1(int,org.apache.parquet.schema.PrimitiveType,org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName)#org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter$/checkConversionRequirement(scala.Function0,java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/makeDecimalType$1(int,org.apache.parquet.schema.PrimitiveType,org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/makeDecimalType$1(int,org.apache.parquet.schema.PrimitiveType,org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/makeDecimalType$1(int,org.apache.parquet.schema.PrimitiveType,org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName)#org/apache/spark/sql/types/DecimalType/DecimalType(int,int)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/makeDecimalType$1(int,org.apache.parquet.schema.PrimitiveType,org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName)#org/apache/parquet/schema/DecimalMetadata/getScale()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/makeDecimalType$1(int,org.apache.parquet.schema.PrimitiveType,org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertPrimitiveField(org.apache.parquet.schema.PrimitiveType)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertPrimitiveField(org.apache.parquet.schema.PrimitiveType)#org/apache/parquet/schema/PrimitiveType/getTypeLength()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertPrimitiveField(org.apache.parquet.schema.PrimitiveType)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertPrimitiveField(org.apache.parquet.schema.PrimitiveType)#org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter$$anonfun$convertPrimitiveField$1/1(org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertPrimitiveField(org.apache.parquet.schema.PrimitiveType)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertPrimitiveField(org.apache.parquet.schema.PrimitiveType)#org/apache/parquet/schema/PrimitiveType/getPrimitiveTypeName()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertPrimitiveField(org.apache.parquet.schema.PrimitiveType)#org/apache/spark/sql/types/Decimal$/MAX_INT_DIGITS()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertPrimitiveField(org.apache.parquet.schema.PrimitiveType)#org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter$/maxPrecisionForBytes(int)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertPrimitiveField(org.apache.parquet.schema.PrimitiveType)#org/apache/spark/sql/types/Decimal$/MAX_LONG_DIGITS()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertPrimitiveField(org.apache.parquet.schema.PrimitiveType)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertPrimitiveField(org.apache.parquet.schema.PrimitiveType)#org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter$/checkConversionRequirement(scala.Function0,java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertPrimitiveField(org.apache.parquet.schema.PrimitiveType)#org/apache/parquet/schema/PrimitiveType/getOriginalType()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertPrimitiveField(org.apache.parquet.schema.PrimitiveType)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertPrimitiveField(org.apache.parquet.schema.PrimitiveType)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertPrimitiveField(org.apache.parquet.schema.PrimitiveType)#org/apache/spark/sql/internal/SQLConf$/PARQUET_INT96_AS_TIMESTAMP()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertPrimitiveField(org.apache.parquet.schema.PrimitiveType)#org/apache/spark/internal/config/ConfigEntry/key()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertPrimitiveField(org.apache.parquet.schema.PrimitiveType)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertPrimitiveField(org.apache.parquet.schema.PrimitiveType)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/illegalType$1(org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName,org.apache.parquet.schema.OriginalType)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/illegalType$1(org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName,org.apache.parquet.schema.OriginalType)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/illegalType$1(org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName,org.apache.parquet.schema.OriginalType)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/illegalType$1(org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName,org.apache.parquet.schema.OriginalType)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/illegalType$1(org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName,org.apache.parquet.schema.OriginalType)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/illegalType$1(org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName,org.apache.parquet.schema.OriginalType)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/illegalType$1(org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName,org.apache.parquet.schema.OriginalType)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/illegalType$1(org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName,org.apache.parquet.schema.OriginalType)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/illegalType$1(org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName,org.apache.parquet.schema.OriginalType)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetSchemaConverter$$isElementType(org.apache.parquet.schema.Type,java.lang.String)#org/apache/parquet/schema/Type/getName()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetSchemaConverter$$isElementType(org.apache.parquet.schema.Type,java.lang.String)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetSchemaConverter$$isElementType(org.apache.parquet.schema.Type,java.lang.String)#org/apache/parquet/schema/Type/asGroupType()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetSchemaConverter$$isElementType(org.apache.parquet.schema.Type,java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetSchemaConverter$$isElementType(org.apache.parquet.schema.Type,java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetSchemaConverter$$isElementType(org.apache.parquet.schema.Type,java.lang.String)#org/apache/parquet/schema/GroupType/getFieldCount()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetSchemaConverter$$isElementType(org.apache.parquet.schema.Type,java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetSchemaConverter$$isElementType(org.apache.parquet.schema.Type,java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetSchemaConverter$$isElementType(org.apache.parquet.schema.Type,java.lang.String)#org/apache/parquet/schema/Type/isPrimitive()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/spark/sql/types/StructField/copy$default$4()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/spark/sql/types/Decimal$/MAX_INT_DIGITS()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/parquet/schema/Types$GroupBuilder/as(org.apache.parquet.schema.OriginalType)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/parquet/schema/Types$BasePrimitiveBuilder/length(int)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter$/minBytesForPrecision()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#scala/Option/get()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter$$anonfun$convertField$1/1(org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/parquet/schema/Types$BasePrimitiveBuilder/precision(int)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/spark/sql/types/StructField/copy$default$1()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/parquet/schema/ConversionPatterns/mapType(org.apache.parquet.schema.Type$Repetition,java.lang.String,org.apache.parquet.schema.Type,org.apache.parquet.schema.Type)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter$/checkFieldName(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/spark/sql/types/StructField/copy(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.types.Metadata)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/spark/sql/types/StructField/copy$default$3()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/spark/sql/types/StructField$/apply$default$4()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/spark/sql/types/StructField/dataType()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/spark/sql/types/StructField/StructField(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.types.Metadata)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/parquet/schema/Types$GroupBuilder/addField(org.apache.parquet.schema.Type)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/parquet/schema/Types/buildGroup(org.apache.parquet.schema.Type$Repetition)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/parquet/schema/Types$BasePrimitiveBuilder/named(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#scala/Option/isEmpty()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/spark/sql/types/UserDefinedType/sqlType()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/spark/sql/types/Decimal$/MAX_LONG_DIGITS()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/spark/sql/types/MapType/valueType()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#scala/collection/mutable/ArrayOps/foldLeft(java.lang.Object,scala.Function2)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/parquet/schema/Types/repeatedGroup()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/spark/sql/types/StructField/name()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/parquet/schema/Types$BaseGroupBuilder/named(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/parquet/schema/Types$Builder/named(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/spark/sql/types/ArrayType/elementType()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/parquet/schema/Types$BasePrimitiveBuilder/scale(int)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/parquet/schema/Types$BaseGroupBuilder/addField(org.apache.parquet.schema.Type)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/parquet/schema/Types/primitive(org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName,org.apache.parquet.schema.Type$Repetition)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/parquet/schema/Types$PrimitiveBuilder/as(org.apache.parquet.schema.OriginalType)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/spark/sql/types/MapType/valueContainsNull()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/parquet/schema/Types$PrimitiveBuilder/named(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/spark/sql/types/MapType/keyType()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/spark/sql/types/DecimalType$Fixed$/unapply(org.apache.spark.sql.types.DecimalType)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#org/apache/spark/sql/types/ArrayType/containsNull()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#scala/Tuple2/_2$mcI$sp()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/convertField(org.apache.spark.sql.types.StructField,org.apache.parquet.schema.Type$Repetition)#scala/Tuple2/_1$mcI$sp()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/typeNotImplemented$1(org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName,org.apache.parquet.schema.OriginalType)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/typeNotImplemented$1(org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName,org.apache.parquet.schema.OriginalType)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/typeNotImplemented$1(org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName,org.apache.parquet.schema.OriginalType)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/typeNotImplemented$1(org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName,org.apache.parquet.schema.OriginalType)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/typeNotImplemented$1(org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName,org.apache.parquet.schema.OriginalType)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/typeNotImplemented$1(org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName,org.apache.parquet.schema.OriginalType)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/typeNotImplemented$1(org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName,org.apache.parquet.schema.OriginalType)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/typeNotImplemented$1(org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName,org.apache.parquet.schema.OriginalType)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/typeNotImplemented$1(org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName,org.apache.parquet.schema.OriginalType)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/typeNotSupported$1(org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName,org.apache.parquet.schema.OriginalType)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/typeNotSupported$1(org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName,org.apache.parquet.schema.OriginalType)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/typeNotSupported$1(org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName,org.apache.parquet.schema.OriginalType)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/typeNotSupported$1(org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName,org.apache.parquet.schema.OriginalType)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/typeNotSupported$1(org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName,org.apache.parquet.schema.OriginalType)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/typeNotSupported$1(org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName,org.apache.parquet.schema.OriginalType)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/typeNotSupported$1(org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName,org.apache.parquet.schema.OriginalType)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/typeNotSupported$1(org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName,org.apache.parquet.schema.OriginalType)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/typeNotSupported$1(org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName,org.apache.parquet.schema.OriginalType)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/7/applyOrElse(org.apache.spark.sql.execution.SparkPlan,scala.Function1)#scala/Predef$/Map()
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/7/applyOrElse(org.apache.spark.sql.execution.SparkPlan,scala.Function1)#scala/Predef$/ArrowAssoc(java.lang.Object)
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/7/applyOrElse(org.apache.spark.sql.execution.SparkPlan,scala.Function1)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/7/applyOrElse(org.apache.spark.sql.execution.SparkPlan,scala.Function1)#scala/collection/immutable/MapLike/mapValues(scala.Function1)
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/7/applyOrElse(org.apache.spark.sql.execution.SparkPlan,scala.Function1)#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/7/applyOrElse(org.apache.spark.sql.execution.SparkPlan,scala.Function1)#scala/collection/immutable/Map$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/7/applyOrElse(org.apache.spark.sql.execution.SparkPlan,scala.Function1)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/7/applyOrElse(org.apache.spark.sql.execution.SparkPlan,scala.Function1)#org/apache/spark/sql/execution/streaming/ProgressReporter$$anonfun$7$$anonfun$applyOrElse$1/1(org.apache.spark.sql.execution.streaming.ProgressReporter$$anonfun$7)
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/7/applyOrElse(org.apache.spark.sql.execution.SparkPlan,scala.Function1)#scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readBatch(int,org.apache.spark.sql.execution.vectorized.ColumnVector)#java/io/IOException/IOException(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readBatch(int,org.apache.spark.sql.execution.vectorized.ColumnVector)#org/apache/parquet/schema/PrimitiveType$PrimitiveTypeName/ordinal()
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readBatch(int,org.apache.spark.sql.execution.vectorized.ColumnVector)#org/apache/parquet/column/ColumnDescriptor/getTypeLength()
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readBatch(int,org.apache.spark.sql.execution.vectorized.ColumnVector)#java/lang/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readBatch(int,org.apache.spark.sql.execution.vectorized.ColumnVector)#java/lang/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readBatch(int,org.apache.spark.sql.execution.vectorized.ColumnVector)#java/lang/StringBuilder/append(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readBatch(int,org.apache.spark.sql.execution.vectorized.ColumnVector)#org/apache/parquet/column/ColumnDescriptor/getType()
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readBatch(int,org.apache.spark.sql.execution.vectorized.ColumnVector)#java/lang/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readBatch(int,org.apache.spark.sql.execution.vectorized.ColumnVector)#java/lang/Math/min(int,int)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readFixedLenByteArrayBatch(int,int,org.apache.spark.sql.execution.vectorized.ColumnVector,int)#org/apache/spark/sql/types/DecimalType/is32BitDecimalType(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readFixedLenByteArrayBatch(int,int,org.apache.spark.sql.execution.vectorized.ColumnVector,int)#org/apache/spark/sql/types/DecimalType/isByteArrayDecimalType(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readFixedLenByteArrayBatch(int,int,org.apache.spark.sql.execution.vectorized.ColumnVector,int)#java/lang/StringBuilder/append(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readFixedLenByteArrayBatch(int,int,org.apache.spark.sql.execution.vectorized.ColumnVector,int)#java/lang/UnsupportedOperationException/UnsupportedOperationException(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readFixedLenByteArrayBatch(int,int,org.apache.spark.sql.execution.vectorized.ColumnVector,int)#java/lang/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readFixedLenByteArrayBatch(int,int,org.apache.spark.sql.execution.vectorized.ColumnVector,int)#org/apache/spark/sql/types/DecimalType/is64BitDecimalType(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readFixedLenByteArrayBatch(int,int,org.apache.spark.sql.execution.vectorized.ColumnVector,int)#org/apache/parquet/io/api/Binary/getBytes()
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readFixedLenByteArrayBatch(int,int,org.apache.spark.sql.execution.vectorized.ColumnVector,int)#java/lang/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readFixedLenByteArrayBatch(int,int,org.apache.spark.sql.execution.vectorized.ColumnVector,int)#java/lang/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/initDataReader(org.apache.parquet.column.Encoding,byte[],int)#java/lang/StringBuilder/append(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/initDataReader(org.apache.parquet.column.Encoding,byte[],int)#org/apache/parquet/column/Encoding/usesDictionary()
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/initDataReader(org.apache.parquet.column.Encoding,byte[],int)#java/lang/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/initDataReader(org.apache.parquet.column.Encoding,byte[],int)#java/lang/UnsupportedOperationException/UnsupportedOperationException(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/initDataReader(org.apache.parquet.column.Encoding,byte[],int)#java/lang/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/initDataReader(org.apache.parquet.column.Encoding,byte[],int)#org/apache/parquet/column/values/ValuesReader/initFromPage(int,byte[],int)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/initDataReader(org.apache.parquet.column.Encoding,byte[],int)#java/lang/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/initDataReader(org.apache.parquet.column.Encoding,byte[],int)#java/io/IOException/IOException(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/initDataReader(org.apache.parquet.column.Encoding,byte[],int)#java/io/IOException/IOException(java.lang.String,java.lang.Throwable)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/decodeDictionaryIds(int,int,org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.execution.vectorized.ColumnVector)#org/apache/spark/sql/types/DecimalType/is64BitDecimalType(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/decodeDictionaryIds(int,int,org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.execution.vectorized.ColumnVector)#org/apache/spark/sql/types/DecimalType/isByteArrayDecimalType(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/decodeDictionaryIds(int,int,org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.execution.vectorized.ColumnVector)#org/apache/parquet/column/Dictionary/decodeToLong(int)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/decodeDictionaryIds(int,int,org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.execution.vectorized.ColumnVector)#org/apache/spark/sql/catalyst/util/DateTimeUtils/fromMillis(long)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/decodeDictionaryIds(int,int,org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.execution.vectorized.ColumnVector)#org/apache/parquet/schema/PrimitiveType$PrimitiveTypeName/ordinal()
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/decodeDictionaryIds(int,int,org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.execution.vectorized.ColumnVector)#java/lang/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/decodeDictionaryIds(int,int,org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.execution.vectorized.ColumnVector)#java/lang/UnsupportedOperationException/UnsupportedOperationException(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/decodeDictionaryIds(int,int,org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.execution.vectorized.ColumnVector)#org/apache/parquet/column/Dictionary/decodeToFloat(int)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/decodeDictionaryIds(int,int,org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.execution.vectorized.ColumnVector)#java/lang/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/decodeDictionaryIds(int,int,org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.execution.vectorized.ColumnVector)#org/apache/parquet/column/Dictionary/decodeToBinary(int)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/decodeDictionaryIds(int,int,org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.execution.vectorized.ColumnVector)#org/apache/spark/sql/types/DecimalType/is32BitDecimalType(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/decodeDictionaryIds(int,int,org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.execution.vectorized.ColumnVector)#java/lang/StringBuilder/append(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/decodeDictionaryIds(int,int,org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.execution.vectorized.ColumnVector)#org/apache/parquet/column/ColumnDescriptor/getType()
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/decodeDictionaryIds(int,int,org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.execution.vectorized.ColumnVector)#org/apache/parquet/column/Dictionary/decodeToInt(int)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/decodeDictionaryIds(int,int,org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.execution.vectorized.ColumnVector)#org/apache/parquet/column/Dictionary/decodeToDouble(int)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/decodeDictionaryIds(int,int,org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.execution.vectorized.ColumnVector)#org/apache/parquet/io/api/Binary/getBytes()
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/decodeDictionaryIds(int,int,org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.execution.vectorized.ColumnVector)#java/lang/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/decodeDictionaryIds(int,int,org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.execution.vectorized.ColumnVector)#java/lang/UnsupportedOperationException/UnsupportedOperationException()
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readPageV2(org.apache.parquet.column.page.DataPageV2)#org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase$ValuesReaderIntIterator/ValuesReaderIntIterator(org.apache.parquet.column.values.ValuesReader)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readPageV2(org.apache.parquet.column.page.DataPageV2)#org/apache/parquet/bytes/BytesUtils/getWidthFromMaxInt(int)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readPageV2(org.apache.parquet.column.page.DataPageV2)#java/io/IOException/IOException(java.lang.String,java.lang.Throwable)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readPageV2(org.apache.parquet.column.page.DataPageV2)#java/lang/StringBuilder/append(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readPageV2(org.apache.parquet.column.page.DataPageV2)#org/apache/parquet/column/page/DataPageV2/getDataEncoding()
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readPageV2(org.apache.parquet.column.page.DataPageV2)#org/apache/parquet/column/page/DataPageV2/getValueCount()
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readPageV2(org.apache.parquet.column.page.DataPageV2)#org/apache/parquet/column/page/DataPageV2/getRepetitionLevels()
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readPageV2(org.apache.parquet.column.page.DataPageV2)#org/apache/parquet/column/page/DataPageV2/getDefinitionLevels()
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readPageV2(org.apache.parquet.column.page.DataPageV2)#java/lang/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readPageV2(org.apache.parquet.column.page.DataPageV2)#org/apache/parquet/bytes/BytesInput/toByteArray()
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readPageV2(org.apache.parquet.column.page.DataPageV2)#org/apache/parquet/column/ColumnDescriptor/getMaxDefinitionLevel()
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readPageV2(org.apache.parquet.column.page.DataPageV2)#java/lang/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readPageV2(org.apache.parquet.column.page.DataPageV2)#java/lang/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readPageV2(org.apache.parquet.column.page.DataPageV2)#org/apache/parquet/column/page/DataPageV2/getData()
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readPageV2(org.apache.parquet.column.page.DataPageV2)#org/apache/parquet/column/ColumnDescriptor/getMaxRepetitionLevel()
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readPageV1(org.apache.parquet.column.page.DataPageV1)#org/apache/parquet/bytes/BytesUtils/getWidthFromMaxInt(int)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readPageV1(org.apache.parquet.column.page.DataPageV1)#java/io/IOException/IOException(java.lang.String,java.lang.Throwable)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readPageV1(org.apache.parquet.column.page.DataPageV1)#java/lang/StringBuilder/append(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readPageV1(org.apache.parquet.column.page.DataPageV1)#org/apache/parquet/column/page/DataPageV1/getValueEncoding()
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readPageV1(org.apache.parquet.column.page.DataPageV1)#org/apache/parquet/column/page/DataPageV1/getValueCount()
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readPageV1(org.apache.parquet.column.page.DataPageV1)#org/apache/parquet/column/page/DataPageV1/getBytes()
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readPageV1(org.apache.parquet.column.page.DataPageV1)#org/apache/parquet/column/values/ValuesReader/getNextOffset()
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readPageV1(org.apache.parquet.column.page.DataPageV1)#java/lang/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readPageV1(org.apache.parquet.column.page.DataPageV1)#org/apache/parquet/bytes/BytesInput/toByteArray()
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readPageV1(org.apache.parquet.column.page.DataPageV1)#java/lang/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readPageV1(org.apache.parquet.column.page.DataPageV1)#org/apache/parquet/column/ColumnDescriptor/getMaxDefinitionLevel()
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readPageV1(org.apache.parquet.column.page.DataPageV1)#org/apache/parquet/column/page/DataPageV1/getRlEncoding()
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readPageV1(org.apache.parquet.column.page.DataPageV1)#org/apache/parquet/column/values/ValuesReader/initFromPage(int,byte[],int)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readPageV1(org.apache.parquet.column.page.DataPageV1)#java/lang/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readPageV1(org.apache.parquet.column.page.DataPageV1)#org/apache/parquet/column/Encoding/getValuesReader(org.apache.parquet.column.ColumnDescriptor,org.apache.parquet.column.ValuesType)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readPageV1(org.apache.parquet.column.page.DataPageV1)#org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase$ValuesReaderIntIterator/ValuesReaderIntIterator(org.apache.parquet.column.values.ValuesReader)
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readPageV1(org.apache.parquet.column.page.DataPageV1)#org/apache/parquet/column/page/DataPageV1/getDlEncoding()
org/apache/spark/sql/execution/datasources/parquet/VectorizedColumnReader/readPageV1(org.apache.parquet.column.page.DataPageV1)#java/lang/UnsupportedOperationException/UnsupportedOperationException(java.lang.String)
org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding/Decoder/next(org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/execution/columnar/NativeColumnType/setField(org.apache.spark.sql.catalyst.InternalRow,int,java.lang.Object)
org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding/Decoder/next(org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding$Decoder/run()
org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding/Decoder/next(org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding$Decoder/currentValue_$eq(java.lang.Object)
org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding/Decoder/next(org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding$Decoder/currentValue()
org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding/Decoder/next(org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding$Decoder/valueCount_$eq(int)
org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding/Decoder/next(org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding$Decoder/valueCount()
org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding/Decoder/next(org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding$Decoder/run_$eq(int)
org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding/Decoder/next(org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/execution/columnar/ByteBufferHelper$/getInt(java.nio.ByteBuffer)
org/apache/spark/sql/execution/columnar/compression/RunLengthEncoding/Decoder/next(org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/execution/columnar/NativeColumnType/extract(java.nio.ByteBuffer)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/toString()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/toString()#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/toString()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/toString()#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/toString()#scala/collection/mutable/ArrayOps/nonEmpty()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/toString()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/toString()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/toString()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/toString()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/toString()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/columnPartition(org.apache.spark.sql.execution.datasources.jdbc.JDBCPartitioningInfo)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/columnPartition(org.apache.spark.sql.execution.datasources.jdbc.JDBCPartitioningInfo)#scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/columnPartition(org.apache.spark.sql.execution.datasources.jdbc.JDBCPartitioningInfo)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/columnPartition(org.apache.spark.sql.execution.datasources.jdbc.JDBCPartitioningInfo)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/columnPartition(org.apache.spark.sql.execution.datasources.jdbc.JDBCPartitioningInfo)#scala/collection/mutable/ArrayBuffer/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/columnPartition(org.apache.spark.sql.execution.datasources.jdbc.JDBCPartitioningInfo)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation$$anonfun$columnPartition$1/1(long,long)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/columnPartition(org.apache.spark.sql.execution.datasources.jdbc.JDBCPartitioningInfo)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/columnPartition(org.apache.spark.sql.execution.datasources.jdbc.JDBCPartitioningInfo)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation$/columnPartition(org.apache.spark.sql.execution.datasources.jdbc.JDBCPartitioningInfo)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/columnPartition(org.apache.spark.sql.execution.datasources.jdbc.JDBCPartitioningInfo)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation$$anonfun$1/1(org.apache.spark.sql.execution.datasources.jdbc.JDBCPartitioningInfo,long,long)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/columnPartition(org.apache.spark.sql.execution.datasources.jdbc.JDBCPartitioningInfo)#scala/Predef$/require(boolean,scala.Function0)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/columnPartition(org.apache.spark.sql.execution.datasources.jdbc.JDBCPartitioningInfo)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation$/logWarning(scala.Function0)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/columnPartition(org.apache.spark.sql.execution.datasources.jdbc.JDBCPartitioningInfo)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/columnPartition(org.apache.spark.sql.execution.datasources.jdbc.JDBCPartitioningInfo)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/columnPartition(org.apache.spark.sql.execution.datasources.jdbc.JDBCPartitioningInfo)#scala/collection/mutable/ArrayBuffer/ArrayBuffer()
org/apache/spark/sql/execution/streaming/RateStreamSource/toString()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/RateStreamSource/toString()#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/streaming/RateStreamSource/toString()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/streaming/RateStreamSource/toString()#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/streaming/RateStreamSource/toString()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/RateStreamSource/toString()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/streaming/RateStreamSource/toString()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/RateStreamSource/toString()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/RateStreamSource/toString()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/streaming/RateStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/streaming/RateStreamSource$$anonfun$getBatch$1/1(org.apache.spark.sql.execution.streaming.RateStreamSource,long,long)
org/apache/spark/sql/execution/streaming/RateStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/streaming/RateStreamSource$/valueAtSecond(long,long,long)
org/apache/spark/sql/execution/streaming/RateStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/Option/flatMap(scala.Function1)
org/apache/spark/sql/execution/streaming/RateStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/RateStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/streaming/RateStreamSource$$anonfun$5/5(org.apache.spark.sql.execution.streaming.RateStreamSource)
org/apache/spark/sql/execution/streaming/RateStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/streaming/RateStreamSource$$anonfun$4/4(org.apache.spark.sql.execution.streaming.RateStreamSource)
org/apache/spark/sql/execution/streaming/RateStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/streaming/RateStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/streaming/RateStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/streaming/RateStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/streaming/LongOffset$/convert(org.apache.spark.sql.execution.streaming.Offset)
org/apache/spark/sql/execution/streaming/RateStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/Predef$/assert(boolean,scala.Function0)
org/apache/spark/sql/execution/streaming/RateStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#java/util/concurrent/TimeUnit/toMillis(long)
org/apache/spark/sql/execution/streaming/RateStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#java/lang/ArithmeticException/ArithmeticException(java.lang.String)
org/apache/spark/sql/execution/streaming/RateStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/SparkContext/range(long,long,long,int)
org/apache/spark/sql/execution/streaming/RateStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/streaming/RateStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/streaming/RateStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/RateStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/streaming/RateStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/RateStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/streaming/RateStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/streaming/RateStreamSource$$anonfun$11/11(org.apache.spark.sql.execution.streaming.RateStreamSource)
org/apache/spark/sql/execution/streaming/RateStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/streaming/RateStreamSource$$anonfun$10/10(org.apache.spark.sql.execution.streaming.RateStreamSource)
org/apache/spark/sql/execution/streaming/RateStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/SparkContext/emptyRDD(scala.reflect.ClassTag)
org/apache/spark/sql/execution/streaming/RateStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/RateStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/rdd/RDD/map(scala.Function1,scala.reflect.ClassTag)
org/apache/spark/sql/execution/streaming/RateStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/streaming/RateStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/streaming/RateStreamSource$$anonfun$12/12(org.apache.spark.sql.execution.streaming.RateStreamSource,long,long,double)
org/apache/spark/sql/execution/streaming/RateStreamSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/streaming/RateStreamSource$$anonfun$getBatch$2/2(org.apache.spark.sql.execution.streaming.RateStreamSource,long,long,long,long)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$QualifiedNameContext/getText()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTableType$/EXTERNAL()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$9/9(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$8/8(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/catalyst/parser/ParseException/ParseException(java.lang.String,org.antlr.v4.runtime.ParserRuleContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$10()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$12()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$9()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$15()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTable/CatalogTable(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableContext/bucketSpec()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/catalyst/parser/ParserUtils$/operationNotAllowed(java.lang.String,org.antlr.v4.runtime.ParserRuleContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$7/7(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#scala/Option/isDefined()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#scala/Tuple4/_1()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#scala/Tuple4/_2()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1/apply()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#scala/Tuple4/_3()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#scala/Tuple4/_4()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableContext/colTypeList()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#scala/Option/nonEmpty()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$TableProviderContext/qualifiedName()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#scala/Option/orElse(scala.Function0)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$apply$11/11(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder/plan(org.antlr.v4.runtime.ParserRuleContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$2()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#scala/runtime/BoxesRunTime/boxToBoolean(boolean)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$3()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/execution/datasources/DataSource$/buildStorageFormatFromOptions(scala.collection.immutable.Map)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$4()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableContext/createTableHeader()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableContext/tableProvider()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$5()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$6()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$14/14(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1,scala.Option)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableContext/locationSpec()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$11()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$10/10(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$8()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$13()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$16/16(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$15/15(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$13/13(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$12/12(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy(scala.Option,scala.Option,scala.Option,scala.Option,boolean,scala.collection.immutable.Map)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTable$1$$anonfun$11/11(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateTable$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableContext/query()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$16()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$17()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/locationUri()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTableType$/MANAGED()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#scala/Tuple4/Tuple4(java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder/logWarning(scala.Function0)
org/apache/spark/sql/internal/HiveSerDe/getDefaultStorage(org.apache.spark.sql.internal.SQLConf)#org/apache/spark/sql/internal/HiveSerDe$$anonfun$4/4()
org/apache/spark/sql/internal/HiveSerDe/getDefaultStorage(org.apache.spark.sql.internal.SQLConf)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat$/empty()
org/apache/spark/sql/internal/HiveSerDe/getDefaultStorage(org.apache.spark.sql.internal.SQLConf)#org/apache/spark/sql/internal/HiveSerDe$$anonfun$5/5()
org/apache/spark/sql/internal/HiveSerDe/getDefaultStorage(org.apache.spark.sql.internal.SQLConf)#org/apache/spark/sql/internal/HiveSerDe$$anonfun$6/6()
org/apache/spark/sql/internal/HiveSerDe/getDefaultStorage(org.apache.spark.sql.internal.SQLConf)#org/apache/spark/sql/internal/HiveSerDe$/getDefaultStorage(org.apache.spark.sql.internal.SQLConf)
org/apache/spark/sql/internal/HiveSerDe/getDefaultStorage(org.apache.spark.sql.internal.SQLConf)#scala/Option/flatMap(scala.Function1)
org/apache/spark/sql/internal/HiveSerDe/getDefaultStorage(org.apache.spark.sql.internal.SQLConf)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$1()
org/apache/spark/sql/internal/HiveSerDe/getDefaultStorage(org.apache.spark.sql.internal.SQLConf)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$5()
org/apache/spark/sql/internal/HiveSerDe/getDefaultStorage(org.apache.spark.sql.internal.SQLConf)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$6()
org/apache/spark/sql/internal/HiveSerDe/getDefaultStorage(org.apache.spark.sql.internal.SQLConf)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy(scala.Option,scala.Option,scala.Option,scala.Option,boolean,scala.collection.immutable.Map)
org/apache/spark/sql/internal/HiveSerDe/getDefaultStorage(org.apache.spark.sql.internal.SQLConf)#org/apache/spark/sql/internal/SQLConf/getConfString(java.lang.String,java.lang.String)
org/apache/spark/sql/internal/HiveSerDe/getDefaultStorage(org.apache.spark.sql.internal.SQLConf)#org/apache/spark/sql/internal/HiveSerDe$$anonfun$1/1()
org/apache/spark/sql/internal/HiveSerDe/getDefaultStorage(org.apache.spark.sql.internal.SQLConf)#org/apache/spark/sql/internal/HiveSerDe$$anonfun$2/2()
org/apache/spark/sql/internal/HiveSerDe/getDefaultStorage(org.apache.spark.sql.internal.SQLConf)#org/apache/spark/sql/internal/HiveSerDe$$anonfun$3/3()
org/apache/spark/sql/internal/HiveSerDe/getDefaultStorage(org.apache.spark.sql.internal.SQLConf)#org/apache/spark/sql/internal/HiveSerDe$/sourceToSerDe(java.lang.String)
org/apache/spark/sql/internal/HiveSerDe/getDefaultStorage(org.apache.spark.sql.internal.SQLConf)#scala/Option/orElse(scala.Function0)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/org/apache/spark/sql/execution/datasources/json/JsonInferSchema/compatibleRootType/1/apply(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/org/apache/spark/sql/execution/datasources/json/JsonInferSchema/compatibleRootType/1/apply(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/json/JsonInferSchema$/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$compatibleRootType(java.lang.String,org.apache.spark.sql.catalyst.util.ParseMode)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/org/apache/spark/sql/execution/datasources/json/JsonInferSchema/compatibleRootType/1/apply(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#scala/Tuple2/_1()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/org/apache/spark/sql/execution/datasources/json/JsonInferSchema/compatibleRootType/1/apply(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#scala/Tuple2/_2()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/org/apache/spark/sql/execution/datasources/json/JsonInferSchema/compatibleRootType/1/apply(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#scala/Function2/apply(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/org/apache/spark/sql/execution/datasources/json/JsonInferSchema/compatibleRootType/1/apply(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/json/JsonInferSchema$/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$withCorruptField(org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.catalyst.util.ParseMode)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/org/apache/spark/sql/execution/datasources/json/JsonInferSchema/compatibleRootType/1/apply(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/ArrayType/elementType()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/org/apache/spark/sql/execution/datasources/json/JsonInferSchema/compatibleRootType/1/apply(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/org/apache/spark/sql/execution/datasources/json/JsonInferSchema/compatibleRootType/1/apply(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/json/JsonInferSchema$/compatibleType(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/org/apache/spark/sql/execution/datasources/json/JsonInferSchema/compatibleRootType/1/apply(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/debug/package/codegenString(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/zipWithIndex(scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/debug/package/codegenString(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/debug/package$$anonfun$codegenString$3/3(scala.collection.mutable.HashSet,scala.runtime.ObjectRef)
org/apache/spark/sql/execution/debug/package/codegenString(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/debug/package/codegenString(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/debug/package$$anonfun$codegenString$1/1(scala.collection.mutable.HashSet)
org/apache/spark/sql/execution/debug/package/codegenString(org.apache.spark.sql.execution.SparkPlan)#scala/collection/TraversableLike/withFilter(scala.Function1)
org/apache/spark/sql/execution/debug/package/codegenString(org.apache.spark.sql.execution.SparkPlan)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/debug/package/codegenString(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/debug/package$$anonfun$codegenString$2/2()
org/apache/spark/sql/execution/debug/package/codegenString(org.apache.spark.sql.execution.SparkPlan)#scala/collection/generic/FilterMonadic/foreach(scala.Function1)
org/apache/spark/sql/execution/debug/package/codegenString(org.apache.spark.sql.execution.SparkPlan)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/debug/package/codegenString(org.apache.spark.sql.execution.SparkPlan)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/debug/package/codegenString(org.apache.spark.sql.execution.SparkPlan)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/debug/package/codegenString(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/SparkPlan/transform(scala.PartialFunction)
org/apache/spark/sql/execution/debug/package/codegenString(org.apache.spark.sql.execution.SparkPlan)#scala/collection/mutable/HashSet/HashSet()
org/apache/spark/sql/execution/debug/package/codegenString(org.apache.spark.sql.execution.SparkPlan)#scala/collection/mutable/HashSet/size()
org/apache/spark/sql/execution/debug/package/codegenString(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/debug/package$/codegenString(org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/debug/package/codegenString(org.apache.spark.sql.execution.SparkPlan)#scala/collection/mutable/HashSet/toSeq()
org/apache/spark/sql/execution/debug/package/codegenString(org.apache.spark.sql.execution.SparkPlan)#scala/runtime/ObjectRef/create(java.lang.Object)
org/apache/spark/sql/execution/debug/package/codegenString(org.apache.spark.sql.execution.SparkPlan)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/SQLImplicits/typecreator4/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/selectType(scala.reflect.api.Symbols$SymbolApi,java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator4/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ModuleSymbolApi/asModule()
org/apache/spark/sql/SQLImplicits/typecreator4/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticPackage(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator4/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$InternalApi/reificationSupport()
org/apache/spark/sql/SQLImplicits/typecreator4/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/RootClass()
org/apache/spark/sql/SQLImplicits/typecreator4/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticModule(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator4/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ClassSymbolApi/asType()
org/apache/spark/sql/SQLImplicits/typecreator4/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ModuleSymbolApi/moduleClass()
org/apache/spark/sql/SQLImplicits/typecreator4/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/thisPrefix(scala.reflect.api.Symbols$SymbolApi)
org/apache/spark/sql/SQLImplicits/typecreator4/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/TypeRef(scala.reflect.api.Types$TypeApi,scala.reflect.api.Symbols$SymbolApi,scala.collection.immutable.List)
org/apache/spark/sql/SQLImplicits/typecreator4/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticClass(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator4/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/universe()
org/apache/spark/sql/SQLImplicits/typecreator4/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$TypeSymbolApi/toTypeConstructor()
org/apache/spark/sql/SQLImplicits/typecreator4/1/apply(scala.reflect.api.Mirror)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/SQLImplicits/typecreator4/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/SingleType(scala.reflect.api.Types$TypeApi,scala.reflect.api.Symbols$SymbolApi)
org/apache/spark/sql/SQLImplicits/typecreator4/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Universe/internal()
org/apache/spark/sql/SQLImplicits/typecreator4/1/apply(scala.reflect.api.Mirror)#scala/collection/immutable/List$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/FileSourceScanExec/anonfun/createNonBucketedReadRDD/1/apply()#org/apache/spark/sql/execution/FileSourceScanExec$$anonfun$createNonBucketedReadRDD$1/apply()
org/apache/spark/sql/execution/FileSourceScanExec/anonfun/createNonBucketedReadRDD/1/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/FileSourceScanExec/anonfun/createNonBucketedReadRDD/1/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/FileSourceScanExec/anonfun/createNonBucketedReadRDD/1/apply()#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/FileSourceScanExec/anonfun/createNonBucketedReadRDD/1/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/FileSourceScanExec/anonfun/createNonBucketedReadRDD/1/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/FileSourceScanExec/anonfun/createNonBucketedReadRDD/1/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/FileSourceScanExec/anonfun/createNonBucketedReadRDD/1/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/FileSourceScanExec/anonfun/createNonBucketedReadRDD/1/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/streaming/RateStreamSource/anonfun/getBatch/2/apply()#org/apache/spark/sql/execution/streaming/RateStreamSource$$anonfun$getBatch$2/apply()
org/apache/spark/sql/execution/streaming/RateStreamSource/anonfun/getBatch/2/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/RateStreamSource/anonfun/getBatch/2/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/streaming/RateStreamSource/anonfun/getBatch/2/apply()#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/streaming/RateStreamSource/anonfun/getBatch/2/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/RateStreamSource/anonfun/getBatch/2/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/streaming/RateStreamSource/anonfun/getBatch/2/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/RateStreamSource/anonfun/getBatch/2/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/RateStreamSource/anonfun/getBatch/2/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/text/TextFileFormat/anonfun/buildReader/2/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#scala/Option/foreach(scala.Function1)
org/apache/spark/sql/execution/datasources/text/TextFileFormat/anonfun/buildReader/2/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/spark/sql/execution/datasources/text/TextFileFormat$$anonfun$buildReader$2$$anonfun$apply$4/4(org.apache.spark.sql.execution.datasources.text.TextFileFormat$$anonfun$buildReader$2,org.apache.spark.sql.catalyst.expressions.UnsafeRow,org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder,org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter)
org/apache/spark/sql/execution/datasources/text/TextFileFormat/anonfun/buildReader/2/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/spark/TaskContext$/get()
org/apache/spark/sql/execution/datasources/text/TextFileFormat/anonfun/buildReader/2/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/spark/broadcast/Broadcast/value()
org/apache/spark/sql/execution/datasources/text/TextFileFormat/anonfun/buildReader/2/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/UnsafeRow(int)
org/apache/spark/sql/execution/datasources/text/TextFileFormat/anonfun/buildReader/2/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/spark/util/SerializableConfiguration/value()
org/apache/spark/sql/execution/datasources/text/TextFileFormat/anonfun/buildReader/2/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/spark/sql/catalyst/expressions/codegen/BufferHolder/BufferHolder(org.apache.spark.sql.catalyst.expressions.UnsafeRow)
org/apache/spark/sql/execution/datasources/text/TextFileFormat/anonfun/buildReader/2/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/spark/sql/execution/datasources/text/TextFileFormat$$anonfun$buildReader$2$$anonfun$apply$3/3(org.apache.spark.sql.execution.datasources.text.TextFileFormat$$anonfun$buildReader$2,org.apache.spark.sql.catalyst.expressions.UnsafeRow)
org/apache/spark/sql/execution/datasources/text/TextFileFormat/anonfun/buildReader/2/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/datasources/text/TextFileFormat/anonfun/buildReader/2/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/spark/sql/catalyst/expressions/codegen/UnsafeRowWriter/UnsafeRowWriter(org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder,int)
org/apache/spark/sql/execution/datasources/text/TextFileFormat/anonfun/buildReader/2/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/spark/sql/execution/datasources/text/TextFileFormat$$anonfun$buildReader$2$$anonfun$apply$1/1(org.apache.spark.sql.execution.datasources.text.TextFileFormat$$anonfun$buildReader$2,org.apache.spark.sql.execution.datasources.HadoopFileLinesReader)
org/apache/spark/sql/execution/datasources/text/TextFileFormat/anonfun/buildReader/2/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/spark/sql/types/StructType/isEmpty()
org/apache/spark/sql/execution/streaming/TextSocketSource/commit(org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/streaming/LongOffset$/convert(org.apache.spark.sql.execution.streaming.Offset)
org/apache/spark/sql/execution/streaming/TextSocketSource/commit(org.apache.spark.sql.execution.streaming.Offset)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/TextSocketSource/commit(org.apache.spark.sql.execution.streaming.Offset)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/TextSocketSource/commit(org.apache.spark.sql.execution.streaming.Offset)#scala/sys/package$/error(java.lang.String)
org/apache/spark/sql/execution/streaming/TextSocketSource/commit(org.apache.spark.sql.execution.streaming.Offset)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/TextSocketSource/commit(org.apache.spark.sql.execution.streaming.Offset)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/TextSocketSource/commit(org.apache.spark.sql.execution.streaming.Offset)#scala/collection/mutable/ListBuffer/trimStart(int)
org/apache/spark/sql/execution/streaming/TextSocketSource/commit(org.apache.spark.sql.execution.streaming.Offset)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/streaming/TextSocketSource/commit(org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/streaming/TextSocketSource$$anonfun$5/5(org.apache.spark.sql.execution.streaming.TextSocketSource,org.apache.spark.sql.execution.streaming.Offset)
org/apache/spark/sql/execution/streaming/TextSocketSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/streaming/LongOffset$/convert(org.apache.spark.sql.execution.streaming.Offset)
org/apache/spark/sql/execution/streaming/TextSocketSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/collection/mutable/ListBuffer/slice(int,int)
org/apache/spark/sql/execution/streaming/TextSocketSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/TextSocketSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/SQLContext$implicits$/newProductEncoder(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/execution/streaming/TextSocketSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/reflect/api/TypeTags/TypeTag()
org/apache/spark/sql/execution/streaming/TextSocketSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/streaming/TextSocketSource$$anonfun$4/4(org.apache.spark.sql.execution.streaming.TextSocketSource)
org/apache/spark/sql/execution/streaming/TextSocketSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/streaming/TextSocketSource$$anonfun$3/3(org.apache.spark.sql.execution.streaming.TextSocketSource)
org/apache/spark/sql/execution/streaming/TextSocketSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/streaming/TextSocketSource$$anonfun$2/2(org.apache.spark.sql.execution.streaming.TextSocketSource)
org/apache/spark/sql/execution/streaming/TextSocketSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#java/lang/Class/getClassLoader()
org/apache/spark/sql/execution/streaming/TextSocketSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/reflect/runtime/package$/universe()
org/apache/spark/sql/execution/streaming/TextSocketSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/Option/flatMap(scala.Function1)
org/apache/spark/sql/execution/streaming/TextSocketSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/reflect/api/JavaUniverse/runtimeMirror(java.lang.ClassLoader)
org/apache/spark/sql/execution/streaming/TextSocketSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/streaming/TextSocketSource$$typecreator2$1/1(org.apache.spark.sql.execution.streaming.TextSocketSource)
org/apache/spark/sql/execution/streaming/TextSocketSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/reflect/api/TypeTags$TypeTag$/apply(scala.reflect.api.Mirror,scala.reflect.api.TypeCreator)
org/apache/spark/sql/execution/streaming/TextSocketSource/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/readParquetFootersInParallel/1/apply(org.apache.hadoop.fs.FileStatus)#org/apache/parquet/hadoop/Footer/Footer(org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.ParquetMetadata)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/readParquetFootersInParallel/1/apply(org.apache.hadoop.fs.FileStatus)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/readParquetFootersInParallel/1/apply(org.apache.hadoop.fs.FileStatus)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$readParquetFootersInParallel$1$$anonfun$apply$9/9(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1,org.apache.hadoop.fs.FileStatus)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/readParquetFootersInParallel/1/apply(org.apache.hadoop.fs.FileStatus)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$/logWarning(scala.Function0,java.lang.Throwable)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/readParquetFootersInParallel/1/apply(org.apache.hadoop.fs.FileStatus)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/readParquetFootersInParallel/1/apply(org.apache.hadoop.fs.FileStatus)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/readParquetFootersInParallel/1/apply(org.apache.hadoop.fs.FileStatus)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/readParquetFootersInParallel/1/apply(org.apache.hadoop.fs.FileStatus)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/readParquetFootersInParallel/1/apply(org.apache.hadoop.fs.FileStatus)#org/apache/parquet/hadoop/ParquetFileReader/readFooter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileStatus,org.apache.parquet.format.converter.ParquetMetadataConverter$MetadataFilter)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/readParquetFootersInParallel/1/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/FileStatus/getPath()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/readParquetFootersInParallel/1/apply(org.apache.hadoop.fs.FileStatus)#scala/Option$/option2Iterable(scala.Option)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/readParquetFootersInParallel/1/apply(org.apache.hadoop.fs.FileStatus)#java/io/IOException/IOException(java.lang.String,java.lang.Throwable)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/initializeAggregateHashMap()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/initializeAggregateHashMap()#org/apache/spark/sql/types/StructType/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/initializeAggregateHashMap()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/initializeAggregateHashMap()#scala/collection/TraversableOnce/mkString(java.lang.String)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/initializeAggregateHashMap()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/initializeAggregateHashMap()#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/initializeAggregateHashMap()#org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/groupingKeys()
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/initializeAggregateHashMap()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/initializeAggregateHashMap()#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/initializeAggregateHashMap()#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/initializeAggregateHashMap()#org/apache/spark/sql/types/StructType/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/initializeAggregateHashMap()#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/initializeAggregateHashMap()#org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator$$anonfun$2/2(org.apache.spark.sql.execution.aggregate.VectorizedHashMapGenerator)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/initializeAggregateHashMap()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/initializeAggregateHashMap()#org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator$$anonfun$1/1(org.apache.spark.sql.execution.aggregate.VectorizedHashMapGenerator)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/initializeAggregateHashMap()#java/lang/String/concat(java.lang.String)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/initializeAggregateHashMap()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/initializeAggregateHashMap()#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/initializeAggregateHashMap()#scala/collection/Seq/length()
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/initializeAggregateHashMap()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/initializeAggregateHashMap()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/generateFindOrInsert()#org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/buffVars()
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/generateFindOrInsert()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/generateFindOrInsert()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/generateFindOrInsert()#scala/collection/TraversableOnce/mkString(java.lang.String)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/generateFindOrInsert()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/generateFindOrInsert()#org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator$$anonfun$generateFindOrInsert$3/3(org.apache.spark.sql.execution.aggregate.VectorizedHashMapGenerator)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/generateFindOrInsert()#org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/groupingKeys()
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/generateFindOrInsert()#org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator$$anonfun$generateFindOrInsert$2/2(org.apache.spark.sql.execution.aggregate.VectorizedHashMapGenerator)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/generateFindOrInsert()#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/generateFindOrInsert()#org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator$$anonfun$generateFindOrInsert$1/1(org.apache.spark.sql.execution.aggregate.VectorizedHashMapGenerator)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/generateFindOrInsert()#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/generateFindOrInsert()#org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/bufferValues()
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/generateFindOrInsert()#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/generateFindOrInsert()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/generateFindOrInsert()#org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/groupingKeySignature()
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/generateFindOrInsert()#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/generateFindOrInsert()#scala/collection/Seq/mkString(java.lang.String)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/generateFindOrInsert()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/generateEquals()#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/generateEquals()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/generateEquals()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/generateEquals()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/generateEquals()#org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/groupingKeys()
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/generateEquals()#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/generateEquals()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/generateEquals()#org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/groupingKeySignature()
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/generateEquals()#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/9/anonfun/apply/7/apply(org.apache.spark.sql.catalyst.InternalRow)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/9/anonfun/apply/7/apply(org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/util/collection/CompactBuffer/iterator()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/9/anonfun/apply/7/apply(org.apache.spark.sql.catalyst.InternalRow)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/9/anonfun/apply/7/apply(org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/sql/catalyst/expressions/JoinedRow/apply(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/9/anonfun/apply/7/apply(org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/sql/catalyst/expressions/JoinedRow/copy()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/9/anonfun/apply/7/apply(org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/util/collection/CompactBuffer/$plus$eq(java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/9/anonfun/apply/7/apply(org.apache.spark.sql.catalyst.InternalRow)#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/9/anonfun/apply/7/apply(org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec$$anonfun$9/org$apache$spark$sql$execution$joins$BroadcastNestedLoopJoinExec$$anonfun$$$outer()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/9/anonfun/apply/7/apply(org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/util/collection/CompactBuffer/CompactBuffer(scala.reflect.ClassTag)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/9/anonfun/apply/7/apply(org.apache.spark.sql.catalyst.InternalRow)#scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/infer(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource$/infer(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/infer(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource$/makeSafeHeader(java.lang.String[],boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/infer(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/infer(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/rdd/RDD/flatMap(scala.Function1,scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/infer(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVInferSchema$/infer(org.apache.spark.rdd.RDD,java.lang.String[],org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/infer(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource$$anonfun$8/8(org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/infer(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/rdd/RDD/take(int)
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/infer(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/collection/mutable/ArrayOps/headOption()
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/infer(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/infer(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/infer(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/infer(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/Some/x()
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/infer(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource$$anonfun$9/9(org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/infer(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/infer(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/internal/SQLConf/caseSensitiveAnalysis()
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/infer(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/types/StructType$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource/infer(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/MultiLineCSVDataSource$/createBaseRdd(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/ProjectExec/usedInputs()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/ProjectExec/usedInputs()#org/apache/spark/sql/execution/ProjectExec/references()
org/apache/spark/sql/execution/ProjectExec/usedInputs()#org/apache/spark/sql/execution/ProjectExec$$anonfun$3/3(org.apache.spark.sql.execution.ProjectExec)
org/apache/spark/sql/execution/ProjectExec/usedInputs()#org/apache/spark/sql/execution/ProjectExec$$anonfun$4/4(org.apache.spark.sql.execution.ProjectExec)
org/apache/spark/sql/execution/ProjectExec/usedInputs()#scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/ProjectExec/usedInputs()#org/apache/spark/sql/execution/ProjectExec$$anonfun$usedInputs$1/1(org.apache.spark.sql.execution.ProjectExec,scala.collection.immutable.Set)
org/apache/spark/sql/execution/ProjectExec/usedInputs()#scala/collection/Seq/groupBy(scala.Function1)
org/apache/spark/sql/execution/ProjectExec/usedInputs()#org/apache/spark/sql/execution/ProjectExec$$anonfun$2/2(org.apache.spark.sql.execution.ProjectExec)
org/apache/spark/sql/execution/ProjectExec/usedInputs()#scala/collection/immutable/MapLike/keySet()
org/apache/spark/sql/execution/ProjectExec/usedInputs()#scala/collection/immutable/Map/filter(scala.Function1)
org/apache/spark/sql/execution/ProjectExec/usedInputs()#org/apache/spark/sql/catalyst/expressions/AttributeSet/filter(scala.Function1)
org/apache/spark/sql/execution/ProjectExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/ProjectExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/ProjectExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/AttributeSet$/apply(scala.collection.Iterable)
org/apache/spark/sql/execution/ProjectExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/ProjectExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/execution/ProjectExec$$anonfun$5/5(org.apache.spark.sql.execution.ProjectExec)
org/apache/spark/sql/execution/ProjectExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/ProjectExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/ProjectExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/ProjectExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/currentVars_$eq(scala.collection.Seq)
org/apache/spark/sql/execution/ProjectExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/ProjectExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/execution/ProjectExec$$anonfun$6/6(org.apache.spark.sql.execution.ProjectExec,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)
org/apache/spark/sql/execution/ProjectExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/Seq/filterNot(scala.Function1)
org/apache/spark/sql/execution/ProjectExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/execution/ProjectExec$$anonfun$7/7(org.apache.spark.sql.execution.ProjectExec)
org/apache/spark/sql/execution/ProjectExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/ProjectExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/execution/ProjectExec$$anonfun$8/8(org.apache.spark.sql.execution.ProjectExec)
org/apache/spark/sql/execution/ProjectExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/ProjectExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/8/apply(org.apache.spark.sql.execution.streaming.Source)#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/8/apply(org.apache.spark.sql.execution.streaming.Source)#java/lang/Object/toString()
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/8/apply(org.apache.spark.sql.execution.streaming.Source)#org/apache/spark/sql/execution/streaming/ProgressReporter$$anonfun$8$$anonfun$1/1(org.apache.spark.sql.execution.streaming.ProgressReporter$$anonfun$8)
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/8/apply(org.apache.spark.sql.execution.streaming.Source)#scala/collection/immutable/Map/getOrElse(java.lang.Object,scala.Function0)
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/8/apply(org.apache.spark.sql.execution.streaming.Source)#org/apache/spark/sql/execution/streaming/ProgressReporter$$anonfun$8$$anonfun$apply$2/2(org.apache.spark.sql.execution.streaming.ProgressReporter$$anonfun$8)
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/8/apply(org.apache.spark.sql.execution.streaming.Source)#org/apache/spark/sql/execution/streaming/ProgressReporter$ExecutionStats/inputRows()
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/8/apply(org.apache.spark.sql.execution.streaming.Source)#scala/Option/orNull(scala.Predef$$less$colon$less)
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/8/apply(org.apache.spark.sql.execution.streaming.Source)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/8/apply(org.apache.spark.sql.execution.streaming.Source)#scala/Predef$/$conforms()
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/8/apply(org.apache.spark.sql.execution.streaming.Source)#org/apache/spark/sql/execution/streaming/ProgressReporter$$anonfun$8$$anonfun$apply$3/3(org.apache.spark.sql.execution.streaming.ProgressReporter$$anonfun$8)
org/apache/spark/sql/execution/columnar/InMemoryRelation/computeStats(org.apache.spark.sql.internal.SQLConf)#org/apache/spark/sql/internal/SQLConf/defaultSizeInBytes()
org/apache/spark/sql/execution/columnar/InMemoryRelation/computeStats(org.apache.spark.sql.internal.SQLConf)#scala/math/BigInt$/long2bigInt(long)
org/apache/spark/sql/execution/columnar/InMemoryRelation/computeStats(org.apache.spark.sql.internal.SQLConf)#org/apache/spark/sql/catalyst/plans/logical/Statistics$/apply$default$2()
org/apache/spark/sql/execution/columnar/InMemoryRelation/computeStats(org.apache.spark.sql.internal.SQLConf)#org/apache/spark/sql/catalyst/plans/logical/Statistics/Statistics(scala.math.BigInt,scala.Option,org.apache.spark.sql.catalyst.expressions.AttributeMap,org.apache.spark.sql.catalyst.plans.logical.HintInfo)
org/apache/spark/sql/execution/columnar/InMemoryRelation/computeStats(org.apache.spark.sql.internal.SQLConf)#scala/runtime/BoxesRunTime/equalsNumObject(java.lang.Number,java.lang.Object)
org/apache/spark/sql/execution/columnar/InMemoryRelation/computeStats(org.apache.spark.sql.internal.SQLConf)#org/apache/spark/util/LongAccumulator/value()
org/apache/spark/sql/execution/columnar/InMemoryRelation/computeStats(org.apache.spark.sql.internal.SQLConf)#java/lang/Long/longValue()
org/apache/spark/sql/execution/columnar/InMemoryRelation/computeStats(org.apache.spark.sql.internal.SQLConf)#org/apache/spark/sql/catalyst/plans/logical/Statistics$/apply$default$3()
org/apache/spark/sql/execution/columnar/InMemoryRelation/computeStats(org.apache.spark.sql.internal.SQLConf)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/columnar/InMemoryRelation/computeStats(org.apache.spark.sql.internal.SQLConf)#org/apache/spark/sql/catalyst/plans/logical/Statistics$/apply$default$4()
org/apache/spark/sql/execution/columnar/InMemoryRelation/buildBuffers()#org/apache/spark/sql/execution/columnar/InMemoryRelation$$anonfun$1/1(org.apache.spark.sql.execution.columnar.InMemoryRelation,scala.collection.Seq)
org/apache/spark/sql/execution/columnar/InMemoryRelation/buildBuffers()#org/apache/spark/sql/execution/columnar/InMemoryRelation$$anonfun$buildBuffers$1/1(org.apache.spark.sql.execution.columnar.InMemoryRelation)
org/apache/spark/sql/execution/columnar/InMemoryRelation/buildBuffers()#org/apache/spark/rdd/RDD/mapPartitionsInternal$default$2()
org/apache/spark/sql/execution/columnar/InMemoryRelation/buildBuffers()#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/columnar/InMemoryRelation/buildBuffers()#org/apache/spark/rdd/RDD/persist(org.apache.spark.storage.StorageLevel)
org/apache/spark/sql/execution/columnar/InMemoryRelation/buildBuffers()#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/columnar/InMemoryRelation/buildBuffers()#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/columnar/InMemoryRelation/buildBuffers()#org/apache/spark/rdd/RDD/setName(java.lang.String)
org/apache/spark/sql/execution/columnar/InMemoryRelation/buildBuffers()#org/apache/spark/rdd/RDD/mapPartitionsInternal(scala.Function1,boolean,scala.reflect.ClassTag)
org/apache/spark/sql/execution/columnar/InMemoryRelation/buildBuffers()#org/apache/spark/sql/execution/columnar/InMemoryRelation$$anonfun$buildBuffers$2/2(org.apache.spark.sql.execution.columnar.InMemoryRelation)
org/apache/spark/sql/execution/columnar/InMemoryRelation/buildBuffers()#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/anonfun/6/apply(long)#java/util/LinkedHashMap/get(java.lang.Object)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/anonfun/6/apply(long)#java/util/LinkedHashMap/containsKey(java.lang.Object)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/anonfun/6/apply(long)#org/apache/spark/sql/execution/streaming/FileStreamSourceLog/compactInterval()
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/anonfun/6/apply(long)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/anonfun/6/apply(long)#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog$/isCompactionBatch(long,int)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/anonfun/6/apply(long)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/anonfun/6/apply(long)#org/apache/spark/sql/execution/streaming/FileStreamSourceLog$$anonfun$6$$anonfun$7/7(org.apache.spark.sql.execution.streaming.FileStreamSourceLog$$anonfun$6,long)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/anonfun/6/apply(long)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/anonfun/6/apply(long)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource/readFile(org.apache.hadoop.conf.Configuration,org.apache.spark.sql.execution.datasources.PartitionedFile,org.apache.spark.sql.catalyst.json.JacksonParser,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/catalyst/json/JacksonParser/options()
org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource/readFile(org.apache.hadoop.conf.Configuration,org.apache.spark.sql.execution.datasources.PartitionedFile,org.apache.spark.sql.catalyst.json.JacksonParser,org.apache.spark.sql.types.StructType)#org/apache/spark/TaskContext$/get()
org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource/readFile(org.apache.hadoop.conf.Configuration,org.apache.spark.sql.execution.datasources.PartitionedFile,org.apache.spark.sql.catalyst.json.JacksonParser,org.apache.spark.sql.types.StructType)#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource/readFile(org.apache.hadoop.conf.Configuration,org.apache.spark.sql.execution.datasources.PartitionedFile,org.apache.spark.sql.catalyst.json.JacksonParser,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource$$anonfun$readFile$1/1(org.apache.spark.sql.execution.datasources.HadoopFileLinesReader)
org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource/readFile(org.apache.hadoop.conf.Configuration,org.apache.spark.sql.execution.datasources.PartitionedFile,org.apache.spark.sql.catalyst.json.JacksonParser,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/catalyst/json/JSONOptions/parseMode()
org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource/readFile(org.apache.hadoop.conf.Configuration,org.apache.spark.sql.execution.datasources.PartitionedFile,org.apache.spark.sql.catalyst.json.JacksonParser,org.apache.spark.sql.types.StructType)#scala/Option/foreach(scala.Function1)
org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource/readFile(org.apache.hadoop.conf.Configuration,org.apache.spark.sql.execution.datasources.PartitionedFile,org.apache.spark.sql.catalyst.json.JacksonParser,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource$$anonfun$readFile$2/2(org.apache.spark.sql.execution.datasources.FailureSafeParser)
org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource/readFile(org.apache.hadoop.conf.Configuration,org.apache.spark.sql.execution.datasources.PartitionedFile,org.apache.spark.sql.catalyst.json.JacksonParser,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource$/readFile(org.apache.hadoop.conf.Configuration,org.apache.spark.sql.execution.datasources.PartitionedFile,org.apache.spark.sql.catalyst.json.JacksonParser,org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource/readFile(org.apache.hadoop.conf.Configuration,org.apache.spark.sql.execution.datasources.PartitionedFile,org.apache.spark.sql.catalyst.json.JacksonParser,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/catalyst/json/JSONOptions/columnNameOfCorruptRecord()
org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource/readFile(org.apache.hadoop.conf.Configuration,org.apache.spark.sql.execution.datasources.PartitionedFile,org.apache.spark.sql.catalyst.json.JacksonParser,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource$$anonfun$5/5(org.apache.spark.sql.catalyst.json.JacksonParser)
org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource/createBaseDataset(org.apache.spark.sql.SparkSession,scala.collection.Seq)#org/apache/spark/sql/execution/datasources/DataSource$/apply$default$6()
org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource/createBaseDataset(org.apache.spark.sql.SparkSession,scala.collection.Seq)#org/apache/spark/sql/execution/datasources/DataSource$/apply$default$7()
org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource/createBaseDataset(org.apache.spark.sql.SparkSession,scala.collection.Seq)#org/apache/spark/sql/execution/datasources/DataSource$/apply$default$8()
org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource/createBaseDataset(org.apache.spark.sql.SparkSession,scala.collection.Seq)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource/createBaseDataset(org.apache.spark.sql.SparkSession,scala.collection.Seq)#org/apache/spark/sql/Encoders$/STRING()
org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource/createBaseDataset(org.apache.spark.sql.SparkSession,scala.collection.Seq)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource/createBaseDataset(org.apache.spark.sql.SparkSession,scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource/createBaseDataset(org.apache.spark.sql.SparkSession,scala.collection.Seq)#org/apache/spark/sql/execution/datasources/DataSource$/apply$default$4()
org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource/createBaseDataset(org.apache.spark.sql.SparkSession,scala.collection.Seq)#org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource$$anonfun$4/4()
org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource/createBaseDataset(org.apache.spark.sql.SparkSession,scala.collection.Seq)#org/apache/spark/sql/execution/datasources/DataSource$/apply$default$5()
org/apache/spark/sql/execution/datasources/json/TextInputJsonDataSource/createBaseDataset(org.apache.spark.sql.SparkSession,scala.collection.Seq)#java/lang/Class/getName()
org/apache/spark/sql/execution/CollapseCodegenStages/org$apache$spark$sql$execution$CollapseCodegenStages$$insertWholeStageCodegen(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/CollapseCodegenStages/org$apache$spark$sql$execution$CollapseCodegenStages$$insertWholeStageCodegen(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/SparkPlan/withNewChildren(scala.collection.Seq)
org/apache/spark/sql/execution/CollapseCodegenStages/org$apache$spark$sql$execution$CollapseCodegenStages$$insertWholeStageCodegen(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/head()
org/apache/spark/sql/execution/CollapseCodegenStages/org$apache$spark$sql$execution$CollapseCodegenStages$$insertWholeStageCodegen(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/catalyst/expressions/Expression/dataType()
org/apache/spark/sql/execution/CollapseCodegenStages/org$apache$spark$sql$execution$CollapseCodegenStages$$insertWholeStageCodegen(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/length()
org/apache/spark/sql/execution/CollapseCodegenStages/org$apache$spark$sql$execution$CollapseCodegenStages$$insertWholeStageCodegen(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/CollapseCodegenStages/org$apache$spark$sql$execution$CollapseCodegenStages$$insertWholeStageCodegen(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/CollapseCodegenStages$$anonfun$org$apache$spark$sql$execution$CollapseCodegenStages$$insertWholeStageCodegen$1/1(org.apache.spark.sql.execution.CollapseCodegenStages)
org/apache/spark/sql/execution/CollapseCodegenStages/org$apache$spark$sql$execution$CollapseCodegenStages$$insertWholeStageCodegen(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/CollapseCodegenStages$$anonfun$org$apache$spark$sql$execution$CollapseCodegenStages$$insertWholeStageCodegen$2/2(org.apache.spark.sql.execution.CollapseCodegenStages)
org/apache/spark/sql/execution/CollapseCodegenStages/org$apache$spark$sql$execution$CollapseCodegenStages$$insertWholeStageCodegen(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/CollapseCodegenStages/org$apache$spark$sql$execution$CollapseCodegenStages$$insertWholeStageCodegen(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/SparkPlan/children()
org/apache/spark/sql/execution/CollapseCodegenStages/supportCodegen(org.apache.spark.sql.execution.SparkPlan)#scala/collection/IterableLike/exists(scala.Function1)
org/apache/spark/sql/execution/CollapseCodegenStages/supportCodegen(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/internal/SQLConf/wholeStageMaxNumFields()
org/apache/spark/sql/execution/CollapseCodegenStages/supportCodegen(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/catalyst/trees/TreeNode/children()
org/apache/spark/sql/execution/CollapseCodegenStages/supportCodegen(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/exists(scala.Function1)
org/apache/spark/sql/execution/CollapseCodegenStages/supportCodegen(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/catalyst/plans/QueryPlan/expressions()
org/apache/spark/sql/execution/CollapseCodegenStages/supportCodegen(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/catalyst/plans/QueryPlan/schema()
org/apache/spark/sql/execution/CollapseCodegenStages/supportCodegen(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/CollapseCodegenStages$$anonfun$1/1(org.apache.spark.sql.execution.CollapseCodegenStages)
org/apache/spark/sql/execution/CollapseCodegenStages/supportCodegen(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/CollapseCodegenStages/supportCodegen(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/CollapseCodegenStages/supportCodegen(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/CollapseCodegenStages$$anonfun$12/12(org.apache.spark.sql.execution.CollapseCodegenStages)
org/apache/spark/sql/execution/CollapseCodegenStages/supportCodegen(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/CollapseCodegenStages$$anonfun$11/11(org.apache.spark.sql.execution.CollapseCodegenStages)
org/apache/spark/sql/execution/CollapseCodegenStages/org$apache$spark$sql$execution$CollapseCodegenStages$$numOfNestedFields(org.apache.spark.sql.types.DataType)#scala/reflect/ClassTag$/Int()
org/apache/spark/sql/execution/CollapseCodegenStages/org$apache$spark$sql$execution$CollapseCodegenStages$$numOfNestedFields(org.apache.spark.sql.types.DataType)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/CollapseCodegenStages/org$apache$spark$sql$execution$CollapseCodegenStages$$numOfNestedFields(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/CollapseCodegenStages$$anonfun$org$apache$spark$sql$execution$CollapseCodegenStages$$numOfNestedFields$1/1(org.apache.spark.sql.execution.CollapseCodegenStages)
org/apache/spark/sql/execution/CollapseCodegenStages/org$apache$spark$sql$execution$CollapseCodegenStages$$numOfNestedFields(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/UserDefinedType/sqlType()
org/apache/spark/sql/execution/CollapseCodegenStages/org$apache$spark$sql$execution$CollapseCodegenStages$$numOfNestedFields(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/MapType/valueType()
org/apache/spark/sql/execution/CollapseCodegenStages/org$apache$spark$sql$execution$CollapseCodegenStages$$numOfNestedFields(org.apache.spark.sql.types.DataType)#scala/collection/mutable/ArrayOps/sum(scala.math.Numeric)
org/apache/spark/sql/execution/CollapseCodegenStages/org$apache$spark$sql$execution$CollapseCodegenStages$$numOfNestedFields(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/execution/CollapseCodegenStages/org$apache$spark$sql$execution$CollapseCodegenStages$$numOfNestedFields(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/ArrayType/elementType()
org/apache/spark/sql/execution/CollapseCodegenStages/org$apache$spark$sql$execution$CollapseCodegenStages$$numOfNestedFields(org.apache.spark.sql.types.DataType)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/CollapseCodegenStages/org$apache$spark$sql$execution$CollapseCodegenStages$$numOfNestedFields(org.apache.spark.sql.types.DataType)#scala/Predef$/intArrayOps(int[])
org/apache/spark/sql/execution/CollapseCodegenStages/org$apache$spark$sql$execution$CollapseCodegenStages$$numOfNestedFields(org.apache.spark.sql.types.DataType)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/CollapseCodegenStages/org$apache$spark$sql$execution$CollapseCodegenStages$$numOfNestedFields(org.apache.spark.sql.types.DataType)#scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)
org/apache/spark/sql/execution/CollapseCodegenStages/org$apache$spark$sql$execution$CollapseCodegenStages$$numOfNestedFields(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/MapType/keyType()
org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder/class/build(org.apache.spark.sql.execution.columnar.NativeColumnBuilder)#java/nio/ByteBuffer/allocate(int)
org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder/class/build(org.apache.spark.sql.execution.columnar.NativeColumnBuilder)#org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder$$anonfun$build$1/1(org.apache.spark.sql.execution.columnar.NativeColumnBuilder,org.apache.spark.sql.execution.columnar.compression.Encoder)
org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder/class/build(org.apache.spark.sql.execution.columnar.NativeColumnBuilder)#java/nio/ByteBuffer/order(java.nio.ByteOrder)
org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder/class/build(org.apache.spark.sql.execution.columnar.NativeColumnBuilder)#java/nio/ByteBuffer/put(java.nio.ByteBuffer)
org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder/class/build(org.apache.spark.sql.execution.columnar.NativeColumnBuilder)#org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder$$anonfun$1/1(org.apache.spark.sql.execution.columnar.NativeColumnBuilder)
org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder/class/build(org.apache.spark.sql.execution.columnar.NativeColumnBuilder)#org/apache/spark/sql/execution/columnar/compression/PassThrough$/encoder(org.apache.spark.sql.execution.columnar.NativeColumnType)
org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder/class/build(org.apache.spark.sql.execution.columnar.NativeColumnBuilder)#java/nio/ByteBuffer/limit()
org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder/class/build(org.apache.spark.sql.execution.columnar.NativeColumnBuilder)#java/nio/ByteBuffer/putInt(int)
org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder/class/build(org.apache.spark.sql.execution.columnar.NativeColumnBuilder)#java/nio/ByteOrder/nativeOrder()
org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder/class/build(org.apache.spark.sql.execution.columnar.NativeColumnBuilder)#scala/collection/Seq/minBy(scala.Function1,scala.math.Ordering)
org/apache/spark/sql/execution/columnar/compression/CompressibleColumnBuilder/class/build(org.apache.spark.sql.execution.columnar.NativeColumnBuilder)#java/nio/ByteBuffer/remaining()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/3/apply(java.lang.String)#scala/Some/x()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/3/apply(java.lang.String)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/3/apply(java.lang.String)#scala/Option$/option2Iterable(scala.Option)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/3/apply(java.lang.String)#org/apache/spark/sql/catalyst/catalog/ExternalCatalogUtils$/escapePathName(java.lang.String)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/3/apply(java.lang.String)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/3/apply(java.lang.String)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/3/apply(java.lang.String)#scala/collection/immutable/Map/get(java.lang.Object)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/3/apply(java.lang.String)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/3/apply(java.lang.String)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/3/apply(java.lang.String)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/SQLImplicits/typecreator5/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/selectType(scala.reflect.api.Symbols$SymbolApi,java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator5/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ModuleSymbolApi/asModule()
org/apache/spark/sql/SQLImplicits/typecreator5/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticPackage(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator5/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$InternalApi/reificationSupport()
org/apache/spark/sql/SQLImplicits/typecreator5/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/RootClass()
org/apache/spark/sql/SQLImplicits/typecreator5/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticModule(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator5/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ClassSymbolApi/asType()
org/apache/spark/sql/SQLImplicits/typecreator5/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ModuleSymbolApi/moduleClass()
org/apache/spark/sql/SQLImplicits/typecreator5/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/thisPrefix(scala.reflect.api.Symbols$SymbolApi)
org/apache/spark/sql/SQLImplicits/typecreator5/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/TypeRef(scala.reflect.api.Types$TypeApi,scala.reflect.api.Symbols$SymbolApi,scala.collection.immutable.List)
org/apache/spark/sql/SQLImplicits/typecreator5/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticClass(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator5/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/universe()
org/apache/spark/sql/SQLImplicits/typecreator5/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$TypeSymbolApi/toTypeConstructor()
org/apache/spark/sql/SQLImplicits/typecreator5/1/apply(scala.reflect.api.Mirror)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/SQLImplicits/typecreator5/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/SingleType(scala.reflect.api.Types$TypeApi,scala.reflect.api.Symbols$SymbolApi)
org/apache/spark/sql/SQLImplicits/typecreator5/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Universe/internal()
org/apache/spark/sql/SQLImplicits/typecreator5/1/apply(scala.reflect.api.Mirror)#scala/collection/immutable/List$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/command/DDLUtils/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/command/DDLUtils/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/command/DDLUtils/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/DDLUtils/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/command/DDLUtils/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/command/DDLUtils/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/command/DDLUtils/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/command/DDLUtils/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/DDLUtils/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/DDLUtils/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)#org/apache/spark/sql/catalyst/catalog/CatalogTable/tableType()
org/apache/spark/sql/execution/command/DDLUtils/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/isTemporaryTable(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/DDLUtils/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)#org/apache/spark/sql/execution/command/DDLUtils$/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)
org/apache/spark/sql/execution/command/DDLUtils/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)#org/apache/spark/sql/catalyst/catalog/CatalogTableType$/VIEW()
org/apache/spark/sql/execution/command/DDLUtils/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)#org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()
org/apache/spark/sql/execution/command/DDLUtils/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)#org/apache/spark/sql/catalyst/catalog/CatalogTable/tracksPartitionsInCatalog()
org/apache/spark/sql/execution/command/DDLUtils/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/command/DDLUtils/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/DDLUtils/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)#org/apache/spark/sql/execution/command/DDLUtils$/isDatasourceTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)
org/apache/spark/sql/execution/command/DDLUtils/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/DDLUtils/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/command/DDLUtils/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/command/DDLUtils/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/command/DDLUtils/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)#org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()
org/apache/spark/sql/execution/command/DDLUtils/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)#org/apache/spark/sql/internal/SQLConf/manageFilesourcePartitions()
org/apache/spark/sql/execution/command/DDLUtils/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)#org/apache/spark/sql/catalyst/TableIdentifier/table()
org/apache/spark/sql/execution/command/DDLUtils/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/command/DDLUtils/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/command/DDLUtils/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/DDLUtils/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/command/DDLUtils/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)#org/apache/spark/sql/execution/command/DDLUtils$/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)
org/apache/spark/sql/execution/command/DDLUtils/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/DDLUtils/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/Dataset/anonfun/withWatermark/1/anonfun/13/apply()#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/Dataset/anonfun/withWatermark/1/anonfun/13/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/Dataset/anonfun/withWatermark/1/anonfun/13/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/Dataset/anonfun/withWatermark/1/anonfun/13/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/Dataset/anonfun/withWatermark/1/anonfun/13/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/Dataset/anonfun/withWatermark/1/anonfun/13/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/Dataset/anonfun/withWatermark/1/anonfun/13/apply()#org/apache/spark/sql/Dataset$$anonfun$withWatermark$1$$anonfun$13/apply()
org/apache/spark/sql/Dataset/anonfun/withWatermark/1/anonfun/13/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/Dataset/anonfun/withWatermark/1/anonfun/13/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/Dataset/anonfun/withWatermark/1/anonfun/13/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/anonfun/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/makeWriter/10/apply(org.apache.spark.sql.catalyst.expressions.SpecializedGetters,int)#org/apache/parquet/io/api/Binary/fromReusedByteArray(byte[])
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/anonfun/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/makeWriter/10/apply(org.apache.spark.sql.catalyst.expressions.SpecializedGetters,int)#java/nio/ByteBuffer/putLong(long)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/anonfun/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/makeWriter/10/apply(org.apache.spark.sql.catalyst.expressions.SpecializedGetters,int)#org/apache/parquet/io/api/RecordConsumer/addBinary(org.apache.parquet.io.api.Binary)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/anonfun/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/makeWriter/10/apply(org.apache.spark.sql.catalyst.expressions.SpecializedGetters,int)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/anonfun/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/makeWriter/10/apply(org.apache.spark.sql.catalyst.expressions.SpecializedGetters,int)#java/nio/ByteBuffer/wrap(byte[])
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/anonfun/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/makeWriter/10/apply(org.apache.spark.sql.catalyst.expressions.SpecializedGetters,int)#org/apache/spark/sql/catalyst/expressions/SpecializedGetters/getLong(int)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/anonfun/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/makeWriter/10/apply(org.apache.spark.sql.catalyst.expressions.SpecializedGetters,int)#scala/Tuple2/_2$mcJ$sp()
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/anonfun/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/makeWriter/10/apply(org.apache.spark.sql.catalyst.expressions.SpecializedGetters,int)#scala/Tuple2$mcIJ$sp/sp(int,long)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/anonfun/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/makeWriter/10/apply(org.apache.spark.sql.catalyst.expressions.SpecializedGetters,int)#java/nio/ByteBuffer/order(java.nio.ByteOrder)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/anonfun/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/makeWriter/10/apply(org.apache.spark.sql.catalyst.expressions.SpecializedGetters,int)#org/apache/spark/sql/catalyst/util/DateTimeUtils$/toJulianDay(long)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/anonfun/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/makeWriter/10/apply(org.apache.spark.sql.catalyst.expressions.SpecializedGetters,int)#java/nio/ByteBuffer/putInt(int)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/anonfun/org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/makeWriter/10/apply(org.apache.spark.sql.catalyst.expressions.SpecializedGetters,int)#scala/Tuple2/_1$mcI$sp()
org/apache/spark/sql/execution/streaming/FileStreamSource/SeenFilesMap/purge()#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/streaming/FileStreamSource/SeenFilesMap/purge()#java/util/Iterator/hasNext()
org/apache/spark/sql/execution/streaming/FileStreamSource/SeenFilesMap/purge()#org/apache/spark/sql/execution/streaming/FileStreamSource$SeenFilesMap/map()
org/apache/spark/sql/execution/streaming/FileStreamSource/SeenFilesMap/purge()#java/util/HashMap/entrySet()
org/apache/spark/sql/execution/streaming/FileStreamSource/SeenFilesMap/purge()#org/apache/spark/sql/execution/streaming/FileStreamSource$SeenFilesMap/lastPurgeTimestamp()
org/apache/spark/sql/execution/streaming/FileStreamSource/SeenFilesMap/purge()#java/util/Iterator/next()
org/apache/spark/sql/execution/streaming/FileStreamSource/SeenFilesMap/purge()#java/util/Set/iterator()
org/apache/spark/sql/execution/streaming/FileStreamSource/SeenFilesMap/purge()#org/apache/spark/sql/execution/streaming/FileStreamSource$SeenFilesMap/lastPurgeTimestamp_$eq(long)
org/apache/spark/sql/execution/streaming/FileStreamSource/SeenFilesMap/purge()#java/util/Iterator/remove()
org/apache/spark/sql/execution/streaming/FileStreamSource/SeenFilesMap/purge()#java/util/Map$Entry/getValue()
org/apache/spark/sql/execution/streaming/FileStreamSource/SeenFilesMap/purge()#org/apache/spark/sql/execution/streaming/FileStreamSource$SeenFilesMap/latestTimestamp()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch(org.apache.spark.sql.SparkSession)#java/util/concurrent/locks/Condition/signalAll()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch$3/3(org.apache.spark.sql.execution.streaming.StreamExecution,org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/expressions/AttributeMap$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/encoders/RowEncoder$/apply(org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/schema()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch$1/1(org.apache.spark.sql.execution.streaming.StreamExecution,org.apache.spark.sql.Dataset)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch(org.apache.spark.sql.SparkSession)#java/util/concurrent/locks/ReentrantLock/lock()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch(org.apache.spark.sql.SparkSession)#scala/runtime/ObjectRef/create(java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/transform(scala.PartialFunction)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch$2/2(org.apache.spark.sql.execution.streaming.StreamExecution)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$3/3(org.apache.spark.sql.execution.streaming.StreamExecution,scala.runtime.ObjectRef)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/transformAllExpressions(scala.PartialFunction)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch(org.apache.spark.sql.SparkSession)#scala/collection/mutable/ArrayBuffer/ArrayBuffer()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch(org.apache.spark.sql.SparkSession)#java/util/concurrent/locks/ReentrantLock/unlock()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$4/4(org.apache.spark.sql.execution.streaming.StreamExecution,org.apache.spark.sql.catalyst.expressions.AttributeMap)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/streaming/IncrementalExecution/analyzed()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch()#java/util/concurrent/locks/Condition/signalAll()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch()#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch$2/2(org.apache.spark.sql.execution.streaming.StreamExecution)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch()#scala/collection/Seq/headOption()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch()#scala/runtime/LongRef/create(long)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch()#scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch()#scala/collection/immutable/Map/filter(scala.Function1)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch()#org/apache/spark/sql/execution/SparkPlan/collect(scala.PartialFunction)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch()#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$9/9(org.apache.spark.sql.execution.streaming.StreamExecution)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch()#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch()#org/apache/spark/util/Clock/getTimeMillis()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch()#scala/Option/foreach(scala.Function1)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch()#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$10/10(org.apache.spark.sql.execution.streaming.StreamExecution)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch()#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$11/11(org.apache.spark.sql.execution.streaming.StreamExecution)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch()#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch$3/3(org.apache.spark.sql.execution.streaming.StreamExecution)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch()#scala/collection/immutable/MapLike/mapValues(scala.Function1)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch()#org/apache/spark/sql/execution/streaming/IncrementalExecution/executedPlan()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch()#java/util/concurrent/locks/ReentrantLock/unlock()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch()#scala/Predef$/$conforms()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch()#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch$1/1(org.apache.spark.sql.execution.streaming.StreamExecution,scala.runtime.LongRef)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch()#java/util/concurrent/locks/ReentrantLock/lock()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$populateStartOffsets(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$populateStartOffsets$6/6(org.apache.spark.sql.execution.streaming.StreamExecution)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$populateStartOffsets(org.apache.spark.sql.SparkSession)#scala/Tuple2/_2()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$populateStartOffsets(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/streaming/OffsetSeqLog/get(long)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$populateStartOffsets(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/streaming/OffsetSeqLog/getLatest()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$populateStartOffsets(org.apache.spark.sql.SparkSession)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$populateStartOffsets(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$populateStartOffsets$1/1(org.apache.spark.sql.execution.streaming.StreamExecution)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$populateStartOffsets(org.apache.spark.sql.SparkSession)#scala/Some/x()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$populateStartOffsets(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$populateStartOffsets$3/3(org.apache.spark.sql.execution.streaming.StreamExecution)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$populateStartOffsets(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$populateStartOffsets$4/4(org.apache.spark.sql.execution.streaming.StreamExecution,long,long)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$populateStartOffsets(org.apache.spark.sql.SparkSession)#scala/Option/foreach(scala.Function1)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$populateStartOffsets(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$populateStartOffsets$5/5(org.apache.spark.sql.execution.streaming.StreamExecution)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$populateStartOffsets(org.apache.spark.sql.SparkSession)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$populateStartOffsets(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$populateStartOffsets$7/7(org.apache.spark.sql.execution.streaming.StreamExecution)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$populateStartOffsets(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$populateStartOffsets$2/2(org.apache.spark.sql.execution.streaming.StreamExecution,org.apache.spark.sql.SparkSession)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$populateStartOffsets(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/streaming/BatchCommitLog/getLatest()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$populateStartOffsets(org.apache.spark.sql.SparkSession)#scala/Tuple2/_1$mcJ$sp()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$getBatchDescriptionString()#java/lang/Object/toString()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$getBatchDescriptionString()#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$getBatchDescriptionString()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$getBatchDescriptionString()#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$getBatchDescriptionString()#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$getBatchDescriptionString()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$getBatchDescriptionString()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$getBatchDescriptionString()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$getBatchDescriptionString()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$getBatchDescriptionString()#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$getBatchDescriptionString$1/1(org.apache.spark.sql.execution.streaming.StreamExecution)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$getBatchDescriptionString()#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$getBatchDescriptionString$2/2(org.apache.spark.sql.execution.streaming.StreamExecution)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$getBatchDescriptionString()#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$getBatchDescriptionString()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$getBatchDescriptionString()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#scala/util/control/NonFatal$/unapply(java.lang.Throwable)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#org/apache/spark/SparkContext/env()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#java/util/concurrent/locks/Condition/signalAll()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#scala/collection/immutable/Map$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#org/apache/spark/sql/internal/SQLConf$/ADAPTIVE_EXECUTION_ENABLED()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#org/apache/spark/sql/streaming/StreamingQueryListener$QueryTerminatedEvent/QueryTerminatedEvent(java.util.UUID,java.util.UUID,scala.Option)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#scala/Predef$/Map()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#scala/Predef$/ArrowAssoc(java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#java/util/concurrent/CountDownLatch/countDown()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#java/util/concurrent/atomic/AtomicReference/compareAndSet(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#scala/util/control/NonFatal$/apply(java.lang.Throwable)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#scala/Option/isEmpty()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#java/io/IOException/getMessage()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#java/lang/Throwable/getMessage()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#org/apache/spark/sql/SparkSession$/setActiveSession(org.apache.spark.sql.SparkSession)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#org/apache/spark/metrics/MetricsSystem/registerSource(org.apache.spark.metrics.source.Source)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#java/util/concurrent/atomic/AtomicReference/set(java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#org/apache/spark/metrics/MetricsSystem/removeSource(org.apache.spark.metrics.source.Source)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$5/5(org.apache.spark.sql.execution.streaming.StreamExecution,org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#java/lang/Class/getName()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#org/apache/spark/SparkContext/setJobGroup(java.lang.String,java.lang.String,boolean)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$2/2(org.apache.spark.sql.execution.streaming.StreamExecution)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$3/3(org.apache.spark.sql.execution.streaming.StreamExecution)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#org/apache/spark/sql/internal/SQLConf/streamingMetricsEnabled()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#java/util/concurrent/locks/ReentrantLock/unlock()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#org/apache/spark/SparkEnv/metricsSystem()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$4/4(org.apache.spark.sql.execution.streaming.StreamExecution)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#org/apache/spark/internal/config/ConfigEntry/key()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#java/util/concurrent/atomic/AtomicReference/get()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#java/lang/String/startsWith(java.lang.String)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#java/util/UUID/toString()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#org/apache/hadoop/fs/FileSystem/delete(org.apache.hadoop.fs.Path,boolean)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#java/util/concurrent/locks/ReentrantLock/lock()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#org/apache/spark/sql/streaming/StreamingQueryListener$QueryStartedEvent/QueryStartedEvent(java.util.UUID,java.util.UUID,java.lang.String)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches$1/1(org.apache.spark.sql.execution.streaming.StreamExecution,org.apache.spark.sql.SparkSession)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#org/apache/hadoop/fs/Path/Path(java.lang.String)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#scala/Option/get()
org/apache/spark/sql/execution/streaming/StreamExecution/org$apache$spark$sql$execution$streaming$StreamExecution$$runBatches()#org/apache/spark/sql/internal/SQLConf$/SHUFFLE_PARTITIONS()
org/apache/spark/sql/execution/streaming/StreamExecution/logicalPlan$lzycompute()#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/transform(scala.PartialFunction)
org/apache/spark/sql/execution/streaming/StreamExecution/logicalPlan$lzycompute()#java/lang/Thread/currentThread()
org/apache/spark/sql/execution/streaming/StreamExecution/logicalPlan$lzycompute()#scala/collection/Seq/distinct()
org/apache/spark/sql/execution/streaming/StreamExecution/logicalPlan$lzycompute()#scala/runtime/LongRef/create(long)
org/apache/spark/sql/execution/streaming/StreamExecution/logicalPlan$lzycompute()#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$logicalPlan$1/1(org.apache.spark.sql.execution.streaming.StreamExecution)
org/apache/spark/sql/execution/streaming/StreamExecution/logicalPlan$lzycompute()#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$logicalPlan$2/2(org.apache.spark.sql.execution.streaming.StreamExecution)
org/apache/spark/sql/execution/streaming/StreamExecution/logicalPlan$lzycompute()#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$2/2(org.apache.spark.sql.execution.streaming.StreamExecution,scala.runtime.LongRef,scala.collection.mutable.Map)
org/apache/spark/sql/execution/streaming/StreamExecution/logicalPlan$lzycompute()#scala/collection/mutable/Map$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/StreamExecution/logicalPlan$lzycompute()#scala/Predef$/assert(boolean,scala.Function0)
org/apache/spark/sql/execution/streaming/StreamExecution/logicalPlan$lzycompute()#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/collect(scala.PartialFunction)
org/apache/spark/sql/execution/streaming/StreamExecution/toDebugString(boolean)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/streaming/StreamExecution/toDebugString(boolean)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/StreamExecution/toDebugString(boolean)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/StreamExecution/toDebugString(boolean)#org/apache/spark/sql/execution/streaming/StreamExecutionThread/getState()
org/apache/spark/sql/execution/streaming/StreamExecution/toDebugString(boolean)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/streaming/StreamExecution/toDebugString(boolean)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/streaming/StreamExecution/toDebugString(boolean)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/streaming/StreamExecution/toDebugString(boolean)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamExecution/toDebugString(boolean)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamExecution/toDebugString(boolean)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/StreamExecution/toDebugString(boolean)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/PartitionedRelation/unapply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Project/projectList()
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/PartitionedRelation/unapply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Project/child()
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/PartitionedRelation/unapply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Filter/child()
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/PartitionedRelation/unapply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery$PartitionedRelation$$anonfun$unapply$3/3(org.apache.spark.sql.execution.OptimizeMetadataOnlyQuery$PartitionedRelation$,org.apache.spark.sql.catalyst.plans.logical.Filter)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/PartitionedRelation/unapply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/types/StructType/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/PartitionedRelation/unapply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/PartitionedRelation/unapply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery$PartitionedRelation$$anonfun$7/7(org.apache.spark.sql.execution.OptimizeMetadataOnlyQuery$PartitionedRelation$)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/PartitionedRelation/unapply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery$PartitionedRelation$$anonfun$unapply$2/2(org.apache.spark.sql.execution.OptimizeMetadataOnlyQuery$PartitionedRelation$,org.apache.spark.sql.catalyst.plans.logical.Project)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/PartitionedRelation/unapply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/Seq/forall(scala.Function1)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/PartitionedRelation/unapply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Option/flatMap(scala.Function1)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/PartitionedRelation/unapply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/CatalogRelation/tableMeta()
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/PartitionedRelation/unapply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/PartitionedRelation/unapply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/expressions/AttributeSet$/apply(scala.collection.Iterable)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/PartitionedRelation/unapply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Filter/condition()
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/PartitionedRelation/unapply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/expressions/Expression/deterministic()
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/PartitionedRelation/unapply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery$PartitionedRelation$/unapply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/PartitionedRelation/unapply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/PartitionedRelation/unapply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/PartitionedRelation/unapply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/types/StructType/nonEmpty()
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/PartitionedRelation/unapply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery$PartitionedRelation$$anonfun$unapply$1/1(org.apache.spark.sql.execution.OptimizeMetadataOnlyQuery$PartitionedRelation$)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/PartitionedRelation/unapply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#scala/collection/immutable/List$/canBuildFrom()
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#scala/collection/Iterator/toSeq()
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#org/apache/spark/sql/types/StructType/toAttributes()
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#scala/collection/Seq/zip(scala.collection.GenIterable,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#scala/collection/immutable/List$/apply(scala.collection.Seq)
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#org/apache/spark/sql/types/StructType$/apply(scala.collection.Seq)
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#scala/collection/mutable/ArrayOps/toSeq()
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#org/apache/spark/sql/Row/toSeq()
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#scala/collection/Seq/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#scala/collection/Seq/isEmpty()
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#org/apache/spark/sql/types/StructField$/apply$default$3()
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#scala/collection/immutable/List/tail()
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#org/apache/spark/sql/types/StructField$/apply$default$4()
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#org/apache/spark/sql/types/StructField/StructField(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.types.Metadata)
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#scala/Predef$/ArrowAssoc(java.lang.Object)
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#org/apache/spark/sql/Dataset$$anonfun$describe$1$$anonfun$44/44(org.apache.spark.sql.Dataset$$anonfun$describe$1,scala.collection.immutable.List)
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#org/apache/spark/sql/Dataset$$anonfun$describe$1$$anonfun$41/41(org.apache.spark.sql.Dataset$$anonfun$describe$1)
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#org/apache/spark/sql/Dataset$$anonfun$describe$1$$anonfun$42/42(org.apache.spark.sql.Dataset$$anonfun$describe$1)
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#org/apache/spark/sql/Dataset$$anonfun$describe$1$$anonfun$43/43(org.apache.spark.sql.Dataset$$anonfun$describe$1)
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#org/apache/spark/sql/Dataset$$anonfun$describe$1$$anonfun$45/45(org.apache.spark.sql.Dataset$$anonfun$describe$1)
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#org/apache/spark/sql/Dataset$$anonfun$describe$1$$anonfun$46/46(org.apache.spark.sql.Dataset$$anonfun$describe$1)
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#org/apache/spark/sql/Dataset$$anonfun$describe$1$$anonfun$47/47(org.apache.spark.sql.Dataset$$anonfun$describe$1)
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#org/apache/spark/sql/Dataset$$anonfun$describe$1$$anonfun$38/38(org.apache.spark.sql.Dataset$$anonfun$describe$1)
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#org/apache/spark/sql/Dataset$$anonfun$describe$1$$anonfun$39/39(org.apache.spark.sql.Dataset$$anonfun$describe$1)
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#org/apache/spark/sql/Dataset$$anonfun$describe$1$$anonfun$40/40(org.apache.spark.sql.Dataset$$anonfun$describe$1)
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#org/apache/spark/sql/Dataset$$anonfun$describe$1/apply()
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#scala/collection/immutable/List/head()
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#org/apache/spark/sql/catalyst/plans/logical/LocalRelation$/fromExternalRows(scala.collection.Seq,scala.collection.Seq)
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#scala/collection/immutable/List/nonEmpty()
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#scala/collection/Seq/grouped(int)
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#scala/collection/immutable/List/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#scala/collection/immutable/List/size()
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#scala/collection/immutable/List/$colon$colon(java.lang.Object)
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#scala/collection/immutable/List/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/Dataset/anonfun/describe/1/apply()#scala/collection/TraversableOnce/toList()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/11/anonfun/apply/10/apply(org.apache.parquet.hadoop.Footer)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$/readSchemaFromFooter(org.apache.parquet.hadoop.Footer,org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/11/anonfun/apply/10/apply(org.apache.parquet.hadoop.Footer)#org/apache/spark/SparkException/SparkException(java.lang.String,java.lang.Throwable)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/11/anonfun/apply/10/apply(org.apache.parquet.hadoop.Footer)#org/apache/spark/sql/types/StructType/merge(org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/11/anonfun/apply/10/apply(org.apache.parquet.hadoop.Footer)#org/apache/spark/sql/types/StructType/treeString()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/11/anonfun/apply/10/apply(org.apache.parquet.hadoop.Footer)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/11/anonfun/apply/10/apply(org.apache.parquet.hadoop.Footer)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/11/anonfun/apply/10/apply(org.apache.parquet.hadoop.Footer)#org/apache/parquet/hadoop/Footer/getFile()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/11/anonfun/apply/10/apply(org.apache.parquet.hadoop.Footer)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/11/anonfun/apply/10/apply(org.apache.parquet.hadoop.Footer)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/SortAggregateExec/anonfun/doExecute/1/anonfun/3/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/aggregate/SortAggregateExec/anonfun/doExecute/1/anonfun/3/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/aggregate/SortAggregateExec$$anonfun$doExecute$1/org$apache$spark$sql$execution$aggregate$SortAggregateExec$$anonfun$$$outer()
org/apache/spark/sql/execution/aggregate/SortAggregateExec/anonfun/doExecute/1/anonfun/3/apply(scala.collection.Iterator)#scala/package$/Iterator()
org/apache/spark/sql/execution/aggregate/SortAggregateExec/anonfun/doExecute/1/anonfun/3/apply(scala.collection.Iterator)#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/execution/aggregate/SortAggregateExec/anonfun/doExecute/1/anonfun/3/apply(scala.collection.Iterator)#scala/collection/Seq/isEmpty()
org/apache/spark/sql/execution/aggregate/SortAggregateExec/anonfun/doExecute/1/anonfun/3/apply(scala.collection.Iterator)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/SortAggregateExec/anonfun/doExecute/1/anonfun/3/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/aggregate/SortAggregateExec$$anonfun$doExecute$1$$anonfun$3$$anonfun$4/4(org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3)
org/apache/spark/sql/execution/aggregate/SortAggregateExec/anonfun/doExecute/1/anonfun/3/apply(scala.collection.Iterator)#scala/collection/Iterator/hasNext()
org/apache/spark/sql/execution/aggregate/SortAggregateExec/anonfun/doExecute/1/anonfun/3/apply(scala.collection.Iterator)#scala/collection/Iterator$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/resolveTable(org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#java/sql/Connection/prepareStatement(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/resolveTable(org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#java/sql/PreparedStatement/close()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/resolveTable(org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#scala/Function0/apply()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/resolveTable(org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#org/apache/spark/sql/jdbc/JdbcDialects$/get(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/resolveTable(org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#java/sql/Connection/close()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/resolveTable(org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/resolveTable(org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/createConnectionFactory(org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/resolveTable(org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$/resolveTable(org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/resolveTable(org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#java/sql/ResultSet/close()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/resolveTable(org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#java/sql/PreparedStatement/executeQuery()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/pruneSchema(org.apache.spark.sql.types.StructType,java.lang.String[])#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/pruneSchema(org.apache.spark.sql.types.StructType,java.lang.String[])#scala/collection/immutable/Map$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/pruneSchema(org.apache.spark.sql.types.StructType,java.lang.String[])#scala/Predef$/Map()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/pruneSchema(org.apache.spark.sql.types.StructType,java.lang.String[])#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/pruneSchema(org.apache.spark.sql.types.StructType,java.lang.String[])#org/apache/spark/sql/types/StructType/StructType(org.apache.spark.sql.types.StructField[])
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/pruneSchema(org.apache.spark.sql.types.StructType,java.lang.String[])#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/pruneSchema(org.apache.spark.sql.types.StructType,java.lang.String[])#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/pruneSchema(org.apache.spark.sql.types.StructType,java.lang.String[])#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/pruneSchema(org.apache.spark.sql.types.StructType,java.lang.String[])#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/pruneSchema(org.apache.spark.sql.types.StructType,java.lang.String[])#org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$$anonfun$pruneSchema$1/1(scala.collection.immutable.Map)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/pruneSchema(org.apache.spark.sql.types.StructType,java.lang.String[])#org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$$anonfun$1/1()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/scanTable(org.apache.spark.SparkContext,org.apache.spark.sql.types.StructType,java.lang.String[],org.apache.spark.sql.sources.Filter[],org.apache.spark.Partition[],org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/scanTable(org.apache.spark.SparkContext,org.apache.spark.sql.types.StructType,java.lang.String[],org.apache.spark.sql.sources.Filter[],org.apache.spark.Partition[],org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/scanTable(org.apache.spark.SparkContext,org.apache.spark.sql.types.StructType,java.lang.String[],org.apache.spark.sql.sources.Filter[],org.apache.spark.Partition[],org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/scanTable(org.apache.spark.SparkContext,org.apache.spark.sql.types.StructType,java.lang.String[],org.apache.spark.sql.sources.Filter[],org.apache.spark.Partition[],org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$/pruneSchema(org.apache.spark.sql.types.StructType,java.lang.String[])
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/scanTable(org.apache.spark.SparkContext,org.apache.spark.sql.types.StructType,java.lang.String[],org.apache.spark.sql.sources.Filter[],org.apache.spark.Partition[],org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#org/apache/spark/sql/jdbc/JdbcDialects$/get(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/scanTable(org.apache.spark.SparkContext,org.apache.spark.sql.types.StructType,java.lang.String[],org.apache.spark.sql.sources.Filter[],org.apache.spark.Partition[],org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$$anonfun$4/4(org.apache.spark.sql.jdbc.JdbcDialect)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/scanTable(org.apache.spark.SparkContext,org.apache.spark.sql.types.StructType,java.lang.String[],org.apache.spark.sql.sources.Filter[],org.apache.spark.Partition[],org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$/scanTable(org.apache.spark.SparkContext,org.apache.spark.sql.types.StructType,java.lang.String[],org.apache.spark.sql.sources.Filter[],org.apache.spark.Partition[],org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/scanTable(org.apache.spark.SparkContext,org.apache.spark.sql.types.StructType,java.lang.String[],org.apache.spark.sql.sources.Filter[],org.apache.spark.Partition[],org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/scanTable(org.apache.spark.SparkContext,org.apache.spark.sql.types.StructType,java.lang.String[],org.apache.spark.sql.sources.Filter[],org.apache.spark.Partition[],org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/createConnectionFactory(org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$compileValue(java.lang.Object)#scala/reflect/ClassTag$/Any()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$compileValue(java.lang.Object)#scala/Predef$/genericArrayOps(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$compileValue(java.lang.Object)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$compileValue(java.lang.Object)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$compileValue(java.lang.Object)#scala/collection/mutable/ArrayOps/mkString(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$compileValue(java.lang.Object)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$compileValue(java.lang.Object)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$compileValue(java.lang.Object)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$/escapeSql(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$compileValue(java.lang.Object)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$compileValue(java.lang.Object)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$compileValue$1/1()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$compileValue(java.lang.Object)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$compileValue(java.lang.Object)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$compileValue(java.lang.Object)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$compileValue(java.lang.Object)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$$anonfun$compileFilter$3/3()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$$anonfun$3/3(org.apache.spark.sql.jdbc.JdbcDialect)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#scala/Predef$/genericArrayOps(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/sources/In/values()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$$anonfun$compileFilter$1/1()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$$anonfun$compileFilter$2/2()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$$anonfun$compileFilter$4/4()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#scala/collection/TraversableOnce/mkString(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#scala/collection/TraversableLike/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$$anonfun$2/2(org.apache.spark.sql.jdbc.JdbcDialect)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#scala/collection/Seq/size()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$/org$apache$spark$sql$execution$datasources$jdbc$JDBCRDD$$compileValue(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#scala/collection/mutable/ArrayOps/isEmpty()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD$/quote$1(java.lang.String,org.apache.spark.sql.jdbc.JdbcDialect)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD/compileFilter(org.apache.spark.sql.sources.Filter,org.apache.spark.sql.jdbc.JdbcDialect)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/createRLEIterator(int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.ColumnDescriptor)#org/apache/parquet/bytes/BytesUtils/getWidthFromMaxInt(int)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/createRLEIterator(int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.ColumnDescriptor)#java/lang/StringBuilder/append(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/createRLEIterator(int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.ColumnDescriptor)#java/io/ByteArrayInputStream/ByteArrayInputStream(byte[])
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/createRLEIterator(int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.ColumnDescriptor)#java/lang/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/createRLEIterator(int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.ColumnDescriptor)#org/apache/parquet/column/values/rle/RunLengthBitPackingHybridDecoder/RunLengthBitPackingHybridDecoder(int,java.io.ByteArrayInputStream)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/createRLEIterator(int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.ColumnDescriptor)#org/apache/parquet/bytes/BytesInput/toByteArray()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/createRLEIterator(int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.ColumnDescriptor)#java/lang/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/createRLEIterator(int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.ColumnDescriptor)#org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase$RLEIntIterator/RLEIntIterator(org.apache.parquet.column.values.rle.RunLengthBitPackingHybridDecoder)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/createRLEIterator(int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.ColumnDescriptor)#java/lang/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/createRLEIterator(int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.ColumnDescriptor)#org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase$NullIntIterator/NullIntIterator()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/createRLEIterator(int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.ColumnDescriptor)#java/io/IOException/IOException(java.lang.String,java.lang.Throwable)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(java.lang.String,java.util.List)#org/apache/parquet/schema/Types$MessageTypeBuilder/named(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(java.lang.String,java.util.List)#org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(java.lang.String,java.util.List)#org/apache/parquet/hadoop/metadata/ParquetMetadata/getBlocks()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(java.lang.String,java.util.List)#java/io/IOException/IOException(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(java.lang.String,java.util.List)#org/apache/parquet/hadoop/metadata/BlockMetaData/getRowCount()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(java.lang.String,java.util.List)#org/apache/parquet/hadoop/ParquetFileReader/readFooter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.format.converter.ParquetMetadataConverter$MetadataFilter)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(java.lang.String,java.util.List)#java/lang/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(java.lang.String,java.util.List)#java/lang/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(java.lang.String,java.util.List)#org/apache/parquet/schema/MessageType/getType(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(java.lang.String,java.util.List)#org/apache/hadoop/fs/Path/Path(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(java.lang.String,java.util.List)#java/util/Iterator/hasNext()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(java.lang.String,java.util.List)#org/apache/parquet/schema/Types/buildMessage()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(java.lang.String,java.util.List)#org/apache/parquet/schema/MessageType/containsField(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(java.lang.String,java.util.List)#org/apache/hadoop/conf/Configuration/Configuration()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(java.lang.String,java.util.List)#java/lang/StringBuilder/append(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(java.lang.String,java.util.List)#org/apache/parquet/hadoop/metadata/ParquetMetadata/getFileMetaData()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(java.lang.String,java.util.List)#org/apache/hadoop/fs/FileSystem/getFileStatus(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(java.lang.String,java.util.List)#org/apache/parquet/schema/MessageType/getColumns()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(java.lang.String,java.util.List)#org/apache/hadoop/fs/FileStatus/getLen()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(java.lang.String,java.util.List)#java/util/List/size()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(java.lang.String,java.util.List)#org/apache/hadoop/conf/Configuration/set(java.lang.String,java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(java.lang.String,java.util.List)#org/apache/parquet/schema/Types$MessageTypeBuilder/addFields(org.apache.parquet.schema.Type[])
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(java.lang.String,java.util.List)#java/util/Iterator/next()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(java.lang.String,java.util.List)#org/apache/parquet/hadoop/metadata/FileMetaData/getSchema()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(java.lang.String,java.util.List)#org/apache/parquet/hadoop/ParquetFileReader/ParquetFileReader(org.apache.hadoop.conf.Configuration,org.apache.parquet.hadoop.metadata.FileMetaData,org.apache.hadoop.fs.Path,java.util.List,java.util.List)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(java.lang.String,java.util.List)#java/util/List/iterator()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(java.lang.String,java.util.List)#java/lang/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(java.lang.String,java.util.List)#org/apache/parquet/format/converter/ParquetMetadataConverter/range(long,long)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/parquet/hadoop/ParquetInputFormat/getFilter(org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport$/SPARK_ROW_REQUESTED_SCHEMA()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/spark/TaskContext$/get()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#java/lang/Long/valueOf(long)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/parquet/hadoop/ParquetFileReader/readFooter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.format.converter.ParquetMetadataConverter$MetadataFilter)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#java/util/Iterator/next()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#java/lang/StringBuilder/append(long)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#java/lang/String/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#java/util/List/add(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#java/util/HashSet/HashSet()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/parquet/hadoop/api/ReadSupport$ReadContext/getRequestedSchema()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#java/util/ArrayList/ArrayList()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/parquet/hadoop/metadata/ParquetMetadata/getBlocks()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/parquet/hadoop/ParquetInputSplit/getEnd()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/hadoop/mapreduce/TaskAttemptContext/getConfiguration()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#java/lang/IllegalStateException/IllegalStateException(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/parquet/hadoop/ParquetInputSplit/getRowGroupOffsets()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#java/lang/Class/getSimpleName()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/spark/executor/TaskMetrics/externalAccums()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#java/util/Iterator/hasNext()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/parquet/hadoop/api/ReadSupport/init(org.apache.parquet.hadoop.api.InitContext)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#scala/Option/isDefined()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#java/lang/StringBuilder/append(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/parquet/hadoop/metadata/ParquetMetadata/getFileMetaData()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/parquet/hadoop/ParquetInputSplit/getStart()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/spark/sql/types/StructType$/fromString(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/spark/TaskContext/taskMetrics()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/parquet/hadoop/metadata/BlockMetaData/getRowCount()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/parquet/schema/MessageType/getColumns()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#scala/collection/mutable/ArrayBuffer/lastOption()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#java/util/List/size()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/parquet/hadoop/ParquetInputSplit/getPath()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#java/util/Set/contains(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#java/lang/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/parquet/hadoop/metadata/BlockMetaData/getStartingPos()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/parquet/hadoop/metadata/FileMetaData/getSchema()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/parquet/hadoop/metadata/FileMetaData/getKeyValueMetaData()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/parquet/hadoop/ParquetFileReader/ParquetFileReader(org.apache.hadoop.conf.Configuration,org.apache.parquet.hadoop.metadata.FileMetaData,org.apache.hadoop.fs.Path,java.util.List,java.util.List)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#java/lang/Integer/valueOf(int)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/spark/util/AccumulatorV2/add(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#java/util/List/get(int)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#java/lang/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#java/lang/Object/getClass()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#java/util/List/iterator()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/hadoop/conf/Configuration/get(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#java/util/Set/add(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/parquet/filter2/compat/RowGroupFilter/filterRowGroups(org.apache.parquet.filter2.compat.FilterCompat$Filter,java.util.List,org.apache.parquet.schema.MessageType)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/parquet/hadoop/api/InitContext/InitContext(org.apache.hadoop.conf.Configuration,java.util.Map,org.apache.parquet.schema.MessageType)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#java/lang/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#scala/Option/get()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#java/util/Arrays/toString(long[])
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/parquet/format/converter/ParquetMetadataConverter/range(long,long)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/toSetMultiMap(java.util.Map)#java/util/Map/put(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/toSetMultiMap(java.util.Map)#java/util/Iterator/hasNext()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/toSetMultiMap(java.util.Map)#java/util/Map$Entry/getKey()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/toSetMultiMap(java.util.Map)#java/util/Collections/unmodifiableMap(java.util.Map)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/toSetMultiMap(java.util.Map)#java/util/Set/iterator()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/toSetMultiMap(java.util.Map)#java/util/Map$Entry/getValue()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/toSetMultiMap(java.util.Map)#java/util/HashSet/HashSet()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/toSetMultiMap(java.util.Map)#java/util/Iterator/next()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/toSetMultiMap(java.util.Map)#java/util/HashMap/HashMap()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/toSetMultiMap(java.util.Map)#java/util/Map/entrySet()
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/toSetMultiMap(java.util.Map)#java/util/Set/add(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/SpecificParquetRecordReaderBase/toSetMultiMap(java.util.Map)#java/util/Collections/unmodifiableSet(java.util.Set)
org/apache/spark/sql/execution/LogicalRDD/newInstance()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/LogicalRDD/newInstance()#org/apache/spark/sql/catalyst/expressions/Expression/transform(scala.PartialFunction)
org/apache/spark/sql/execution/LogicalRDD/newInstance()#org/apache/spark/sql/execution/LogicalRDD$$anonfun$1/1(org.apache.spark.sql.execution.LogicalRDD,scala.collection.immutable.Map)
org/apache/spark/sql/execution/LogicalRDD/newInstance()#org/apache/spark/sql/execution/LogicalRDD$$anonfun$5/5(org.apache.spark.sql.execution.LogicalRDD,scala.collection.immutable.Map)
org/apache/spark/sql/execution/LogicalRDD/newInstance()#scala/Predef$/$conforms()
org/apache/spark/sql/execution/LogicalRDD/newInstance()#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/LogicalRDD/newInstance()#scala/collection/Seq/zip(scala.collection.GenIterable,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/LogicalRDD/newInstance()#org/apache/spark/sql/execution/LogicalRDD$$anonfun$4/4(org.apache.spark.sql.execution.LogicalRDD)
org/apache/spark/sql/execution/LogicalRDD/newInstance()#scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)
org/apache/spark/sql/streaming/StreamingQueryProgress/safeMapToJValue$1(java.util.Map,scala.Function1)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/streaming/StreamingQueryProgress/safeMapToJValue$1(java.util.Map,scala.Function1)#org/apache/spark/sql/streaming/StreamingQueryProgress$$anonfun$safeMapToJValue$1$1/1(org.apache.spark.sql.streaming.StreamingQueryProgress,java.util.Map,scala.Function1)
org/apache/spark/sql/streaming/StreamingQueryProgress/safeMapToJValue$1(java.util.Map,scala.Function1)#scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)
org/apache/spark/sql/streaming/StreamingQueryProgress/safeMapToJValue$1(java.util.Map,scala.Function1)#java/util/Map/isEmpty()
org/apache/spark/sql/streaming/StreamingQueryProgress/safeMapToJValue$1(java.util.Map,scala.Function1)#scala/collection/convert/Decorators$AsScala/asScala()
org/apache/spark/sql/streaming/StreamingQueryProgress/safeMapToJValue$1(java.util.Map,scala.Function1)#org/json4s/package$/JNothing()
org/apache/spark/sql/streaming/StreamingQueryProgress/safeMapToJValue$1(java.util.Map,scala.Function1)#org/apache/spark/sql/streaming/StreamingQueryProgress$$anonfun$safeMapToJValue$1$2/2(org.apache.spark.sql.streaming.StreamingQueryProgress)
org/apache/spark/sql/streaming/StreamingQueryProgress/safeMapToJValue$1(java.util.Map,scala.Function1)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/streaming/StreamingQueryProgress/safeMapToJValue$1(java.util.Map,scala.Function1)#scala/collection/MapLike/keySet()
org/apache/spark/sql/streaming/StreamingQueryProgress/safeMapToJValue$1(java.util.Map,scala.Function1)#scala/collection/Seq/sorted(scala.math.Ordering)
org/apache/spark/sql/streaming/StreamingQueryProgress/safeMapToJValue$1(java.util.Map,scala.Function1)#scala/collection/TraversableOnce/reduce(scala.Function2)
org/apache/spark/sql/streaming/StreamingQueryProgress/safeMapToJValue$1(java.util.Map,scala.Function1)#scala/collection/Set/toSeq()
org/apache/spark/sql/streaming/StreamingQueryProgress/jsonValue()#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/streaming/StreamingQueryProgress/jsonValue()#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/streaming/StreamingQueryProgress/jsonValue()#org/json4s/JsonAST$JString$/apply(java.lang.String)
org/apache/spark/sql/streaming/StreamingQueryProgress/jsonValue()#org/json4s/JsonAST$JInt$/apply(scala.math.BigInt)
org/apache/spark/sql/streaming/StreamingQueryProgress/jsonValue()#org/apache/spark/sql/streaming/StreamingQueryProgress$$anonfun$jsonValue$1/1(org.apache.spark.sql.streaming.StreamingQueryProgress)
org/apache/spark/sql/streaming/StreamingQueryProgress/jsonValue()#scala/Predef$/$conforms()
org/apache/spark/sql/streaming/StreamingQueryProgress/jsonValue()#scala/collection/mutable/ArrayOps/toList()
org/apache/spark/sql/streaming/StreamingQueryProgress/jsonValue()#org/json4s/JsonDSL$/pair2Assoc(scala.Tuple2,scala.Function1)
org/apache/spark/sql/streaming/StreamingQueryProgress/jsonValue()#org/json4s/JsonDSL$/jobject2assoc(org.json4s.JsonAST$JObject)
org/apache/spark/sql/streaming/StreamingQueryProgress/jsonValue()#org/json4s/package$/JString()
org/apache/spark/sql/streaming/StreamingQueryProgress/jsonValue()#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/streaming/StreamingQueryProgress/jsonValue()#org/json4s/JsonDSL$JsonAssoc/$tilde(scala.Tuple2,scala.Function1)
org/apache/spark/sql/streaming/StreamingQueryProgress/jsonValue()#org/apache/spark/sql/streaming/StreamingQueryProgress$$anonfun$jsonValue$4/4(org.apache.spark.sql.streaming.StreamingQueryProgress)
org/apache/spark/sql/streaming/StreamingQueryProgress/jsonValue()#scala/Predef$/ArrowAssoc(java.lang.Object)
org/apache/spark/sql/streaming/StreamingQueryProgress/jsonValue()#org/json4s/JsonDSL$JsonListAssoc/$tilde(scala.Tuple2)
org/apache/spark/sql/streaming/StreamingQueryProgress/jsonValue()#scala/math/BigInt$/long2bigInt(long)
org/apache/spark/sql/streaming/StreamingQueryProgress/jsonValue()#org/apache/spark/sql/streaming/StreamingQueryProgress$$anonfun$jsonValue$3/3(org.apache.spark.sql.streaming.StreamingQueryProgress)
org/apache/spark/sql/streaming/StreamingQueryProgress/jsonValue()#org/json4s/JsonAST$JArray$/apply(scala.collection.immutable.List)
org/apache/spark/sql/streaming/StreamingQueryProgress/jsonValue()#org/apache/spark/sql/streaming/StreamingQueryProgress$$anonfun$jsonValue$2/2(org.apache.spark.sql.streaming.StreamingQueryProgress)
org/apache/spark/sql/streaming/StreamingQueryProgress/jsonValue()#scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)
org/apache/spark/sql/streaming/StreamingQueryProgress/jsonValue()#org/json4s/package$/JInt()
org/apache/spark/sql/streaming/StreamingQueryProgress/jsonValue()#java/util/UUID/toString()
org/apache/spark/sql/streaming/StreamingQueryProgress/jsonValue()#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/streaming/StreamingQueryProgress/jsonValue()#org/json4s/package$/JArray()
org/apache/spark/sql/execution/aggregate/ObjectAggregationMap/dumpToExternalSorter(scala.collection.Seq,scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/aggregate/ObjectAggregationMap/dumpToExternalSorter(scala.collection.Seq,scala.collection.Seq)#org/apache/spark/TaskContext$/get()
org/apache/spark/sql/execution/aggregate/ObjectAggregationMap/dumpToExternalSorter(scala.collection.Seq,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/UnsafeProjection/apply(org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/aggregate/ObjectAggregationMap/dumpToExternalSorter(scala.collection.Seq,scala.collection.Seq)#scala/collection/Iterator/hasNext()
org/apache/spark/sql/execution/aggregate/ObjectAggregationMap/dumpToExternalSorter(scala.collection.Seq,scala.collection.Seq)#org/apache/spark/SparkEnv/blockManager()
org/apache/spark/sql/execution/aggregate/ObjectAggregationMap/dumpToExternalSorter(scala.collection.Seq,scala.collection.Seq)#org/apache/spark/sql/execution/aggregate/ObjectAggregationMap$$anonfun$2/2(org.apache.spark.sql.execution.aggregate.ObjectAggregationMap)
org/apache/spark/sql/execution/aggregate/ObjectAggregationMap/dumpToExternalSorter(scala.collection.Seq,scala.collection.Seq)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/ObjectAggregationMap/dumpToExternalSorter(scala.collection.Seq,scala.collection.Seq)#org/apache/spark/sql/types/StructType$/fromAttributes(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/ObjectAggregationMap/dumpToExternalSorter(scala.collection.Seq,scala.collection.Seq)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/aggregate/ObjectAggregationMap/dumpToExternalSorter(scala.collection.Seq,scala.collection.Seq)#org/apache/spark/SparkEnv/serializerManager()
org/apache/spark/sql/execution/aggregate/ObjectAggregationMap/dumpToExternalSorter(scala.collection.Seq,scala.collection.Seq)#scala/collection/Iterator/next()
org/apache/spark/sql/execution/aggregate/ObjectAggregationMap/dumpToExternalSorter(scala.collection.Seq,scala.collection.Seq)#scala/collection/Seq/foreach(scala.Function1)
org/apache/spark/sql/execution/aggregate/ObjectAggregationMap/dumpToExternalSorter(scala.collection.Seq,scala.collection.Seq)#org/apache/spark/memory/TaskMemoryManager/pageSizeBytes()
org/apache/spark/sql/execution/aggregate/ObjectAggregationMap/dumpToExternalSorter(scala.collection.Seq,scala.collection.Seq)#org/apache/spark/SparkEnv$/get()
org/apache/spark/sql/execution/aggregate/ObjectAggregationMap/dumpToExternalSorter(scala.collection.Seq,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/UnsafeProjection$/create(org.apache.spark.sql.types.DataType[])
org/apache/spark/sql/execution/aggregate/ObjectAggregationMap/dumpToExternalSorter(scala.collection.Seq,scala.collection.Seq)#org/apache/spark/SparkConf/getLong(java.lang.String,long)
org/apache/spark/sql/execution/aggregate/ObjectAggregationMap/dumpToExternalSorter(scala.collection.Seq,scala.collection.Seq)#org/apache/spark/sql/execution/aggregate/ObjectAggregationMap$$anonfun$dumpToExternalSorter$1/1(org.apache.spark.sql.execution.aggregate.ObjectAggregationMap,org.apache.spark.sql.execution.aggregate.AggregationBufferEntry)
org/apache/spark/sql/execution/aggregate/ObjectAggregationMap/dumpToExternalSorter(scala.collection.Seq,scala.collection.Seq)#java/util/LinkedHashMap/clear()
org/apache/spark/sql/execution/aggregate/ObjectAggregationMap/dumpToExternalSorter(scala.collection.Seq,scala.collection.Seq)#org/apache/spark/TaskContext/taskMemoryManager()
org/apache/spark/sql/execution/aggregate/ObjectAggregationMap/dumpToExternalSorter(scala.collection.Seq,scala.collection.Seq)#scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/aggregate/ObjectAggregationMap/dumpToExternalSorter(scala.collection.Seq,scala.collection.Seq)#scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/ObjectAggregationMap/dumpToExternalSorter(scala.collection.Seq,scala.collection.Seq)#org/apache/spark/sql/execution/aggregate/ObjectAggregationMap$$anonfun$1/1(org.apache.spark.sql.execution.aggregate.ObjectAggregationMap)
org/apache/spark/sql/execution/aggregate/ObjectAggregationMap/dumpToExternalSorter(scala.collection.Seq,scala.collection.Seq)#org/apache/spark/SparkEnv/conf()
org/apache/spark/sql/execution/FilterExec/org$apache$spark$sql$execution$FilterExec$$genPredicate$1(org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/FilterExec/org$apache$spark$sql$execution$FilterExec$$genPredicate$1(org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/BindReferences$/bindReference$default$3()
org/apache/spark/sql/execution/FilterExec/org$apache$spark$sql$execution$FilterExec$$genPredicate$1(org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/FilterExec/org$apache$spark$sql$execution$FilterExec$$genPredicate$1(org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/isNull()
org/apache/spark/sql/execution/FilterExec/org$apache$spark$sql$execution$FilterExec$$genPredicate$1(org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/FilterExec/org$apache$spark$sql$execution$FilterExec$$genPredicate$1(org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/Expression/nullable()
org/apache/spark/sql/execution/FilterExec/org$apache$spark$sql$execution$FilterExec$$genPredicate$1(org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/FilterExec/org$apache$spark$sql$execution$FilterExec$$genPredicate$1(org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/package$/AttributeSeq(scala.collection.Seq)
org/apache/spark/sql/execution/FilterExec/org$apache$spark$sql$execution$FilterExec$$genPredicate$1(org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/BindReferences$/bindReference(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.package$AttributeSeq,boolean)
org/apache/spark/sql/execution/FilterExec/org$apache$spark$sql$execution$FilterExec$$genPredicate$1(org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/FilterExec/org$apache$spark$sql$execution$FilterExec$$genPredicate$1(org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/FilterExec/org$apache$spark$sql$execution$FilterExec$$genPredicate$1(org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/code()
org/apache/spark/sql/execution/FilterExec/org$apache$spark$sql$execution$FilterExec$$genPredicate$1(org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/Expression/genCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)
org/apache/spark/sql/execution/FilterExec/org$apache$spark$sql$execution$FilterExec$$genPredicate$1(org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/value()
org/apache/spark/sql/execution/FilterExec/org$apache$spark$sql$execution$FilterExec$$genPredicate$1(org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/FilterExec/org$apache$spark$sql$execution$FilterExec$$genPredicate$1(org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/package$ExpressionCanonicalizer$/execute(org.apache.spark.sql.catalyst.trees.TreeNode)
org/apache/spark/sql/execution/FilterExec/org$apache$spark$sql$execution$FilterExec$$genPredicate$1(org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/FilterExec/org$apache$spark$sql$execution$FilterExec$$genPredicate$1(org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/Expression/references()
org/apache/spark/sql/execution/FilterExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/Seq/zipWithIndex(scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/FilterExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/FilterExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/FilterExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/TraversableOnce/mkString(java.lang.String)
org/apache/spark/sql/execution/FilterExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/FilterExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/FilterExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/FilterExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/execution/FilterExec$$anonfun$17/17(org.apache.spark.sql.execution.FilterExec)
org/apache/spark/sql/execution/FilterExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/FilterExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/FilterExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/FilterExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/currentVars_$eq(scala.collection.Seq)
org/apache/spark/sql/execution/FilterExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/FilterExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/Seq/length()
org/apache/spark/sql/execution/FilterExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/execution/FilterExec$$anonfun$16/16(org.apache.spark.sql.execution.FilterExec,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,boolean[])
org/apache/spark/sql/execution/FilterExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/execution/FilterExec$$anonfun$13/13(org.apache.spark.sql.execution.FilterExec,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,boolean[])
org/apache/spark/sql/execution/FilterExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/buildReaderWithPartitionValues(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/buildReaderWithPartitionValues(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport$/SPARK_ROW_REQUESTED_SCHEMA()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/buildReaderWithPartitionValues(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/types/StructType/json()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/buildReaderWithPartitionValues(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/internal/SQLConf$/PARQUET_INT64_AS_TIMESTAMP_MILLIS()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/buildReaderWithPartitionValues(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/SparkContext/broadcast(java.lang.Object,scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/buildReaderWithPartitionValues(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport$/setSchema(org.apache.spark.sql.types.StructType,org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/buildReaderWithPartitionValues(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/conf/Configuration/set(java.lang.String,java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/buildReaderWithPartitionValues(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/internal/SQLConf/isParquetINT96AsTimestamp()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/buildReaderWithPartitionValues(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/internal/SQLConf$/PARQUET_BINARY_AS_STRING()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/buildReaderWithPartitionValues(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/types/StructType/forall(scala.Function1)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/buildReaderWithPartitionValues(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/buildReaderWithPartitionValues(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/buildReaderWithPartitionValues(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/internal/SQLConf/isParquetBinaryAsString()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/buildReaderWithPartitionValues(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#scala/collection/mutable/ArrayOps/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/buildReaderWithPartitionValues(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/types/StructType/StructType(org.apache.spark.sql.types.StructField[])
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/buildReaderWithPartitionValues(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1/1(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.Option,org.apache.spark.broadcast.Broadcast,boolean,boolean)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/buildReaderWithPartitionValues(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$4/4(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat,org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/buildReaderWithPartitionValues(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/conf/Configuration/setBoolean(java.lang.String,boolean)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/buildReaderWithPartitionValues(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#java/lang/Class/getName()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/buildReaderWithPartitionValues(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$5/5(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/buildReaderWithPartitionValues(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#scala/collection/TraversableOnce/reduceOption(scala.Function2)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/buildReaderWithPartitionValues(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$6/6(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/buildReaderWithPartitionValues(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/buildReaderWithPartitionValues(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/buildReaderWithPartitionValues(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport$/SPARK_ROW_SCHEMA()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/buildReaderWithPartitionValues(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/internal/SQLConf/parquetVectorizedReaderEnabled()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/buildReaderWithPartitionValues(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/internal/SQLConf$/PARQUET_INT96_AS_TIMESTAMP()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/buildReaderWithPartitionValues(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/internal/config/ConfigEntry/key()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/buildReaderWithPartitionValues(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/internal/SQLConf/isParquetINT64AsTimestampMillis()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/buildReaderWithPartitionValues(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter$/checkFieldNames(org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/buildReaderWithPartitionValues(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/buildReaderWithPartitionValues(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/util/SerializableConfiguration/SerializableConfiguration(org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/buildReaderWithPartitionValues(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/internal/SQLConf/parquetFilterPushDown()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/splitFiles(scala.collection.Seq)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/splitFiles(scala.collection.Seq)#scala/collection/mutable/ArrayOps/filterNot(scala.Function1)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/splitFiles(scala.collection.Seq)#scala/collection/Seq/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/splitFiles(scala.collection.Seq)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/splitFiles(scala.collection.Seq)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$splitFiles$1/1(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/splitFiles(scala.collection.Seq)#scala/collection/mutable/ArrayOps/filter(scala.Function1)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/splitFiles(scala.collection.Seq)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$3/3(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/splitFiles(scala.collection.Seq)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$FileTypes/FileTypes(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/splitFiles(scala.collection.Seq)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$splitFiles$2/2(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/splitFiles(scala.collection.Seq)#scala/collection/mutable/ArrayOps/sortBy(scala.Function1,scala.math.Ordering)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/splitFiles(scala.collection.Seq)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$splitFiles$3/3(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/splitFiles(scala.collection.Seq)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/readSchemaFromFooter(org.apache.parquet.hadoop.Footer,org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$/readSchemaFromFooter(org.apache.parquet.hadoop.Footer,org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/readSchemaFromFooter(org.apache.parquet.hadoop.Footer,org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$readSchemaFromFooter$2/2(org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter,org.apache.parquet.hadoop.metadata.FileMetaData)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/readSchemaFromFooter(org.apache.parquet.hadoop.Footer,org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter)#org/apache/parquet/hadoop/Footer/getParquetMetadata()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/readSchemaFromFooter(org.apache.parquet.hadoop.Footer,org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter)#scala/Option/flatMap(scala.Function1)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/readSchemaFromFooter(org.apache.parquet.hadoop.Footer,org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter)#scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/readSchemaFromFooter(org.apache.parquet.hadoop.Footer,org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter)#scala/collection/convert/Decorators$AsScala/asScala()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/readSchemaFromFooter(org.apache.parquet.hadoop.Footer,org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter)#org/apache/parquet/hadoop/metadata/FileMetaData/getKeyValueMetaData()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/readSchemaFromFooter(org.apache.parquet.hadoop.Footer,org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter)#scala/Predef$/$conforms()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/readSchemaFromFooter(org.apache.parquet.hadoop.Footer,org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter)#org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport$/SPARK_METADATA_KEY()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/readSchemaFromFooter(org.apache.parquet.hadoop.Footer,org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/readSchemaFromFooter(org.apache.parquet.hadoop.Footer,org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$readSchemaFromFooter$1/1()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/readSchemaFromFooter(org.apache.parquet.hadoop.Footer,org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter)#org/apache/parquet/hadoop/metadata/ParquetMetadata/getFileMetaData()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/readSchemaFromFooter(org.apache.parquet.hadoop.Footer,org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter)#scala/collection/immutable/Map/get(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/readSchemaFromFooter(org.apache.parquet.hadoop.Footer,org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter)#scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/inferSchema(org.apache.spark.sql.SparkSession,scala.collection.immutable.Map,scala.collection.Seq)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$FileTypes/commonMetadata()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/inferSchema(org.apache.spark.sql.SparkSession,scala.collection.immutable.Map,scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/inferSchema(org.apache.spark.sql.SparkSession,scala.collection.immutable.Map,scala.collection.Seq)#scala/Option$/option2Iterable(scala.Option)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/inferSchema(org.apache.spark.sql.SparkSession,scala.collection.immutable.Map,scala.collection.Seq)#scala/collection/Seq/headOption()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/inferSchema(org.apache.spark.sql.SparkSession,scala.collection.immutable.Map,scala.collection.Seq)#scala/Option/orElse(scala.Function0)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/inferSchema(org.apache.spark.sql.SparkSession,scala.collection.immutable.Map,scala.collection.Seq)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$2/2(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat,org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$FileTypes)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/inferSchema(org.apache.spark.sql.SparkSession,scala.collection.immutable.Map,scala.collection.Seq)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$1/1(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat,org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$FileTypes)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/inferSchema(org.apache.spark.sql.SparkSession,scala.collection.immutable.Map,scala.collection.Seq)#org/apache/spark/sql/internal/SQLConf/isParquetSchemaRespectSummaries()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/inferSchema(org.apache.spark.sql.SparkSession,scala.collection.immutable.Map,scala.collection.Seq)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/inferSchema(org.apache.spark.sql.SparkSession,scala.collection.immutable.Map,scala.collection.Seq)#scala/collection/Iterable/toSeq()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/inferSchema(org.apache.spark.sql.SparkSession,scala.collection.immutable.Map,scala.collection.Seq)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/inferSchema(org.apache.spark.sql.SparkSession,scala.collection.immutable.Map,scala.collection.Seq)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/inferSchema(org.apache.spark.sql.SparkSession,scala.collection.immutable.Map,scala.collection.Seq)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$FileTypes/data()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/inferSchema(org.apache.spark.sql.SparkSession,scala.collection.immutable.Map,scala.collection.Seq)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$FileTypes/metadata()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/inferSchema(org.apache.spark.sql.SparkSession,scala.collection.immutable.Map,scala.collection.Seq)#scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/prepareWrite(org.apache.spark.sql.SparkSession,org.apache.hadoop.mapreduce.Job,scala.collection.immutable.Map,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/internal/SQLConf$/PARQUET_OUTPUT_COMMITTER_CLASS()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/prepareWrite(org.apache.spark.sql.SparkSession,org.apache.hadoop.mapreduce.Job,scala.collection.immutable.Map,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/internal/SQLConf/writeLegacyParquetFormat()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/prepareWrite(org.apache.spark.sql.SparkSession,org.apache.hadoop.mapreduce.Job,scala.collection.immutable.Map,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/internal/SQLConf$/PARQUET_INT64_AS_TIMESTAMP_MILLIS()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/prepareWrite(org.apache.spark.sql.SparkSession,org.apache.hadoop.mapreduce.Job,scala.collection.immutable.Map,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/internal/SQLConf$/PARQUET_WRITE_LEGACY_FORMAT()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/prepareWrite(org.apache.spark.sql.SparkSession,org.apache.hadoop.mapreduce.Job,scala.collection.immutable.Map,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport$/setSchema(org.apache.spark.sql.types.StructType,org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/prepareWrite(org.apache.spark.sql.SparkSession,org.apache.hadoop.mapreduce.Job,scala.collection.immutable.Map,org.apache.spark.sql.types.StructType)#org/apache/hadoop/conf/Configuration/setClass(java.lang.String,java.lang.Class,java.lang.Class)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/prepareWrite(org.apache.spark.sql.SparkSession,org.apache.hadoop.mapreduce.Job,scala.collection.immutable.Map,org.apache.spark.sql.types.StructType)#org/apache/hadoop/conf/Configuration/setBoolean(java.lang.String,boolean)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/prepareWrite(org.apache.spark.sql.SparkSession,org.apache.hadoop.mapreduce.Job,scala.collection.immutable.Map,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/internal/SQLConf$/OUTPUT_COMMITTER_CLASS()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/prepareWrite(org.apache.spark.sql.SparkSession,org.apache.hadoop.mapreduce.Job,scala.collection.immutable.Map,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/internal/SQLConf/isParquetINT96AsTimestamp()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/prepareWrite(org.apache.spark.sql.SparkSession,org.apache.hadoop.mapreduce.Job,scala.collection.immutable.Map,org.apache.spark.sql.types.StructType)#org/apache/hadoop/conf/Configuration/getClass(java.lang.String,java.lang.Class,java.lang.Class)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/prepareWrite(org.apache.spark.sql.SparkSession,org.apache.hadoop.mapreduce.Job,scala.collection.immutable.Map,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$prepareWrite$2/2(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat,java.lang.Class)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/prepareWrite(org.apache.spark.sql.SparkSession,org.apache.hadoop.mapreduce.Job,scala.collection.immutable.Map,org.apache.spark.sql.types.StructType)#java/lang/Object/toString()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/prepareWrite(org.apache.spark.sql.SparkSession,org.apache.hadoop.mapreduce.Job,scala.collection.immutable.Map,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/internal/SQLConf/isParquetBinaryAsString()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/prepareWrite(org.apache.spark.sql.SparkSession,org.apache.hadoop.mapreduce.Job,scala.collection.immutable.Map,org.apache.spark.sql.types.StructType)#org/apache/hadoop/conf/Configuration/set(java.lang.String,java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/prepareWrite(org.apache.spark.sql.SparkSession,org.apache.hadoop.mapreduce.Job,scala.collection.immutable.Map,org.apache.spark.sql.types.StructType)#scala/runtime/BoxesRunTime/boxToBoolean(boolean)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/prepareWrite(org.apache.spark.sql.SparkSession,org.apache.hadoop.mapreduce.Job,scala.collection.immutable.Map,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$prepareWrite$1/1(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/prepareWrite(org.apache.spark.sql.SparkSession,org.apache.hadoop.mapreduce.Job,scala.collection.immutable.Map,org.apache.spark.sql.types.StructType)#org/apache/parquet/hadoop/ParquetOutputFormat/setWriteSupportClass(org.apache.hadoop.mapreduce.Job,java.lang.Class)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/prepareWrite(org.apache.spark.sql.SparkSession,org.apache.hadoop.mapreduce.Job,scala.collection.immutable.Map,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/internal/SQLConf$/PARQUET_INT96_AS_TIMESTAMP()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/prepareWrite(org.apache.spark.sql.SparkSession,org.apache.hadoop.mapreduce.Job,scala.collection.immutable.Map,org.apache.spark.sql.types.StructType)#org/apache/hadoop/mapreduce/Job/setOutputFormatClass(java.lang.Class)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/prepareWrite(org.apache.spark.sql.SparkSession,org.apache.hadoop.mapreduce.Job,scala.collection.immutable.Map,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anon$1/1(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/prepareWrite(org.apache.spark.sql.SparkSession,org.apache.hadoop.mapreduce.Job,scala.collection.immutable.Map,org.apache.spark.sql.types.StructType)#org/apache/spark/internal/config/ConfigEntry/key()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/prepareWrite(org.apache.spark.sql.SparkSession,org.apache.hadoop.mapreduce.Job,scala.collection.immutable.Map,org.apache.spark.sql.types.StructType)#org/apache/spark/internal/config/OptionalConfigEntry/key()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/prepareWrite(org.apache.spark.sql.SparkSession,org.apache.hadoop.mapreduce.Job,scala.collection.immutable.Map,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/internal/SQLConf$/PARQUET_BINARY_AS_STRING()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/prepareWrite(org.apache.spark.sql.SparkSession,org.apache.hadoop.mapreduce.Job,scala.collection.immutable.Map,org.apache.spark.sql.types.StructType)#org/apache/hadoop/conf/Configuration/get(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/prepareWrite(org.apache.spark.sql.SparkSession,org.apache.hadoop.mapreduce.Job,scala.collection.immutable.Map,org.apache.spark.sql.types.StructType)#org/apache/parquet/hadoop/util/ContextUtil/getConfiguration(org.apache.hadoop.mapreduce.JobContext)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/prepareWrite(org.apache.spark.sql.SparkSession,org.apache.hadoop.mapreduce.Job,scala.collection.immutable.Map,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/internal/SQLConf/isParquetINT64AsTimestampMillis()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#org/apache/spark/SparkContext/parallelize(scala.collection.Seq,int,scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#org/apache/spark/rdd/RDD/mapPartitions$default$2()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#java/lang/Math/max(int,int)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#org/apache/spark/SparkContext/defaultParallelism()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$mergeSchemasInParallel$1/1(scala.runtime.ObjectRef)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#scala/collection/mutable/ArrayOps/tail()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#scala/collection/mutable/ArrayOps/foreach(scala.Function1)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#scala/collection/Seq/size()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#scala/collection/mutable/ArrayOps/isEmpty()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#java/lang/Math/min(int,int)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#scala/collection/mutable/ArrayOps/head()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$11/11(boolean,boolean,boolean,boolean,org.apache.spark.util.SerializableConfiguration,boolean)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#org/apache/spark/rdd/RDD/collect()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#org/apache/spark/rdd/RDD/mapPartitions(scala.Function1,boolean,scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#org/apache/spark/sql/internal/SQLConf/writeLegacyParquetFormat()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#org/apache/spark/sql/internal/SQLConf/isParquetBinaryAsString()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$10/10()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#org/apache/spark/sql/internal/SQLConf/isParquetINT96AsTimestamp()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#org/apache/spark/sql/internal/SQLConf/ignoreCorruptFiles()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#org/apache/spark/sql/internal/SQLConf/isParquetINT64AsTimestampMillis()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#scala/runtime/ObjectRef/create(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/mergeSchemasInParallel(scala.collection.Seq,org.apache.spark.sql.SparkSession)#org/apache/spark/util/SerializableConfiguration/SerializableConfiguration(org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/anonfun/1/applyOrElse(org.apache.spark.sql.types.DataType,scala.Function1)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/anonfun/1/applyOrElse(org.apache.spark.sql.types.DataType,scala.Function1)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$1$$anonfun$applyOrElse$1/1(org.apache.spark.sql.execution.datasources.parquet.ParquetFilters$$anonfun$1)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/anonfun/1/applyOrElse(org.apache.spark.sql.types.DataType,scala.Function1)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$1$$anonfun$applyOrElse$2/2(org.apache.spark.sql.execution.datasources.parquet.ParquetFilters$$anonfun$1)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/anonfun/1/applyOrElse(org.apache.spark.sql.types.DataType,scala.Function1)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$1$$anonfun$applyOrElse$3/3(org.apache.spark.sql.execution.datasources.parquet.ParquetFilters$$anonfun$1)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/anonfun/1/applyOrElse(org.apache.spark.sql.types.DataType,scala.Function1)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$1$$anonfun$applyOrElse$4/4(org.apache.spark.sql.execution.datasources.parquet.ParquetFilters$$anonfun$1)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/anonfun/1/applyOrElse(org.apache.spark.sql.types.DataType,scala.Function1)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$1$$anonfun$applyOrElse$5/5(org.apache.spark.sql.execution.datasources.parquet.ParquetFilters$$anonfun$1)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/anonfun/1/applyOrElse(org.apache.spark.sql.types.DataType,scala.Function1)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$1$$anonfun$applyOrElse$6/6(org.apache.spark.sql.execution.datasources.parquet.ParquetFilters$$anonfun$1)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/anonfun/1/applyOrElse(org.apache.spark.sql.types.DataType,scala.Function1)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$1$$anonfun$applyOrElse$7/7(org.apache.spark.sql.execution.datasources.parquet.ParquetFilters$$anonfun$1)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/anonfun/1/applyOrElse(org.apache.spark.sql.types.DataType,scala.Function1)#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeSetter(java.sql.Connection,org.apache.spark.sql.jdbc.JdbcDialect,org.apache.spark.sql.types.DataType)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeSetter(java.sql.Connection,org.apache.spark.sql.jdbc.JdbcDialect,org.apache.spark.sql.types.DataType)#java/lang/String/split(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeSetter(java.sql.Connection,org.apache.spark.sql.jdbc.JdbcDialect,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeSetter$1/1()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeSetter(java.sql.Connection,org.apache.spark.sql.jdbc.JdbcDialect,org.apache.spark.sql.types.DataType)#java/lang/String/toLowerCase(java.util.Locale)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeSetter(java.sql.Connection,org.apache.spark.sql.jdbc.JdbcDialect,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeSetter$2/2()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeSetter(java.sql.Connection,org.apache.spark.sql.jdbc.JdbcDialect,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeSetter$3/3()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeSetter(java.sql.Connection,org.apache.spark.sql.jdbc.JdbcDialect,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeSetter$4/4()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeSetter(java.sql.Connection,org.apache.spark.sql.jdbc.JdbcDialect,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeSetter$7/7()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeSetter(java.sql.Connection,org.apache.spark.sql.jdbc.JdbcDialect,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeSetter$8/8()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeSetter(java.sql.Connection,org.apache.spark.sql.jdbc.JdbcDialect,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeSetter$9/9()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeSetter(java.sql.Connection,org.apache.spark.sql.jdbc.JdbcDialect,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeSetter$13/13(java.sql.Connection,java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeSetter(java.sql.Connection,org.apache.spark.sql.jdbc.JdbcDialect,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$getJdbcType(org.apache.spark.sql.types.DataType,org.apache.spark.sql.jdbc.JdbcDialect)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeSetter(java.sql.Connection,org.apache.spark.sql.jdbc.JdbcDialect,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeSetter$10/10()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeSetter(java.sql.Connection,org.apache.spark.sql.jdbc.JdbcDialect,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeSetter$11/11()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeSetter(java.sql.Connection,org.apache.spark.sql.jdbc.JdbcDialect,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeSetter$12/12()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeSetter(java.sql.Connection,org.apache.spark.sql.jdbc.JdbcDialect,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeSetter$14/14()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeSetter(java.sql.Connection,org.apache.spark.sql.jdbc.JdbcDialect,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeSetter$5/5()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeSetter(java.sql.Connection,org.apache.spark.sql.jdbc.JdbcDialect,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/ArrayType/elementType()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeSetter(java.sql.Connection,org.apache.spark.sql.jdbc.JdbcDialect,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeSetter$6/6()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/types/MetadataBuilder/putString(java.lang.String,java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#java/sql/ResultSetMetaData/getColumnCount()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/types/StructField/StructField(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.types.Metadata)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#java/sql/ResultSetMetaData/isSigned(int)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#java/lang/Class/getName()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#java/lang/Object/getClass()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#java/sql/ResultSetMetaData/getScale(int)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#java/sql/ResultSetMetaData/getColumnType(int)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/types/MetadataBuilder/putLong(java.lang.String,long)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/types/MetadataBuilder/build()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/types/StructType/StructType(org.apache.spark.sql.types.StructField[])
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#java/sql/ResultSetMetaData/isNullable(int)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#java/sql/ResultSet/getMetaData()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#java/sql/ResultSetMetaData/getPrecision(int)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#java/sql/ResultSetMetaData/getColumnLabel(int)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/types/MetadataBuilder/MetadataBuilder()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#java/sql/SQLException/getMessage()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$8/8(int,int,int,boolean)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getSchema(java.sql.ResultSet,org.apache.spark.sql.jdbc.JdbcDialect)#java/sql/ResultSetMetaData/getColumnTypeName(int)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getCommonJDBCType(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/DecimalType/scale()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getCommonJDBCType(org.apache.spark.sql.types.DataType)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getCommonJDBCType(org.apache.spark.sql.types.DataType)#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getCommonJDBCType(org.apache.spark.sql.types.DataType)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getCommonJDBCType(org.apache.spark.sql.types.DataType)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getCommonJDBCType(org.apache.spark.sql.types.DataType)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getCommonJDBCType(org.apache.spark.sql.types.DataType)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getCommonJDBCType(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/getCommonJDBCType(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getCommonJDBCType(org.apache.spark.sql.types.DataType)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getCommonJDBCType(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/DecimalType/precision()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#java/sql/Connection/rollback()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#java/sql/DatabaseMetaData/supportsTransactionIsolationLevel(int)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#java/sql/SQLException/initCause(java.lang.Throwable)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#java/sql/SQLException/addSuppressed(java.lang.Throwable)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$savePartition$4/4()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#scala/collection/Iterator/hasNext()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#java/sql/Connection/close()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#scala/package$/Iterator()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$19/19(org.apache.spark.sql.jdbc.JdbcDialect,java.sql.Connection)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#java/sql/PreparedStatement/setNull(int,int)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#java/sql/Connection/getMetaData()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#scala/Option/get()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#java/sql/Connection/prepareStatement(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#java/sql/SQLException/getNextException()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#java/sql/PreparedStatement/addBatch()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#scala/reflect/ClassTag$/Int()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#scala/util/control/NonFatal$/unapply(java.lang.Throwable)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#scala/collection/Iterator/next()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#scala/collection/Iterator$/empty()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#java/sql/PreparedStatement/close()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#java/sql/DatabaseMetaData/supportsTransactions()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#scala/Function0/apply()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#java/sql/Connection/setTransactionIsolation(int)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#scala/Option/isEmpty()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#java/sql/DatabaseMetaData/getDefaultTransactionIsolation()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#java/sql/PreparedStatement/executeBatch()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#java/sql/SQLException/getCause()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$savePartition$3/3()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/logWarning(scala.Function0,java.lang.Throwable)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#java/sql/Connection/commit()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/logWarning(scala.Function0)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$20/20(org.apache.spark.sql.jdbc.JdbcDialect)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#org/apache/spark/sql/Row/isNullAt(int)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$savePartition$1/1(int,int)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$savePartition$2/2(int)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#scala/Function3/apply(java.lang.Object,java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#java/sql/Connection/setAutoCommit(boolean)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/savePartition(scala.Function0,java.lang.String,scala.collection.Iterator,org.apache.spark.sql.types.StructType,java.lang.String,int,org.apache.spark.sql.jdbc.JdbcDialect,int)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/catalyst/analysis/package$/caseSensitiveResolution()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)#scala/Option/get()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/types/StructType/fieldNames()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)#scala/collection/mutable/ArrayOps/mkString(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)#scala/Option/isEmpty()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$7/7()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$3/3(org.apache.spark.sql.jdbc.JdbcDialect)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$4/4(scala.Option,org.apache.spark.sql.jdbc.JdbcDialect,scala.Function2,java.lang.String[])
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)#org/apache/spark/sql/catalyst/analysis/package$/caseInsensitiveResolution()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/createTable(java.sql.Connection,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#java/sql/Statement/close()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/createTable(java.sql.Connection,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/createTable(java.sql.Connection,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/schemaString(org.apache.spark.sql.Dataset,java.lang.String,scala.Option)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/createTable(java.sql.Connection,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/createTable(java.sql.Connection,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/createTable(java.sql.Connection,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#java/sql/Statement/executeUpdate(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/createTable(java.sql.Connection,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/createTable(java.sql.Connection,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/createTable(java.sql.Connection,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/createTable(java.sql.Connection,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#java/sql/Connection/createStatement()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/schemaString(org.apache.spark.sql.Dataset,java.lang.String,scala.Option)#scala/collection/mutable/StringBuilder/substring(int)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/schemaString(org.apache.spark.sql.Dataset,java.lang.String,scala.Option)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$21/21(org.apache.spark.sql.Dataset)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/schemaString(org.apache.spark.sql.Dataset,java.lang.String,scala.Option)#org/apache/spark/sql/jdbc/JdbcDialects$/get(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/schemaString(org.apache.spark.sql.Dataset,java.lang.String,scala.Option)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/schemaString(org.apache.spark.sql.Dataset,java.lang.String,scala.Option)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/schemaString(org.apache.spark.sql.Dataset,java.lang.String,scala.Option)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/schemaString(org.apache.spark.sql.Dataset,java.lang.String,scala.Option)#scala/collection/mutable/StringBuilder/length()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/schemaString(org.apache.spark.sql.Dataset,java.lang.String,scala.Option)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/schemaString(org.apache.spark.sql.Dataset,java.lang.String,scala.Option)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$22/22()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/schemaString(org.apache.spark.sql.Dataset,java.lang.String,scala.Option)#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/schemaString(org.apache.spark.sql.Dataset,java.lang.String,scala.Option)#scala/collection/mutable/ArrayOps/foreach(scala.Function1)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/schemaString(org.apache.spark.sql.Dataset,java.lang.String,scala.Option)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$schemaString$1/1(scala.collection.mutable.StringBuilder,org.apache.spark.sql.jdbc.JdbcDialect,scala.collection.immutable.Map)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/schemaString(org.apache.spark.sql.Dataset,java.lang.String,scala.Option)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/schemaString(org.apache.spark.sql.Dataset,java.lang.String,scala.Option)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#scala/Some/x()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions$/JDBC_NUM_PARTITIONS()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/getInsertStatement(java.lang.String,org.apache.spark.sql.types.StructType,scala.Option,boolean,org.apache.spark.sql.jdbc.JdbcDialect)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#org/apache/spark/sql/jdbc/JdbcDialects$/get(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$saveTable$1/1(java.lang.String,org.apache.spark.sql.jdbc.JdbcDialect,org.apache.spark.sql.types.StructType,scala.Function0,int,int,java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#org/apache/spark/rdd/RDD/getNumPartitions()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/createConnectionFactory(org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$parseUserSpecifiedCreateTableColumnTypes(org.apache.spark.sql.Dataset,java.lang.String)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$parseUserSpecifiedCreateTableColumnTypes(org.apache.spark.sql.Dataset,java.lang.String)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$parseUserSpecifiedCreateTableColumnTypes(org.apache.spark.sql.Dataset,java.lang.String)#org/apache/spark/sql/internal/SQLConf/resolver()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$parseUserSpecifiedCreateTableColumnTypes(org.apache.spark.sql.Dataset,java.lang.String)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$parseUserSpecifiedCreateTableColumnTypes(org.apache.spark.sql.Dataset,java.lang.String)#org/apache/spark/sql/types/StructType/fieldNames()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$parseUserSpecifiedCreateTableColumnTypes(org.apache.spark.sql.Dataset,java.lang.String)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$parseUserSpecifiedCreateTableColumnTypes$2/2(org.apache.spark.sql.Dataset,scala.Function2)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$parseUserSpecifiedCreateTableColumnTypes(org.apache.spark.sql.Dataset,java.lang.String)#org/apache/spark/sql/catalyst/parser/CatalystSqlParser$/parseTableSchema(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$parseUserSpecifiedCreateTableColumnTypes(org.apache.spark.sql.Dataset,java.lang.String)#scala/collection/mutable/ArrayOps/toMap(scala.Predef$$less$colon$less)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$parseUserSpecifiedCreateTableColumnTypes(org.apache.spark.sql.Dataset,java.lang.String)#scala/Predef$/$conforms()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$parseUserSpecifiedCreateTableColumnTypes(org.apache.spark.sql.Dataset,java.lang.String)#org/apache/spark/sql/internal/SQLConf/caseSensitiveAnalysis()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$parseUserSpecifiedCreateTableColumnTypes(org.apache.spark.sql.Dataset,java.lang.String)#org/apache/spark/sql/catalyst/util/CaseInsensitiveMap$/apply(scala.collection.immutable.Map)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$parseUserSpecifiedCreateTableColumnTypes(org.apache.spark.sql.Dataset,java.lang.String)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$25/25()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$parseUserSpecifiedCreateTableColumnTypes(org.apache.spark.sql.Dataset,java.lang.String)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$parseUserSpecifiedCreateTableColumnTypes(org.apache.spark.sql.Dataset,java.lang.String)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$parseUserSpecifiedCreateTableColumnTypes$1/1(org.apache.spark.sql.types.StructType,scala.Function2)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$parseUserSpecifiedCreateTableColumnTypes(org.apache.spark.sql.Dataset,java.lang.String)#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$parseUserSpecifiedCreateTableColumnTypes(org.apache.spark.sql.Dataset,java.lang.String)#scala/collection/mutable/ArrayOps/foreach(scala.Function1)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/resultSetToRows(java.sql.ResultSet,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/catalyst/encoders/ExpressionEncoder/resolveAndBind$default$1()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/resultSetToRows(java.sql.ResultSet,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/resultSetToSparkInternalRows(java.sql.ResultSet,org.apache.spark.sql.types.StructType,org.apache.spark.executor.InputMetrics)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/resultSetToRows(java.sql.ResultSet,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/catalyst/encoders/RowEncoder$/apply(org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/resultSetToRows(java.sql.ResultSet,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/resultSetToRows(java.sql.ResultSet,org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/resultSetToRows(java.sql.ResultSet,org.apache.spark.sql.types.StructType)#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/resultSetToRows(java.sql.ResultSet,org.apache.spark.sql.types.StructType)#scala/collection/Iterator/map(scala.Function1)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/resultSetToRows(java.sql.ResultSet,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$resultSetToRows$1/1(org.apache.spark.sql.catalyst.encoders.ExpressionEncoder)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/resultSetToRows(java.sql.ResultSet,org.apache.spark.sql.types.StructType)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/resultSetToRows(java.sql.ResultSet,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$10/10()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/resultSetToRows(java.sql.ResultSet,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/catalyst/encoders/ExpressionEncoder/resolveAndBind$default$2()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/resultSetToRows(java.sql.ResultSet,org.apache.spark.sql.types.StructType)#org/apache/spark/TaskContext$/get()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/resultSetToRows(java.sql.ResultSet,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/catalyst/encoders/ExpressionEncoder/resolveAndBind(scala.collection.Seq,org.apache.spark.sql.catalyst.analysis.Analyzer)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/resultSetToRows(java.sql.ResultSet,org.apache.spark.sql.types.StructType)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/resultSetToRows(java.sql.ResultSet,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$9/9()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.Metadata)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter$6/6()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.Metadata)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter$7/7()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.Metadata)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.Metadata)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter$8/8()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.Metadata)#org/apache/spark/sql/types/DataType/simpleString()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.Metadata)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.Metadata)#scala/Option/isEmpty()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.Metadata)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$16/16(org.apache.spark.sql.types.DecimalType)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.Metadata)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter$1/1()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.Metadata)#org/apache/spark/sql/types/DecimalType$Fixed$/unapply(org.apache.spark.sql.types.DecimalType)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.Metadata)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter$9/9()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.Metadata)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter$3/3(int,int)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.Metadata)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter$13/13(scala.Function1)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.Metadata)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.Metadata)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter$10/10()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.Metadata)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter$11/11()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.Metadata)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter$12/12()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.Metadata)#org/apache/spark/sql/types/Metadata/contains(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.Metadata)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.Metadata)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.Metadata)#org/apache/spark/sql/types/ArrayType/elementType()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.Metadata)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter$2/2()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.Metadata)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.Metadata)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.Metadata)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$13/13()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.Metadata)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$14/14()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.Metadata)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$15/15()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.Metadata)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$17/17()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.Metadata)#scala/Tuple2/_2$mcI$sp()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.Metadata)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter$4/4()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.Metadata)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.Metadata)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.Metadata)#scala/Tuple2/_1$mcI$sp()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.Metadata)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter$5/5()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$makeGetter(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.Metadata)#scala/Option/get()
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#org/apache/spark/SparkContext/getCallSite()
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#java/lang/Object/toString()
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#scala/Function0/apply()
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#org/apache/spark/SparkContext/setLocalProperty(java.lang.String,java.lang.String)
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#org/apache/spark/SparkContext/getLocalProperty(java.lang.String)
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#org/apache/spark/sql/execution/SQLExecution$/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#java/lang/System/currentTimeMillis()
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#org/apache/spark/scheduler/LiveListenerBus/post(org.apache.spark.scheduler.SparkListenerEvent)
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#java/util/concurrent/ConcurrentHashMap/remove(java.lang.Object)
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#org/apache/spark/util/CallSite/longForm()
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#java/util/concurrent/ConcurrentHashMap/put(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#org/apache/spark/sql/execution/SQLExecution$/executionIdToQueryExecution()
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#org/apache/spark/sql/execution/SQLExecution$/nextExecutionId()
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#org/apache/spark/sql/execution/SparkPlanInfo$/fromSparkPlan(org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#org/apache/spark/sql/execution/SQLExecution$/EXECUTION_ID_KEY()
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#org/apache/spark/SparkContext/listenerBus()
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#org/apache/spark/util/CallSite/shortForm()
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/SQLExecution/withNewExecutionId(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,scala.Function0)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$/makeLt()
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$/makeGt()
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$/makeGtEq()
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$createFilter$8/8(java.lang.String,java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$createFilter$1/1(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$createFilter$3/3(java.lang.String,java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$/makeNotEq()
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$/getFieldMap(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#scala/PartialFunction/lift()
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$createFilter$10/10(java.lang.String,java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$/makeEq()
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#scala/Option/flatMap(scala.Function1)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$/canMakeFilterOn$1(java.lang.String,scala.collection.immutable.Map)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$createFilter$12/12(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$createFilter$11/11(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$/makeLtEq()
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#scala/collection/immutable/Map/apply(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$createFilter$9/9(java.lang.String,java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$createFilter$7/7(java.lang.String,java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$createFilter$13/13()
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$createFilter$6/6(java.lang.String,java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$createFilter$2/2(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$createFilter$5/5(java.lang.String,java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/createFilter(org.apache.spark.sql.types.StructType,org.apache.spark.sql.sources.Filter)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$createFilter$4/4(java.lang.String,java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/getFieldMap(org.apache.spark.sql.types.DataType)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/getFieldMap(org.apache.spark.sql.types.DataType)#scala/Predef$/Map()
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/getFieldMap(org.apache.spark.sql.types.DataType)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/getFieldMap(org.apache.spark.sql.types.DataType)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/getFieldMap(org.apache.spark.sql.types.DataType)#scala/collection/mutable/ArrayOps/toMap(scala.Predef$$less$colon$less)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/getFieldMap(org.apache.spark.sql.types.DataType)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/getFieldMap(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$getFieldMap$1/1()
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/getFieldMap(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/getFieldMap(org.apache.spark.sql.types.DataType)#scala/Predef$/$conforms()
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/getFieldMap(org.apache.spark.sql.types.DataType)#scala/collection/immutable/Map$/empty()
org/apache/spark/sql/SQLImplicits/typecreator6/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/selectType(scala.reflect.api.Symbols$SymbolApi,java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator6/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ModuleSymbolApi/asModule()
org/apache/spark/sql/SQLImplicits/typecreator6/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticPackage(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator6/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$InternalApi/reificationSupport()
org/apache/spark/sql/SQLImplicits/typecreator6/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/RootClass()
org/apache/spark/sql/SQLImplicits/typecreator6/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticModule(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator6/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ClassSymbolApi/asType()
org/apache/spark/sql/SQLImplicits/typecreator6/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ModuleSymbolApi/moduleClass()
org/apache/spark/sql/SQLImplicits/typecreator6/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/thisPrefix(scala.reflect.api.Symbols$SymbolApi)
org/apache/spark/sql/SQLImplicits/typecreator6/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/TypeRef(scala.reflect.api.Types$TypeApi,scala.reflect.api.Symbols$SymbolApi,scala.collection.immutable.List)
org/apache/spark/sql/SQLImplicits/typecreator6/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticClass(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator6/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/universe()
org/apache/spark/sql/SQLImplicits/typecreator6/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$TypeSymbolApi/toTypeConstructor()
org/apache/spark/sql/SQLImplicits/typecreator6/1/apply(scala.reflect.api.Mirror)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/SQLImplicits/typecreator6/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/SingleType(scala.reflect.api.Types$TypeApi,scala.reflect.api.Symbols$SymbolApi)
org/apache/spark/sql/SQLImplicits/typecreator6/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Universe/internal()
org/apache/spark/sql/SQLImplicits/typecreator6/1/apply(scala.reflect.api.Mirror)#scala/collection/immutable/List$/apply(scala.collection.Seq)
org/apache/spark/sql/RelationalGroupedDataset/toDF(scala.collection.Seq)#org/apache/spark/sql/RelationalGroupedDataset$PivotType/pivotCol()
org/apache/spark/sql/RelationalGroupedDataset/toDF(scala.collection.Seq)#org/apache/spark/sql/RelationalGroupedDataset$PivotType/values()
org/apache/spark/sql/RelationalGroupedDataset/toDF(scala.collection.Seq)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/RelationalGroupedDataset/toDF(scala.collection.Seq)#org/apache/spark/sql/catalyst/plans/logical/Pivot/Pivot(scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/RelationalGroupedDataset/toDF(scala.collection.Seq)#org/apache/spark/sql/catalyst/plans/logical/Aggregate/Aggregate(scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/RelationalGroupedDataset/toDF(scala.collection.Seq)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/RelationalGroupedDataset/toDF(scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/RelationalGroupedDataset/toDF(scala.collection.Seq)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/RelationalGroupedDataset/toDF(scala.collection.Seq)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/RelationalGroupedDataset/toDF(scala.collection.Seq)#org/apache/spark/sql/RelationalGroupedDataset$$anonfun$2/2(org.apache.spark.sql.RelationalGroupedDataset)
org/apache/spark/sql/RelationalGroupedDataset/toDF(scala.collection.Seq)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/RelationalGroupedDataset/toDF(scala.collection.Seq)#org/apache/spark/sql/RelationalGroupedDataset$$anonfun$1/1(org.apache.spark.sql.RelationalGroupedDataset)
org/apache/spark/sql/RelationalGroupedDataset/toDF(scala.collection.Seq)#org/apache/spark/sql/Dataset$/ofRows(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/RelationalGroupedDataset/toDF(scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/Cube/Cube(scala.collection.Seq)
org/apache/spark/sql/RelationalGroupedDataset/toDF(scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/Rollup/Rollup(scala.collection.Seq)
org/apache/spark/sql/RelationalGroupedDataset/toDF(scala.collection.Seq)#org/apache/spark/sql/internal/SQLConf/dataFrameRetainGroupColumns()
org/apache/spark/sql/RelationalGroupedDataset/toDF(scala.collection.Seq)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/RelationalGroupedDataset/count()#org/apache/spark/sql/catalyst/expressions/aggregate/Count/toAggregateExpression()
org/apache/spark/sql/RelationalGroupedDataset/count()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/RelationalGroupedDataset/count()#org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$5(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)
org/apache/spark/sql/RelationalGroupedDataset/count()#org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$3(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)
org/apache/spark/sql/RelationalGroupedDataset/count()#org/apache/spark/sql/catalyst/expressions/Literal$/apply(java.lang.Object)
org/apache/spark/sql/RelationalGroupedDataset/count()#org/apache/spark/sql/catalyst/expressions/Alias/Alias(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String,org.apache.spark.sql.catalyst.expressions.ExprId,scala.Option,scala.Option,java.lang.Boolean)
org/apache/spark/sql/RelationalGroupedDataset/count()#org/apache/spark/sql/catalyst/expressions/aggregate/Count$/apply(org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/RelationalGroupedDataset/count()#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/RelationalGroupedDataset/count()#org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$6(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)
org/apache/spark/sql/RelationalGroupedDataset/count()#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/RelationalGroupedDataset/count()#org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$4(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)
org/apache/spark/sql/RelationalGroupedDataset/org$apache$spark$sql$RelationalGroupedDataset$$alias(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/RelationalGroupedDataset$$anonfun$org$apache$spark$sql$RelationalGroupedDataset$$alias$1/1(org.apache.spark.sql.RelationalGroupedDataset)
org/apache/spark/sql/RelationalGroupedDataset/org$apache$spark$sql$RelationalGroupedDataset$$alias(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/Alias/Alias(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String,org.apache.spark.sql.catalyst.expressions.ExprId,scala.Option,scala.Option,java.lang.Boolean)
org/apache/spark/sql/RelationalGroupedDataset/org$apache$spark$sql$RelationalGroupedDataset$$alias(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/util/package$/usePrettyExpression(org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/RelationalGroupedDataset/org$apache$spark$sql$RelationalGroupedDataset$$alias(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/aggregate/AggregateExpression/aggregateFunction()
org/apache/spark/sql/RelationalGroupedDataset/org$apache$spark$sql$RelationalGroupedDataset$$alias(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$4(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)
org/apache/spark/sql/RelationalGroupedDataset/org$apache$spark$sql$RelationalGroupedDataset$$alias(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$3(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)
org/apache/spark/sql/RelationalGroupedDataset/org$apache$spark$sql$RelationalGroupedDataset$$alias(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/analysis/UnresolvedAlias/UnresolvedAlias(org.apache.spark.sql.catalyst.expressions.Expression,scala.Option)
org/apache/spark/sql/RelationalGroupedDataset/org$apache$spark$sql$RelationalGroupedDataset$$alias(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$6(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)
org/apache/spark/sql/RelationalGroupedDataset/org$apache$spark$sql$RelationalGroupedDataset$$alias(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/analysis/UnresolvedAlias$/apply$default$2()
org/apache/spark/sql/RelationalGroupedDataset/org$apache$spark$sql$RelationalGroupedDataset$$alias(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/Expression/sql()
org/apache/spark/sql/RelationalGroupedDataset/org$apache$spark$sql$RelationalGroupedDataset$$alias(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$5(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)
org/apache/spark/sql/RelationalGroupedDataset/org$apache$spark$sql$RelationalGroupedDataset$$alias(org.apache.spark.sql.catalyst.expressions.Expression)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/RelationalGroupedDataset/org$apache$spark$sql$RelationalGroupedDataset$$alias(org.apache.spark.sql.catalyst.expressions.Expression)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/RelationalGroupedDataset/flatMapGroupsInR(byte[],byte[],org.apache.spark.broadcast.Broadcast[],org.apache.spark.sql.types.StructType)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/RelationalGroupedDataset/flatMapGroupsInR(byte[],byte[],org.apache.spark.broadcast.Broadcast[],org.apache.spark.sql.types.StructType)#org/apache/spark/sql/RelationalGroupedDataset$$anonfun$7/7(org.apache.spark.sql.RelationalGroupedDataset)
org/apache/spark/sql/RelationalGroupedDataset/flatMapGroupsInR(byte[],byte[],org.apache.spark.broadcast.Broadcast[],org.apache.spark.sql.types.StructType)#org/apache/spark/sql/RelationalGroupedDataset$$anonfun$6/6(org.apache.spark.sql.RelationalGroupedDataset)
org/apache/spark/sql/RelationalGroupedDataset/flatMapGroupsInR(byte[],byte[],org.apache.spark.broadcast.Broadcast[],org.apache.spark.sql.types.StructType)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/output()
org/apache/spark/sql/RelationalGroupedDataset/flatMapGroupsInR(byte[],byte[],org.apache.spark.broadcast.Broadcast[],org.apache.spark.sql.types.StructType)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/RelationalGroupedDataset/flatMapGroupsInR(byte[],byte[],org.apache.spark.broadcast.Broadcast[],org.apache.spark.sql.types.StructType)#org/apache/spark/sql/RelationalGroupedDataset$$anonfun$8/8(org.apache.spark.sql.RelationalGroupedDataset)
org/apache/spark/sql/RelationalGroupedDataset/flatMapGroupsInR(byte[],byte[],org.apache.spark.broadcast.Broadcast[],org.apache.spark.sql.types.StructType)#org/apache/spark/sql/catalyst/encoders/ExpressionEncoder/deserializer()
org/apache/spark/sql/RelationalGroupedDataset/flatMapGroupsInR(byte[],byte[],org.apache.spark.broadcast.Broadcast[],org.apache.spark.sql.types.StructType)#org/apache/spark/sql/catalyst/encoders/ExpressionEncoder/schema()
org/apache/spark/sql/RelationalGroupedDataset/flatMapGroupsInR(byte[],byte[],org.apache.spark.broadcast.Broadcast[],org.apache.spark.sql.types.StructType)#org/apache/spark/sql/Dataset$/ofRows(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/RelationalGroupedDataset/flatMapGroupsInR(byte[],byte[],org.apache.spark.broadcast.Broadcast[],org.apache.spark.sql.types.StructType)#org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsInR$/apply(byte[],byte[],org.apache.spark.broadcast.Broadcast[],org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/RelationalGroupedDataset/pivot(java.lang.String)#scala/reflect/ClassTag$/Any()
org/apache/spark/sql/RelationalGroupedDataset/pivot(java.lang.String)#scala/Predef$/genericArrayOps(java.lang.Object)
org/apache/spark/sql/RelationalGroupedDataset/pivot(java.lang.String)#org/apache/spark/sql/internal/SQLConf$/DATAFRAME_PIVOT_MAX_VALUES()
org/apache/spark/sql/RelationalGroupedDataset/pivot(java.lang.String)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/RelationalGroupedDataset/pivot(java.lang.String)#org/apache/spark/rdd/RDD/take(int)
org/apache/spark/sql/RelationalGroupedDataset/pivot(java.lang.String)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/RelationalGroupedDataset/pivot(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/RelationalGroupedDataset/pivot(java.lang.String)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/RelationalGroupedDataset/pivot(java.lang.String)#scala/collection/Seq/length()
org/apache/spark/sql/RelationalGroupedDataset/pivot(java.lang.String)#scala/collection/mutable/ArrayOps/toSeq()
org/apache/spark/sql/RelationalGroupedDataset/pivot(java.lang.String)#org/apache/spark/sql/internal/SQLConf/dataFramePivotMaxValues()
org/apache/spark/sql/RelationalGroupedDataset/pivot(java.lang.String)#org/apache/spark/sql/RelationalGroupedDataset$$anonfun$5/5(org.apache.spark.sql.RelationalGroupedDataset)
org/apache/spark/sql/RelationalGroupedDataset/pivot(java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/RelationalGroupedDataset/pivot(java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/RelationalGroupedDataset/pivot(java.lang.String)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/RelationalGroupedDataset/pivot(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/RelationalGroupedDataset/pivot(java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/RelationalGroupedDataset/pivot(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/RelationalGroupedDataset/pivot(java.lang.String)#org/apache/spark/internal/config/ConfigEntry/key()
org/apache/spark/sql/RelationalGroupedDataset/pivot(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/RelationalGroupedDataset/pivot(java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/RelationalGroupedDataset/pivot(java.lang.String)#org/apache/spark/rdd/RDD/map(scala.Function1,scala.reflect.ClassTag)
org/apache/spark/sql/RelationalGroupedDataset/pivot(java.lang.String)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/crossTabulate/2/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/crossTabulate/2/apply()#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/crossTabulate/2/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/crossTabulate/2/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/crossTabulate/2/apply()#org/apache/spark/sql/execution/stat/StatFunctions$$anonfun$crossTabulate$2/apply()
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/crossTabulate/2/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/crossTabulate/2/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/crossTabulate/2/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/crossTabulate/2/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/internal/SharedState/org$apache$spark$sql$internal$SharedState$$reflect(java.lang.String,java.lang.Object,java.lang.Object,scala.reflect.ClassTag,scala.reflect.ClassTag)#java/lang/reflect/Constructor/newInstance(java.lang.Object[])
org/apache/spark/sql/internal/SharedState/org$apache$spark$sql$internal$SharedState$$reflect(java.lang.String,java.lang.Object,java.lang.Object,scala.reflect.ClassTag,scala.reflect.ClassTag)#scala/util/control/NonFatal$/unapply(java.lang.Throwable)
org/apache/spark/sql/internal/SharedState/org$apache$spark$sql$internal$SharedState$$reflect(java.lang.String,java.lang.Object,java.lang.Object,scala.reflect.ClassTag,scala.reflect.ClassTag)#org/apache/spark/util/Utils$/classForName(java.lang.String)
org/apache/spark/sql/internal/SharedState/org$apache$spark$sql$internal$SharedState$$reflect(java.lang.String,java.lang.Object,java.lang.Object,scala.reflect.ClassTag,scala.reflect.ClassTag)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/internal/SharedState/org$apache$spark$sql$internal$SharedState$$reflect(java.lang.String,java.lang.Object,java.lang.Object,scala.reflect.ClassTag,scala.reflect.ClassTag)#scala/Option/isEmpty()
org/apache/spark/sql/internal/SharedState/org$apache$spark$sql$internal$SharedState$$reflect(java.lang.String,java.lang.Object,java.lang.Object,scala.reflect.ClassTag,scala.reflect.ClassTag)#scala/reflect/ClassTag/runtimeClass()
org/apache/spark/sql/internal/SharedState/org$apache$spark$sql$internal$SharedState$$reflect(java.lang.String,java.lang.Object,java.lang.Object,scala.reflect.ClassTag,scala.reflect.ClassTag)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/internal/SharedState/org$apache$spark$sql$internal$SharedState$$reflect(java.lang.String,java.lang.Object,java.lang.Object,scala.reflect.ClassTag,scala.reflect.ClassTag)#java/lang/Class/getDeclaredConstructor(java.lang.Class[])
org/apache/spark/sql/internal/SharedState/org$apache$spark$sql$internal$SharedState$$reflect(java.lang.String,java.lang.Object,java.lang.Object,scala.reflect.ClassTag,scala.reflect.ClassTag)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/internal/SharedState/org$apache$spark$sql$internal$SharedState$$reflect(java.lang.String,java.lang.Object,java.lang.Object,scala.reflect.ClassTag,scala.reflect.ClassTag)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String,java.lang.Throwable)
org/apache/spark/sql/internal/SharedState/org$apache$spark$sql$internal$SharedState$$reflect(java.lang.String,java.lang.Object,java.lang.Object,scala.reflect.ClassTag,scala.reflect.ClassTag)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/internal/SharedState/org$apache$spark$sql$internal$SharedState$$reflect(java.lang.String,java.lang.Object,java.lang.Object,scala.reflect.ClassTag,scala.reflect.ClassTag)#scala/Option/get()
org/apache/spark/sql/execution/columnar/ColumnAccessor/apply(org.apache.spark.sql.types.DataType,java.nio.ByteBuffer)#java/nio/ByteOrder/nativeOrder()
org/apache/spark/sql/execution/columnar/ColumnAccessor/apply(org.apache.spark.sql.types.DataType,java.nio.ByteBuffer)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/columnar/ColumnAccessor/apply(org.apache.spark.sql.types.DataType,java.nio.ByteBuffer)#org/apache/spark/sql/types/UserDefinedType/sqlType()
org/apache/spark/sql/execution/columnar/ColumnAccessor/apply(org.apache.spark.sql.types.DataType,java.nio.ByteBuffer)#org/apache/spark/sql/types/Decimal$/MAX_LONG_DIGITS()
org/apache/spark/sql/execution/columnar/ColumnAccessor/apply(org.apache.spark.sql.types.DataType,java.nio.ByteBuffer)#java/lang/Exception/Exception(java.lang.String)
org/apache/spark/sql/execution/columnar/ColumnAccessor/apply(org.apache.spark.sql.types.DataType,java.nio.ByteBuffer)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/columnar/ColumnAccessor/apply(org.apache.spark.sql.types.DataType,java.nio.ByteBuffer)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/columnar/ColumnAccessor/apply(org.apache.spark.sql.types.DataType,java.nio.ByteBuffer)#java/nio/ByteBuffer/order(java.nio.ByteOrder)
org/apache/spark/sql/execution/columnar/ColumnAccessor/apply(org.apache.spark.sql.types.DataType,java.nio.ByteBuffer)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/columnar/ColumnAccessor/apply(org.apache.spark.sql.types.DataType,java.nio.ByteBuffer)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/columnar/ColumnAccessor/apply(org.apache.spark.sql.types.DataType,java.nio.ByteBuffer)#org/apache/spark/sql/types/DecimalType/precision()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeCatalogTable/1/anonfun/apply/11/apply(java.lang.String)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeCatalogTable/1/anonfun/apply/11/apply(java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeCatalogTable/1/anonfun/apply/11/apply(java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeCatalogTable/1/anonfun/apply/11/apply(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeCatalogTable/1/anonfun/apply/11/apply(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeCatalogTable/1/anonfun/apply/11/apply(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeCatalogTable/1/anonfun/apply/11/apply(java.lang.String)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeCatalogTable/1/anonfun/apply/11/apply(java.lang.String)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeCatalogTable/1/anonfun/apply/11/apply(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeCatalogTable/1/anonfun/apply/11/apply(java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeCatalogTable/1/anonfun/apply/11/apply(java.lang.String)#scala/collection/Seq/mkString(java.lang.String)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeCatalogTable/1/anonfun/apply/11/apply(java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeCatalogTable/1/anonfun/apply/11/apply(java.lang.String)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/anonfun/genBuildSideVars/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/ExprCode(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/anonfun/genBuildSideVars/1/apply(scala.Tuple2)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/anonfun/genBuildSideVars/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/code()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/anonfun/genBuildSideVars/1/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/anonfun/genBuildSideVars/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/isNull()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/anonfun/genBuildSideVars/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/Attribute/nullable()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/anonfun/genBuildSideVars/1/apply(scala.Tuple2)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/anonfun/genBuildSideVars/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/Attribute/dataType()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/anonfun/genBuildSideVars/1/apply(scala.Tuple2)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/anonfun/genBuildSideVars/1/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/anonfun/genBuildSideVars/1/apply(scala.Tuple2)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/anonfun/genBuildSideVars/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/BoundReference/BoundReference(int,org.apache.spark.sql.types.DataType,boolean)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/anonfun/genBuildSideVars/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/defaultValue(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/anonfun/genBuildSideVars/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/value()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/anonfun/genBuildSideVars/1/apply(scala.Tuple2)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/anonfun/genBuildSideVars/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/freshName(java.lang.String)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/anonfun/genBuildSideVars/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/javaType(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/anonfun/genBuildSideVars/1/apply(scala.Tuple2)#scala/Tuple2/_2$mcI$sp()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/anonfun/genBuildSideVars/1/apply(scala.Tuple2)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/anonfun/genBuildSideVars/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/BoundReference/genCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)
org/apache/spark/sql/execution/joins/BroadcastHashJoinExec/anonfun/genBuildSideVars/1/apply(scala.Tuple2)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/SparkSqlAstBuilder/org$apache$spark$sql$execution$SparkSqlAstBuilder$$visitPropertyKeys(org.apache.spark.sql.catalyst.parser.SqlBaseParser$TablePropertyListContext)#org/apache/spark/sql/catalyst/parser/ParserUtils$/operationNotAllowed(java.lang.String,org.antlr.v4.runtime.ParserRuleContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/org$apache$spark$sql$execution$SparkSqlAstBuilder$$visitPropertyKeys(org.apache.spark.sql.catalyst.parser.SqlBaseParser$TablePropertyListContext)#scala/collection/Iterable/nonEmpty()
org/apache/spark/sql/execution/SparkSqlAstBuilder/org$apache$spark$sql$execution$SparkSqlAstBuilder$$visitPropertyKeys(org.apache.spark.sql.catalyst.parser.SqlBaseParser$TablePropertyListContext)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/SparkSqlAstBuilder/org$apache$spark$sql$execution$SparkSqlAstBuilder$$visitPropertyKeys(org.apache.spark.sql.catalyst.parser.SqlBaseParser$TablePropertyListContext)#scala/collection/MapLike/keys()
org/apache/spark/sql/execution/SparkSqlAstBuilder/org$apache$spark$sql$execution$SparkSqlAstBuilder$$visitPropertyKeys(org.apache.spark.sql.catalyst.parser.SqlBaseParser$TablePropertyListContext)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/org$apache$spark$sql$execution$SparkSqlAstBuilder$$visitPropertyKeys(org.apache.spark.sql.catalyst.parser.SqlBaseParser$TablePropertyListContext)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/SparkSqlAstBuilder/org$apache$spark$sql$execution$SparkSqlAstBuilder$$visitPropertyKeys(org.apache.spark.sql.catalyst.parser.SqlBaseParser$TablePropertyListContext)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/SparkSqlAstBuilder/org$apache$spark$sql$execution$SparkSqlAstBuilder$$visitPropertyKeys(org.apache.spark.sql.catalyst.parser.SqlBaseParser$TablePropertyListContext)#scala/collection/Iterable/toSeq()
org/apache/spark/sql/execution/SparkSqlAstBuilder/org$apache$spark$sql$execution$SparkSqlAstBuilder$$visitPropertyKeys(org.apache.spark.sql.catalyst.parser.SqlBaseParser$TablePropertyListContext)#scala/collection/immutable/Map/keys()
org/apache/spark/sql/execution/SparkSqlAstBuilder/org$apache$spark$sql$execution$SparkSqlAstBuilder$$visitPropertyKeys(org.apache.spark.sql.catalyst.parser.SqlBaseParser$TablePropertyListContext)#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$18/18(org.apache.spark.sql.execution.SparkSqlAstBuilder)
org/apache/spark/sql/execution/SparkSqlAstBuilder/org$apache$spark$sql$execution$SparkSqlAstBuilder$$visitPropertyKeys(org.apache.spark.sql.catalyst.parser.SqlBaseParser$TablePropertyListContext)#scala/collection/immutable/Map/filter(scala.Function1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/org$apache$spark$sql$execution$SparkSqlAstBuilder$$visitPropertyKeys(org.apache.spark.sql.catalyst.parser.SqlBaseParser$TablePropertyListContext)#scala/collection/Iterable/mkString(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/SparkSqlAstBuilder/org$apache$spark$sql$execution$SparkSqlAstBuilder$$visitPropertyKeyValues(org.apache.spark.sql.catalyst.parser.SqlBaseParser$TablePropertyListContext)#scala/collection/immutable/Iterable/nonEmpty()
org/apache/spark/sql/execution/SparkSqlAstBuilder/org$apache$spark$sql$execution$SparkSqlAstBuilder$$visitPropertyKeyValues(org.apache.spark.sql.catalyst.parser.SqlBaseParser$TablePropertyListContext)#org/apache/spark/sql/catalyst/parser/ParserUtils$/operationNotAllowed(java.lang.String,org.antlr.v4.runtime.ParserRuleContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/org$apache$spark$sql$execution$SparkSqlAstBuilder$$visitPropertyKeyValues(org.apache.spark.sql.catalyst.parser.SqlBaseParser$TablePropertyListContext)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/SparkSqlAstBuilder/org$apache$spark$sql$execution$SparkSqlAstBuilder$$visitPropertyKeyValues(org.apache.spark.sql.catalyst.parser.SqlBaseParser$TablePropertyListContext)#scala/collection/immutable/Iterable/mkString(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/SparkSqlAstBuilder/org$apache$spark$sql$execution$SparkSqlAstBuilder$$visitPropertyKeyValues(org.apache.spark.sql.catalyst.parser.SqlBaseParser$TablePropertyListContext)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/SparkSqlAstBuilder/org$apache$spark$sql$execution$SparkSqlAstBuilder$$visitPropertyKeyValues(org.apache.spark.sql.catalyst.parser.SqlBaseParser$TablePropertyListContext)#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$1/1(org.apache.spark.sql.execution.SparkSqlAstBuilder)
org/apache/spark/sql/execution/SparkSqlAstBuilder/org$apache$spark$sql$execution$SparkSqlAstBuilder$$visitPropertyKeyValues(org.apache.spark.sql.catalyst.parser.SqlBaseParser$TablePropertyListContext)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/org$apache$spark$sql$execution$SparkSqlAstBuilder$$visitPropertyKeyValues(org.apache.spark.sql.catalyst.parser.SqlBaseParser$TablePropertyListContext)#scala/collection/immutable/Iterable$/canBuildFrom()
org/apache/spark/sql/execution/SparkSqlAstBuilder/org$apache$spark$sql$execution$SparkSqlAstBuilder$$visitPropertyKeyValues(org.apache.spark.sql.catalyst.parser.SqlBaseParser$TablePropertyListContext)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/SparkSqlAstBuilder/org$apache$spark$sql$execution$SparkSqlAstBuilder$$visitPropertyKeyValues(org.apache.spark.sql.catalyst.parser.SqlBaseParser$TablePropertyListContext)#scala/collection/immutable/Map/collect(scala.PartialFunction,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/SparkSqlAstBuilder/org$apache$spark$sql$execution$SparkSqlAstBuilder$$validateRowFormatFileFormat(org.apache.spark.sql.catalyst.parser.SqlBaseParser$RowFormatContext,org.apache.spark.sql.catalyst.parser.SqlBaseParser$CreateFileFormatContext,org.antlr.v4.runtime.ParserRuleContext)#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateFileFormatContext/fileFormat()
org/apache/spark/sql/execution/SparkSqlAstBuilder/org$apache$spark$sql$execution$SparkSqlAstBuilder$$validateRowFormatFileFormat(org.apache.spark.sql.catalyst.parser.SqlBaseParser$RowFormatContext,org.apache.spark.sql.catalyst.parser.SqlBaseParser$CreateFileFormatContext,org.antlr.v4.runtime.ParserRuleContext)#org/apache/spark/sql/catalyst/parser/ParserUtils$/operationNotAllowed(java.lang.String,org.antlr.v4.runtime.ParserRuleContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/org$apache$spark$sql$execution$SparkSqlAstBuilder$$validateRowFormatFileFormat(org.apache.spark.sql.catalyst.parser.SqlBaseParser$RowFormatContext,org.apache.spark.sql.catalyst.parser.SqlBaseParser$CreateFileFormatContext,org.antlr.v4.runtime.ParserRuleContext)#scala/Tuple2/_1()
org/apache/spark/sql/execution/SparkSqlAstBuilder/org$apache$spark$sql$execution$SparkSqlAstBuilder$$validateRowFormatFileFormat(org.apache.spark.sql.catalyst.parser.SqlBaseParser$RowFormatContext,org.apache.spark.sql.catalyst.parser.SqlBaseParser$CreateFileFormatContext,org.antlr.v4.runtime.ParserRuleContext)#java/lang/String/toLowerCase(java.util.Locale)
org/apache/spark/sql/execution/SparkSqlAstBuilder/org$apache$spark$sql$execution$SparkSqlAstBuilder$$validateRowFormatFileFormat(org.apache.spark.sql.catalyst.parser.SqlBaseParser$RowFormatContext,org.apache.spark.sql.catalyst.parser.SqlBaseParser$CreateFileFormatContext,org.antlr.v4.runtime.ParserRuleContext)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/SparkSqlAstBuilder/org$apache$spark$sql$execution$SparkSqlAstBuilder$$validateRowFormatFileFormat(org.apache.spark.sql.catalyst.parser.SqlBaseParser$RowFormatContext,org.apache.spark.sql.catalyst.parser.SqlBaseParser$CreateFileFormatContext,org.antlr.v4.runtime.ParserRuleContext)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/org$apache$spark$sql$execution$SparkSqlAstBuilder$$validateRowFormatFileFormat(org.apache.spark.sql.catalyst.parser.SqlBaseParser$RowFormatContext,org.apache.spark.sql.catalyst.parser.SqlBaseParser$CreateFileFormatContext,org.antlr.v4.runtime.ParserRuleContext)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/SparkSqlAstBuilder/org$apache$spark$sql$execution$SparkSqlAstBuilder$$validateRowFormatFileFormat(org.apache.spark.sql.catalyst.parser.SqlBaseParser$RowFormatContext,org.apache.spark.sql.catalyst.parser.SqlBaseParser$CreateFileFormatContext,org.antlr.v4.runtime.ParserRuleContext)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/org$apache$spark$sql$execution$SparkSqlAstBuilder$$validateRowFormatFileFormat(org.apache.spark.sql.catalyst.parser.SqlBaseParser$RowFormatContext,org.apache.spark.sql.catalyst.parser.SqlBaseParser$CreateFileFormatContext,org.antlr.v4.runtime.ParserRuleContext)#org/apache/spark/sql/catalyst/parser/SqlBaseParser$GenericFileFormatContext/identifier()
org/apache/spark/sql/execution/SparkSqlAstBuilder/org$apache$spark$sql$execution$SparkSqlAstBuilder$$validateRowFormatFileFormat(org.apache.spark.sql.catalyst.parser.SqlBaseParser$RowFormatContext,org.apache.spark.sql.catalyst.parser.SqlBaseParser$CreateFileFormatContext,org.antlr.v4.runtime.ParserRuleContext)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/org$apache$spark$sql$execution$SparkSqlAstBuilder$$validateRowFormatFileFormat(org.apache.spark.sql.catalyst.parser.SqlBaseParser$RowFormatContext,org.apache.spark.sql.catalyst.parser.SqlBaseParser$CreateFileFormatContext,org.antlr.v4.runtime.ParserRuleContext)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/SparkSqlAstBuilder/org$apache$spark$sql$execution$SparkSqlAstBuilder$$validateRowFormatFileFormat(org.apache.spark.sql.catalyst.parser.SqlBaseParser$RowFormatContext,org.apache.spark.sql.catalyst.parser.SqlBaseParser$CreateFileFormatContext,org.antlr.v4.runtime.ParserRuleContext)#scala/Tuple2/_2()
org/apache/spark/sql/execution/SparkSqlAstBuilder/org$apache$spark$sql$execution$SparkSqlAstBuilder$$validateRowFormatFileFormat(org.apache.spark.sql.catalyst.parser.SqlBaseParser$RowFormatContext,org.apache.spark.sql.catalyst.parser.SqlBaseParser$CreateFileFormatContext,org.antlr.v4.runtime.ParserRuleContext)#org/apache/spark/sql/catalyst/parser/SqlBaseParser$IdentifierContext/getText()
org/apache/spark/sql/execution/SparkSqlAstBuilder/format$1(org.apache.spark.sql.catalyst.parser.SqlBaseParser$RowFormatContext,java.lang.String,java.lang.String)#scala/Some/x()
org/apache/spark/sql/execution/SparkSqlAstBuilder/format$1(org.apache.spark.sql.catalyst.parser.SqlBaseParser$RowFormatContext,java.lang.String,java.lang.String)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/properties()
org/apache/spark/sql/execution/SparkSqlAstBuilder/format$1(org.apache.spark.sql.catalyst.parser.SqlBaseParser$RowFormatContext,java.lang.String,java.lang.String)#scala/Predef$/ArrowAssoc(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/format$1(org.apache.spark.sql.catalyst.parser.SqlBaseParser$RowFormatContext,java.lang.String,java.lang.String)#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/format$1(org.apache.spark.sql.catalyst.parser.SqlBaseParser$RowFormatContext,java.lang.String,java.lang.String)#org/apache/spark/sql/internal/SQLConf/getConfString(java.lang.String,java.lang.String)
org/apache/spark/sql/execution/SparkSqlAstBuilder/format$1(org.apache.spark.sql.catalyst.parser.SqlBaseParser$RowFormatContext,java.lang.String,java.lang.String)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/locationUri()
org/apache/spark/sql/execution/SparkSqlAstBuilder/format$1(org.apache.spark.sql.catalyst.parser.SqlBaseParser$RowFormatContext,java.lang.String,java.lang.String)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/format$1(org.apache.spark.sql.catalyst.parser.SqlBaseParser$RowFormatContext,java.lang.String,java.lang.String)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/SparkSqlAstBuilder/format$1(org.apache.spark.sql.catalyst.parser.SqlBaseParser$RowFormatContext,java.lang.String,java.lang.String)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/outputFormat()
org/apache/spark/sql/execution/SparkSqlAstBuilder/format$1(org.apache.spark.sql.catalyst.parser.SqlBaseParser$RowFormatContext,java.lang.String,java.lang.String)#scala/collection/Seq$/empty()
org/apache/spark/sql/execution/SparkSqlAstBuilder/format$1(org.apache.spark.sql.catalyst.parser.SqlBaseParser$RowFormatContext,java.lang.String,java.lang.String)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/inputFormat()
org/apache/spark/sql/execution/SparkSqlAstBuilder/format$1(org.apache.spark.sql.catalyst.parser.SqlBaseParser$RowFormatContext,java.lang.String,java.lang.String)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/serde()
org/apache/spark/sql/execution/SparkSqlAstBuilder/format$1(org.apache.spark.sql.catalyst.parser.SqlBaseParser$RowFormatContext,java.lang.String,java.lang.String)#scala/Tuple2/_1()
org/apache/spark/sql/execution/SparkSqlAstBuilder/format$1(org.apache.spark.sql.catalyst.parser.SqlBaseParser$RowFormatContext,java.lang.String,java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/SparkSqlAstBuilder/format$1(org.apache.spark.sql.catalyst.parser.SqlBaseParser$RowFormatContext,java.lang.String,java.lang.String)#scala/Tuple2/_2()
org/apache/spark/sql/execution/SparkSqlAstBuilder/format$1(org.apache.spark.sql.catalyst.parser.SqlBaseParser$RowFormatContext,java.lang.String,java.lang.String)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/SparkSqlAstBuilder/format$1(org.apache.spark.sql.catalyst.parser.SqlBaseParser$RowFormatContext,java.lang.String,java.lang.String)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/SparkSqlAstBuilder/format$1(org.apache.spark.sql.catalyst.parser.SqlBaseParser$RowFormatContext,java.lang.String,java.lang.String)#scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/format$1(org.apache.spark.sql.catalyst.parser.SqlBaseParser$RowFormatContext,java.lang.String,java.lang.String)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/format$1(org.apache.spark.sql.catalyst.parser.SqlBaseParser$RowFormatContext,java.lang.String,java.lang.String)#scala/collection/immutable/Map/toSeq()
org/apache/spark/sql/execution/SparkSqlAstBuilder/format$1(org.apache.spark.sql.catalyst.parser.SqlBaseParser$RowFormatContext,java.lang.String,java.lang.String)#scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/SparkSqlAstBuilder/format$1(org.apache.spark.sql.catalyst.parser.SqlBaseParser$RowFormatContext,java.lang.String,java.lang.String)#scala/Tuple4/Tuple4(java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/format$1(org.apache.spark.sql.catalyst.parser.SqlBaseParser$RowFormatContext,java.lang.String,java.lang.String)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/createConnectionFactory/1/apply()#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$createConnectionFactory$1/apply()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/createConnectionFactory/1/apply()#scala/collection/JavaConverters$/enumerationAsScalaIteratorConverter(java.util.Enumeration)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/createConnectionFactory/1/apply()#java/sql/DriverManager/getDrivers()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/createConnectionFactory/1/apply()#scala/collection/TraversableOnce/collectFirst(scala.PartialFunction)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/createConnectionFactory/1/apply()#scala/collection/convert/Decorators$AsScala/asScala()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/createConnectionFactory/1/apply()#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$createConnectionFactory$1$$anonfun$1/1(org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$createConnectionFactory$1)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/createConnectionFactory/1/apply()#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$createConnectionFactory$1$$anonfun$2/2(org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$createConnectionFactory$1)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/createConnectionFactory/1/apply()#java/sql/Driver/connect(java.lang.String,java.util.Properties)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/createConnectionFactory/1/apply()#org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry$/register(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/createConnectionFactory/1/apply()#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/internal/SharedState/anonfun/2/apply()#org/apache/spark/sql/internal/StaticSQLConf$/WAREHOUSE_PATH()
org/apache/spark/sql/internal/SharedState/anonfun/2/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/internal/SharedState/anonfun/2/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/internal/SharedState/anonfun/2/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/internal/SharedState/anonfun/2/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/internal/SharedState/anonfun/2/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/internal/SharedState/anonfun/2/apply()#org/apache/spark/sql/internal/SharedState$$anonfun$2/apply()
org/apache/spark/sql/internal/SharedState/anonfun/2/apply()#org/apache/spark/internal/config/ConfigEntry/key()
org/apache/spark/sql/internal/SharedState/anonfun/2/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/internal/SharedState/anonfun/2/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/streaming/StreamingDeduplicateExec/anonfun/doExecute/4/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamingDeduplicateExec/anonfun/doExecute/4/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/streaming/StreamingDeduplicateExec/anonfun/doExecute/4/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamingDeduplicateExec/anonfun/doExecute/4/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#org/apache/spark/util/CompletionIterator$/apply(scala.collection.Iterator,scala.Function0)
org/apache/spark/sql/execution/streaming/StreamingDeduplicateExec/anonfun/doExecute/4/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#scala/Some/x()
org/apache/spark/sql/execution/streaming/StreamingDeduplicateExec/anonfun/doExecute/4/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#org/apache/spark/sql/execution/streaming/StreamingDeduplicateExec$$anonfun$doExecute$4$$anonfun$3/3(org.apache.spark.sql.execution.streaming.StreamingDeduplicateExec$$anonfun$doExecute$4,org.apache.spark.sql.catalyst.expressions.codegen.Predicate)
org/apache/spark/sql/execution/streaming/StreamingDeduplicateExec/anonfun/doExecute/4/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#org/apache/spark/sql/catalyst/expressions/codegen/GenerateUnsafeProjection$/generate(java.lang.Object,scala.collection.Seq)
org/apache/spark/sql/execution/streaming/StreamingDeduplicateExec/anonfun/doExecute/4/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#org/apache/spark/sql/execution/streaming/StreamingDeduplicateExec$$anonfun$doExecute$4$$anonfun$4/4(org.apache.spark.sql.execution.streaming.StreamingDeduplicateExec$$anonfun$doExecute$4,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,org.apache.spark.sql.execution.metric.SQLMetric,org.apache.spark.sql.execution.metric.SQLMetric,org.apache.spark.sql.execution.streaming.state.StateStore)
org/apache/spark/sql/execution/streaming/StreamingDeduplicateExec/anonfun/doExecute/4/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#scala/collection/Iterator/filter(scala.Function1)
org/apache/spark/sql/execution/streaming/StreamingDeduplicateExec/anonfun/doExecute/4/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#org/apache/spark/sql/execution/streaming/StreamingDeduplicateExec$$anonfun$doExecute$4$$anonfun$apply$1/1(org.apache.spark.sql.execution.streaming.StreamingDeduplicateExec$$anonfun$doExecute$4,org.apache.spark.sql.execution.metric.SQLMetric,org.apache.spark.sql.execution.streaming.state.StateStore)
org/apache/spark/sql/execution/streaming/StreamingDeduplicateExec/anonfun/doExecute/4/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#org/apache/spark/sql/execution/streaming/StreamingDeduplicateExec/longMetric(java.lang.String)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/3/apply(org.apache.spark.sql.types.StructField)#scala/collection/immutable/Map/head()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/3/apply(org.apache.spark.sql.types.StructField)#scala/collection/immutable/Map/size()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/3/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/catalyst/expressions/Alias/Alias(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String,org.apache.spark.sql.catalyst.expressions.ExprId,scala.Option,scala.Option,java.lang.Boolean)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/3/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/types/StructField/dataType()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/3/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/3/apply(org.apache.spark.sql.types.StructField)#scala/collection/immutable/Map/mkString(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/3/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$6(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/3/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$5(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/3/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$4(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/3/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$3(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/3/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/3/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/catalyst/expressions/Literal$/apply(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/3/apply(org.apache.spark.sql.types.StructField)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/3/apply(org.apache.spark.sql.types.StructField)#scala/Tuple2/_2()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/3/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/types/StructField/name()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/3/apply(org.apache.spark.sql.types.StructField)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/3/apply(org.apache.spark.sql.types.StructField)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/3/apply(org.apache.spark.sql.types.StructField)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/3/apply(org.apache.spark.sql.types.StructField)#scala/collection/immutable/Map/isEmpty()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/3/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/3/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/3/apply(org.apache.spark.sql.types.StructField)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/3/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/execution/datasources/DataSourceAnalysis$$anonfun$3$$anonfun$4/4(org.apache.spark.sql.execution.datasources.DataSourceAnalysis$$anonfun$3,org.apache.spark.sql.types.StructField)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/3/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/3/apply(org.apache.spark.sql.types.StructField)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/3/apply(org.apache.spark.sql.types.StructField)#scala/collection/immutable/Map/filter(scala.Function1)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/3/apply(org.apache.spark.sql.types.StructField)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSourceAnalysis/anonfun/3/apply(org.apache.spark.sql.types.StructField)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/command/TruncateTableCommand/anonfun/run/2/apply(scala.Option)#org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/command/TruncateTableCommand/anonfun/run/2/apply(scala.Option)#scala/util/control/NonFatal$/unapply(java.lang.Throwable)
org/apache/spark/sql/execution/command/TruncateTableCommand/anonfun/run/2/apply(scala.Option)#scala/Option/isDefined()
org/apache/spark/sql/execution/command/TruncateTableCommand/anonfun/run/2/apply(scala.Option)#java/lang/Throwable/toString()
org/apache/spark/sql/execution/command/TruncateTableCommand/anonfun/run/2/apply(scala.Option)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/command/TruncateTableCommand/anonfun/run/2/apply(scala.Option)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/TruncateTableCommand/anonfun/run/2/apply(scala.Option)#scala/Option/isEmpty()
org/apache/spark/sql/execution/command/TruncateTableCommand/anonfun/run/2/apply(scala.Option)#org/apache/hadoop/fs/Path/Path(java.net.URI)
org/apache/spark/sql/execution/command/TruncateTableCommand/anonfun/run/2/apply(scala.Option)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/command/TruncateTableCommand/anonfun/run/2/apply(scala.Option)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/command/TruncateTableCommand/anonfun/run/2/apply(scala.Option)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/command/TruncateTableCommand/anonfun/run/2/apply(scala.Option)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/command/TruncateTableCommand/anonfun/run/2/apply(scala.Option)#org/apache/hadoop/fs/FileSystem/delete(org.apache.hadoop.fs.Path,boolean)
org/apache/spark/sql/execution/command/TruncateTableCommand/anonfun/run/2/apply(scala.Option)#scala/runtime/BoxesRunTime/boxToBoolean(boolean)
org/apache/spark/sql/execution/command/TruncateTableCommand/anonfun/run/2/apply(scala.Option)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/TruncateTableCommand/anonfun/run/2/apply(scala.Option)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/command/TruncateTableCommand/anonfun/run/2/apply(scala.Option)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/command/TruncateTableCommand/anonfun/run/2/apply(scala.Option)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/TruncateTableCommand/anonfun/run/2/apply(scala.Option)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/TruncateTableCommand/anonfun/run/2/apply(scala.Option)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/command/TruncateTableCommand/anonfun/run/2/apply(scala.Option)#org/apache/hadoop/fs/FileSystem/mkdirs(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/command/TruncateTableCommand/anonfun/run/2/apply(scala.Option)#scala/Option/get()
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/anonfun/2/applyOrElse(org.apache.spark.sql.types.DataType,scala.Function1)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/anonfun/2/applyOrElse(org.apache.spark.sql.types.DataType,scala.Function1)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$2$$anonfun$applyOrElse$9/9(org.apache.spark.sql.execution.datasources.parquet.ParquetFilters$$anonfun$2)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/anonfun/2/applyOrElse(org.apache.spark.sql.types.DataType,scala.Function1)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$2$$anonfun$applyOrElse$13/13(org.apache.spark.sql.execution.datasources.parquet.ParquetFilters$$anonfun$2)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/anonfun/2/applyOrElse(org.apache.spark.sql.types.DataType,scala.Function1)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$2$$anonfun$applyOrElse$14/14(org.apache.spark.sql.execution.datasources.parquet.ParquetFilters$$anonfun$2)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/anonfun/2/applyOrElse(org.apache.spark.sql.types.DataType,scala.Function1)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$2$$anonfun$applyOrElse$10/10(org.apache.spark.sql.execution.datasources.parquet.ParquetFilters$$anonfun$2)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/anonfun/2/applyOrElse(org.apache.spark.sql.types.DataType,scala.Function1)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$2$$anonfun$applyOrElse$11/11(org.apache.spark.sql.execution.datasources.parquet.ParquetFilters$$anonfun$2)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/anonfun/2/applyOrElse(org.apache.spark.sql.types.DataType,scala.Function1)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$2$$anonfun$applyOrElse$12/12(org.apache.spark.sql.execution.datasources.parquet.ParquetFilters$$anonfun$2)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/anonfun/2/applyOrElse(org.apache.spark.sql.types.DataType,scala.Function1)#org/apache/spark/sql/execution/datasources/parquet/ParquetFilters$$anonfun$2$$anonfun$applyOrElse$8/8(org.apache.spark.sql.execution.datasources.parquet.ParquetFilters$$anonfun$2)
org/apache/spark/sql/execution/datasources/parquet/ParquetFilters/anonfun/2/applyOrElse(org.apache.spark.sql.types.DataType,scala.Function1)#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateTotalSize(org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateTotalSize(org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateTotalSize(org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/collection/Seq/isEmpty()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateTotalSize(org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/listPartitions(org.apache.spark.sql.catalyst.TableIdentifier,scala.Option)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateTotalSize(org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/collection/TraversableOnce/sum(scala.math.Numeric)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateTotalSize(org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/execution/command/AnalyzeTableCommand$$anonfun$calculateTotalSize$1/1(org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.catalog.CatalogTable)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateTotalSize(org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/listPartitions$default$2()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateTotalSize(org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateTotalSize(org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/execution/command/AnalyzeTableCommand$/org$apache$spark$sql$execution$command$AnalyzeTableCommand$$calculateLocationSize(org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.TableIdentifier,scala.Option)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateTotalSize(org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateTotalSize(org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateTotalSize(org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/execution/command/AnalyzeTableCommand$/calculateTotalSize(org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.catalog.CatalogTable)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateTotalSize(org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/locationUri()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/calculateTotalSize(org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/storage()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/org$apache$spark$sql$execution$command$AnalyzeTableCommand$$calculateLocationSize$1(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.String)#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/org$apache$spark$sql$execution$command$AnalyzeTableCommand$$calculateLocationSize$1(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.String)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/org$apache$spark$sql$execution$command$AnalyzeTableCommand$$calculateLocationSize$1(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.String)#org/apache/hadoop/fs/FileSystem/getFileStatus(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/org$apache$spark$sql$execution$command$AnalyzeTableCommand$$calculateLocationSize$1(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.String)#org/apache/spark/sql/execution/command/AnalyzeTableCommand$$anonfun$6/6(java.lang.String,org.apache.hadoop.fs.FileSystem)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/org$apache$spark$sql$execution$command$AnalyzeTableCommand$$calculateLocationSize$1(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.String)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/command/AnalyzeTableCommand/org$apache$spark$sql$execution$command$AnalyzeTableCommand$$calculateLocationSize$1(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.String)#scala/reflect/ClassTag$/Long()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/org$apache$spark$sql$execution$command$AnalyzeTableCommand$$calculateLocationSize$1(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.String)#scala/Predef$/longArrayOps(long[])
org/apache/spark/sql/execution/command/AnalyzeTableCommand/org$apache$spark$sql$execution$command$AnalyzeTableCommand$$calculateLocationSize$1(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.String)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/org$apache$spark$sql$execution$command$AnalyzeTableCommand$$calculateLocationSize$1(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.String)#org/apache/hadoop/fs/FileStatus/isDirectory()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/org$apache$spark$sql$execution$command$AnalyzeTableCommand$$calculateLocationSize$1(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.String)#org/apache/hadoop/fs/FileStatus/getLen()
org/apache/spark/sql/execution/command/AnalyzeTableCommand/org$apache$spark$sql$execution$command$AnalyzeTableCommand$$calculateLocationSize$1(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.String)#org/apache/hadoop/fs/FileSystem/listStatus(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/command/AnalyzeTableCommand/org$apache$spark$sql$execution$command$AnalyzeTableCommand$$calculateLocationSize$1(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.String)#scala/collection/mutable/ArrayOps/sum(scala.math.Numeric)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/doExecute/1/anonfun/6/anonfun/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/anonfun/statsString/1/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/AttributeReference/name()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/doExecute/1/anonfun/6/anonfun/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/anonfun/statsString/1/1/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/doExecute/1/anonfun/6/anonfun/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/anonfun/statsString/1/1/apply(scala.Tuple2)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/doExecute/1/anonfun/6/anonfun/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/anonfun/statsString/1/1/apply(scala.Tuple2)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/doExecute/1/anonfun/6/anonfun/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/anonfun/statsString/1/1/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/doExecute/1/anonfun/6/anonfun/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/anonfun/statsString/1/1/apply(scala.Tuple2)#scala/Tuple2/_2$mcI$sp()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/doExecute/1/anonfun/6/anonfun/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/anonfun/statsString/1/1/apply(scala.Tuple2)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/doExecute/1/anonfun/6/anonfun/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/anonfun/statsString/1/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/AttributeReference/dataType()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/doExecute/1/anonfun/6/anonfun/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/anonfun/statsString/1/1/apply(scala.Tuple2)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/doExecute/1/anonfun/6/anonfun/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/anonfun/statsString/1/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/InternalRow/get(int,org.apache.spark.sql.types.DataType)
org/apache/spark/sql/SQLImplicits/typecreator7/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/selectType(scala.reflect.api.Symbols$SymbolApi,java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator7/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ModuleSymbolApi/asModule()
org/apache/spark/sql/SQLImplicits/typecreator7/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticPackage(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator7/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$InternalApi/reificationSupport()
org/apache/spark/sql/SQLImplicits/typecreator7/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/RootClass()
org/apache/spark/sql/SQLImplicits/typecreator7/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticModule(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator7/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ClassSymbolApi/asType()
org/apache/spark/sql/SQLImplicits/typecreator7/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ModuleSymbolApi/moduleClass()
org/apache/spark/sql/SQLImplicits/typecreator7/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/thisPrefix(scala.reflect.api.Symbols$SymbolApi)
org/apache/spark/sql/SQLImplicits/typecreator7/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/TypeRef(scala.reflect.api.Types$TypeApi,scala.reflect.api.Symbols$SymbolApi,scala.collection.immutable.List)
org/apache/spark/sql/SQLImplicits/typecreator7/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticClass(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator7/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/universe()
org/apache/spark/sql/SQLImplicits/typecreator7/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$TypeSymbolApi/toTypeConstructor()
org/apache/spark/sql/SQLImplicits/typecreator7/1/apply(scala.reflect.api.Mirror)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/SQLImplicits/typecreator7/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/SingleType(scala.reflect.api.Types$TypeApi,scala.reflect.api.Symbols$SymbolApi)
org/apache/spark/sql/SQLImplicits/typecreator7/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Universe/internal()
org/apache/spark/sql/SQLImplicits/typecreator7/1/apply(scala.reflect.api.Mirror)#scala/collection/immutable/List$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/exchange/EnsureRequirements/anonfun/org/apache/spark/sql/execution/exchange/EnsureRequirements/ensureDistributionAndOrdering/5/apply(scala.Tuple2)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/exchange/EnsureRequirements/anonfun/org/apache/spark/sql/execution/exchange/EnsureRequirements/ensureDistributionAndOrdering/5/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/exchange/EnsureRequirements/anonfun/org/apache/spark/sql/execution/exchange/EnsureRequirements/ensureDistributionAndOrdering/5/apply(scala.Tuple2)#scala/Tuple2/_2()
org/apache/spark/sql/execution/exchange/EnsureRequirements/anonfun/org/apache/spark/sql/execution/exchange/EnsureRequirements/ensureDistributionAndOrdering/5/apply(scala.Tuple2)#scala/collection/Seq/zip(scala.collection.GenIterable,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/exchange/EnsureRequirements/anonfun/org/apache/spark/sql/execution/exchange/EnsureRequirements/ensureDistributionAndOrdering/5/apply(scala.Tuple2)#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/execution/exchange/EnsureRequirements/anonfun/org/apache/spark/sql/execution/exchange/EnsureRequirements/ensureDistributionAndOrdering/5/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/exchange/EnsureRequirements/anonfun/org/apache/spark/sql/execution/exchange/EnsureRequirements/ensureDistributionAndOrdering/5/apply(scala.Tuple2)#org/apache/spark/sql/execution/SortExec$/apply$default$4()
org/apache/spark/sql/execution/exchange/EnsureRequirements/anonfun/org/apache/spark/sql/execution/exchange/EnsureRequirements/ensureDistributionAndOrdering/5/apply(scala.Tuple2)#org/apache/spark/sql/execution/exchange/EnsureRequirements$$anonfun$org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering$5$$anonfun$8/8(org.apache.spark.sql.execution.exchange.EnsureRequirements$$anonfun$org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering$5)
org/apache/spark/sql/execution/exchange/EnsureRequirements/anonfun/org/apache/spark/sql/execution/exchange/EnsureRequirements/ensureDistributionAndOrdering/5/apply(scala.Tuple2)#scala/collection/IterableLike/forall(scala.Function1)
org/apache/spark/sql/execution/exchange/EnsureRequirements/anonfun/org/apache/spark/sql/execution/exchange/EnsureRequirements/ensureDistributionAndOrdering/5/apply(scala.Tuple2)#scala/collection/Seq/length()
org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator/newBuffer()#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator/newBuffer()#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator/newBuffer()#org/apache/spark/sql/catalyst/expressions/UnsafeProjection/apply(org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator/newBuffer()#org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator/initializeBuffer(org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator/newBuffer()#org/apache/spark/sql/catalyst/expressions/UnsafeProjection$/create(org.apache.spark.sql.types.DataType[])
org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator/newBuffer()#org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator$$anonfun$4/4(org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator)
org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator/newBuffer()#org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator$$anonfun$3/3(org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator)
org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator/newBuffer()#org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator$$anonfun$2/2(org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator)
org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator/newBuffer()#org/apache/spark/sql/catalyst/expressions/GenericInternalRow/GenericInternalRow(int)
org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator/newBuffer()#org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator$$anonfun$1/1(org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator)
org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator/newBuffer()#org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator/aggregateFunctions()
org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator/newBuffer()#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator/newBuffer()#scala/collection/mutable/ArrayOps/forall(scala.Function1)
org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator/newBuffer()#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator/newBuffer()#scala/collection/mutable/ArrayOps/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator/processCurrentSortedGroup()#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator/processCurrentSortedGroup()#scala/collection/Iterator/next()
org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator/processCurrentSortedGroup()#org/apache/spark/sql/catalyst/expressions/UnsafeProjection/apply(org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator/processCurrentSortedGroup()#org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator/processRow()
org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator/processCurrentSortedGroup()#org/apache/spark/sql/catalyst/expressions/UnsafeRow/copy()
org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator/processCurrentSortedGroup()#org/apache/spark/sql/catalyst/expressions/package$Projection/apply(java.lang.Object)
org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator/processCurrentSortedGroup()#org/apache/spark/sql/catalyst/InternalRow/copy()
org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator/processCurrentSortedGroup()#scala/Function2/apply(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator/processCurrentSortedGroup()#scala/collection/Iterator/hasNext()
org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator/processCurrentSortedGroup()#org/apache/spark/sql/execution/aggregate/SortBasedAggregationIterator/groupingProjection()
org/apache/spark/sql/execution/WholeStageCodegenExec/doExecute()#org/apache/spark/sql/execution/WholeStageCodegenExec/longMetric(java.lang.String)
org/apache/spark/sql/execution/WholeStageCodegenExec/doExecute()#scala/collection/Seq/apply(int)
org/apache/spark/sql/execution/WholeStageCodegenExec/doExecute()#org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator$/compile(org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment)
org/apache/spark/sql/execution/WholeStageCodegenExec/doExecute()#scala/Tuple2/_1()
org/apache/spark/sql/execution/WholeStageCodegenExec/doExecute()#scala/Tuple2/_2()
org/apache/spark/sql/execution/WholeStageCodegenExec/doExecute()#org/apache/spark/rdd/RDD/mapPartitionsWithIndex(scala.Function2,boolean,scala.reflect.ClassTag)
org/apache/spark/sql/execution/WholeStageCodegenExec/doExecute()#org/apache/spark/rdd/RDD/zipPartitions(org.apache.spark.rdd.RDD,scala.Function2,scala.reflect.ClassTag,scala.reflect.ClassTag)
org/apache/spark/sql/execution/WholeStageCodegenExec/doExecute()#org/apache/spark/sql/execution/WholeStageCodegenExec$$anonfun$doExecute$1/1(org.apache.spark.sql.execution.WholeStageCodegenExec)
org/apache/spark/sql/execution/WholeStageCodegenExec/doExecute()#org/apache/spark/sql/execution/WholeStageCodegenExec$$anonfun$8/8(org.apache.spark.sql.execution.WholeStageCodegenExec,org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment,java.lang.Object[],org.apache.spark.sql.execution.metric.SQLMetric)
org/apache/spark/sql/execution/WholeStageCodegenExec/doExecute()#scala/collection/Seq/size()
org/apache/spark/sql/execution/WholeStageCodegenExec/doExecute()#org/apache/spark/sql/execution/WholeStageCodegenExec$$anonfun$9/9(org.apache.spark.sql.execution.WholeStageCodegenExec)
org/apache/spark/sql/execution/WholeStageCodegenExec/doExecute()#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/WholeStageCodegenExec/doExecute()#org/apache/spark/sql/execution/WholeStageCodegenExec/logWarning(scala.Function0)
org/apache/spark/sql/execution/WholeStageCodegenExec/doExecute()#scala/reflect/ClassTag$/Any()
org/apache/spark/sql/execution/WholeStageCodegenExec/doExecute()#scala/Predef$/assert(boolean,scala.Function0)
org/apache/spark/sql/execution/WholeStageCodegenExec/doExecute()#org/apache/spark/sql/internal/SQLConf/wholeStageFallback()
org/apache/spark/sql/execution/WholeStageCodegenExec/doExecute()#org/apache/spark/util/Utils$/isTesting()
org/apache/spark/sql/execution/WholeStageCodegenExec/doExecute()#org/apache/spark/sql/execution/WholeStageCodegenExec$$anonfun$doExecute$2/2(org.apache.spark.sql.execution.WholeStageCodegenExec)
org/apache/spark/sql/execution/WholeStageCodegenExec/doExecute()#org/apache/spark/rdd/RDD/mapPartitionsWithIndex$default$2()
org/apache/spark/sql/execution/WholeStageCodegenExec/doExecute()#scala/collection/Seq/head()
org/apache/spark/sql/execution/WholeStageCodegenExec/doExecute()#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/WholeStageCodegenExec/doExecute()#scala/collection/mutable/ArrayBuffer/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/WholeStageCodegenExec/doExecute()#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/references()
org/apache/spark/sql/execution/WholeStageCodegenExec/doExecute()#scala/collection/Seq/length()
org/apache/spark/sql/execution/WholeStageCodegenExec/doExecute()#org/apache/spark/sql/execution/WholeStageCodegenExec/sqlContext()
org/apache/spark/sql/execution/WholeStageCodegenExec/doExecute()#org/apache/spark/sql/execution/WholeStageCodegenExec$$anonfun$10/10(org.apache.spark.sql.execution.WholeStageCodegenExec,org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment,java.lang.Object[],org.apache.spark.sql.execution.metric.SQLMetric)
org/apache/spark/sql/execution/WholeStageCodegenExec/doExecute()#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/WholeStageCodegenExec/doCodeGen()#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/CodegenContext()
org/apache/spark/sql/execution/WholeStageCodegenExec/doCodeGen()#java/lang/String/trim()
org/apache/spark/sql/execution/WholeStageCodegenExec/doCodeGen()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/WholeStageCodegenExec/doCodeGen()#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/registerComment(scala.Function0)
org/apache/spark/sql/execution/WholeStageCodegenExec/doCodeGen()#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/initMutableStates()
org/apache/spark/sql/execution/WholeStageCodegenExec/doCodeGen()#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/WholeStageCodegenExec/doCodeGen()#org/apache/spark/sql/execution/WholeStageCodegenExec/logDebug(scala.Function0)
org/apache/spark/sql/execution/WholeStageCodegenExec/doCodeGen()#org/apache/spark/sql/catalyst/expressions/codegen/CodeFormatter$/stripExtraNewLines(java.lang.String)
org/apache/spark/sql/execution/WholeStageCodegenExec/doCodeGen()#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/declareAddedFunctions()
org/apache/spark/sql/execution/WholeStageCodegenExec/doCodeGen()#org/apache/spark/sql/catalyst/expressions/codegen/CodeFormatter$/stripOverlappingComments(org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment)
org/apache/spark/sql/execution/WholeStageCodegenExec/doCodeGen()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/WholeStageCodegenExec/doCodeGen()#org/apache/spark/sql/execution/WholeStageCodegenExec$$anonfun$doCodeGen$1/1(org.apache.spark.sql.execution.WholeStageCodegenExec,org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment)
org/apache/spark/sql/execution/WholeStageCodegenExec/doCodeGen()#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/declareMutableStates()
org/apache/spark/sql/execution/WholeStageCodegenExec/doCodeGen()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/WholeStageCodegenExec/doCodeGen()#org/apache/spark/sql/execution/WholeStageCodegenExec$$anonfun$7/7(org.apache.spark.sql.execution.WholeStageCodegenExec)
org/apache/spark/sql/execution/WholeStageCodegenExec/doCodeGen()#org/apache/spark/sql/catalyst/expressions/codegen/CodeAndComment/CodeAndComment(java.lang.String,scala.collection.Map)
org/apache/spark/sql/execution/WholeStageCodegenExec/doCodeGen()#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/initPartition()
org/apache/spark/sql/execution/WholeStageCodegenExec/doCodeGen()#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/getPlaceHolderToComments()
org/apache/spark/sql/execution/WholeStageCodegenExec/doCodeGen()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/WholeStageCodegenExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/WholeStageCodegenExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/copyResult()
org/apache/spark/sql/execution/WholeStageCodegenExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#java/lang/String/trim()
org/apache/spark/sql/execution/WholeStageCodegenExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/code()
org/apache/spark/sql/execution/WholeStageCodegenExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/WholeStageCodegenExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/WholeStageCodegenExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/WholeStageCodegenExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/WholeStageCodegenExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/WholeStageCodegenExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/value()
org/apache/spark/sql/execution/WholeStageCodegenExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/SparkSession/org$apache$spark$sql$SparkSession$$instantiateSessionState(java.lang.String,org.apache.spark.sql.SparkSession)#java/lang/Class/getConstructors()
org/apache/spark/sql/SparkSession/org$apache$spark$sql$SparkSession$$instantiateSessionState(java.lang.String,org.apache.spark.sql.SparkSession)#scala/util/control/NonFatal$/unapply(java.lang.Throwable)
org/apache/spark/sql/SparkSession/org$apache$spark$sql$SparkSession$$instantiateSessionState(java.lang.String,org.apache.spark.sql.SparkSession)#org/apache/spark/util/Utils$/classForName(java.lang.String)
org/apache/spark/sql/SparkSession/org$apache$spark$sql$SparkSession$$instantiateSessionState(java.lang.String,org.apache.spark.sql.SparkSession)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/SparkSession/org$apache$spark$sql$SparkSession$$instantiateSessionState(java.lang.String,org.apache.spark.sql.SparkSession)#scala/Option/isEmpty()
org/apache/spark/sql/SparkSession/org$apache$spark$sql$SparkSession$$instantiateSessionState(java.lang.String,org.apache.spark.sql.SparkSession)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/SparkSession/org$apache$spark$sql$SparkSession$$instantiateSessionState(java.lang.String,org.apache.spark.sql.SparkSession)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/SparkSession/org$apache$spark$sql$SparkSession$$instantiateSessionState(java.lang.String,org.apache.spark.sql.SparkSession)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String,java.lang.Throwable)
org/apache/spark/sql/SparkSession/org$apache$spark$sql$SparkSession$$instantiateSessionState(java.lang.String,org.apache.spark.sql.SparkSession)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/SparkSession/org$apache$spark$sql$SparkSession$$instantiateSessionState(java.lang.String,org.apache.spark.sql.SparkSession)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/SparkSession/org$apache$spark$sql$SparkSession$$instantiateSessionState(java.lang.String,org.apache.spark.sql.SparkSession)#java/lang/reflect/Constructor/newInstance(java.lang.Object[])
org/apache/spark/sql/SparkSession/org$apache$spark$sql$SparkSession$$instantiateSessionState(java.lang.String,org.apache.spark.sql.SparkSession)#scala/collection/mutable/ArrayOps/head()
org/apache/spark/sql/SparkSession/org$apache$spark$sql$SparkSession$$instantiateSessionState(java.lang.String,org.apache.spark.sql.SparkSession)#scala/Option/get()
org/apache/spark/sql/execution/CacheManager/org$apache$spark$sql$execution$CacheManager$$recacheByCondition(org.apache.spark.sql.SparkSession,scala.Function1)#java/util/Iterator/remove()
org/apache/spark/sql/execution/CacheManager/org$apache$spark$sql$execution$CacheManager$$recacheByCondition(org.apache.spark.sql.SparkSession,scala.Function1)#org/apache/spark/rdd/RDD/unpersist(boolean)
org/apache/spark/sql/execution/CacheManager/org$apache$spark$sql$execution$CacheManager$$recacheByCondition(org.apache.spark.sql.SparkSession,scala.Function1)#scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)
org/apache/spark/sql/execution/CacheManager/org$apache$spark$sql$execution$CacheManager$$recacheByCondition(org.apache.spark.sql.SparkSession,scala.Function1)#scala/collection/mutable/ArrayBuffer$/empty()
org/apache/spark/sql/execution/CacheManager/org$apache$spark$sql$execution$CacheManager$$recacheByCondition(org.apache.spark.sql.SparkSession,scala.Function1)#org/apache/spark/sql/execution/CacheManager$$anonfun$org$apache$spark$sql$execution$CacheManager$$recacheByCondition$1/1(org.apache.spark.sql.execution.CacheManager)
org/apache/spark/sql/execution/CacheManager/org$apache$spark$sql$execution$CacheManager$$recacheByCondition(org.apache.spark.sql.SparkSession,scala.Function1)#org/apache/spark/sql/execution/columnar/InMemoryRelation$/apply(boolean,int,org.apache.spark.storage.StorageLevel,org.apache.spark.sql.execution.SparkPlan,scala.Option)
org/apache/spark/sql/execution/CacheManager/org$apache$spark$sql$execution$CacheManager$$recacheByCondition(org.apache.spark.sql.SparkSession,scala.Function1)#org/apache/spark/rdd/RDD/unpersist$default$1()
org/apache/spark/sql/execution/CacheManager/org$apache$spark$sql$execution$CacheManager$$recacheByCondition(org.apache.spark.sql.SparkSession,scala.Function1)#java/util/LinkedList/iterator()
org/apache/spark/sql/execution/CacheManager/org$apache$spark$sql$execution$CacheManager$$recacheByCondition(org.apache.spark.sql.SparkSession,scala.Function1)#scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object)
org/apache/spark/sql/execution/CacheManager/org$apache$spark$sql$execution$CacheManager$$recacheByCondition(org.apache.spark.sql.SparkSession,scala.Function1)#java/util/Iterator/hasNext()
org/apache/spark/sql/execution/CacheManager/org$apache$spark$sql$execution$CacheManager$$recacheByCondition(org.apache.spark.sql.SparkSession,scala.Function1)#java/util/Iterator/next()
org/apache/spark/sql/execution/CacheManager/org$apache$spark$sql$execution$CacheManager$$recacheByCondition(org.apache.spark.sql.SparkSession,scala.Function1)#scala/collection/mutable/ArrayBuffer/foreach(scala.Function1)
org/apache/spark/sql/execution/CacheManager/org$apache$spark$sql$execution$CacheManager$$recacheByCondition(org.apache.spark.sql.SparkSession,scala.Function1)#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/datasources/FileSourceStrategy$/logInfo(scala.Function0)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/datasources/FileSourceStrategy$$anonfun$apply$4/4(org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/expressions/ExpressionSet/$minus$minus(scala.collection.GenTraversableOnce)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/expressions/ExpressionSet$/apply(scala.collection.TraversableOnce)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/expressions/AttributeSet$/apply(scala.collection.Iterable)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/datasources/FileSourceStrategy$$anonfun$3/3(org.apache.spark.sql.catalyst.expressions.AttributeSet)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/expressions/AttributeSet/toSeq()
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/expressions/ExpressionSet/filter(scala.Function1)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/datasources/FileSourceStrategy$$anonfun$4/4()
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Option/get()
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/analysis/Analyzer/resolver()
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/datasources/FileSourceStrategy$$anonfun$7/7()
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/Seq/reduceOption(scala.Function2)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/datasources/FileSourceStrategy$$anonfun$apply$2/2(org.apache.spark.sql.catalyst.expressions.ExpressionSet)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/Seq/filter(scala.Function1)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Tuple3/_1()
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Tuple3/_2()
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/datasources/FileSourceStrategy$$anonfun$9/9(org.apache.spark.sql.execution.FileSourceScanExec)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/QueryPlan/output()
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Tuple3/_3()
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/datasources/FileSourceStrategy$$anonfun$6/6(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Option/isEmpty()
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/datasources/FileSourceStrategy$$anonfun$8/8(org.apache.spark.sql.execution.FileSourceScanExec)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/datasources/FileSourceStrategy$$anonfun$1/1(org.apache.spark.sql.execution.datasources.LogicalRelation)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/planning/PhysicalOperation$/unapply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/TraversableLike/filterNot(scala.Function1)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/datasources/FileSourceStrategy$$anonfun$2/2(org.apache.spark.sql.catalyst.expressions.AttributeSet)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/expressions/package$AttributeSeq/toStructType()
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/expressions/package$/AttributeSeq(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/datasources/FileSourceStrategy$$anonfun$apply$3/3(scala.collection.immutable.Set)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/datasources/FileSourceStrategy$/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/datasources/FileSourceStrategy$$anonfun$5/5(org.apache.spark.sql.catalyst.expressions.AttributeSet)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/expressions/ExpressionSet/toSeq()
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/datasources/LogicalRelation/resolve(org.apache.spark.sql.types.StructType,scala.Function2)
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/immutable/Set/toSeq()
org/apache/spark/sql/execution/datasources/FileSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/AppendColumns/func()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsInR/outputObjAttr()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/CoGroup/leftGroup()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/MapPartitions/func()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsWithState/dataAttributes()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/expressions/IntegerLiteral$/unapply(java.lang.Object)
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Expand/projections()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/MapGroups/child()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsInR/func()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/MapPartitions/outputObjAttr()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#java/lang/IllegalStateException/IllegalStateException(java.lang.String)
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsWithState/func()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/LocalLimit/limitExpr()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/SparkStrategies$BasicOperators$$anonfun$5/5(org.apache.spark.sql.execution.SparkStrategies$BasicOperators$)
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/MapElements/child()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/CoGroup/right()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/LocalRelation/data()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsInR/inputSchema()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Project/projectList()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Sample/upperBound()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Repartition/shuffle()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/CoGroup/leftAttr()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Sample/lowerBound()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsInR/broadcastVars()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/MapPartitionsInR/child()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/CoGroup/leftDeserializer()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Generate/join()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/AppendColumnsWithObject/newColumnsSerializer()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Repartition/child()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/GlobalLimit/child()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsWithState/keyDeserializer()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/MapGroups/valueDeserializer()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Sample/child()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/RepartitionByExpression/numPartitions()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/SerializeFromObject/serializer()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/DeserializeToObject/child()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Project/child()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Generate/generator()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/encoders/RowEncoder$/apply(org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/MapGroups/groupingAttributes()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/AppendColumnsWithObject/childSerializer()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/SparkStrategies$BasicOperators$$anonfun$4/4(org.apache.spark.sql.execution.SparkStrategies$BasicOperators$,org.apache.spark.sql.catalyst.encoders.ExpressionEncoder)
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsInR/keyDeserializer()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Option/isEmpty()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/MapGroups/outputObjAttr()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/RepartitionByExpression/partitionExpressions()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/RDDScanExec$/apply$default$4()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/MapPartitionsInR/outputSchema()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/MapPartitionsInR/packageNames()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/RDDScanExec$/apply$default$5()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/LocalLimit/child()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Window/child()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/CoGroup/outputObjAttr()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Sort/order()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/MapElements/outputObjAttr()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/MapPartitionsInR/func()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/SparkStrategies$BasicOperators$/planLater(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/AppendColumns/serializer()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/physical/HashPartitioning/HashPartitioning(scala.collection.Seq,int)
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/CoGroup/rightGroup()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Union/children()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/AppendColumns/deserializer()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/TypedFilter/deserializer()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/MapGroups/dataAttributes()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/MapElements/func()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/CoGroup/func()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/MapPartitionsInR/outputObjAttr()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsWithState/child()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Generate/outer()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/TypedFilter/child()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/AppendColumns/child()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Generate/child()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/SparkPlanner/singleRowRdd()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/exchange/ShuffleExchange$/apply(org.apache.spark.sql.catalyst.plans.physical.Partitioning,org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/DeserializeToObject/outputObjAttr()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsInR/child()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/CoGroup/left()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/MapGroups/func()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/CoGroup/rightAttr()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Generate/qualifiedGeneratorOutput()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Filter/condition()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/MapPartitionsInR/inputSchema()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/MapPartitions/child()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsInR/groupingAttributes()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/TypedFilter/typedCondition(org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/MapGroups/keyDeserializer()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/RepartitionByExpression/child()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/CoGroup/rightDeserializer()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Filter/child()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Sort/global()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsWithState/timeout()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/MapPartitionsInR/broadcastVars()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Window/partitionSpec()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Expand/child()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/MapGroupsExec$/apply(scala.Function3,org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.Attribute,org.apache.spark.sql.streaming.GroupStateTimeout,org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Sample/withReplacement()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/AppendColumnsWithObject/child()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsWithState/groupingAttributes()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/SerializeFromObject/child()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/CoGroup/keyDeserializer()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Sample/seed()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Expand/output()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/SortExec$/apply$default$4()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsWithState/valueDeserializer()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/physical/RoundRobinPartitioning/RoundRobinPartitioning(int)
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/LocalRelation/output()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Repartition/numPartitions()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/DeserializeToObject/deserializer()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Window/orderSpec()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/AppendColumnsWithObject/func()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Sort/child()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/GlobalLimit/limitExpr()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsInR/valueDeserializer()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsWithState/outputObjAttr()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsInR/dataAttributes()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Window/windowExpressions()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/ResolvedHint/child()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsInR/packageNames()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Option/get()
org/apache/spark/sql/execution/SparkStrategies/BasicOperators/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/FlatMapGroupsInR/outputSchema()
org/apache/spark/sql/execution/RowDataSourceScanExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/Seq/zipWithIndex(scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/RowDataSourceScanExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/RowDataSourceScanExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#java/lang/String/trim()
org/apache/spark/sql/execution/RowDataSourceScanExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/RowDataSourceScanExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/execution/RowDataSourceScanExec$$anonfun$6/6(org.apache.spark.sql.execution.RowDataSourceScanExec)
org/apache/spark/sql/execution/RowDataSourceScanExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/RowDataSourceScanExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/freshName(java.lang.String)
org/apache/spark/sql/execution/RowDataSourceScanExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/RowDataSourceScanExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/RowDataSourceScanExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/RowDataSourceScanExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/currentVars_$eq(scala.collection.Seq)
org/apache/spark/sql/execution/RowDataSourceScanExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addMutableState(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/RowDataSourceScanExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/RowDataSourceScanExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/INPUT_ROW_$eq(java.lang.String)
org/apache/spark/sql/execution/RowDataSourceScanExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/RowDataSourceScanExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/RowDataSourceScanExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/execution/RowDataSourceScanExec$$anonfun$7/7(org.apache.spark.sql.execution.RowDataSourceScanExec,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)
org/apache/spark/sql/execution/RowDataSourceScanExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/2/apply(scala.Tuple2)#scala/Some/x()
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/2/apply(scala.Tuple2)#org/apache/spark/sql/types/MetadataBuilder/putString(java.lang.String,java.lang.String)
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/2/apply(scala.Tuple2)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/2/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/2/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$6(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/2/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$5(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/2/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$4(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/2/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$3(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/2/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/Alias/Alias(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String,org.apache.spark.sql.catalyst.expressions.ExprId,scala.Option,scala.Option,java.lang.Boolean)
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/2/apply(scala.Tuple2)#org/apache/spark/sql/types/MetadataBuilder/build()
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/2/apply(scala.Tuple2)#org/apache/spark/sql/types/MetadataBuilder/MetadataBuilder()
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/2/apply(scala.Tuple2)#scala/Tuple2/_2()
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/2/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/2/apply(scala.Tuple2)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/command/ShowTablePropertiesCommand/run(org.apache.spark.sql.SparkSession)#scala/Some/x()
org/apache/spark/sql/execution/command/ShowTablePropertiesCommand/run(org.apache.spark.sql.SparkSession)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/command/ShowTablePropertiesCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/empty()
org/apache/spark/sql/execution/command/ShowTablePropertiesCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/ShowTablePropertiesCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/immutable/Map/getOrElse(java.lang.Object,scala.Function0)
org/apache/spark/sql/execution/command/ShowTablePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/Row$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/command/ShowTablePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/properties()
org/apache/spark/sql/execution/command/ShowTablePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/ShowTablePropertiesCommand$$anonfun$run$4/4(org.apache.spark.sql.execution.command.ShowTablePropertiesCommand)
org/apache/spark/sql/execution/command/ShowTablePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/ShowTablePropertiesCommand$$anonfun$14/14(org.apache.spark.sql.execution.command.ShowTablePropertiesCommand,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)
org/apache/spark/sql/execution/command/ShowTablePropertiesCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/immutable/Map/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/ShowTablePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/isTemporaryTable(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/ShowTablePropertiesCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/command/ShowTablePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/ShowTablePropertiesCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/ShowTablePropertiesCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/TraversableOnce/toSeq()
org/apache/spark/sql/execution/command/ShowTablePropertiesCommand/run(org.apache.spark.sql.SparkSession)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/command/ShowTablePropertiesCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/immutable/Iterable$/canBuildFrom()
org/apache/spark/sql/execution/ExpandExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#java/lang/String/trim()
org/apache/spark/sql/execution/ExpandExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/immutable/IndexedSeq$/canBuildFrom()
org/apache/spark/sql/execution/ExpandExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/ExpandExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/ExpandExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/freshName(java.lang.String)
org/apache/spark/sql/execution/ExpandExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/ExpandExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/Seq/zipWithIndex(scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/ExpandExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/ExpandExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/currentVars_$eq(scala.collection.Seq)
org/apache/spark/sql/execution/ExpandExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/ExpandExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/execution/ExpandExec$$anonfun$4/4(org.apache.spark.sql.execution.ExpandExec,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,boolean[])
org/apache/spark/sql/execution/ExpandExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/Seq/indices()
org/apache/spark/sql/execution/ExpandExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/copyResult_$eq(boolean)
org/apache/spark/sql/execution/ExpandExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/ExpandExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/immutable/Range/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/ExpandExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/execution/ExpandExec$$anonfun$5/5(org.apache.spark.sql.execution.ExpandExec,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,boolean[],scala.collection.immutable.IndexedSeq)
org/apache/spark/sql/execution/ExpandExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/ExpandExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/ExpandExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/execution/ExpandExec$$anonfun$1/1(org.apache.spark.sql.execution.ExpandExec)
org/apache/spark/sql/execution/ExpandExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/ExpandExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/ExpandExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/Seq/length()
org/apache/spark/sql/execution/ExpandExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/reflect/ClassTag$/Boolean()
org/apache/spark/sql/execution/ExpandExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/Seq/mkString(java.lang.String)
org/apache/spark/sql/execution/ExpandExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/WatermarkSupport/anonfun/watermarkExpression/1/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/streaming/WatermarkSupport/anonfun/watermarkExpression/1/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#org/apache/spark/internal/Logging/logInfo(scala.Function0)
org/apache/spark/sql/execution/streaming/WatermarkSupport/anonfun/watermarkExpression/1/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#org/apache/spark/sql/execution/streaming/WatermarkSupport$$anonfun$watermarkExpression$1$$anonfun$apply$3/3(org.apache.spark.sql.execution.streaming.WatermarkSupport$$anonfun$watermarkExpression$1,org.apache.spark.sql.catalyst.expressions.LessThanOrEqual)
org/apache/spark/sql/execution/streaming/WatermarkSupport/anonfun/watermarkExpression/1/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#org/apache/spark/sql/catalyst/expressions/LessThanOrEqual/LessThanOrEqual(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/execution/streaming/WatermarkSupport/anonfun/watermarkExpression/1/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/streaming/WatermarkSupport/anonfun/watermarkExpression/1/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#org/apache/spark/sql/catalyst/expressions/Attribute/dataType()
org/apache/spark/sql/execution/streaming/WatermarkSupport/anonfun/watermarkExpression/1/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#org/apache/spark/sql/catalyst/expressions/GetStructField/GetStructField(org.apache.spark.sql.catalyst.expressions.Expression,int,scala.Option)
org/apache/spark/sql/execution/streaming/WatermarkSupport/anonfun/watermarkExpression/1/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#org/apache/spark/sql/catalyst/expressions/GetStructField$/apply$default$3()
org/apache/spark/sql/execution/streaming/WatermarkSupport/anonfun/watermarkExpression/1/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#org/apache/spark/sql/catalyst/expressions/Literal$/apply(java.lang.Object)
org/apache/spark/sql/execution/streaming/WatermarkSupport/anonfun/watermarkExpression/1/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#scala/Option/get()
org/apache/spark/sql/execution/aggregate/AggregationIterator/anonfun/8/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/aggregate/DeclarativeAggregate/updateExpressions()
org/apache/spark/sql/execution/aggregate/AggregationIterator/anonfun/8/apply(scala.Tuple2)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/aggregate/AggregationIterator/anonfun/8/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/aggregate/AggregateExpression/mode()
org/apache/spark/sql/execution/aggregate/AggregationIterator/anonfun/8/apply(scala.Tuple2)#scala/collection/Seq/apply(int)
org/apache/spark/sql/execution/aggregate/AggregationIterator/anonfun/8/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/aggregate/DeclarativeAggregate/mergeExpressions()
org/apache/spark/sql/execution/aggregate/AggregationIterator/anonfun/8/apply(scala.Tuple2)#org/apache/spark/sql/execution/aggregate/AggregationIterator$$anonfun$8$$anonfun$apply$2/2(org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$8)
org/apache/spark/sql/execution/aggregate/AggregationIterator/anonfun/8/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/aggregate/AggregationIterator/anonfun/8/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/aggregate/AggregateFunction/aggBufferAttributes()
org/apache/spark/sql/execution/aggregate/AggregationIterator/anonfun/8/apply(scala.Tuple2)#scala/collection/Seq$/fill(int,scala.Function0)
org/apache/spark/sql/execution/aggregate/AggregationIterator/anonfun/8/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/aggregate/AggregationIterator/anonfun/8/apply(scala.Tuple2)#scala/collection/Seq/length()
org/apache/spark/sql/execution/aggregate/AggregationIterator/anonfun/8/apply(scala.Tuple2)#scala/Tuple2/_2$mcI$sp()
org/apache/spark/sql/execution/command/AlterDatabasePropertiesCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/empty()
org/apache/spark/sql/execution/command/AlterDatabasePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogDatabase/properties()
org/apache/spark/sql/execution/command/AlterDatabasePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/getDatabaseMetadata(java.lang.String)
org/apache/spark/sql/execution/command/AlterDatabasePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/alterDatabase(org.apache.spark.sql.catalyst.catalog.CatalogDatabase)
org/apache/spark/sql/execution/command/AlterDatabasePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogDatabase/copy(java.lang.String,java.lang.String,java.net.URI,scala.collection.immutable.Map)
org/apache/spark/sql/execution/command/AlterDatabasePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogDatabase/copy$default$1()
org/apache/spark/sql/execution/command/AlterDatabasePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogDatabase/copy$default$2()
org/apache/spark/sql/execution/command/AlterDatabasePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogDatabase/copy$default$3()
org/apache/spark/sql/execution/command/AlterDatabasePropertiesCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/immutable/Map/$plus$plus(scala.collection.GenTraversableOnce)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitSetTableSerDe/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$SetTableSerDeContext/tableIdentifier()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitSetTableSerDe/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitSetTableSerDe$1/apply()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitSetTableSerDe/1/apply()#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitSetTableSerDe/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$SetTableSerDeContext/tablePropertyList()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitSetTableSerDe/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$SetTableSerDeContext/partitionSpec()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitSetTableSerDe/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$SetTableSerDeContext/STRING()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitSetTableSerDe/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder/visitTableIdentifier(org.apache.spark.sql.catalyst.parser.SqlBaseParser$TableIdentifierContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitSetTableSerDe/1/apply()#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitSetTableSerDe/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitSetTableSerDe$1$$anonfun$apply$23/23(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitSetTableSerDe$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitSetTableSerDe/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitSetTableSerDe$1$$anonfun$apply$22/22(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitSetTableSerDe$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitSetTableSerDe/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitSetTableSerDe$1$$anonfun$apply$21/21(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitSetTableSerDe$1)
org/apache/spark/sql/execution/SparkPlan/anonfun/2/apply(scala.collection.Iterator)#org/apache/spark/io/CompressionCodec$/createCodec(org.apache.spark.SparkConf)
org/apache/spark/sql/execution/SparkPlan/anonfun/2/apply(scala.collection.Iterator)#java/io/DataOutputStream/writeInt(int)
org/apache/spark/sql/execution/SparkPlan/anonfun/2/apply(scala.collection.Iterator)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/getSizeInBytes()
org/apache/spark/sql/execution/SparkPlan/anonfun/2/apply(scala.collection.Iterator)#scala/collection/Iterator/next()
org/apache/spark/sql/execution/SparkPlan/anonfun/2/apply(scala.collection.Iterator)#java/io/DataOutputStream/close()
org/apache/spark/sql/execution/SparkPlan/anonfun/2/apply(scala.collection.Iterator)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/SparkPlan/anonfun/2/apply(scala.collection.Iterator)#org/apache/spark/SparkEnv$/get()
org/apache/spark/sql/execution/SparkPlan/anonfun/2/apply(scala.collection.Iterator)#scala/package$/Iterator()
org/apache/spark/sql/execution/SparkPlan/anonfun/2/apply(scala.collection.Iterator)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/writeToStream(java.io.OutputStream,byte[])
org/apache/spark/sql/execution/SparkPlan/anonfun/2/apply(scala.collection.Iterator)#org/apache/spark/SparkEnv/conf()
org/apache/spark/sql/execution/SparkPlan/anonfun/2/apply(scala.collection.Iterator)#java/io/DataOutputStream/flush()
org/apache/spark/sql/execution/SparkPlan/anonfun/2/apply(scala.collection.Iterator)#java/io/ByteArrayOutputStream/ByteArrayOutputStream()
org/apache/spark/sql/execution/SparkPlan/anonfun/2/apply(scala.collection.Iterator)#org/apache/spark/io/CompressionCodec/compressedOutputStream(java.io.OutputStream)
org/apache/spark/sql/execution/SparkPlan/anonfun/2/apply(scala.collection.Iterator)#scala/collection/Iterator/hasNext()
org/apache/spark/sql/execution/SparkPlan/anonfun/2/apply(scala.collection.Iterator)#scala/collection/Iterator$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/SparkPlan/anonfun/2/apply(scala.collection.Iterator)#java/io/DataOutputStream/DataOutputStream(java.io.OutputStream)
org/apache/spark/sql/execution/SparkPlan/anonfun/2/apply(scala.collection.Iterator)#java/io/ByteArrayOutputStream/toByteArray()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anon/1/next()#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anon$1/nextKey_$eq(org.apache.spark.sql.catalyst.expressions.UnsafeRow)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anon/1/next()#org/apache/spark/sql/execution/UnsafeKVExternalSorter$KVSorterIterator/getValue()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anon/1/next()#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anon$1/currentKey()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anon/1/next()#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anon$1/joinedRow()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anon/1/next()#org/apache/spark/sql/catalyst/expressions/JoinedRow/apply(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anon/1/next()#org/apache/spark/sql/catalyst/expressions/UnsafeRow/copy()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anon/1/next()#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anon$1/currentKey_$eq(org.apache.spark.sql.catalyst.expressions.UnsafeRow)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anon/1/next()#org/apache/spark/sql/catalyst/expressions/package$MutableProjection/apply(java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anon/1/next()#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anon$1/currentRow()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anon/1/next()#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anon$1/currentRow_$eq(org.apache.spark.sql.catalyst.expressions.UnsafeRow)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anon/1/next()#org/apache/spark/sql/catalyst/expressions/UnsafeRow/equals(java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anon/1/next()#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anon$1/nextKey()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anon/1/next()#org/apache/spark/sql/catalyst/expressions/package$MutableProjection/target(org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anon/1/next()#org/apache/spark/sql/execution/UnsafeKVExternalSorter$KVSorterIterator/getKey()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anon/1/next()#org/apache/spark/sql/execution/UnsafeKVExternalSorter$KVSorterIterator/next()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anon/1/next()#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anon$1/mergeProjection()
org/apache/spark/sql/execution/streaming/StateStoreSaveExec/doExecute()#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/streaming/StateStoreSaveExec/doExecute()#org/apache/spark/sql/execution/streaming/state/package$StateStoreOps/mapPartitionsWithStateStore(java.lang.String,long,long,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.internal.SessionState,scala.Option,scala.Function2,scala.reflect.ClassTag)
org/apache/spark/sql/execution/streaming/StateStoreSaveExec/doExecute()#scala/Predef$/assert(boolean,scala.Function0)
org/apache/spark/sql/execution/streaming/StateStoreSaveExec/doExecute()#org/apache/spark/sql/execution/streaming/StateStoreSaveExec$$anonfun$doExecute$3/3(org.apache.spark.sql.execution.streaming.StateStoreSaveExec)
org/apache/spark/sql/execution/streaming/StateStoreSaveExec/doExecute()#org/apache/spark/sql/catalyst/expressions/package$AttributeSeq/toStructType()
org/apache/spark/sql/execution/streaming/StateStoreSaveExec/doExecute()#org/apache/spark/sql/catalyst/expressions/package$/AttributeSeq(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/StateStoreSaveExec/doExecute()#org/apache/spark/sql/execution/streaming/StateStoreSaveExec/sqlContext()
org/apache/spark/sql/execution/streaming/StateStoreSaveExec/doExecute()#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/streaming/StateStoreSaveExec/doExecute()#scala/Option/nonEmpty()
org/apache/spark/sql/execution/streaming/StateStoreSaveExec/doExecute()#org/apache/spark/sql/execution/streaming/StateStoreSaveExec$$anonfun$doExecute$2/2(org.apache.spark.sql.execution.streaming.StateStoreSaveExec)
org/apache/spark/sql/execution/streaming/StateStoreSaveExec/doExecute()#org/apache/spark/sql/execution/streaming/state/package$/StateStoreOps(org.apache.spark.rdd.RDD,scala.reflect.ClassTag)
org/apache/spark/sql/execution/streaming/StateStoreSaveExec/doExecute()#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/internal/SharedState/anonfun/3/apply()#org/apache/spark/sql/internal/StaticSQLConf$/WAREHOUSE_PATH()
org/apache/spark/sql/internal/SharedState/anonfun/3/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/internal/SharedState/anonfun/3/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/internal/SharedState/anonfun/3/apply()#org/apache/spark/internal/config/ConfigEntry/key()
org/apache/spark/sql/internal/SharedState/anonfun/3/apply()#org/apache/spark/sql/internal/SharedState$$anonfun$3/apply()
org/apache/spark/sql/internal/SharedState/anonfun/3/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/internal/SharedState/anonfun/3/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/internal/SharedState/anonfun/3/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/internal/SharedState/anonfun/3/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/internal/SharedState/anonfun/3/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/jdbc/PostgresDialect/getJDBCType(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/DecimalType/scale()
org/apache/spark/sql/jdbc/PostgresDialect/getJDBCType(org.apache.spark.sql.types.DataType)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/jdbc/PostgresDialect/getJDBCType(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/jdbc/PostgresDialect$$anonfun$getJDBCType$1/1()
org/apache/spark/sql/jdbc/PostgresDialect/getJDBCType(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/jdbc/PostgresDialect$$anonfun$getJDBCType$2/2(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/jdbc/PostgresDialect/getJDBCType(org.apache.spark.sql.types.DataType)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/jdbc/PostgresDialect/getJDBCType(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/jdbc/PostgresDialect$$anonfun$getJDBCType$3/3()
org/apache/spark/sql/jdbc/PostgresDialect/getJDBCType(org.apache.spark.sql.types.DataType)#scala/Option/orElse(scala.Function0)
org/apache/spark/sql/jdbc/PostgresDialect/getJDBCType(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/jdbc/PostgresDialect$/getJDBCType(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/jdbc/PostgresDialect/getJDBCType(org.apache.spark.sql.types.DataType)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/jdbc/PostgresDialect/getJDBCType(org.apache.spark.sql.types.DataType)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/jdbc/PostgresDialect/getJDBCType(org.apache.spark.sql.types.DataType)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/jdbc/PostgresDialect/getJDBCType(org.apache.spark.sql.types.DataType)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/jdbc/PostgresDialect/getJDBCType(org.apache.spark.sql.types.DataType)#scala/Option/map(scala.Function1)
org/apache/spark/sql/jdbc/PostgresDialect/getJDBCType(org.apache.spark.sql.types.DataType)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/jdbc/PostgresDialect/getJDBCType(org.apache.spark.sql.types.DataType)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/jdbc/PostgresDialect/getJDBCType(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/ArrayType/elementType()
org/apache/spark/sql/jdbc/PostgresDialect/getJDBCType(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/DecimalType/precision()
org/apache/spark/sql/jdbc/PostgresDialect/getCatalystType(int,java.lang.String,int,org.apache.spark.sql.types.MetadataBuilder)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/jdbc/PostgresDialect/getCatalystType(int,java.lang.String,int,org.apache.spark.sql.types.MetadataBuilder)#org/apache/spark/sql/types/Metadata/getLong(java.lang.String)
org/apache/spark/sql/jdbc/PostgresDialect/getCatalystType(int,java.lang.String,int,org.apache.spark.sql.types.MetadataBuilder)#scala/collection/immutable/StringOps/drop(int)
org/apache/spark/sql/jdbc/PostgresDialect/getCatalystType(int,java.lang.String,int,org.apache.spark.sql.types.MetadataBuilder)#org/apache/spark/sql/jdbc/PostgresDialect$/toCatalystType(java.lang.String,int,int)
org/apache/spark/sql/jdbc/PostgresDialect/getCatalystType(int,java.lang.String,int,org.apache.spark.sql.types.MetadataBuilder)#scala/Option/map(scala.Function1)
org/apache/spark/sql/jdbc/PostgresDialect/getCatalystType(int,java.lang.String,int,org.apache.spark.sql.types.MetadataBuilder)#java/lang/String/equals(java.lang.Object)
org/apache/spark/sql/jdbc/PostgresDialect/getCatalystType(int,java.lang.String,int,org.apache.spark.sql.types.MetadataBuilder)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/jdbc/PostgresDialect/getCatalystType(int,java.lang.String,int,org.apache.spark.sql.types.MetadataBuilder)#org/apache/spark/sql/jdbc/PostgresDialect$/getCatalystType(int,java.lang.String,int,org.apache.spark.sql.types.MetadataBuilder)
org/apache/spark/sql/jdbc/PostgresDialect/getCatalystType(int,java.lang.String,int,org.apache.spark.sql.types.MetadataBuilder)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/jdbc/PostgresDialect/getCatalystType(int,java.lang.String,int,org.apache.spark.sql.types.MetadataBuilder)#org/apache/spark/sql/types/MetadataBuilder/build()
org/apache/spark/sql/jdbc/PostgresDialect/getCatalystType(int,java.lang.String,int,org.apache.spark.sql.types.MetadataBuilder)#org/apache/spark/sql/jdbc/PostgresDialect$$anonfun$getCatalystType$1/1()
org/apache/spark/sql/execution/aggregate/TypedAverage/typecreator1/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ModuleSymbolApi/asModule()
org/apache/spark/sql/execution/aggregate/TypedAverage/typecreator1/1/apply(scala.reflect.api.Mirror)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/TypedAverage/typecreator1/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticPackage(java.lang.String)
org/apache/spark/sql/execution/aggregate/TypedAverage/typecreator1/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$InternalApi/reificationSupport()
org/apache/spark/sql/execution/aggregate/TypedAverage/typecreator1/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/ThisType(scala.reflect.api.Symbols$SymbolApi)
org/apache/spark/sql/execution/aggregate/TypedAverage/typecreator1/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ClassSymbolApi/asType()
org/apache/spark/sql/execution/aggregate/TypedAverage/typecreator1/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ModuleSymbolApi/moduleClass()
org/apache/spark/sql/execution/aggregate/TypedAverage/typecreator1/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/TypeRef(scala.reflect.api.Types$TypeApi,scala.reflect.api.Symbols$SymbolApi,scala.collection.immutable.List)
org/apache/spark/sql/execution/aggregate/TypedAverage/typecreator1/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticClass(java.lang.String)
org/apache/spark/sql/execution/aggregate/TypedAverage/typecreator1/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/universe()
org/apache/spark/sql/execution/aggregate/TypedAverage/typecreator1/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$TypeSymbolApi/toTypeConstructor()
org/apache/spark/sql/execution/aggregate/TypedAverage/typecreator1/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Universe/internal()
org/apache/spark/sql/execution/aggregate/TypedAverage/typecreator1/1/apply(scala.reflect.api.Mirror)#scala/collection/immutable/List$/apply(scala.collection.Seq)
org/apache/spark/sql/DataFrameWriter/mode(java.lang.String)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/DataFrameWriter/mode(java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/DataFrameWriter/mode(java.lang.String)#java/lang/String/toLowerCase(java.util.Locale)
org/apache/spark/sql/DataFrameWriter/mode(java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/DataFrameWriter/mode(java.lang.String)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/DataFrameWriter/mode(java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/DataFrameWriter/mode(java.lang.String)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/DataFrameWriter/mode(java.lang.String)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/DataFrameWriter/mode(java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/DataFrameWriter/mode(java.lang.String)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/DataFrameWriter/saveAsTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/refreshTable(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/DataFrameWriter/saveAsTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/DataFrameWriter$$anonfun$1/1(org.apache.spark.sql.DataFrameWriter)
org/apache/spark/sql/DataFrameWriter/saveAsTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/DataFrameWriter/saveAsTable(org.apache.spark.sql.catalyst.TableIdentifier)#scala/Tuple2/_1$mcZ$sp()
org/apache/spark/sql/DataFrameWriter/saveAsTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/TableIdentifier/copy(java.lang.String,scala.Option)
org/apache/spark/sql/DataFrameWriter/saveAsTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/catalog/CatalogRelation/tableMeta()
org/apache/spark/sql/DataFrameWriter/saveAsTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/DataFrameWriter/saveAsTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/TableIdentifier/database()
org/apache/spark/sql/DataFrameWriter/saveAsTable(org.apache.spark.sql.catalyst.TableIdentifier)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/DataFrameWriter/saveAsTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()
org/apache/spark/sql/DataFrameWriter/saveAsTable(org.apache.spark.sql.catalyst.TableIdentifier)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/DataFrameWriter/saveAsTable(org.apache.spark.sql.catalyst.TableIdentifier)#scala/collection/Seq/contains(java.lang.Object)
org/apache/spark/sql/DataFrameWriter/saveAsTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/tableExists(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/DataFrameWriter/saveAsTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/execution/command/DDLUtils$/isHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)
org/apache/spark/sql/DataFrameWriter/saveAsTable(org.apache.spark.sql.catalyst.TableIdentifier)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/DataFrameWriter/saveAsTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/collect(scala.PartialFunction)
org/apache/spark/sql/DataFrameWriter/saveAsTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/analysis/EliminateSubqueryAliases$/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/DataFrameWriter/saveAsTable(org.apache.spark.sql.catalyst.TableIdentifier)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/DataFrameWriter/saveAsTable(org.apache.spark.sql.catalyst.TableIdentifier)#scala/Tuple2/_2()
org/apache/spark/sql/DataFrameWriter/saveAsTable(org.apache.spark.sql.catalyst.TableIdentifier)#scala/runtime/BoxesRunTime/boxToBoolean(boolean)
org/apache/spark/sql/DataFrameWriter/saveAsTable(org.apache.spark.sql.catalyst.TableIdentifier)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/DataFrameWriter/saveAsTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/TableIdentifier/unquotedString()
org/apache/spark/sql/DataFrameWriter/saveAsTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/DataFrameWriter/saveAsTable(org.apache.spark.sql.catalyst.TableIdentifier)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/DataFrameWriter/saveAsTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/dropTable(org.apache.spark.sql.catalyst.TableIdentifier,boolean,boolean)
org/apache/spark/sql/DataFrameWriter/saveAsTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/DataFrameWriter/saveAsTable(org.apache.spark.sql.catalyst.TableIdentifier)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/DataFrameWriter/saveAsTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/DataFrameWriter$$anonfun$2/2(org.apache.spark.sql.DataFrameWriter,org.apache.spark.sql.catalyst.catalog.SessionCatalog)
org/apache/spark/sql/DataFrameWriter/saveAsTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/DataFrameWriter/saveAsTable(org.apache.spark.sql.catalyst.TableIdentifier)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/DataFrameWriter/saveAsTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/TableIdentifier/copy$default$1()
org/apache/spark/sql/DataFrameWriter/insertInto(org.apache.spark.sql.catalyst.TableIdentifier)#scala/Predef$/Map()
org/apache/spark/sql/DataFrameWriter/insertInto(org.apache.spark.sql.catalyst.TableIdentifier)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/DataFrameWriter/insertInto(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/InsertIntoTable(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.immutable.Map,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,boolean,boolean)
org/apache/spark/sql/DataFrameWriter/insertInto(org.apache.spark.sql.catalyst.TableIdentifier)#scala/Option/isDefined()
org/apache/spark/sql/DataFrameWriter/insertInto(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/DataFrameWriter/insertInto(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/DataFrameWriter/insertInto(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/DataFrameWriter/insertInto(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/analysis/UnresolvedRelation/UnresolvedRelation(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/DataFrameWriter/insertInto(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/DataFrameWriter/insertInto(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/DataFrameWriter/insertInto(org.apache.spark.sql.catalyst.TableIdentifier)#scala/collection/immutable/Map$/empty()
org/apache/spark/sql/DataFrameWriter/assertNotBucketed(java.lang.String)#scala/Option/isDefined()
org/apache/spark/sql/DataFrameWriter/assertNotBucketed(java.lang.String)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/DataFrameWriter/assertNotBucketed(java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/DataFrameWriter/assertNotBucketed(java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/DataFrameWriter/assertNotBucketed(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/DataFrameWriter/assertNotBucketed(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/DataFrameWriter/assertNotBucketed(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/DataFrameWriter/assertNotBucketed(java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/DataFrameWriter/assertNotBucketed(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/DataFrameWriter/assertNotBucketed(java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/DataFrameWriter/createTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/catalog/CatalogTableType$/EXTERNAL()
org/apache/spark/sql/DataFrameWriter/createTable(org.apache.spark.sql.catalyst.TableIdentifier)#scala/Option/isDefined()
org/apache/spark/sql/DataFrameWriter/createTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/DataFrameWriter$$anonfun$3/3(org.apache.spark.sql.DataFrameWriter)
org/apache/spark/sql/DataFrameWriter/createTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$15()
org/apache/spark/sql/DataFrameWriter/createTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$17()
org/apache/spark/sql/DataFrameWriter/createTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/execution/datasources/DataSource$/buildStorageFormatFromOptions(scala.collection.immutable.Map)
org/apache/spark/sql/DataFrameWriter/createTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/types/StructType/StructType()
org/apache/spark/sql/DataFrameWriter/createTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$10()
org/apache/spark/sql/DataFrameWriter/createTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$11()
org/apache/spark/sql/DataFrameWriter/createTable(org.apache.spark.sql.catalyst.TableIdentifier)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/DataFrameWriter/createTable(org.apache.spark.sql.catalyst.TableIdentifier)#scala/Predef$/$conforms()
org/apache/spark/sql/DataFrameWriter/createTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$12()
org/apache/spark/sql/DataFrameWriter/createTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$8()
org/apache/spark/sql/DataFrameWriter/createTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$13()
org/apache/spark/sql/DataFrameWriter/createTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$9()
org/apache/spark/sql/DataFrameWriter/createTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$14()
org/apache/spark/sql/DataFrameWriter/createTable(org.apache.spark.sql.catalyst.TableIdentifier)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/DataFrameWriter/createTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$16()
org/apache/spark/sql/DataFrameWriter/createTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/catalog/CatalogTable/CatalogTable(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)
org/apache/spark/sql/DataFrameWriter/createTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/locationUri()
org/apache/spark/sql/DataFrameWriter/createTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/catalog/CatalogTableType$/MANAGED()
org/apache/spark/sql/DataFrameWriter/createTable(org.apache.spark.sql.catalyst.TableIdentifier)#scala/collection/mutable/HashMap/toMap(scala.Predef$$less$colon$less)
org/apache/spark/sql/DataFrameWriter/assertNotPartitioned(java.lang.String)#scala/Option/isDefined()
org/apache/spark/sql/DataFrameWriter/assertNotPartitioned(java.lang.String)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/DataFrameWriter/assertNotPartitioned(java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/DataFrameWriter/assertNotPartitioned(java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/DataFrameWriter/assertNotPartitioned(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/DataFrameWriter/assertNotPartitioned(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/DataFrameWriter/assertNotPartitioned(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/DataFrameWriter/assertNotPartitioned(java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/DataFrameWriter/assertNotPartitioned(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/DataFrameWriter/assertNotPartitioned(java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/DataFrameWriter/save()#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/DataFrameWriter/save()#org/apache/spark/sql/execution/command/DDLUtils$/HIVE_PROVIDER()
org/apache/spark/sql/DataFrameWriter/save()#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/DataFrameWriter/save()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/DataFrameWriter/save()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/DataFrameWriter/save()#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/DataFrameWriter/save()#org/apache/spark/sql/DataFrameWriter$$anonfun$save$1/1(org.apache.spark.sql.DataFrameWriter)
org/apache/spark/sql/DataFrameWriter/save()#java/lang/String/toLowerCase(java.util.Locale)
org/apache/spark/sql/DataFrameWriter/save()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/DataFrameWriter/save()#scala/Predef$/$conforms()
org/apache/spark/sql/DataFrameWriter/save()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/DataFrameWriter/save()#scala/collection/mutable/HashMap/toMap(scala.Predef$$less$colon$less)
org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/alterTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)
org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/properties()
org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/empty()
org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq/foreach(scala.Function1)
org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$1()
org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$2()
org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$17()
org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand$$anonfun$run$4/4(org.apache.spark.sql.execution.command.AlterTableUnsetPropertiesCommand,org.apache.spark.sql.catalyst.catalog.CatalogTable)
org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/DDLUtils$/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)
org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$3()
org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$10()
org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$4()
org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand$$anonfun$1/1(org.apache.spark.sql.execution.command.AlterTableUnsetPropertiesCommand)
org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$5()
org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$12()
org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)
org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$6()
org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$13()
org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()
org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$7()
org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/immutable/Map/filter(scala.Function1)
org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$15()
org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$8()
org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$16()
org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$9()
org/apache/spark/sql/execution/ui/ExecutionPage/planVisualizationResources()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/ui/ExecutionPage/planVisualizationResources()#scala/xml/Text/Text(java.lang.String)
org/apache/spark/sql/execution/ui/ExecutionPage/planVisualizationResources()#scala/xml/NodeBuffer/$amp$plus(java.lang.Object)
org/apache/spark/sql/execution/ui/ExecutionPage/planVisualizationResources()#scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,scala.collection.Seq,scala.xml.MetaData)
org/apache/spark/sql/execution/ui/ExecutionPage/planVisualizationResources()#org/apache/spark/ui/UIUtils$/prependBaseUri(java.lang.String,java.lang.String)
org/apache/spark/sql/execution/ui/ExecutionPage/planVisualizationResources()#scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,java.lang.String,scala.xml.MetaData)
org/apache/spark/sql/execution/ui/ExecutionPage/planVisualizationResources()#org/apache/spark/ui/UIUtils$/prependBaseUri$default$2()
org/apache/spark/sql/execution/ui/ExecutionPage/planVisualizationResources()#scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)
org/apache/spark/sql/execution/ui/ExecutionPage/planVisualizationResources()#scala/xml/NodeBuffer/NodeBuffer()
org/apache/spark/sql/execution/ui/ExecutionPage/org$apache$spark$sql$execution$ui$ExecutionPage$$planVisualization(scala.collection.immutable.Map,org.apache.spark.sql.execution.ui.SparkPlanGraph)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/ui/ExecutionPage/org$apache$spark$sql$execution$ui$ExecutionPage$$planVisualization(scala.collection.immutable.Map,org.apache.spark.sql.execution.ui.SparkPlanGraph)#scala/xml/Text/Text(java.lang.String)
org/apache/spark/sql/execution/ui/ExecutionPage/org$apache$spark$sql$execution$ui$ExecutionPage$$planVisualization(scala.collection.immutable.Map,org.apache.spark.sql.execution.ui.SparkPlanGraph)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/ui/ExecutionPage/org$apache$spark$sql$execution$ui$ExecutionPage$$planVisualization(scala.collection.immutable.Map,org.apache.spark.sql.execution.ui.SparkPlanGraph)#scala/xml/NodeBuffer/$amp$plus(java.lang.Object)
org/apache/spark/sql/execution/ui/ExecutionPage/org$apache$spark$sql$execution$ui$ExecutionPage$$planVisualization(scala.collection.immutable.Map,org.apache.spark.sql.execution.ui.SparkPlanGraph)#scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,scala.collection.Seq,scala.xml.MetaData)
org/apache/spark/sql/execution/ui/ExecutionPage/org$apache$spark$sql$execution$ui$ExecutionPage$$planVisualization(scala.collection.immutable.Map,org.apache.spark.sql.execution.ui.SparkPlanGraph)#java/lang/Object/toString()
org/apache/spark/sql/execution/ui/ExecutionPage/org$apache$spark$sql$execution$ui$ExecutionPage$$planVisualization(scala.collection.immutable.Map,org.apache.spark.sql.execution.ui.SparkPlanGraph)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/ui/ExecutionPage/org$apache$spark$sql$execution$ui$ExecutionPage$$planVisualization(scala.collection.immutable.Map,org.apache.spark.sql.execution.ui.SparkPlanGraph)#org/apache/spark/sql/execution/ui/ExecutionPage$$anonfun$7/7(org.apache.spark.sql.execution.ui.ExecutionPage)
org/apache/spark/sql/execution/ui/ExecutionPage/org$apache$spark$sql$execution$ui$ExecutionPage$$planVisualization(scala.collection.immutable.Map,org.apache.spark.sql.execution.ui.SparkPlanGraph)#scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)
org/apache/spark/sql/execution/ui/ExecutionPage/org$apache$spark$sql$execution$ui$ExecutionPage$$planVisualization(scala.collection.immutable.Map,org.apache.spark.sql.execution.ui.SparkPlanGraph)#scala/xml/NodeBuffer/NodeBuffer()
org/apache/spark/sql/execution/ui/ExecutionPage/org$apache$spark$sql$execution$ui$ExecutionPage$$planVisualization(scala.collection.immutable.Map,org.apache.spark.sql.execution.ui.SparkPlanGraph)#scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/ui/ExecutionPage/org$apache$spark$sql$execution$ui$ExecutionPage$$planVisualization(scala.collection.immutable.Map,org.apache.spark.sql.execution.ui.SparkPlanGraph)#scala/collection/Seq/size()
org/apache/spark/sql/execution/ui/ExecutionPage/render(javax.servlet.http.HttpServletRequest)#javax/servlet/http/HttpServletRequest/getParameter(java.lang.String)
org/apache/spark/sql/execution/ui/ExecutionPage/render(javax.servlet.http.HttpServletRequest)#org/apache/spark/sql/execution/ui/ExecutionPage$$anonfun$render$1/1(org.apache.spark.sql.execution.ui.ExecutionPage)
org/apache/spark/sql/execution/ui/ExecutionPage/render(javax.servlet.http.HttpServletRequest)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/ui/ExecutionPage/render(javax.servlet.http.HttpServletRequest)#org/apache/spark/ui/UIUtils$/stripXSS(java.lang.String)
org/apache/spark/sql/execution/ui/ExecutionPage/render(javax.servlet.http.HttpServletRequest)#scala/Predef$/require(boolean,scala.Function0)
org/apache/spark/sql/execution/ui/ExecutionPage/render(javax.servlet.http.HttpServletRequest)#org/apache/spark/sql/execution/ui/ExecutionPage$$anonfun$render$2/2(org.apache.spark.sql.execution.ui.ExecutionPage,scala.xml.NodeSeq)
org/apache/spark/sql/execution/ui/ExecutionPage/render(javax.servlet.http.HttpServletRequest)#org/apache/spark/ui/UIUtils$/headerSparkPage$default$5()
org/apache/spark/sql/execution/ui/ExecutionPage/render(javax.servlet.http.HttpServletRequest)#org/apache/spark/ui/UIUtils$/headerSparkPage$default$7()
org/apache/spark/sql/execution/ui/ExecutionPage/render(javax.servlet.http.HttpServletRequest)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/ui/ExecutionPage/render(javax.servlet.http.HttpServletRequest)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/ui/ExecutionPage/render(javax.servlet.http.HttpServletRequest)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/ui/ExecutionPage/render(javax.servlet.http.HttpServletRequest)#scala/collection/immutable/StringOps/nonEmpty()
org/apache/spark/sql/execution/ui/ExecutionPage/render(javax.servlet.http.HttpServletRequest)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/ui/ExecutionPage/render(javax.servlet.http.HttpServletRequest)#org/apache/spark/sql/execution/ui/ExecutionPage$$anonfun$6/6(org.apache.spark.sql.execution.ui.ExecutionPage,long)
org/apache/spark/sql/execution/ui/ExecutionPage/render(javax.servlet.http.HttpServletRequest)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/ui/ExecutionPage/render(javax.servlet.http.HttpServletRequest)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/ui/ExecutionPage/render(javax.servlet.http.HttpServletRequest)#org/apache/spark/ui/UIUtils$/headerSparkPage(java.lang.String,scala.Function0,org.apache.spark.ui.SparkUITab,scala.Option,scala.Option,boolean,boolean)
org/apache/spark/sql/execution/ui/ExecutionPage/render(javax.servlet.http.HttpServletRequest)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/ui/ExecutionPage/render(javax.servlet.http.HttpServletRequest)#org/apache/spark/ui/UIUtils$/headerSparkPage$default$6()
org/apache/spark/sql/execution/ui/ExecutionPage/render(javax.servlet.http.HttpServletRequest)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/ui/ExecutionPage/render(javax.servlet.http.HttpServletRequest)#scala/collection/immutable/StringOps/toLong()
org/apache/spark/sql/execution/ui/ExecutionPage/render(javax.servlet.http.HttpServletRequest)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/ui/ExecutionPage/render(javax.servlet.http.HttpServletRequest)#org/apache/spark/sql/execution/ui/ExecutionPage$$anonfun$2/2(org.apache.spark.sql.execution.ui.ExecutionPage,long)
org/apache/spark/sql/execution/ui/ExecutionPage/render(javax.servlet.http.HttpServletRequest)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/scanPartitions/1/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/FileStatus/getPath()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/scanPartitions/1/apply(org.apache.hadoop.fs.FileStatus)#scala/Predef$/ArrowAssoc(java.lang.Object)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/scanPartitions/1/apply(org.apache.hadoop.fs.FileStatus)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/scanPartitions/1/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/FileStatus/isDirectory()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/scanPartitions/1/apply(org.apache.hadoop.fs.FileStatus)#scala/Function2/apply(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/scanPartitions/1/apply(org.apache.hadoop.fs.FileStatus)#org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/logWarning(scala.Function0)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/scanPartitions/1/apply(org.apache.hadoop.fs.FileStatus)#org/apache/spark/sql/catalyst/catalog/ExternalCatalogUtils$/unescapePathName(java.lang.String)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/scanPartitions/1/apply(org.apache.hadoop.fs.FileStatus)#java/lang/String/contains(java.lang.CharSequence)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/scanPartitions/1/apply(org.apache.hadoop.fs.FileStatus)#scala/collection/immutable/Map/$plus$plus(scala.collection.GenTraversableOnce)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/scanPartitions/1/apply(org.apache.hadoop.fs.FileStatus)#org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand$$anonfun$org$apache$spark$sql$execution$command$AlterTableRecoverPartitionsCommand$$scanPartitions$1$$anonfun$apply$3/3(org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsCommand$$anonfun$org$apache$spark$sql$execution$command$AlterTableRecoverPartitionsCommand$$scanPartitions$1,java.lang.String)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/scanPartitions/1/apply(org.apache.hadoop.fs.FileStatus)#org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand$$anonfun$org$apache$spark$sql$execution$command$AlterTableRecoverPartitionsCommand$$scanPartitions$1$$anonfun$apply$2/2(org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsCommand$$anonfun$org$apache$spark$sql$execution$command$AlterTableRecoverPartitionsCommand$$scanPartitions$1,java.lang.String[])
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/scanPartitions/1/apply(org.apache.hadoop.fs.FileStatus)#scala/collection/immutable/Map$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/scanPartitions/1/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/Path/getName()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/scanPartitions/1/apply(org.apache.hadoop.fs.FileStatus)#scala/Predef$/Map()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/scanPartitions/1/apply(org.apache.hadoop.fs.FileStatus)#java/lang/String/split(java.lang.String,int)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/scanPartitions/1/apply(org.apache.hadoop.fs.FileStatus)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/scanPartitions/1/apply(org.apache.hadoop.fs.FileStatus)#scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/scanPartitions/1/apply(org.apache.hadoop.fs.FileStatus)#scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/scanPartitions/1/apply(org.apache.hadoop.fs.FileStatus)#scala/collection/Seq/head()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/scanPartitions/1/apply(org.apache.hadoop.fs.FileStatus)#scala/collection/Seq/drop(int)
org/apache/spark/sql/SQLImplicits/typecreator8/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/selectType(scala.reflect.api.Symbols$SymbolApi,java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator8/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ModuleSymbolApi/asModule()
org/apache/spark/sql/SQLImplicits/typecreator8/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticPackage(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator8/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$InternalApi/reificationSupport()
org/apache/spark/sql/SQLImplicits/typecreator8/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/RootClass()
org/apache/spark/sql/SQLImplicits/typecreator8/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticModule(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator8/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ClassSymbolApi/asType()
org/apache/spark/sql/SQLImplicits/typecreator8/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ModuleSymbolApi/moduleClass()
org/apache/spark/sql/SQLImplicits/typecreator8/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/thisPrefix(scala.reflect.api.Symbols$SymbolApi)
org/apache/spark/sql/SQLImplicits/typecreator8/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/TypeRef(scala.reflect.api.Types$TypeApi,scala.reflect.api.Symbols$SymbolApi,scala.collection.immutable.List)
org/apache/spark/sql/SQLImplicits/typecreator8/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticClass(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator8/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/universe()
org/apache/spark/sql/SQLImplicits/typecreator8/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$TypeSymbolApi/toTypeConstructor()
org/apache/spark/sql/SQLImplicits/typecreator8/1/apply(scala.reflect.api.Mirror)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/SQLImplicits/typecreator8/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/SingleType(scala.reflect.api.Types$TypeApi,scala.reflect.api.Symbols$SymbolApi)
org/apache/spark/sql/SQLImplicits/typecreator8/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Universe/internal()
org/apache/spark/sql/SQLImplicits/typecreator8/1/apply(scala.reflect.api.Mirror)#scala/collection/immutable/List$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/anonfun/6/anonfun/apply/3/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#org/apache/spark/sql/catalyst/expressions/Cast/Cast(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.types.DataType,scala.Option)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/anonfun/6/anonfun/apply/3/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#scala/collection/immutable/Map/apply(java.lang.Object)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/anonfun/6/anonfun/apply/3/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/anonfun/6/anonfun/apply/3/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#org/apache/spark/sql/catalyst/expressions/Cast/eval(org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/anonfun/6/anonfun/apply/3/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#org/apache/spark/sql/catalyst/expressions/Cast/eval$default$1()
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/anonfun/6/anonfun/apply/3/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#org/apache/spark/sql/catalyst/expressions/Attribute/name()
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/anonfun/6/anonfun/apply/3/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#org/apache/spark/sql/catalyst/expressions/Literal$/apply(java.lang.Object)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/anonfun/6/anonfun/apply/3/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#org/apache/spark/sql/catalyst/catalog/CatalogTablePartition/spec()
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/anonfun/6/anonfun/apply/3/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#org/apache/spark/sql/catalyst/expressions/Attribute/dataType()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitFailNativeCommand/1/apply()#scala/collection/mutable/Buffer$/canBuildFrom()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitFailNativeCommand/1/apply()#org/apache/spark/sql/catalyst/parser/ParserUtils$/operationNotAllowed(java.lang.String,org.antlr.v4.runtime.ParserRuleContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitFailNativeCommand/1/apply()#scala/collection/TraversableLike/collect(scala.PartialFunction,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitFailNativeCommand/1/apply()#scala/collection/convert/Decorators$AsScala/asScala()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitFailNativeCommand/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitFailNativeCommand$1/apply()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitFailNativeCommand/1/apply()#scala/collection/TraversableOnce/mkString(java.lang.String)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitFailNativeCommand/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$FailNativeCommandContext/unsupportedHiveNativeCommands()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitFailNativeCommand/1/apply()#scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitFailNativeCommand/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitFailNativeCommand$1$$anonfun$2/2(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitFailNativeCommand$1)
org/apache/spark/sql/execution/ui/SparkPlanGraph/org$apache$spark$sql$execution$ui$SparkPlanGraph$$buildSparkPlanGraphNode(org.apache.spark.sql.execution.SparkPlanInfo,java.util.concurrent.atomic.AtomicLong,scala.collection.mutable.ArrayBuffer,scala.collection.mutable.ArrayBuffer,org.apache.spark.sql.execution.ui.SparkPlanGraphNode,org.apache.spark.sql.execution.ui.SparkPlanGraphCluster,scala.collection.mutable.HashMap)#scala/collection/mutable/ArrayBuffer$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/ui/SparkPlanGraph/org$apache$spark$sql$execution$ui$SparkPlanGraph$$buildSparkPlanGraphNode(org.apache.spark.sql.execution.SparkPlanInfo,java.util.concurrent.atomic.AtomicLong,scala.collection.mutable.ArrayBuffer,scala.collection.mutable.ArrayBuffer,org.apache.spark.sql.execution.ui.SparkPlanGraphNode,org.apache.spark.sql.execution.ui.SparkPlanGraphCluster,scala.collection.mutable.HashMap)#scala/collection/mutable/HashMap/apply(java.lang.Object)
org/apache/spark/sql/execution/ui/SparkPlanGraph/org$apache$spark$sql$execution$ui$SparkPlanGraph$$buildSparkPlanGraphNode(org.apache.spark.sql.execution.SparkPlanInfo,java.util.concurrent.atomic.AtomicLong,scala.collection.mutable.ArrayBuffer,scala.collection.mutable.ArrayBuffer,org.apache.spark.sql.execution.ui.SparkPlanGraphNode,org.apache.spark.sql.execution.ui.SparkPlanGraphCluster,scala.collection.mutable.HashMap)#scala/collection/mutable/HashMap/$plus$eq(scala.Tuple2)
org/apache/spark/sql/execution/ui/SparkPlanGraph/org$apache$spark$sql$execution$ui$SparkPlanGraph$$buildSparkPlanGraphNode(org.apache.spark.sql.execution.SparkPlanInfo,java.util.concurrent.atomic.AtomicLong,scala.collection.mutable.ArrayBuffer,scala.collection.mutable.ArrayBuffer,org.apache.spark.sql.execution.ui.SparkPlanGraphNode,org.apache.spark.sql.execution.ui.SparkPlanGraphCluster,scala.collection.mutable.HashMap)#scala/collection/Seq/head()
org/apache/spark/sql/execution/ui/SparkPlanGraph/org$apache$spark$sql$execution$ui$SparkPlanGraph$$buildSparkPlanGraphNode(org.apache.spark.sql.execution.SparkPlanInfo,java.util.concurrent.atomic.AtomicLong,scala.collection.mutable.ArrayBuffer,scala.collection.mutable.ArrayBuffer,org.apache.spark.sql.execution.ui.SparkPlanGraphNode,org.apache.spark.sql.execution.ui.SparkPlanGraphCluster,scala.collection.mutable.HashMap)#org/apache/spark/sql/execution/ui/SparkPlanGraph$$anonfun$org$apache$spark$sql$execution$ui$SparkPlanGraph$$buildSparkPlanGraphNode$1/1(java.util.concurrent.atomic.AtomicLong,scala.collection.mutable.ArrayBuffer,scala.collection.mutable.ArrayBuffer,org.apache.spark.sql.execution.ui.SparkPlanGraphCluster,scala.collection.mutable.HashMap,org.apache.spark.sql.execution.ui.SparkPlanGraphNode)
org/apache/spark/sql/execution/ui/SparkPlanGraph/org$apache$spark$sql$execution$ui$SparkPlanGraph$$buildSparkPlanGraphNode(org.apache.spark.sql.execution.SparkPlanInfo,java.util.concurrent.atomic.AtomicLong,scala.collection.mutable.ArrayBuffer,scala.collection.mutable.ArrayBuffer,org.apache.spark.sql.execution.ui.SparkPlanGraphNode,org.apache.spark.sql.execution.ui.SparkPlanGraphCluster,scala.collection.mutable.HashMap)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/ui/SparkPlanGraph/org$apache$spark$sql$execution$ui$SparkPlanGraph$$buildSparkPlanGraphNode(org.apache.spark.sql.execution.SparkPlanInfo,java.util.concurrent.atomic.AtomicLong,scala.collection.mutable.ArrayBuffer,scala.collection.mutable.ArrayBuffer,org.apache.spark.sql.execution.ui.SparkPlanGraphNode,org.apache.spark.sql.execution.ui.SparkPlanGraphCluster,scala.collection.mutable.HashMap)#scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object)
org/apache/spark/sql/execution/ui/SparkPlanGraph/org$apache$spark$sql$execution$ui$SparkPlanGraph$$buildSparkPlanGraphNode(org.apache.spark.sql.execution.SparkPlanInfo,java.util.concurrent.atomic.AtomicLong,scala.collection.mutable.ArrayBuffer,scala.collection.mutable.ArrayBuffer,org.apache.spark.sql.execution.ui.SparkPlanGraphNode,org.apache.spark.sql.execution.ui.SparkPlanGraphCluster,scala.collection.mutable.HashMap)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/ui/SparkPlanGraph/org$apache$spark$sql$execution$ui$SparkPlanGraph$$buildSparkPlanGraphNode(org.apache.spark.sql.execution.SparkPlanInfo,java.util.concurrent.atomic.AtomicLong,scala.collection.mutable.ArrayBuffer,scala.collection.mutable.ArrayBuffer,org.apache.spark.sql.execution.ui.SparkPlanGraphNode,org.apache.spark.sql.execution.ui.SparkPlanGraphCluster,scala.collection.mutable.HashMap)#org/apache/spark/sql/execution/ui/SparkPlanGraph$$anonfun$2/2()
org/apache/spark/sql/execution/ui/SparkPlanGraph/org$apache$spark$sql$execution$ui$SparkPlanGraph$$buildSparkPlanGraphNode(org.apache.spark.sql.execution.SparkPlanInfo,java.util.concurrent.atomic.AtomicLong,scala.collection.mutable.ArrayBuffer,scala.collection.mutable.ArrayBuffer,org.apache.spark.sql.execution.ui.SparkPlanGraphNode,org.apache.spark.sql.execution.ui.SparkPlanGraphCluster,scala.collection.mutable.HashMap)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/ui/SparkPlanGraph/org$apache$spark$sql$execution$ui$SparkPlanGraph$$buildSparkPlanGraphNode(org.apache.spark.sql.execution.SparkPlanInfo,java.util.concurrent.atomic.AtomicLong,scala.collection.mutable.ArrayBuffer,scala.collection.mutable.ArrayBuffer,org.apache.spark.sql.execution.ui.SparkPlanGraphNode,org.apache.spark.sql.execution.ui.SparkPlanGraphCluster,scala.collection.mutable.HashMap)#org/apache/spark/sql/execution/ui/SparkPlanGraph$$anonfun$3/3()
org/apache/spark/sql/execution/ui/SparkPlanGraph/org$apache$spark$sql$execution$ui$SparkPlanGraph$$buildSparkPlanGraphNode(org.apache.spark.sql.execution.SparkPlanInfo,java.util.concurrent.atomic.AtomicLong,scala.collection.mutable.ArrayBuffer,scala.collection.mutable.ArrayBuffer,org.apache.spark.sql.execution.ui.SparkPlanGraphNode,org.apache.spark.sql.execution.ui.SparkPlanGraphCluster,scala.collection.mutable.HashMap)#scala/Predef$/ArrowAssoc(java.lang.Object)
org/apache/spark/sql/execution/ui/SparkPlanGraph/org$apache$spark$sql$execution$ui$SparkPlanGraph$$buildSparkPlanGraphNode(org.apache.spark.sql.execution.SparkPlanInfo,java.util.concurrent.atomic.AtomicLong,scala.collection.mutable.ArrayBuffer,scala.collection.mutable.ArrayBuffer,org.apache.spark.sql.execution.ui.SparkPlanGraphNode,org.apache.spark.sql.execution.ui.SparkPlanGraphCluster,scala.collection.mutable.HashMap)#scala/collection/Seq/foreach(scala.Function1)
org/apache/spark/sql/execution/ui/SparkPlanGraph/org$apache$spark$sql$execution$ui$SparkPlanGraph$$buildSparkPlanGraphNode(org.apache.spark.sql.execution.SparkPlanInfo,java.util.concurrent.atomic.AtomicLong,scala.collection.mutable.ArrayBuffer,scala.collection.mutable.ArrayBuffer,org.apache.spark.sql.execution.ui.SparkPlanGraphNode,org.apache.spark.sql.execution.ui.SparkPlanGraphCluster,scala.collection.mutable.HashMap)#java/util/concurrent/atomic/AtomicLong/getAndIncrement()
org/apache/spark/sql/execution/ui/SparkPlanGraph/org$apache$spark$sql$execution$ui$SparkPlanGraph$$buildSparkPlanGraphNode(org.apache.spark.sql.execution.SparkPlanInfo,java.util.concurrent.atomic.AtomicLong,scala.collection.mutable.ArrayBuffer,scala.collection.mutable.ArrayBuffer,org.apache.spark.sql.execution.ui.SparkPlanGraphNode,org.apache.spark.sql.execution.ui.SparkPlanGraphCluster,scala.collection.mutable.HashMap)#scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/ui/SparkPlanGraph/org$apache$spark$sql$execution$ui$SparkPlanGraph$$buildSparkPlanGraphNode(org.apache.spark.sql.execution.SparkPlanInfo,java.util.concurrent.atomic.AtomicLong,scala.collection.mutable.ArrayBuffer,scala.collection.mutable.ArrayBuffer,org.apache.spark.sql.execution.ui.SparkPlanGraphNode,org.apache.spark.sql.execution.ui.SparkPlanGraphCluster,scala.collection.mutable.HashMap)#scala/collection/mutable/HashMap/contains(java.lang.Object)
org/apache/spark/sql/execution/ui/SparkPlanGraph/org$apache$spark$sql$execution$ui$SparkPlanGraph$$buildSparkPlanGraphNode(org.apache.spark.sql.execution.SparkPlanInfo,java.util.concurrent.atomic.AtomicLong,scala.collection.mutable.ArrayBuffer,scala.collection.mutable.ArrayBuffer,org.apache.spark.sql.execution.ui.SparkPlanGraphNode,org.apache.spark.sql.execution.ui.SparkPlanGraphCluster,scala.collection.mutable.HashMap)#java/lang/String/contains(java.lang.CharSequence)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#org/apache/spark/sql/execution/stat/StatFunctions$$anonfun$6/6()
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#org/apache/spark/sql/execution/stat/StatFunctions$$anonfun$7/7()
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/collection/mutable/ArrayOps/groupBy(scala.Function1)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/collection/immutable/Map/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#org/apache/spark/sql/execution/stat/StatFunctions$$anonfun$crossTabulate$1/1()
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#org/apache/spark/sql/execution/stat/StatFunctions$$anonfun$8/8(scala.collection.immutable.Map,int)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/collection/mutable/ArrayOps/distinct()
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#org/apache/spark/sql/types/StructField$/apply$default$3()
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/collection/immutable/Map/size()
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/collection/Seq/sortBy(scala.Function1,scala.math.Ordering)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#org/apache/spark/sql/types/StructField$/apply$default$4()
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#org/apache/spark/sql/execution/stat/StatFunctions$$anonfun$9/9()
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#org/apache/spark/sql/functions$/count(java.lang.String)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#org/apache/spark/sql/types/StructField/StructField(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.types.Metadata)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#org/apache/spark/sql/execution/stat/StatFunctions$/logWarning(scala.Function0)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/collection/Seq/$plus$colon(java.lang.Object,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#org/apache/spark/sql/types/StructType/toAttributes()
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/Predef$/require(boolean,scala.Function0)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/collection/mutable/ArrayOps/toMap(scala.Predef$$less$colon$less)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#org/apache/spark/sql/execution/stat/StatFunctions$$anonfun$crossTabulate$2/2(java.lang.String,int)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#org/apache/spark/sql/Dataset$/ofRows(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#org/apache/spark/sql/execution/stat/StatFunctions$$anonfun$10/10()
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/collection/mutable/ArrayOps/zipWithIndex(scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/collection/TraversableOnce/toSeq()
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/collection/immutable/Map/toSeq()
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/collection/immutable/Iterable$/canBuildFrom()
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/Predef$/$conforms()
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#org/apache/spark/sql/execution/stat/StatFunctions$/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/collection/mutable/ArrayOps/sorted(scala.math.Ordering)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#org/apache/spark/sql/types/StructType$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#org/apache/spark/sql/catalyst/plans/logical/LocalRelation/LocalRelation(scala.collection.Seq,scala.collection.Seq)
org/apache/spark/sql/execution/stat/StatFunctions/crossTabulate(org.apache.spark.sql.Dataset,java.lang.String,java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/stat/StatFunctions/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/stat/StatFunctions/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/stat/StatFunctions/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)#scala/Predef$DummyImplicit$/dummyImplicit()
org/apache/spark/sql/execution/stat/StatFunctions/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)#org/apache/spark/sql/execution/stat/StatFunctions$/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)
org/apache/spark/sql/execution/stat/StatFunctions/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)#scala/Array$/fallbackCanBuildFrom(scala.Predef$DummyImplicit)
org/apache/spark/sql/execution/stat/StatFunctions/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)#scala/Predef$/require(boolean,scala.Function0)
org/apache/spark/sql/execution/stat/StatFunctions/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)#scala/collection/Seq/size()
org/apache/spark/sql/execution/stat/StatFunctions/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)#org/apache/spark/sql/execution/stat/StatFunctions$$anonfun$3/3()
org/apache/spark/sql/execution/stat/StatFunctions/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/stat/StatFunctions/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)#scala/Array$/fill(int,scala.Function0,scala.reflect.ClassTag)
org/apache/spark/sql/execution/stat/StatFunctions/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)#org/apache/spark/sql/execution/stat/StatFunctions$$anonfun$1/1(org.apache.spark.sql.Dataset)
org/apache/spark/sql/execution/stat/StatFunctions/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)#org/apache/spark/rdd/RDD/aggregate(java.lang.Object,scala.Function2,scala.Function2,scala.reflect.ClassTag)
org/apache/spark/sql/execution/stat/StatFunctions/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/stat/StatFunctions/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)#org/apache/spark/sql/execution/stat/StatFunctions$$anonfun$multipleApproxQuantiles$2/2(scala.collection.Seq)
org/apache/spark/sql/execution/stat/StatFunctions/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)#org/apache/spark/sql/execution/stat/StatFunctions$$anonfun$2/2(double)
org/apache/spark/sql/execution/stat/StatFunctions/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)#scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)
org/apache/spark/sql/execution/stat/StatFunctions/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)#org/apache/spark/sql/execution/stat/StatFunctions$$anonfun$multipleApproxQuantiles$1/1(double)
org/apache/spark/sql/execution/stat/StatFunctions/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)#org/apache/spark/sql/execution/stat/StatFunctions$$anonfun$4/4()
org/apache/spark/sql/execution/stat/StatFunctions/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/streaming/FileStreamSource/anonfun/getBatch/2/apply()#org/apache/spark/sql/execution/streaming/FileStreamSource$$anonfun$getBatch$2/apply()
org/apache/spark/sql/execution/streaming/FileStreamSource/anonfun/getBatch/2/apply()#scala/collection/mutable/ArrayOps/mkString(java.lang.String)
org/apache/spark/sql/execution/streaming/FileStreamSource/anonfun/getBatch/2/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/FileStreamSource/anonfun/getBatch/2/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/FileStreamSource/anonfun/getBatch/2/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/streaming/FileStreamSource/anonfun/getBatch/2/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/streaming/FileStreamSource/anonfun/getBatch/2/apply()#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/streaming/FileStreamSource/anonfun/getBatch/2/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/FileStreamSource/anonfun/getBatch/2/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/streaming/FileStreamSinkLog/anonfun/1/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/FileStreamSinkLog/anonfun/1/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/FileStreamSinkLog/anonfun/1/apply()#org/apache/spark/internal/config/ConfigEntry/key()
org/apache/spark/sql/execution/streaming/FileStreamSinkLog/anonfun/1/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/streaming/FileStreamSinkLog/anonfun/1/apply()#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/streaming/FileStreamSinkLog/anonfun/1/apply()#org/apache/spark/sql/execution/streaming/FileStreamSinkLog$$anonfun$1/apply()
org/apache/spark/sql/execution/streaming/FileStreamSinkLog/anonfun/1/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/streaming/FileStreamSinkLog/anonfun/1/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/FileStreamSinkLog/anonfun/1/apply()#org/apache/spark/sql/internal/SQLConf$/FILE_SINK_LOG_COMPACT_INTERVAL()
org/apache/spark/sql/execution/streaming/FileStreamSinkLog/anonfun/1/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/FileStreamSinkLog/anonfun/1/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/expressions/ExpressionSet/nonEmpty()
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Tuple3/_2()
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Option/isEmpty()
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/plans/logical/Filter/Filter(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Option/get()
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/analysis/Analyzer/resolver()
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions$$anonfun$apply$1$$anonfun$3/3(org.apache.spark.sql.execution.datasources.PruneFileSourcePartitions$$anonfun$apply$1,org.apache.spark.sql.execution.datasources.InMemoryFileIndex)
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/plans/logical/Project/Project(scala.collection.Seq,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Option/isDefined()
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/Seq/filter(scala.Function1)
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Tuple3/_1()
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Tuple3/_3()
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions$$anonfun$apply$1$$anonfun$1/1(org.apache.spark.sql.execution.datasources.PruneFileSourcePartitions$$anonfun$apply$1,org.apache.spark.sql.execution.datasources.LogicalRelation)
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions$$anonfun$apply$1$$anonfun$2/2(org.apache.spark.sql.execution.datasources.PruneFileSourcePartitions$$anonfun$apply$1,org.apache.spark.sql.catalyst.expressions.AttributeSet)
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/expressions/ExpressionSet$/apply(scala.collection.TraversableOnce)
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/planning/PhysicalOperation$/unapply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/Seq/reduceLeft(scala.Function2)
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/expressions/AttributeSet$/apply(scala.collection.Iterable)
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/expressions/ExpressionSet/toSeq()
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/datasources/LogicalRelation/resolve(org.apache.spark.sql.types.StructType,scala.Function2)
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$withCorruptField(org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.catalyst.util.ParseMode)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$withCorruptField(org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.catalyst.util.ParseMode)#scala/collection/mutable/ArrayOps/$plus$colon(java.lang.Object,scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$withCorruptField(org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.catalyst.util.ParseMode)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$withCorruptField(org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.catalyst.util.ParseMode)#java/util/Arrays/sort(java.lang.Object[],java.util.Comparator)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$withCorruptField(org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.catalyst.util.ParseMode)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$withCorruptField(org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.catalyst.util.ParseMode)#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$withCorruptField(org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.catalyst.util.ParseMode)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$withCorruptField(org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.catalyst.util.ParseMode)#org/apache/spark/sql/types/DataType/catalogString()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$withCorruptField(org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.catalyst.util.ParseMode)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$withCorruptField(org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.catalyst.util.ParseMode)#org/apache/spark/sql/types/StructType/StructType(org.apache.spark.sql.types.StructField[])
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$withCorruptField(org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.catalyst.util.ParseMode)#org/apache/spark/sql/types/StructField$/apply$default$4()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$withCorruptField(org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.catalyst.util.ParseMode)#org/apache/spark/sql/types/StructType/fieldNames()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$withCorruptField(org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.catalyst.util.ParseMode)#org/apache/spark/sql/types/StructField/StructField(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.types.Metadata)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$withCorruptField(org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.catalyst.util.ParseMode)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$withCorruptField(org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.catalyst.util.ParseMode)#scala/collection/mutable/ArrayOps/contains(java.lang.Object)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$withCorruptField(org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.catalyst.util.ParseMode)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$withCorruptField(org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.catalyst.util.ParseMode)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$withCorruptField(org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.catalyst.util.ParseMode)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$withCorruptField(org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.catalyst.util.ParseMode)#java/lang/RuntimeException/RuntimeException(java.lang.String)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$withCorruptField(org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.catalyst.util.ParseMode)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$withCorruptField(org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.catalyst.util.ParseMode)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/infer(org.apache.spark.rdd.RDD,org.apache.spark.sql.catalyst.json.JSONOptions,scala.Function2)#org/apache/spark/rdd/RDD/mapPartitions(scala.Function1,boolean,scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/infer(org.apache.spark.rdd.RDD,org.apache.spark.sql.catalyst.json.JSONOptions,scala.Function2)#org/apache/spark/rdd/RDD/fold(java.lang.Object,scala.Function2)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/infer(org.apache.spark.rdd.RDD,org.apache.spark.sql.catalyst.json.JSONOptions,scala.Function2)#org/apache/spark/rdd/RDD/mapPartitions$default$2()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/infer(org.apache.spark.rdd.RDD,org.apache.spark.sql.catalyst.json.JSONOptions,scala.Function2)#org/apache/spark/sql/catalyst/json/JSONOptions/columnNameOfCorruptRecord()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/infer(org.apache.spark.rdd.RDD,org.apache.spark.sql.catalyst.json.JSONOptions,scala.Function2)#org/apache/spark/sql/execution/datasources/json/JsonInferSchema$$anonfun$1/1(org.apache.spark.sql.catalyst.json.JSONOptions,scala.Function2,org.apache.spark.sql.catalyst.util.ParseMode,java.lang.String)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/infer(org.apache.spark.rdd.RDD,org.apache.spark.sql.catalyst.json.JSONOptions,scala.Function2)#org/apache/spark/sql/types/StructType$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/infer(org.apache.spark.rdd.RDD,org.apache.spark.sql.catalyst.json.JSONOptions,scala.Function2)#org/apache/spark/sql/catalyst/json/JSONOptions/parseMode()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/infer(org.apache.spark.rdd.RDD,org.apache.spark.sql.catalyst.json.JSONOptions,scala.Function2)#org/apache/spark/sql/execution/datasources/json/JsonInferSchema$/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$canonicalizeType(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/infer(org.apache.spark.rdd.RDD,org.apache.spark.sql.catalyst.json.JSONOptions,scala.Function2)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/infer(org.apache.spark.rdd.RDD,org.apache.spark.sql.catalyst.json.JSONOptions,scala.Function2)#scala/Some/x()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/infer(org.apache.spark.rdd.RDD,org.apache.spark.sql.catalyst.json.JSONOptions,scala.Function2)#org/apache/spark/sql/execution/datasources/json/JsonInferSchema$/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$compatibleRootType(java.lang.String,org.apache.spark.sql.catalyst.util.ParseMode)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/infer(org.apache.spark.rdd.RDD,org.apache.spark.sql.catalyst.json.JSONOptions,scala.Function2)#org/apache/spark/sql/execution/datasources/json/JsonInferSchema$/infer(org.apache.spark.rdd.RDD,org.apache.spark.sql.catalyst.json.JSONOptions,scala.Function2)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$canonicalizeType(org.apache.spark.sql.types.DataType)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$canonicalizeType(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/json/JsonInferSchema$$anonfun$org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$canonicalizeType$1/1(org.apache.spark.sql.types.ArrayType)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$canonicalizeType(org.apache.spark.sql.types.DataType)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$canonicalizeType(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$canonicalizeType(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/ArrayType/elementType()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$canonicalizeType(org.apache.spark.sql.types.DataType)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$canonicalizeType(org.apache.spark.sql.types.DataType)#scala/collection/mutable/ArrayOps/withFilter(scala.Function1)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$canonicalizeType(org.apache.spark.sql.types.DataType)#scala/collection/generic/FilterMonadic/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$canonicalizeType(org.apache.spark.sql.types.DataType)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$canonicalizeType(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/json/JsonInferSchema$/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$canonicalizeType(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$canonicalizeType(org.apache.spark.sql.types.DataType)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$canonicalizeType(org.apache.spark.sql.types.DataType)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$canonicalizeType(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/json/JsonInferSchema$$anonfun$2/2()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$canonicalizeType(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/json/JsonInferSchema$$anonfun$3/3()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$canonicalizeType(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/StructType/StructType(org.apache.spark.sql.types.StructField[])
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$inferField(com.fasterxml.jackson.core.JsonParser,org.apache.spark.sql.catalyst.json.JSONOptions)#com/fasterxml/jackson/core/JsonParser/getTextLength()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$inferField(com.fasterxml.jackson.core.JsonParser,org.apache.spark.sql.catalyst.json.JSONOptions)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$inferField(com.fasterxml.jackson.core.JsonParser,org.apache.spark.sql.catalyst.json.JSONOptions)#org/apache/spark/sql/types/DecimalType$/MAX_PRECISION()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$inferField(com.fasterxml.jackson.core.JsonParser,org.apache.spark.sql.catalyst.json.JSONOptions)#java/util/Arrays/sort(java.lang.Object[],java.util.Comparator)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$inferField(com.fasterxml.jackson.core.JsonParser,org.apache.spark.sql.catalyst.json.JSONOptions)#com/fasterxml/jackson/core/JsonParser/getCurrentName()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$inferField(com.fasterxml.jackson.core.JsonParser,org.apache.spark.sql.catalyst.json.JSONOptions)#com/fasterxml/jackson/core/JsonParser/getNumberType()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$inferField(com.fasterxml.jackson.core.JsonParser,org.apache.spark.sql.catalyst.json.JSONOptions)#scala/collection/mutable/ArrayBuilder/result()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$inferField(com.fasterxml.jackson.core.JsonParser,org.apache.spark.sql.catalyst.json.JSONOptions)#java/math/BigDecimal/precision()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$inferField(com.fasterxml.jackson.core.JsonParser,org.apache.spark.sql.catalyst.json.JSONOptions)#scala/collection/mutable/ArrayBuilder/$plus$eq(java.lang.Object)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$inferField(com.fasterxml.jackson.core.JsonParser,org.apache.spark.sql.catalyst.json.JSONOptions)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$inferField(com.fasterxml.jackson.core.JsonParser,org.apache.spark.sql.catalyst.json.JSONOptions)#scala/Array$/newBuilder(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$inferField(com.fasterxml.jackson.core.JsonParser,org.apache.spark.sql.catalyst.json.JSONOptions)#org/apache/spark/sql/types/DecimalType/DecimalType(int,int)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$inferField(com.fasterxml.jackson.core.JsonParser,org.apache.spark.sql.catalyst.json.JSONOptions)#org/apache/spark/sql/catalyst/json/JacksonUtils$/nextUntil(com.fasterxml.jackson.core.JsonParser,com.fasterxml.jackson.core.JsonToken)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$inferField(com.fasterxml.jackson.core.JsonParser,org.apache.spark.sql.catalyst.json.JSONOptions)#org/apache/spark/sql/types/ArrayType$/apply(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$inferField(com.fasterxml.jackson.core.JsonParser,org.apache.spark.sql.catalyst.json.JSONOptions)#org/apache/spark/sql/execution/datasources/json/JsonInferSchema$/compatibleType(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$inferField(com.fasterxml.jackson.core.JsonParser,org.apache.spark.sql.catalyst.json.JSONOptions)#org/apache/spark/sql/catalyst/json/JSONOptions/prefersDecimal()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$inferField(com.fasterxml.jackson.core.JsonParser,org.apache.spark.sql.catalyst.json.JSONOptions)#com/fasterxml/jackson/core/JsonParser/nextToken()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$inferField(com.fasterxml.jackson.core.JsonParser,org.apache.spark.sql.catalyst.json.JSONOptions)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$inferField(com.fasterxml.jackson.core.JsonParser,org.apache.spark.sql.catalyst.json.JSONOptions)#java/math/BigDecimal/scale()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$inferField(com.fasterxml.jackson.core.JsonParser,org.apache.spark.sql.catalyst.json.JSONOptions)#org/apache/spark/sql/types/StructType/StructType(org.apache.spark.sql.types.StructField[])
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$inferField(com.fasterxml.jackson.core.JsonParser,org.apache.spark.sql.catalyst.json.JSONOptions)#org/apache/spark/sql/types/StructField$/apply$default$4()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$inferField(com.fasterxml.jackson.core.JsonParser,org.apache.spark.sql.catalyst.json.JSONOptions)#java/lang/Math/max(int,int)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$inferField(com.fasterxml.jackson.core.JsonParser,org.apache.spark.sql.catalyst.json.JSONOptions)#org/apache/spark/sql/types/StructField/StructField(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.types.Metadata)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$inferField(com.fasterxml.jackson.core.JsonParser,org.apache.spark.sql.catalyst.json.JSONOptions)#com/fasterxml/jackson/core/JsonParser/getCurrentToken()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$inferField(com.fasterxml.jackson.core.JsonParser,org.apache.spark.sql.catalyst.json.JSONOptions)#com/fasterxml/jackson/core/JsonParser/getDecimalValue()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$inferField(com.fasterxml.jackson.core.JsonParser,org.apache.spark.sql.catalyst.json.JSONOptions)#org/apache/spark/sql/execution/datasources/json/JsonInferSchema$/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$inferField(com.fasterxml.jackson.core.JsonParser,org.apache.spark.sql.catalyst.json.JSONOptions)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$inferField(com.fasterxml.jackson.core.JsonParser,org.apache.spark.sql.catalyst.json.JSONOptions)#org/apache/spark/sql/catalyst/json/JSONOptions/primitivesAsString()
org/apache/spark/sql/execution/CoGroupExec/anonfun/doExecute/1/anonfun/apply/5/apply(scala.Tuple3)#scala/collection/TraversableOnce$/MonadOps(scala.collection.TraversableOnce)
org/apache/spark/sql/execution/CoGroupExec/anonfun/doExecute/1/anonfun/apply/5/apply(scala.Tuple3)#scala/Tuple3/_1()
org/apache/spark/sql/execution/CoGroupExec/anonfun/doExecute/1/anonfun/apply/5/apply(scala.Tuple3)#scala/Tuple3/_2()
org/apache/spark/sql/execution/CoGroupExec/anonfun/doExecute/1/anonfun/apply/5/apply(scala.Tuple3)#org/apache/spark/sql/execution/CoGroupExec$$anonfun$doExecute$1/org$apache$spark$sql$execution$CoGroupExec$$anonfun$$$outer()
org/apache/spark/sql/execution/CoGroupExec/anonfun/doExecute/1/anonfun/apply/5/apply(scala.Tuple3)#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/execution/CoGroupExec/anonfun/doExecute/1/anonfun/apply/5/apply(scala.Tuple3)#scala/Function3/apply(java.lang.Object,java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/CoGroupExec/anonfun/doExecute/1/anonfun/apply/5/apply(scala.Tuple3)#scala/collection/TraversableOnce$MonadOps/map(scala.Function1)
org/apache/spark/sql/execution/CoGroupExec/anonfun/doExecute/1/anonfun/apply/5/apply(scala.Tuple3)#scala/Tuple3/_3()
org/apache/spark/sql/execution/CoGroupExec/anonfun/doExecute/1/anonfun/apply/5/apply(scala.Tuple3)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/CoGroupExec/anonfun/doExecute/1/anonfun/apply/5/apply(scala.Tuple3)#scala/collection/Iterator/map(scala.Function1)
org/apache/spark/sql/execution/command/AlterTableDropPartitionCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/empty()
org/apache/spark/sql/execution/command/AlterTableDropPartitionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/dropPartitions(org.apache.spark.sql.catalyst.TableIdentifier,scala.collection.Seq,boolean,boolean,boolean)
org/apache/spark/sql/execution/command/AlterTableDropPartitionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/DDLUtils$/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)
org/apache/spark/sql/execution/command/AlterTableDropPartitionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/AlterTableDropPartitionCommand$$anonfun$9/9(org.apache.spark.sql.execution.command.AlterTableDropPartitionCommand,org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable)
org/apache/spark/sql/execution/command/AlterTableDropPartitionCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/command/AlterTableDropPartitionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/AlterTableDropPartitionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/DDLUtils$/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)
org/apache/spark/sql/execution/command/AlterTableDropPartitionCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/AlterTableDropPartitionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/1/anonfun/apply/1/apply(java.lang.Object)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/1/anonfun/apply/1/apply(java.lang.Object)#org/apache/spark/sql/types/StructField/StructField(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.types.Metadata)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/1/anonfun/apply/1/apply(java.lang.Object)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/1/anonfun/apply/1/apply(java.lang.Object)#org/apache/spark/sql/execution/datasources/json/JsonInferSchema$$anonfun$1$$anonfun$apply$1$$anonfun$apply$2/2(org.apache.spark.sql.execution.datasources.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1,java.lang.Object)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/1/anonfun/apply/1/apply(java.lang.Object)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/1/anonfun/apply/1/apply(java.lang.Object)#org/apache/spark/sql/execution/datasources/json/JsonInferSchema$$anonfun$1$$anonfun$apply$1/apply(java.lang.Object)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/1/anonfun/apply/1/apply(java.lang.Object)#org/apache/spark/sql/types/StructType$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/1/anonfun/apply/1/apply(java.lang.Object)#org/apache/spark/util/Utils$/tryWithResource(scala.Function0,scala.Function1)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/1/anonfun/apply/1/apply(java.lang.Object)#org/apache/spark/sql/types/StructField$/apply$default$3()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/1/anonfun/apply/1/apply(java.lang.Object)#scala/Option$/option2Iterable(scala.Option)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/1/anonfun/apply/1/apply(java.lang.Object)#org/apache/spark/sql/execution/datasources/json/JsonInferSchema$$anonfun$1$$anonfun$apply$1$$anonfun$apply$3/3(org.apache.spark.sql.execution.datasources.json.JsonInferSchema$$anonfun$1$$anonfun$apply$1)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/1/anonfun/apply/1/apply(java.lang.Object)#org/apache/spark/sql/types/StructField$/apply$default$4()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/1/anonfun/apply/1/apply(java.lang.Object)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/1/anonfun/apply/1/apply(java.lang.Object)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/GenerateExec/anonfun/1/apply(int,scala.collection.Iterator)#org/apache/spark/sql/types/StructType/length()
org/apache/spark/sql/execution/GenerateExec/anonfun/1/apply(int,scala.collection.Iterator)#org/apache/spark/sql/execution/GenerateExec$$anonfun$1$$anonfun$3/3(org.apache.spark.sql.execution.GenerateExec$$anonfun$1,org.apache.spark.sql.catalyst.expressions.JoinedRow)
org/apache/spark/sql/execution/GenerateExec/anonfun/1/apply(int,scala.collection.Iterator)#scala/collection/Iterator/flatMap(scala.Function1)
org/apache/spark/sql/execution/GenerateExec/anonfun/1/apply(int,scala.collection.Iterator)#org/apache/spark/sql/catalyst/expressions/GenericInternalRow/GenericInternalRow(int)
org/apache/spark/sql/execution/GenerateExec/anonfun/1/apply(int,scala.collection.Iterator)#org/apache/spark/sql/catalyst/expressions/JoinedRow/JoinedRow()
org/apache/spark/sql/execution/GenerateExec/anonfun/1/apply(int,scala.collection.Iterator)#org/apache/spark/sql/catalyst/expressions/Generator/elementSchema()
org/apache/spark/sql/execution/GenerateExec/anonfun/1/apply(int,scala.collection.Iterator)#org/apache/spark/sql/catalyst/expressions/UnsafeProjection$/create(scala.collection.Seq,scala.collection.Seq)
org/apache/spark/sql/execution/GenerateExec/anonfun/1/apply(int,scala.collection.Iterator)#scala/collection/Iterator/$plus$plus(scala.Function0)
org/apache/spark/sql/execution/GenerateExec/anonfun/1/apply(int,scala.collection.Iterator)#org/apache/spark/sql/execution/GenerateExec$$anonfun$1$$anonfun$4/4(org.apache.spark.sql.execution.GenerateExec$$anonfun$1,org.apache.spark.sql.catalyst.expressions.GenericInternalRow)
org/apache/spark/sql/execution/GenerateExec/anonfun/1/apply(int,scala.collection.Iterator)#org/apache/spark/sql/execution/GenerateExec$$anonfun$1$$anonfun$2/2(org.apache.spark.sql.execution.GenerateExec$$anonfun$1,org.apache.spark.sql.catalyst.expressions.GenericInternalRow,org.apache.spark.sql.catalyst.expressions.JoinedRow)
org/apache/spark/sql/execution/GenerateExec/anonfun/1/apply(int,scala.collection.Iterator)#org/apache/spark/sql/catalyst/expressions/UnsafeProjection/initialize(int)
org/apache/spark/sql/execution/GenerateExec/anonfun/1/apply(int,scala.collection.Iterator)#org/apache/spark/sql/execution/GenerateExec$$anonfun$1$$anonfun$apply$5/5(org.apache.spark.sql.execution.GenerateExec$$anonfun$1,org.apache.spark.sql.catalyst.expressions.UnsafeProjection)
org/apache/spark/sql/execution/GenerateExec/anonfun/1/apply(int,scala.collection.Iterator)#scala/collection/Iterator/map(scala.Function1)
org/apache/spark/sql/execution/GenerateExec/anonfun/1/apply(int,scala.collection.Iterator)#org/apache/spark/sql/execution/GenerateExec$$anonfun$1$$anonfun$5/5(org.apache.spark.sql.execution.GenerateExec$$anonfun$1)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetSchema(org.apache.parquet.schema.MessageType,org.apache.spark.sql.types.StructType)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetSchema(org.apache.parquet.schema.MessageType,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter$/SPARK_PARQUET_SCHEMA_NAME()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetSchema(org.apache.parquet.schema.MessageType,org.apache.spark.sql.types.StructType)#org/apache/parquet/schema/Types/buildMessage()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetSchema(org.apache.parquet.schema.MessageType,org.apache.spark.sql.types.StructType)#org/apache/parquet/schema/Types$MessageTypeBuilder/addFields(org.apache.parquet.schema.Type[])
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetSchema(org.apache.parquet.schema.MessageType,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport$/clipParquetGroupFields(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetSchema(org.apache.parquet.schema.MessageType,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport$/clipParquetSchema(org.apache.parquet.schema.MessageType,org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetSchema(org.apache.parquet.schema.MessageType,org.apache.spark.sql.types.StructType)#org/apache/parquet/schema/Types$BaseGroupBuilder/named(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetSchema(org.apache.parquet.schema.MessageType,org.apache.spark.sql.types.StructType)#org/apache/parquet/schema/MessageType/asGroupType()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetSchema(org.apache.parquet.schema.MessageType,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter$/EMPTY_MESSAGE()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetSchema(org.apache.parquet.schema.MessageType,org.apache.spark.sql.types.StructType)#scala/collection/Seq/isEmpty()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetSchema(org.apache.parquet.schema.MessageType,org.apache.spark.sql.types.StructType)#scala/collection/Seq/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetReadSupport$$expand$1(org.apache.spark.sql.types.DataType)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetReadSupport$$expand$1(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/StructType/copy(org.apache.spark.sql.types.StructField[])
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetReadSupport$$expand$1(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/UserDefinedType/sqlType()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetReadSupport$$expand$1(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetReadSupport$$expand$1(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/ArrayType/copy$default$2()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetReadSupport$$expand$1(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/MapType/keyType()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetReadSupport$$expand$1(org.apache.spark.sql.types.DataType)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetReadSupport$$expand$1(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/ArrayType/copy(org.apache.spark.sql.types.DataType,boolean)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetReadSupport$$expand$1(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport$$anonfun$2/2()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetReadSupport$$expand$1(org.apache.spark.sql.types.DataType)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetReadSupport$$expand$1(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport$/org$apache$spark$sql$execution$datasources$parquet$ParquetReadSupport$$expand$1(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetReadSupport$$expand$1(org.apache.spark.sql.types.DataType)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetReadSupport$$expand$1(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/MapType/copy(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType,boolean)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetReadSupport$$expand$1(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/MapType/valueType()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetReadSupport$$expand$1(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/ArrayType/elementType()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/org$apache$spark$sql$execution$datasources$parquet$ParquetReadSupport$$expand$1(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/MapType/copy$default$3()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetMapType(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#org/apache/parquet/schema/Type/asGroupType()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetMapType(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#org/apache/parquet/schema/Types/repeatedGroup()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetMapType(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#org/apache/parquet/schema/GroupType/getRepetition()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetMapType(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#org/apache/parquet/schema/Types$BaseGroupBuilder/named(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetMapType(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#org/apache/parquet/schema/Types$BaseGroupBuilder/addField(org.apache.parquet.schema.Type)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetMapType(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#org/apache/parquet/schema/GroupType/getType(int)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetMapType(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#scala/Predef$/assert(boolean)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetMapType(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport$/isPrimitiveCatalystType(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetMapType(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#org/apache/parquet/schema/GroupType/getOriginalType()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetMapType(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#org/apache/parquet/schema/Types/buildGroup(org.apache.parquet.schema.Type$Repetition)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetMapType(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#org/apache/parquet/schema/GroupType/getName()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetMapType(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#org/apache/parquet/schema/Types$GroupBuilder/as(org.apache.parquet.schema.OriginalType)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetMapType(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport$/org$apache$spark$sql$execution$datasources$parquet$ParquetReadSupport$$clipParquetType(org.apache.parquet.schema.Type,org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetGroup(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.StructType)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetGroup(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.StructType)#org/apache/parquet/schema/Types$BaseGroupBuilder/addFields(org.apache.parquet.schema.Type[])
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetGroup(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.StructType)#scala/collection/Seq/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetGroup(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport$/clipParquetGroupFields(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetGroup(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.StructType)#org/apache/parquet/schema/GroupType/getRepetition()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetGroup(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.StructType)#org/apache/parquet/schema/Types$GroupBuilder/as(org.apache.parquet.schema.OriginalType)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetGroup(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.StructType)#org/apache/parquet/schema/Types$BaseGroupBuilder/named(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetGroup(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.StructType)#org/apache/parquet/schema/GroupType/getOriginalType()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetGroup(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.StructType)#org/apache/parquet/schema/Types/buildGroup(org.apache.parquet.schema.Type$Repetition)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetGroup(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.StructType)#org/apache/parquet/schema/GroupType/getName()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetListType(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.DataType)#org/apache/parquet/schema/GroupType/isRepetition(org.apache.parquet.schema.Type$Repetition)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetListType(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.DataType)#org/apache/parquet/schema/Types/repeatedGroup()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetListType(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.DataType)#org/apache/parquet/schema/GroupType/getRepetition()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetListType(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.DataType)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetListType(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.DataType)#org/apache/parquet/schema/Types$BaseGroupBuilder/addField(org.apache.parquet.schema.Type)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetListType(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport$$anonfun$clipParquetListType$2/2(org.apache.parquet.schema.GroupType)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetListType(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.DataType)#org/apache/parquet/schema/Type/isPrimitive()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetListType(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.DataType)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetListType(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.DataType)#scala/Predef$/assert(boolean,scala.Function0)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetListType(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.DataType)#scala/Predef$/assert(boolean)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetListType(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport$/isPrimitiveCatalystType(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetListType(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.DataType)#org/apache/parquet/schema/Types$GroupBuilder/addField(org.apache.parquet.schema.Type)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetListType(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.DataType)#org/apache/parquet/schema/Type/asGroupType()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetListType(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.DataType)#org/apache/parquet/schema/GroupType/getOriginalType()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetListType(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.DataType)#org/apache/parquet/schema/Types/buildGroup(org.apache.parquet.schema.Type$Repetition)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetListType(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.DataType)#org/apache/parquet/schema/GroupType/getName()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetListType(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.DataType)#org/apache/parquet/schema/Types$GroupBuilder/as(org.apache.parquet.schema.OriginalType)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetListType(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport$/org$apache$spark$sql$execution$datasources$parquet$ParquetReadSupport$$clipParquetType(org.apache.parquet.schema.Type,org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetListType(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.DataType)#org/apache/parquet/schema/Types$BaseGroupBuilder/named(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetListType(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.DataType)#org/apache/parquet/schema/GroupType/getFieldCount()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetListType(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.DataType)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetListType(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.DataType)#org/apache/parquet/schema/Type/isRepetition(org.apache.parquet.schema.Type$Repetition)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetListType(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport$$anonfun$clipParquetListType$1/1(org.apache.parquet.schema.GroupType)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetListType(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.DataType)#org/apache/parquet/schema/GroupType/getType(int)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetListType(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.DataType)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetGroupFields(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.StructType)#scala/collection/mutable/Buffer$/canBuildFrom()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetGroupFields(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport$$anonfun$1/1()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetGroupFields(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/types/StructType/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetGroupFields(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.StructType)#scala/collection/convert/Decorators$AsScala/asScala()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetGroupFields(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.StructType)#scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetGroupFields(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetGroupFields(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport$$anonfun$clipParquetGroupFields$1/1(scala.collection.immutable.Map,org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetGroupFields(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.StructType)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetGroupFields(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.StructType)#scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetGroupFields(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter$/$lessinit$greater$default$1()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetGroupFields(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.StructType)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetGroupFields(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.StructType)#scala/Predef$/$conforms()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetGroupFields(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.StructType)#org/apache/parquet/schema/GroupType/getFields()
org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport/clipParquetGroupFields(org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/RangeExec/doExecute()#org/apache/spark/sql/execution/RangeExec/longMetric(java.lang.String)
org/apache/spark/sql/execution/RangeExec/doExecute()#org/apache/spark/SparkContext/parallelize(scala.collection.Seq,int,scala.reflect.ClassTag)
org/apache/spark/sql/execution/RangeExec/doExecute()#org/apache/spark/rdd/RDD/mapPartitionsWithIndex(scala.Function2,boolean,scala.reflect.ClassTag)
org/apache/spark/sql/execution/RangeExec/doExecute()#org/apache/spark/rdd/RDD/mapPartitionsWithIndex$default$2()
org/apache/spark/sql/execution/RangeExec/doExecute()#org/apache/spark/sql/execution/RangeExec/sqlContext()
org/apache/spark/sql/execution/RangeExec/doExecute()#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/RangeExec/doExecute()#scala/runtime/RichInt$/until$extension0(int,int)
org/apache/spark/sql/execution/RangeExec/doExecute()#scala/Predef$/intWrapper(int)
org/apache/spark/sql/execution/RangeExec/doExecute()#scala/reflect/ClassTag$/Int()
org/apache/spark/sql/execution/RangeExec/doExecute()#org/apache/spark/sql/execution/RangeExec$$anonfun$20/20(org.apache.spark.sql.execution.RangeExec,org.apache.spark.sql.execution.metric.SQLMetric)
org/apache/spark/sql/execution/RangeExec/inputRDDs()#org/apache/spark/sql/execution/RangeExec$$anonfun$19/19(org.apache.spark.sql.execution.RangeExec)
org/apache/spark/sql/execution/RangeExec/inputRDDs()#scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)
org/apache/spark/sql/execution/RangeExec/inputRDDs()#org/apache/spark/SparkContext/parallelize(scala.collection.Seq,int,scala.reflect.ClassTag)
org/apache/spark/sql/execution/RangeExec/inputRDDs()#org/apache/spark/rdd/EmptyRDD/EmptyRDD(org.apache.spark.SparkContext,scala.reflect.ClassTag)
org/apache/spark/sql/execution/RangeExec/inputRDDs()#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/RangeExec/inputRDDs()#scala/runtime/RichInt$/until$extension0(int,int)
org/apache/spark/sql/execution/RangeExec/inputRDDs()#scala/Predef$/intWrapper(int)
org/apache/spark/sql/execution/RangeExec/inputRDDs()#scala/reflect/ClassTag$/Int()
org/apache/spark/sql/execution/RangeExec/inputRDDs()#org/apache/spark/rdd/RDD/map(scala.Function1,scala.reflect.ClassTag)
org/apache/spark/sql/execution/RangeExec/inputRDDs()#org/apache/spark/sql/execution/RangeExec/sqlContext()
org/apache/spark/sql/execution/RangeExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/RangeExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#java/lang/Class/getName()
org/apache/spark/sql/execution/RangeExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/RangeExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/freshName(java.lang.String)
org/apache/spark/sql/execution/RangeExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/RangeExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/RangeExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/RangeExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/ExprCode(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/RangeExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addMutableState(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/RangeExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/RangeExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/RangeExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/RangeExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/RangeExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addNewFunction(java.lang.String,java.lang.String)
org/apache/spark/sql/execution/RangeExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/RangeExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/math/BigInt/toLong()
org/apache/spark/sql/execution/command/DescribeTableCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/immutable/Map/nonEmpty()
org/apache/spark/sql/execution/command/DescribeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/command/DescribeTableCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/DescribeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/schema()
org/apache/spark/sql/execution/command/DescribeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/command/DescribeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/command/DescribeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/command/DescribeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()
org/apache/spark/sql/execution/command/DescribeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/lookupRelation(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/DescribeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/TableIdentifier/identifier()
org/apache/spark/sql/execution/command/DescribeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/schema()
org/apache/spark/sql/execution/command/DescribeTableCommand/run(org.apache.spark.sql.SparkSession)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/DescribeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/isTemporaryTable(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/DescribeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/DescribeTableCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/ArrayBuffer/ArrayBuffer()
org/apache/spark/sql/execution/command/DescribeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/command/DescribeTableCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/DescribeTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/types/StructType/isEmpty()
org/apache/spark/sql/execution/command/DescribeTableCommand/run(org.apache.spark.sql.SparkSession)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/DescribeTableCommand/describeDetailedPartitionInfo(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.ArrayBuffer)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/command/DescribeTableCommand/describeDetailedPartitionInfo(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.ArrayBuffer)#org/apache/spark/sql/catalyst/TableIdentifier/identifier()
org/apache/spark/sql/execution/command/DescribeTableCommand/describeDetailedPartitionInfo(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.ArrayBuffer)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/command/DescribeTableCommand/describeDetailedPartitionInfo(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.ArrayBuffer)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/getPartition(org.apache.spark.sql.catalyst.TableIdentifier,scala.collection.immutable.Map)
org/apache/spark/sql/execution/command/DescribeTableCommand/describeDetailedPartitionInfo(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.ArrayBuffer)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/DescribeTableCommand/describeDetailedPartitionInfo(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.ArrayBuffer)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/command/DescribeTableCommand/describeDetailedPartitionInfo(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.ArrayBuffer)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/command/DescribeTableCommand/describeDetailedPartitionInfo(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.ArrayBuffer)#org/apache/spark/sql/catalyst/catalog/CatalogTableType$/VIEW()
org/apache/spark/sql/execution/command/DescribeTableCommand/describeDetailedPartitionInfo(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.ArrayBuffer)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/DescribeTableCommand/describeDetailedPartitionInfo(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.ArrayBuffer)#org/apache/spark/sql/catalyst/catalog/CatalogTable/tableType()
org/apache/spark/sql/execution/command/DescribeTableCommand/describeDetailedPartitionInfo(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.ArrayBuffer)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/command/DescribeTableCommand/describeDetailedPartitionInfo(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.ArrayBuffer)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/DescribeTableCommand/describeDetailedPartitionInfo(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.ArrayBuffer)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/command/DescribeTableCommand/describeDetailedPartitionInfo(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.ArrayBuffer)#org/apache/spark/sql/execution/command/DDLUtils$/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)
org/apache/spark/sql/execution/command/DescribeTableCommand/describeDetailedPartitionInfo(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.ArrayBuffer)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/DescribeTableCommand/describeSchema(org.apache.spark.sql.types.StructType,scala.collection.mutable.ArrayBuffer,boolean)#org/apache/spark/sql/catalyst/expressions/NamedExpression/name()
org/apache/spark/sql/execution/command/DescribeTableCommand/describeSchema(org.apache.spark.sql.types.StructType,scala.collection.mutable.ArrayBuffer,boolean)#scala/collection/Seq/apply(int)
org/apache/spark/sql/execution/command/DescribeTableCommand/describeSchema(org.apache.spark.sql.types.StructType,scala.collection.mutable.ArrayBuffer,boolean)#org/apache/spark/sql/execution/command/DescribeTableCommand$$anonfun$describeSchema$1/1(org.apache.spark.sql.execution.command.DescribeTableCommand,scala.collection.mutable.ArrayBuffer)
org/apache/spark/sql/execution/command/DescribeTableCommand/describeSchema(org.apache.spark.sql.types.StructType,scala.collection.mutable.ArrayBuffer,boolean)#scala/collection/Seq/head()
org/apache/spark/sql/execution/command/DescribeTableCommand/describeSchema(org.apache.spark.sql.types.StructType,scala.collection.mutable.ArrayBuffer,boolean)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/DescribeTableCommand/describeSchema(org.apache.spark.sql.types.StructType,scala.collection.mutable.ArrayBuffer,boolean)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/DescribeTableCommand/describeSchema(org.apache.spark.sql.types.StructType,scala.collection.mutable.ArrayBuffer,boolean)#org/apache/spark/sql/types/StructType/foreach(scala.Function1)
org/apache/spark/sql/execution/command/DescribeTableCommand/describeSchema(org.apache.spark.sql.types.StructType,scala.collection.mutable.ArrayBuffer,boolean)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/DescribeTableCommand/describeSchema(org.apache.spark.sql.types.StructType,scala.collection.mutable.ArrayBuffer,boolean)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/DescribeTableCommand/describeFormattedDetailedPartitionInfo(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTable,org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,scala.collection.mutable.ArrayBuffer)#scala/Some/x()
org/apache/spark/sql/execution/command/DescribeTableCommand/describeFormattedDetailedPartitionInfo(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTable,org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,scala.collection.mutable.ArrayBuffer)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/toLinkedHashMap()
org/apache/spark/sql/execution/command/DescribeTableCommand/describeFormattedDetailedPartitionInfo(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTable,org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,scala.collection.mutable.ArrayBuffer)#org/apache/spark/sql/catalyst/TableIdentifier/table()
org/apache/spark/sql/execution/command/DescribeTableCommand/describeFormattedDetailedPartitionInfo(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTable,org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,scala.collection.mutable.ArrayBuffer)#org/apache/spark/sql/catalyst/catalog/BucketSpec/toLinkedHashMap()
org/apache/spark/sql/execution/command/DescribeTableCommand/describeFormattedDetailedPartitionInfo(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTable,org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,scala.collection.mutable.ArrayBuffer)#org/apache/spark/sql/catalyst/catalog/CatalogTable/database()
org/apache/spark/sql/execution/command/DescribeTableCommand/describeFormattedDetailedPartitionInfo(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTable,org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,scala.collection.mutable.ArrayBuffer)#org/apache/spark/sql/catalyst/catalog/CatalogTable/bucketSpec()
org/apache/spark/sql/execution/command/DescribeTableCommand/describeFormattedDetailedPartitionInfo(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTable,org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,scala.collection.mutable.ArrayBuffer)#org/apache/spark/sql/execution/command/DescribeTableCommand$$anonfun$describeFormattedDetailedPartitionInfo$2/2(org.apache.spark.sql.execution.command.DescribeTableCommand,scala.collection.mutable.ArrayBuffer)
org/apache/spark/sql/execution/command/DescribeTableCommand/describeFormattedDetailedPartitionInfo(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTable,org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,scala.collection.mutable.ArrayBuffer)#scala/collection/mutable/LinkedHashMap/foreach(scala.Function1)
org/apache/spark/sql/execution/command/DescribeTableCommand/describeFormattedDetailedPartitionInfo(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTable,org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,scala.collection.mutable.ArrayBuffer)#org/apache/spark/sql/execution/command/DescribeTableCommand$$anonfun$describeFormattedDetailedPartitionInfo$3/3(org.apache.spark.sql.execution.command.DescribeTableCommand,scala.collection.mutable.ArrayBuffer)
org/apache/spark/sql/execution/command/DescribeTableCommand/describeFormattedDetailedPartitionInfo(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTable,org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,scala.collection.mutable.ArrayBuffer)#org/apache/spark/sql/catalyst/catalog/CatalogTable/storage()
org/apache/spark/sql/execution/command/DescribeTableCommand/describeFormattedDetailedPartitionInfo(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTable,org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,scala.collection.mutable.ArrayBuffer)#org/apache/spark/sql/execution/command/DescribeTableCommand$$anonfun$describeFormattedDetailedPartitionInfo$1/1(org.apache.spark.sql.execution.command.DescribeTableCommand,scala.collection.mutable.ArrayBuffer)
org/apache/spark/sql/execution/command/DescribeTableCommand/describeFormattedDetailedPartitionInfo(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTable,org.apache.spark.sql.catalyst.catalog.CatalogTablePartition,scala.collection.mutable.ArrayBuffer)#org/apache/spark/sql/catalyst/catalog/CatalogTablePartition/toLinkedHashMap()
org/apache/spark/sql/execution/datasources/CreateTempViewUsing/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/datasources/LogicalRelation$/apply(org.apache.spark.sql.sources.BaseRelation)
org/apache/spark/sql/execution/datasources/CreateTempViewUsing/run(org.apache.spark.sql.SparkSession)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/CreateTempViewUsing/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/empty()
org/apache/spark/sql/execution/datasources/CreateTempViewUsing/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/Dataset$/ofRows(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/datasources/CreateTempViewUsing/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/CreateTempViewUsing/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/CreateTempViewUsing/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/CreateTempViewUsing/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/datasources/DataSource$/apply$default$3()
org/apache/spark/sql/execution/datasources/CreateTempViewUsing/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/datasources/DataSource$/apply$default$5()
org/apache/spark/sql/execution/datasources/CreateTempViewUsing/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/TableIdentifier/table()
org/apache/spark/sql/execution/datasources/CreateTempViewUsing/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/DDLUtils$/HIVE_PROVIDER()
org/apache/spark/sql/execution/datasources/CreateTempViewUsing/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/datasources/DataSource$/apply$default$6()
org/apache/spark/sql/execution/datasources/CreateTempViewUsing/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/datasources/DataSource$/apply$default$8()
org/apache/spark/sql/execution/datasources/CreateTempViewUsing/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/CreateTempViewUsing/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/createTempView(java.lang.String,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,boolean)
org/apache/spark/sql/execution/datasources/CreateTempViewUsing/run(org.apache.spark.sql.SparkSession)#java/lang/String/toLowerCase(java.util.Locale)
org/apache/spark/sql/execution/datasources/CreateTempViewUsing/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/createGlobalTempView(java.lang.String,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,boolean)
org/apache/spark/sql/execution/datasources/CreateTempViewUsing/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/CreateTempViewUsing/argString()#org/apache/spark/sql/execution/datasources/CreateTempViewUsing$$anonfun$argString$1/1(org.apache.spark.sql.execution.datasources.CreateTempViewUsing)
org/apache/spark/sql/execution/datasources/CreateTempViewUsing/argString()#org/apache/spark/sql/execution/datasources/CreateTempViewUsing$$anonfun$argString$2/2(org.apache.spark.sql.execution.datasources.CreateTempViewUsing)
org/apache/spark/sql/execution/datasources/CreateTempViewUsing/argString()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/CreateTempViewUsing/argString()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/CreateTempViewUsing/argString()#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/datasources/CreateTempViewUsing/argString()#scala/runtime/BoxesRunTime/boxToBoolean(boolean)
org/apache/spark/sql/execution/datasources/CreateTempViewUsing/argString()#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/datasources/CreateTempViewUsing/argString()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/CreateTempViewUsing/argString()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/CreateTempViewUsing/argString()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/CreateTempViewUsing/argString()#org/apache/spark/sql/catalyst/catalog/CatalogUtils$/maskCredentials(scala.collection.immutable.Map)
org/apache/spark/sql/execution/datasources/CreateTempViewUsing/argString()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/CreateTempViewUsing/argString()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/SQLImplicits/typecreator9/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/universe()
org/apache/spark/sql/SQLImplicits/typecreator9/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/selectType(scala.reflect.api.Symbols$SymbolApi,java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator9/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ModuleSymbolApi/asModule()
org/apache/spark/sql/SQLImplicits/typecreator9/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticPackage(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator9/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$InternalApi/reificationSupport()
org/apache/spark/sql/SQLImplicits/typecreator9/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/RootClass()
org/apache/spark/sql/SQLImplicits/typecreator9/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ModuleSymbolApi/moduleClass()
org/apache/spark/sql/SQLImplicits/typecreator9/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/thisPrefix(scala.reflect.api.Symbols$SymbolApi)
org/apache/spark/sql/SQLImplicits/typecreator9/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/TypeRef(scala.reflect.api.Types$TypeApi,scala.reflect.api.Symbols$SymbolApi,scala.collection.immutable.List)
org/apache/spark/sql/SQLImplicits/typecreator9/1/apply(scala.reflect.api.Mirror)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/SQLImplicits/typecreator9/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/SingleType(scala.reflect.api.Types$TypeApi,scala.reflect.api.Symbols$SymbolApi)
org/apache/spark/sql/SQLImplicits/typecreator9/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticModule(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator9/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/ThisType(scala.reflect.api.Symbols$SymbolApi)
org/apache/spark/sql/SQLImplicits/typecreator9/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Universe/internal()
org/apache/spark/sql/SQLImplicits/typecreator9/1/apply(scala.reflect.api.Mirror)#scala/collection/immutable/List$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/listLeafFiles(scala.collection.Seq)#scala/collection/mutable/ArrayBuffer$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/listLeafFiles(scala.collection.Seq)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$$anonfun$listLeafFiles$2/2(org.apache.spark.sql.execution.datasources.InMemoryFileIndex,scala.collection.mutable.LinkedHashSet)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/listLeafFiles(scala.collection.Seq)#org/apache/hadoop/mapred/FileInputFormat/getInputPathFilter(org.apache.hadoop.mapred.JobConf)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/listLeafFiles(scala.collection.Seq)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex/hadoopConf()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/listLeafFiles(scala.collection.Seq)#scala/collection/Seq/foreach(scala.Function1)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/listLeafFiles(scala.collection.Seq)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$/org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$bulkListLeafFiles(scala.collection.Seq,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.PathFilter,org.apache.spark.sql.SparkSession)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/listLeafFiles(scala.collection.Seq)#org/apache/hadoop/mapred/JobConf/JobConf(org.apache.hadoop.conf.Configuration,java.lang.Class)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/listLeafFiles(scala.collection.Seq)#java/lang/Object/getClass()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/listLeafFiles(scala.collection.Seq)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$$anonfun$listLeafFiles$1/1(org.apache.spark.sql.execution.datasources.InMemoryFileIndex,scala.collection.mutable.LinkedHashSet,scala.collection.mutable.ArrayBuffer)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/listLeafFiles(scala.collection.Seq)#scala/collection/mutable/LinkedHashSet$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/refresh0()#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/refresh0()#scala/collection/mutable/ArrayOps/groupBy(scala.Function1)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/refresh0()#scala/collection/mutable/LinkedHashSet/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/refresh0()#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$$anonfun$refresh0$1/1(org.apache.spark.sql.execution.datasources.InMemoryFileIndex)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/refresh0()#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$$anonfun$refresh0$2/2(org.apache.spark.sql.execution.datasources.InMemoryFileIndex)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/refresh0()#scala/collection/mutable/LinkedHashSet$/canBuildFrom()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/refresh0()#scala/collection/mutable/LinkedHashSet/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/refresh0()#scala/collection/mutable/LinkedHashMap/$plus$plus$eq(scala.collection.TraversableOnce)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/refresh0()#scala/collection/mutable/LinkedHashMap/LinkedHashMap()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/refresh0()#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/getValidBatchesBeforeCompactionBatch(long,int)#scala/Predef$/longWrapper(long)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/getValidBatchesBeforeCompactionBatch(long,int)#scala/Predef$/assert(boolean,scala.Function0)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/getValidBatchesBeforeCompactionBatch(long,int)#scala/math/package$/max(long,long)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/getValidBatchesBeforeCompactionBatch(long,int)#scala/runtime/RichLong/until(java.lang.Object)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/getValidBatchesBeforeCompactionBatch(long,int)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/getValidBatchesBeforeCompactionBatch(long,int)#scala/runtime/RichLong/RichLong(long)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/getValidBatchesBeforeCompactionBatch(long,int)#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog$/getValidBatchesBeforeCompactionBatch(long,int)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/getValidBatchesBeforeCompactionBatch(long,int)#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog$/isCompactionBatch(long,int)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/getValidBatchesBeforeCompactionBatch(long,int)#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog$$anonfun$getValidBatchesBeforeCompactionBatch$1/1(long)
org/apache/spark/sql/catalog/Table/toString()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/catalog/Table/toString()#org/apache/spark/sql/catalog/Table$$anonfun$toString$5/5(org.apache.spark.sql.catalog.Table)
org/apache/spark/sql/catalog/Table/toString()#org/apache/spark/sql/catalog/Table$$anonfun$toString$6/6(org.apache.spark.sql.catalog.Table)
org/apache/spark/sql/catalog/Table/toString()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/catalog/Table/toString()#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/catalog/Table/toString()#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/catalog/Table/toString()#org/apache/spark/sql/catalog/Table$$anonfun$toString$3/3(org.apache.spark.sql.catalog.Table)
org/apache/spark/sql/catalog/Table/toString()#scala/runtime/BoxesRunTime/boxToBoolean(boolean)
org/apache/spark/sql/catalog/Table/toString()#org/apache/spark/sql/catalog/Table$$anonfun$toString$4/4(org.apache.spark.sql.catalog.Table)
org/apache/spark/sql/catalog/Table/toString()#scala/Option/map(scala.Function1)
org/apache/spark/sql/catalog/Table/toString()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/catalog/Table/toString()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/catalog/Table/toString()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/catalog/Table/toString()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/catalog/Table/toString()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateView/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateViewContext/TEMPORARY()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateView/1/apply()#scala/Option$/option2Iterable(scala.Option)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateView/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateViewContext/tableIdentifier()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateView/1/apply()#org/apache/spark/sql/catalyst/parser/ParserUtils$/operationNotAllowed(java.lang.String,org.antlr.v4.runtime.ParserRuleContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateView/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateViewContext/GLOBAL()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateView/1/apply()#scala/collection/Iterable/toSeq()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateView/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateViewContext/identifierList()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateView/1/apply()#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateView/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder/plan(org.antlr.v4.runtime.ParserRuleContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateView/1/apply()#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateView/1/apply()#scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateView/1/apply()#org/apache/spark/sql/catalyst/parser/ParserUtils$/source(org.antlr.v4.runtime.ParserRuleContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateView/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateViewContext/query()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateView/1/apply()#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateView/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder/visitTableIdentifier(org.apache.spark.sql.catalyst.parser.SqlBaseParser$TableIdentifierContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateView/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateView$1$$anonfun$47/47(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateView$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateView/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateViewContext/EXISTS()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateView/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateViewContext/identifierCommentList()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateView/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateViewContext/REPLACE()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateView/1/apply()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateView/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateView$1$$anonfun$apply$36/36(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateView$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateView/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$QueryContext/queryNoWith()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateView/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateView$1$$anonfun$apply$34/34(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateView$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateView/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateView$1$$anonfun$apply$35/35(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateView$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateView/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateViewContext/tablePropertyList()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateView/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$SingleInsertQueryContext/insertInto()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateView/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateViewContext/STRING()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateView/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateView$1/apply()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/equals(java.lang.Object)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/equals(java.lang.Object)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/blockReplication()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/equals(java.lang.Object)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/path()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/equals(java.lang.Object)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/length()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/equals(java.lang.Object)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/modificationTime()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/equals(java.lang.Object)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/isDir()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/equals(java.lang.Object)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/blockSize()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/equals(java.lang.Object)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/accessTime()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/equals(java.lang.Object)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/blockLocations()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/equals(java.lang.Object)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/canEqual(java.lang.Object)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/productElement(int)#java/lang/Object/toString()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/productElement(int)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/blockReplication()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/productElement(int)#scala/runtime/BoxesRunTime/boxToShort(short)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/productElement(int)#java/lang/IndexOutOfBoundsException/IndexOutOfBoundsException(java.lang.String)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/productElement(int)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/path()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/productElement(int)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/length()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/productElement(int)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/isDir()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/productElement(int)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/productElement(int)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/blockSize()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/productElement(int)#scala/runtime/BoxesRunTime/boxToBoolean(boolean)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/productElement(int)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/productElement(int)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/modificationTime()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/productElement(int)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/accessTime()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/productElement(int)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/blockLocations()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/hashCode()#scala/runtime/Statics/finalizeHash(int,int)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/hashCode()#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/path()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/hashCode()#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/length()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/hashCode()#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/modificationTime()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/hashCode()#scala/runtime/Statics/anyHash(java.lang.Object)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/hashCode()#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/blockSize()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/hashCode()#scala/runtime/Statics/mix(int,int)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/hashCode()#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/blockReplication()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/hashCode()#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/isDir()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/hashCode()#scala/runtime/Statics/longHash(long)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/hashCode()#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/accessTime()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/SerializableFileStatus/hashCode()#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/blockLocations()
org/apache/spark/sql/execution/columnar/InMemoryRelation/anonfun/1/anon/1/next()#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/columnar/InMemoryRelation/anonfun/1/anon/1/next()#org/apache/spark/sql/execution/columnar/ColumnBuilder$/MAX_BATCH_SIZE_IN_BYTE()
org/apache/spark/sql/execution/columnar/InMemoryRelation/anonfun/1/anon/1/next()#scala/Predef$DummyImplicit$/dummyImplicit()
org/apache/spark/sql/execution/columnar/InMemoryRelation/anonfun/1/anon/1/next()#scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)
org/apache/spark/sql/execution/columnar/InMemoryRelation/anonfun/1/anon/1/next()#scala/Array$/fallbackCanBuildFrom(scala.Predef$DummyImplicit)
org/apache/spark/sql/execution/columnar/InMemoryRelation/anonfun/1/anon/1/next()#org/apache/spark/sql/execution/columnar/InMemoryRelation$$anonfun$1$$anon$1$$anonfun$3/3(org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1)
org/apache/spark/sql/execution/columnar/InMemoryRelation/anonfun/1/anon/1/next()#org/apache/spark/sql/execution/columnar/InMemoryRelation$$anonfun$1$$anon$1$$anonfun$2/2(org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1)
org/apache/spark/sql/execution/columnar/InMemoryRelation/anonfun/1/anon/1/next()#scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/columnar/InMemoryRelation/anonfun/1/anon/1/next()#org/apache/spark/util/LongAccumulator/add(long)
org/apache/spark/sql/execution/columnar/InMemoryRelation/anonfun/1/anon/1/next()#org/apache/spark/sql/execution/columnar/InMemoryRelation$$anonfun$1$$anon$1$$anonfun$next$1/1(org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1,org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/columnar/InMemoryRelation/anonfun/1/anon/1/next()#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/columnar/InMemoryRelation/anonfun/1/anon/1/next()#scala/collection/mutable/ArrayOps/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/columnar/InMemoryRelation/anonfun/1/anon/1/next()#org/apache/spark/sql/execution/columnar/InMemoryRelation$$anonfun$1$$anon$1$$anonfun$next$2/2(org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1)
org/apache/spark/sql/execution/columnar/InMemoryRelation/anonfun/1/anon/1/next()#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/columnar/InMemoryRelation/anonfun/1/anon/1/next()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/columnar/InMemoryRelation/anonfun/1/anon/1/next()#scala/Predef$/assert(boolean,scala.Function0)
org/apache/spark/sql/execution/columnar/InMemoryRelation/anonfun/1/anon/1/next()#scala/collection/Iterator/next()
org/apache/spark/sql/execution/columnar/InMemoryRelation/anonfun/1/anon/1/next()#org/apache/spark/sql/execution/columnar/InMemoryRelation$$anonfun$1$$anon$1/next()
org/apache/spark/sql/execution/columnar/InMemoryRelation/anonfun/1/anon/1/next()#org/apache/spark/sql/execution/columnar/InMemoryRelation$$anonfun$1/org$apache$spark$sql$execution$columnar$InMemoryRelation$$anonfun$$$outer()
org/apache/spark/sql/execution/columnar/InMemoryRelation/anonfun/1/anon/1/next()#org/apache/spark/sql/execution/columnar/InMemoryRelation$$anonfun$1$$anon$1$$anonfun$4/4(org.apache.spark.sql.execution.columnar.InMemoryRelation$$anonfun$1$$anon$1)
org/apache/spark/sql/execution/columnar/InMemoryRelation/anonfun/1/anon/1/next()#scala/collection/Iterator/hasNext()
org/apache/spark/sql/execution/columnar/InMemoryRelation/anonfun/1/anon/1/next()#org/apache/spark/sql/catalyst/InternalRow/numFields()
org/apache/spark/sql/execution/columnar/InMemoryRelation/anonfun/1/anon/1/next()#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/columnar/InMemoryRelation/anonfun/1/anon/1/next()#org/apache/spark/sql/catalyst/InternalRow$/fromSeq(scala.collection.Seq)
org/apache/spark/sql/execution/columnar/InMemoryRelation/anonfun/1/anon/1/next()#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/joins/SortMergeJoinExec/outputOrdering()#org/apache/spark/sql/catalyst/plans/LeftExistence$/unapply(org.apache.spark.sql.catalyst.plans.JoinType)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/outputOrdering()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/joins/SortMergeJoinExec/outputOrdering()#scala/Option/isEmpty()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/outputOrdering()#scala/collection/Seq/zip(scala.collection.GenIterable,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/outputOrdering()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/outputOrdering()#org/apache/spark/sql/execution/joins/SortMergeJoinExec$$anonfun$outputOrdering$1/1(org.apache.spark.sql.execution.joins.SortMergeJoinExec)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/outputOrdering()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/outputOrdering()#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/outputOrdering()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/outputOrdering()#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/outputOrdering()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/outputOrdering()#java/lang/Class/getSimpleName()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/outputOrdering()#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/outputOrdering()#java/lang/Object/getClass()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/splitVarsByCondition(scala.collection.Seq,scala.collection.Seq)#scala/Option/isDefined()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/splitVarsByCondition(scala.collection.Seq,scala.collection.Seq)#scala/Tuple2/_1()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/splitVarsByCondition(scala.collection.Seq,scala.collection.Seq)#scala/Tuple2/_2()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/splitVarsByCondition(scala.collection.Seq,scala.collection.Seq)#scala/collection/Seq/zip(scala.collection.GenIterable,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/splitVarsByCondition(scala.collection.Seq,scala.collection.Seq)#org/apache/spark/sql/execution/joins/SortMergeJoinExec$$anonfun$8/8(org.apache.spark.sql.execution.joins.SortMergeJoinExec)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/splitVarsByCondition(scala.collection.Seq,scala.collection.Seq)#org/apache/spark/sql/execution/joins/SortMergeJoinExec$$anonfun$7/7(org.apache.spark.sql.execution.joins.SortMergeJoinExec,org.apache.spark.sql.catalyst.expressions.AttributeSet)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/splitVarsByCondition(scala.collection.Seq,scala.collection.Seq)#org/apache/spark/sql/execution/joins/SortMergeJoinExec$$anonfun$9/9(org.apache.spark.sql.execution.joins.SortMergeJoinExec)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/splitVarsByCondition(scala.collection.Seq,scala.collection.Seq)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/splitVarsByCondition(scala.collection.Seq,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/Expression/references()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/splitVarsByCondition(scala.collection.Seq,scala.collection.Seq)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/splitVarsByCondition(scala.collection.Seq,scala.collection.Seq)#scala/collection/TraversableLike/partition(scala.Function1)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/splitVarsByCondition(scala.collection.Seq,scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/splitVarsByCondition(scala.collection.Seq,scala.collection.Seq)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/splitVarsByCondition(scala.collection.Seq,scala.collection.Seq)#scala/Option/get()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/genComparision(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,scala.collection.Seq)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/genComparision(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/genComparision(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,scala.collection.Seq)#org/apache/spark/sql/execution/joins/SortMergeJoinExec$$anonfun$4/4(org.apache.spark.sql.execution.joins.SortMergeJoinExec,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/genComparision(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,scala.collection.Seq)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/joins/SortMergeJoinExec/genComparision(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,scala.collection.Seq)#scala/collection/Seq/zip(scala.collection.GenIterable,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/genComparision(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,scala.collection.Seq)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/genComparision(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,scala.collection.Seq)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/genComparision(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,scala.collection.Seq)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/genComparision(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,scala.collection.Seq)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/genComparision(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,scala.collection.Seq)#scala/collection/IterableLike/zipWithIndex(scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/genComparision(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,scala.collection.Seq)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/genComparision(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,scala.collection.Seq)#scala/collection/Seq/mkString(java.lang.String)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/genComparision(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,scala.collection.Seq)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/genScanner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/genScanner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/genScanner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/genScanner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/genScanner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/freshName(java.lang.String)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/genScanner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/genScanner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/genScanner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/genScanner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/genScanner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/genScanner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/execution/joins/SortMergeJoinExec$$anonfun$genScanner$1/1(org.apache.spark.sql.execution.joins.SortMergeJoinExec)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/genScanner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addMutableState(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/genScanner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/joins/SortMergeJoinExec/genScanner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/execution/joins/SortMergeJoinExec$$anonfun$5/5(org.apache.spark.sql.execution.joins.SortMergeJoinExec)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/genScanner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/execution/joins/SortMergeJoinExec$$anonfun$genScanner$2/2(org.apache.spark.sql.execution.joins.SortMergeJoinExec)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/genScanner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/execution/joins/SortMergeJoinExec$$anonfun$6/6(org.apache.spark.sql.execution.joins.SortMergeJoinExec)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/genScanner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/TraversableOnce/mkString(java.lang.String)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/genScanner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/execution/joins/SortMergeJoinExec$$anonfun$genScanner$3/3(org.apache.spark.sql.execution.joins.SortMergeJoinExec)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/genScanner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/execution/joins/SortMergeJoinExec$$anonfun$genScanner$4/4(org.apache.spark.sql.execution.joins.SortMergeJoinExec)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/genScanner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addNewFunction(java.lang.String,java.lang.String)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/genScanner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#java/lang/Class/getName()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/genScanner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/genScanner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/execution/joins/SortMergeJoinExec$$anonfun$genScanner$5/5(org.apache.spark.sql.execution.joins.SortMergeJoinExec)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/genScanner(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/outputPartitioning()#org/apache/spark/sql/catalyst/plans/LeftExistence$/unapply(org.apache.spark.sql.catalyst.plans.JoinType)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/outputPartitioning()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/joins/SortMergeJoinExec/outputPartitioning()#scala/Option/isEmpty()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/outputPartitioning()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/outputPartitioning()#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/outputPartitioning()#org/apache/spark/sql/catalyst/plans/physical/PartitioningCollection/PartitioningCollection(scala.collection.Seq)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/outputPartitioning()#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/outputPartitioning()#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/outputPartitioning()#java/lang/Class/getSimpleName()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/outputPartitioning()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/outputPartitioning()#org/apache/spark/sql/catalyst/plans/physical/UnknownPartitioning/UnknownPartitioning(int)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/outputPartitioning()#org/apache/spark/sql/catalyst/plans/physical/Partitioning/numPartitions()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/outputPartitioning()#java/lang/Object/getClass()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/outputPartitioning()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/BindReferences$/bindReference$default$3()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/isNull()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/BindReferences$/bindReference(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.package$AttributeSeq,boolean)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/currentVars_$eq(scala.collection.Seq)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#java/lang/String/trim()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Option/isDefined()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/code()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/Expression/genCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/copyResult_$eq(boolean)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Tuple2/_1()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/value()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addMutableState(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/joins/SortMergeJoinExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Tuple2/_2()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/package$/AttributeSeq(scala.collection.Seq)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/freshName(java.lang.String)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Option/get()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/output()#org/apache/spark/sql/execution/joins/SortMergeJoinExec$$anonfun$output$3/3(org.apache.spark.sql.execution.joins.SortMergeJoinExec)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/output()#org/apache/spark/sql/catalyst/plans/LeftExistence$/unapply(org.apache.spark.sql.catalyst.plans.JoinType)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/output()#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/output()#scala/Option/isEmpty()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/output()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/output()#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/output()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/output()#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/output()#java/lang/Object/getClass()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/output()#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/output()#org/apache/spark/sql/execution/joins/SortMergeJoinExec$$anonfun$output$2/2(org.apache.spark.sql.execution.joins.SortMergeJoinExec)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/output()#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/output()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/output()#java/lang/Class/getSimpleName()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/output()#org/apache/spark/sql/catalyst/plans/ExistenceJoin/exists()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/output()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/joins/SortMergeJoinExec/output()#scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/output()#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/output()#scala/collection/Seq/$colon$plus(java.lang.Object,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/output()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/output()#org/apache/spark/sql/execution/joins/SortMergeJoinExec$$anonfun$output$1/1(org.apache.spark.sql.execution.joins.SortMergeJoinExec)
org/apache/spark/sql/SQLImplicits/typecreator10/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/universe()
org/apache/spark/sql/SQLImplicits/typecreator10/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/selectType(scala.reflect.api.Symbols$SymbolApi,java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator10/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/TypeTags$TypeTag/in(scala.reflect.api.Mirror)
org/apache/spark/sql/SQLImplicits/typecreator10/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ModuleSymbolApi/asModule()
org/apache/spark/sql/SQLImplicits/typecreator10/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticPackage(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator10/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$InternalApi/reificationSupport()
org/apache/spark/sql/SQLImplicits/typecreator10/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/RootClass()
org/apache/spark/sql/SQLImplicits/typecreator10/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticModule(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator10/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ModuleSymbolApi/moduleClass()
org/apache/spark/sql/SQLImplicits/typecreator10/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/thisPrefix(scala.reflect.api.Symbols$SymbolApi)
org/apache/spark/sql/SQLImplicits/typecreator10/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/TypeRef(scala.reflect.api.Types$TypeApi,scala.reflect.api.Symbols$SymbolApi,scala.collection.immutable.List)
org/apache/spark/sql/SQLImplicits/typecreator10/1/apply(scala.reflect.api.Mirror)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/SQLImplicits/typecreator10/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/SingleType(scala.reflect.api.Types$TypeApi,scala.reflect.api.Symbols$SymbolApi)
org/apache/spark/sql/SQLImplicits/typecreator10/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/TypeTags$TypeTag/tpe()
org/apache/spark/sql/SQLImplicits/typecreator10/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Universe/internal()
org/apache/spark/sql/SQLImplicits/typecreator10/1/apply(scala.reflect.api.Mirror)#scala/collection/immutable/List$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/constructNextBatch/1/anonfun/apply/mcVJ/sp/2/apply()#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch$1$$anonfun$apply$mcVJ$sp$2/apply()
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/constructNextBatch/1/anonfun/apply/mcVJ/sp/2/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/constructNextBatch/1/anonfun/apply/mcVJ/sp/2/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/constructNextBatch/1/anonfun/apply/mcVJ/sp/2/apply()#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/constructNextBatch/1/anonfun/apply/mcVJ/sp/2/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/constructNextBatch/1/anonfun/apply/mcVJ/sp/2/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/constructNextBatch/1/anonfun/apply/mcVJ/sp/2/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/constructNextBatch/1/anonfun/apply/mcVJ/sp/2/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/constructNextBatch/1/anonfun/apply/mcVJ/sp/2/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/createLeftVars/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/ExprCode(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/createLeftVars/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/getValue(java.lang.String,org.apache.spark.sql.types.DataType,java.lang.String)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/createLeftVars/1/apply(scala.Tuple2)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/createLeftVars/1/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/createLeftVars/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/Attribute/nullable()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/createLeftVars/1/apply(scala.Tuple2)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/createLeftVars/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/Attribute/dataType()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/createLeftVars/1/apply(scala.Tuple2)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/createLeftVars/1/apply(scala.Tuple2)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/createLeftVars/1/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/createLeftVars/1/apply(scala.Tuple2)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/createLeftVars/1/apply(scala.Tuple2)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/createLeftVars/1/apply(scala.Tuple2)#java/lang/Object/toString()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/createLeftVars/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/defaultValue(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/createLeftVars/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addMutableState(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/createLeftVars/1/apply(scala.Tuple2)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/createLeftVars/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/freshName(java.lang.String)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/createLeftVars/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/javaType(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/createLeftVars/1/apply(scala.Tuple2)#scala/Tuple2/_2$mcI$sp()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/createLeftVars/1/apply(scala.Tuple2)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/2/apply(org.apache.spark.sql.execution.streaming.OffsetSeqMetadata)#scala/Predef$/ArrowAssoc(java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/2/apply(org.apache.spark.sql.execution.streaming.OffsetSeqMetadata)#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$populateStartOffsets$2$$anonfun$1/1(org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$populateStartOffsets$2,int)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/2/apply(org.apache.spark.sql.execution.streaming.OffsetSeqMetadata)#scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/2/apply(org.apache.spark.sql.execution.streaming.OffsetSeqMetadata)#org/apache/spark/internal/config/ConfigEntry/key()
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/2/apply(org.apache.spark.sql.execution.streaming.OffsetSeqMetadata)#scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/2/apply(org.apache.spark.sql.execution.streaming.OffsetSeqMetadata)#scala/collection/immutable/Map/$plus(scala.Tuple2)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/2/apply(org.apache.spark.sql.execution.streaming.OffsetSeqMetadata)#java/lang/Object/toString()
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/2/apply(org.apache.spark.sql.execution.streaming.OffsetSeqMetadata)#scala/collection/immutable/Map/getOrElse(java.lang.Object,scala.Function0)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/2/apply(org.apache.spark.sql.execution.streaming.OffsetSeqMetadata)#org/apache/spark/sql/internal/SQLConf$/SHUFFLE_PARTITIONS()
org/apache/spark/sql/DataFrameStatFunctions/sampleBy(java.lang.String,scala.collection.immutable.Map,long)#scala/collection/immutable/Map/values()
org/apache/spark/sql/DataFrameStatFunctions/sampleBy(java.lang.String,scala.collection.immutable.Map,long)#org/apache/spark/sql/DataFrameStatFunctions$$anonfun$sampleBy$2/2(org.apache.spark.sql.DataFrameStatFunctions,scala.collection.immutable.Map)
org/apache/spark/sql/DataFrameStatFunctions/sampleBy(java.lang.String,scala.collection.immutable.Map,long)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/DataFrameStatFunctions/sampleBy(java.lang.String,scala.collection.immutable.Map,long)#org/apache/spark/sql/DataFrameStatFunctions$$anonfun$1/1(org.apache.spark.sql.DataFrameStatFunctions,scala.collection.immutable.Map)
org/apache/spark/sql/DataFrameStatFunctions/sampleBy(java.lang.String,scala.collection.immutable.Map,long)#scala/Predef$/require(boolean,scala.Function0)
org/apache/spark/sql/DataFrameStatFunctions/sampleBy(java.lang.String,scala.collection.immutable.Map,long)#org/apache/spark/sql/DataFrameStatFunctions$$anonfun$sampleBy$1/1(org.apache.spark.sql.DataFrameStatFunctions)
org/apache/spark/sql/DataFrameStatFunctions/sampleBy(java.lang.String,scala.collection.immutable.Map,long)#org/apache/spark/sql/Column$/apply(java.lang.String)
org/apache/spark/sql/DataFrameStatFunctions/sampleBy(java.lang.String,scala.collection.immutable.Map,long)#scala/reflect/api/TypeTags$TypeTag$/Boolean()
org/apache/spark/sql/DataFrameStatFunctions/sampleBy(java.lang.String,scala.collection.immutable.Map,long)#scala/reflect/api/TypeTags$TypeTag$/Any()
org/apache/spark/sql/DataFrameStatFunctions/sampleBy(java.lang.String,scala.collection.immutable.Map,long)#org/apache/spark/sql/functions$/rand(long)
org/apache/spark/sql/DataFrameStatFunctions/sampleBy(java.lang.String,scala.collection.immutable.Map,long)#org/apache/spark/sql/functions$/udf(scala.Function2,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/DataFrameStatFunctions/sampleBy(java.lang.String,scala.collection.immutable.Map,long)#scala/reflect/runtime/package$/universe()
org/apache/spark/sql/DataFrameStatFunctions/sampleBy(java.lang.String,scala.collection.immutable.Map,long)#scala/collection/Iterable/forall(scala.Function1)
org/apache/spark/sql/DataFrameStatFunctions/sampleBy(java.lang.String,scala.collection.immutable.Map,long)#scala/reflect/api/TypeTags/TypeTag()
org/apache/spark/sql/DataFrameStatFunctions/sampleBy(java.lang.String,scala.collection.immutable.Map,long)#scala/reflect/api/TypeTags$TypeTag$/Double()
org/apache/spark/sql/DataFrameStatFunctions/approxQuantile(scala.collection.immutable.List,scala.collection.immutable.List,double)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/DataFrameStatFunctions/approxQuantile(scala.collection.immutable.List,scala.collection.immutable.List,double)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/DataFrameStatFunctions/approxQuantile(scala.collection.immutable.List,scala.collection.immutable.List,double)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/DataFrameStatFunctions/approxQuantile(scala.collection.immutable.List,scala.collection.immutable.List,double)#scala/reflect/ClassTag$/Double()
org/apache/spark/sql/DataFrameStatFunctions/approxQuantile(scala.collection.immutable.List,scala.collection.immutable.List,double)#scala/collection/mutable/ArrayOps/toList()
org/apache/spark/sql/DataFrameStatFunctions/approxQuantile(scala.collection.immutable.List,scala.collection.immutable.List,double)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/DataFrameStatFunctions/approxQuantile(scala.collection.immutable.List,scala.collection.immutable.List,double)#scala/collection/convert/Decorators$AsJava/asJava()
org/apache/spark/sql/DataFrameStatFunctions/approxQuantile(scala.collection.immutable.List,scala.collection.immutable.List,double)#scala/collection/immutable/List/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/DataFrameStatFunctions/approxQuantile(scala.collection.immutable.List,scala.collection.immutable.List,double)#scala/collection/JavaConverters$/seqAsJavaListConverter(scala.collection.Seq)
org/apache/spark/sql/DataFrameStatFunctions/approxQuantile(scala.collection.immutable.List,scala.collection.immutable.List,double)#org/apache/spark/sql/DataFrameStatFunctions$$anonfun$approxQuantile$3/3(org.apache.spark.sql.DataFrameStatFunctions)
org/apache/spark/sql/DataFrameStatFunctions/approxQuantile(java.lang.String[],double[],double)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/DataFrameStatFunctions/approxQuantile(java.lang.String[],double[],double)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/DataFrameStatFunctions/approxQuantile(java.lang.String[],double[],double)#scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)
org/apache/spark/sql/DataFrameStatFunctions/approxQuantile(java.lang.String[],double[],double)#org/apache/spark/sql/execution/stat/StatFunctions$/multipleApproxQuantiles(org.apache.spark.sql.Dataset,scala.collection.Seq,scala.collection.Seq,double)
org/apache/spark/sql/DataFrameStatFunctions/approxQuantile(java.lang.String[],double[],double)#scala/Predef$/wrapDoubleArray(double[])
org/apache/spark/sql/DataFrameStatFunctions/approxQuantile(java.lang.String[],double[],double)#scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/DataFrameStatFunctions/approxQuantile(java.lang.String[],double[],double)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/DataFrameStatFunctions/approxQuantile(java.lang.String[],double[],double)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/DataFrameStatFunctions/approxQuantile(java.lang.String[],double[],double)#org/apache/spark/sql/DataFrameStatFunctions$$anonfun$approxQuantile$2/2(org.apache.spark.sql.DataFrameStatFunctions)
org/apache/spark/sql/DataFrameStatFunctions/approxQuantile(java.lang.String[],double[],double)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/DataFrameStatFunctions/approxQuantile(java.lang.String[],double[],double)#org/apache/spark/sql/DataFrameStatFunctions$$anonfun$approxQuantile$1/1(org.apache.spark.sql.DataFrameStatFunctions)
org/apache/spark/sql/DataFrameStatFunctions/approxQuantile(java.lang.String[],double[],double)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/DataFrameStatFunctions/approxQuantile(java.lang.String[],double[],double)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/DataFrameStatFunctions/buildBloomFilter(org.apache.spark.sql.Column,org.apache.spark.util.sketch.BloomFilter)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/DataFrameStatFunctions/buildBloomFilter(org.apache.spark.sql.Column,org.apache.spark.util.sketch.BloomFilter)#org/apache/spark/sql/DataFrameStatFunctions$$anonfun$9/9(org.apache.spark.sql.DataFrameStatFunctions)
org/apache/spark/sql/DataFrameStatFunctions/buildBloomFilter(org.apache.spark.sql.Column,org.apache.spark.util.sketch.BloomFilter)#org/apache/spark/sql/DataFrameStatFunctions$$anonfun$8/8(org.apache.spark.sql.DataFrameStatFunctions)
org/apache/spark/sql/DataFrameStatFunctions/buildBloomFilter(org.apache.spark.sql.Column,org.apache.spark.util.sketch.BloomFilter)#org/apache/spark/sql/DataFrameStatFunctions$$anonfun$buildBloomFilter$3/3(org.apache.spark.sql.DataFrameStatFunctions)
org/apache/spark/sql/DataFrameStatFunctions/buildBloomFilter(org.apache.spark.sql.Column,org.apache.spark.util.sketch.BloomFilter)#org/apache/spark/sql/types/StructField/dataType()
org/apache/spark/sql/DataFrameStatFunctions/buildBloomFilter(org.apache.spark.sql.Column,org.apache.spark.util.sketch.BloomFilter)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/DataFrameStatFunctions/buildBloomFilter(org.apache.spark.sql.Column,org.apache.spark.util.sketch.BloomFilter)#scala/Predef$/require(boolean,scala.Function0)
org/apache/spark/sql/DataFrameStatFunctions/buildBloomFilter(org.apache.spark.sql.Column,org.apache.spark.util.sketch.BloomFilter)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/DataFrameStatFunctions/buildBloomFilter(org.apache.spark.sql.Column,org.apache.spark.util.sketch.BloomFilter)#org/apache/spark/sql/DataFrameStatFunctions$$anonfun$buildBloomFilter$1/1(org.apache.spark.sql.DataFrameStatFunctions,org.apache.spark.sql.types.DataType)
org/apache/spark/sql/DataFrameStatFunctions/buildBloomFilter(org.apache.spark.sql.Column,org.apache.spark.util.sketch.BloomFilter)#org/apache/spark/sql/types/StructType/head()
org/apache/spark/sql/DataFrameStatFunctions/buildBloomFilter(org.apache.spark.sql.Column,org.apache.spark.util.sketch.BloomFilter)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/DataFrameStatFunctions/buildBloomFilter(org.apache.spark.sql.Column,org.apache.spark.util.sketch.BloomFilter)#org/apache/spark/sql/DataFrameStatFunctions$$anonfun$buildBloomFilter$2/2(org.apache.spark.sql.DataFrameStatFunctions,scala.Function2)
org/apache/spark/sql/DataFrameStatFunctions/buildBloomFilter(org.apache.spark.sql.Column,org.apache.spark.util.sketch.BloomFilter)#org/apache/spark/sql/DataFrameStatFunctions$$anonfun$7/7(org.apache.spark.sql.DataFrameStatFunctions)
org/apache/spark/sql/DataFrameStatFunctions/buildBloomFilter(org.apache.spark.sql.Column,org.apache.spark.util.sketch.BloomFilter)#org/apache/spark/rdd/RDD/aggregate(java.lang.Object,scala.Function2,scala.Function2,scala.reflect.ClassTag)
org/apache/spark/sql/DataFrameStatFunctions/buildBloomFilter(org.apache.spark.sql.Column,org.apache.spark.util.sketch.BloomFilter)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/DataFrameStatFunctions/buildBloomFilter(org.apache.spark.sql.Column,org.apache.spark.util.sketch.BloomFilter)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/DataFrameStatFunctions/buildBloomFilter(org.apache.spark.sql.Column,org.apache.spark.util.sketch.BloomFilter)#org/apache/spark/sql/DataFrameStatFunctions$$anonfun$11/11(org.apache.spark.sql.DataFrameStatFunctions)
org/apache/spark/sql/DataFrameStatFunctions/buildBloomFilter(org.apache.spark.sql.Column,org.apache.spark.util.sketch.BloomFilter)#org/apache/spark/sql/DataFrameStatFunctions$$anonfun$10/10(org.apache.spark.sql.DataFrameStatFunctions)
org/apache/spark/sql/DataFrameStatFunctions/buildBloomFilter(org.apache.spark.sql.Column,org.apache.spark.util.sketch.BloomFilter)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/DataFrameStatFunctions/buildBloomFilter(org.apache.spark.sql.Column,org.apache.spark.util.sketch.BloomFilter)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/DataFrameStatFunctions/buildBloomFilter(org.apache.spark.sql.Column,org.apache.spark.util.sketch.BloomFilter)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/DataFrameStatFunctions/buildBloomFilter(org.apache.spark.sql.Column,org.apache.spark.util.sketch.BloomFilter)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/DataFrameStatFunctions/countMinSketch(org.apache.spark.sql.Column,org.apache.spark.util.sketch.CountMinSketch)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/DataFrameStatFunctions/countMinSketch(org.apache.spark.sql.Column,org.apache.spark.util.sketch.CountMinSketch)#org/apache/spark/rdd/RDD/aggregate(java.lang.Object,scala.Function2,scala.Function2,scala.reflect.ClassTag)
org/apache/spark/sql/DataFrameStatFunctions/countMinSketch(org.apache.spark.sql.Column,org.apache.spark.util.sketch.CountMinSketch)#org/apache/spark/sql/DataFrameStatFunctions$$anonfun$countMinSketch$2/2(org.apache.spark.sql.DataFrameStatFunctions)
org/apache/spark/sql/DataFrameStatFunctions/countMinSketch(org.apache.spark.sql.Column,org.apache.spark.util.sketch.CountMinSketch)#org/apache/spark/sql/DataFrameStatFunctions$$anonfun$4/4(org.apache.spark.sql.DataFrameStatFunctions)
org/apache/spark/sql/DataFrameStatFunctions/countMinSketch(org.apache.spark.sql.Column,org.apache.spark.util.sketch.CountMinSketch)#org/apache/spark/sql/DataFrameStatFunctions$$anonfun$3/3(org.apache.spark.sql.DataFrameStatFunctions)
org/apache/spark/sql/DataFrameStatFunctions/countMinSketch(org.apache.spark.sql.Column,org.apache.spark.util.sketch.CountMinSketch)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/DataFrameStatFunctions/countMinSketch(org.apache.spark.sql.Column,org.apache.spark.util.sketch.CountMinSketch)#org/apache/spark/sql/DataFrameStatFunctions$$anonfun$2/2(org.apache.spark.sql.DataFrameStatFunctions)
org/apache/spark/sql/DataFrameStatFunctions/countMinSketch(org.apache.spark.sql.Column,org.apache.spark.util.sketch.CountMinSketch)#org/apache/spark/sql/types/StructType/head()
org/apache/spark/sql/DataFrameStatFunctions/countMinSketch(org.apache.spark.sql.Column,org.apache.spark.util.sketch.CountMinSketch)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/DataFrameStatFunctions/countMinSketch(org.apache.spark.sql.Column,org.apache.spark.util.sketch.CountMinSketch)#org/apache/spark/sql/DataFrameStatFunctions$$anonfun$countMinSketch$1/1(org.apache.spark.sql.DataFrameStatFunctions,scala.Function2)
org/apache/spark/sql/DataFrameStatFunctions/countMinSketch(org.apache.spark.sql.Column,org.apache.spark.util.sketch.CountMinSketch)#org/apache/spark/sql/types/StructField/dataType()
org/apache/spark/sql/DataFrameStatFunctions/countMinSketch(org.apache.spark.sql.Column,org.apache.spark.util.sketch.CountMinSketch)#org/apache/spark/sql/DataFrameStatFunctions$$anonfun$6/6(org.apache.spark.sql.DataFrameStatFunctions)
org/apache/spark/sql/DataFrameStatFunctions/countMinSketch(org.apache.spark.sql.Column,org.apache.spark.util.sketch.CountMinSketch)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/DataFrameStatFunctions/countMinSketch(org.apache.spark.sql.Column,org.apache.spark.util.sketch.CountMinSketch)#org/apache/spark/sql/DataFrameStatFunctions$$anonfun$5/5(org.apache.spark.sql.DataFrameStatFunctions)
org/apache/spark/sql/DataFrameStatFunctions/countMinSketch(org.apache.spark.sql.Column,org.apache.spark.util.sketch.CountMinSketch)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/DataFrameStatFunctions/countMinSketch(org.apache.spark.sql.Column,org.apache.spark.util.sketch.CountMinSketch)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/DataFrameStatFunctions/countMinSketch(org.apache.spark.sql.Column,org.apache.spark.util.sketch.CountMinSketch)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/DataFrameStatFunctions/countMinSketch(org.apache.spark.sql.Column,org.apache.spark.util.sketch.CountMinSketch)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/DataFrameStatFunctions/countMinSketch(org.apache.spark.sql.Column,org.apache.spark.util.sketch.CountMinSketch)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/DataFrameStatFunctions/countMinSketch(org.apache.spark.sql.Column,org.apache.spark.util.sketch.CountMinSketch)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/csv/CSVFileFormat/buildReader(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/csv/CSVFileFormat/buildReader(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/types/StructType/getFieldIndex(java.lang.String)
org/apache/spark/sql/execution/datasources/csv/CSVFileFormat/buildReader(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/datasources/csv/CSVUtils$/verifySchema(org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/datasources/csv/CSVFileFormat/buildReader(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/SparkContext/broadcast(java.lang.Object,scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/csv/CSVFileFormat/buildReader(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/internal/SQLConf/sessionLocalTimeZone()
org/apache/spark/sql/execution/datasources/csv/CSVFileFormat/buildReader(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/datasources/csv/CSVFileFormat$$anonfun$buildReader$1/1(org.apache.spark.sql.execution.datasources.csv.CSVFileFormat,org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/datasources/csv/CSVFileFormat/buildReader(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/datasources/csv/CSVFileFormat$$anonfun$buildReader$2/2(org.apache.spark.sql.execution.datasources.csv.CSVFileFormat,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.broadcast.Broadcast,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/CSVFileFormat/buildReader(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/internal/SQLConf/columnNameOfCorruptRecord()
org/apache/spark/sql/execution/datasources/csv/CSVFileFormat/buildReader(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#scala/Option/foreach(scala.Function1)
org/apache/spark/sql/execution/datasources/csv/CSVFileFormat/buildReader(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/util/SerializableConfiguration/SerializableConfiguration(org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/window/WindowExec/anonfun/14/anon/1/fetchNextRow()#org/apache/spark/sql/execution/window/WindowExec$$anonfun$14$$anon$1/nextRowAvailable()
org/apache/spark/sql/execution/window/WindowExec/anonfun/14/anon/1/fetchNextRow()#org/apache/spark/sql/execution/window/WindowExec$$anonfun$14$$anon$1/grouping()
org/apache/spark/sql/execution/window/WindowExec/anonfun/14/anon/1/fetchNextRow()#org/apache/spark/sql/execution/window/WindowExec$$anonfun$14$$anon$1/nextRow_$eq(org.apache.spark.sql.catalyst.expressions.UnsafeRow)
org/apache/spark/sql/execution/window/WindowExec/anonfun/14/anon/1/fetchNextRow()#scala/collection/Iterator/next()
org/apache/spark/sql/execution/window/WindowExec/anonfun/14/anon/1/fetchNextRow()#org/apache/spark/sql/catalyst/expressions/UnsafeProjection/apply(org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/window/WindowExec/anonfun/14/anon/1/fetchNextRow()#org/apache/spark/sql/execution/window/WindowExec$$anonfun$14$$anon$1/nextRowAvailable_$eq(boolean)
org/apache/spark/sql/execution/window/WindowExec/anonfun/14/anon/1/fetchNextRow()#scala/collection/Iterator/hasNext()
org/apache/spark/sql/execution/window/WindowExec/anonfun/14/anon/1/fetchNextRow()#org/apache/spark/sql/execution/window/WindowExec$$anonfun$14$$anon$1/nextGroup_$eq(org.apache.spark.sql.catalyst.expressions.UnsafeRow)
org/apache/spark/sql/execution/window/WindowExec/anonfun/14/anon/1/fetchNextRow()#org/apache/spark/sql/execution/window/WindowExec$$anonfun$14$$anon$1/nextRow()
org/apache/spark/sql/execution/window/WindowExec/anonfun/14/anon/1/next()#org/apache/spark/sql/execution/window/WindowExec$$anonfun$14$$anon$1/nextRowAvailable()
org/apache/spark/sql/execution/window/WindowExec/anonfun/14/anon/1/next()#org/apache/spark/sql/catalyst/expressions/JoinedRow/apply(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/window/WindowExec/anonfun/14/anon/1/next()#org/apache/spark/sql/execution/window/WindowExec$$anonfun$14$$anon$1/bufferIterator()
org/apache/spark/sql/execution/window/WindowExec/anonfun/14/anon/1/next()#org/apache/spark/sql/execution/window/WindowExec$$anonfun$14$$anon$1/next()
org/apache/spark/sql/execution/window/WindowExec/anonfun/14/anon/1/next()#java/util/NoSuchElementException/NoSuchElementException()
org/apache/spark/sql/execution/window/WindowExec/anonfun/14/anon/1/next()#org/apache/spark/sql/execution/window/WindowExec$$anonfun$14$$anon$1/rowIndex()
org/apache/spark/sql/execution/window/WindowExec/anonfun/14/anon/1/next()#org/apache/spark/sql/execution/window/WindowExec$$anonfun$14$$anon$1/numFrames()
org/apache/spark/sql/execution/window/WindowExec/anonfun/14/anon/1/next()#org/apache/spark/sql/execution/window/WindowExec$$anonfun$14$$anon$1/frames()
org/apache/spark/sql/execution/window/WindowExec/anonfun/14/anon/1/next()#scala/collection/Iterator/next()
org/apache/spark/sql/execution/window/WindowExec/anonfun/14/anon/1/next()#org/apache/spark/sql/execution/window/WindowExec$$anonfun$14$$anon$1/org$apache$spark$sql$execution$window$WindowExec$$anonfun$$anon$$windowFunctionResult()
org/apache/spark/sql/execution/window/WindowExec/anonfun/14/anon/1/next()#org/apache/spark/sql/catalyst/expressions/UnsafeProjection/apply(org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/window/WindowExec/anonfun/14/anon/1/next()#org/apache/spark/sql/execution/window/WindowExec$$anonfun$14$$anon$1/join()
org/apache/spark/sql/execution/window/WindowExec/anonfun/14/anon/1/next()#scala/collection/Iterator/hasNext()
org/apache/spark/sql/execution/window/WindowExec/anonfun/14/anon/1/next()#org/apache/spark/sql/execution/window/WindowExec$$anonfun$14$$anon$1/rowIndex_$eq(int)
org/apache/spark/sql/execution/window/WindowExec/anonfun/14/anon/1/next()#org/apache/spark/sql/execution/window/WindowExec$$anonfun$14$$anon$1/fetchNextPartition()
org/apache/spark/sql/execution/window/WindowExec/anonfun/14/anon/1/next()#org/apache/spark/sql/execution/window/WindowExec$$anonfun$14$$anon$1/result()
org/apache/spark/sql/execution/window/WindowExec/anonfun/14/anon/1/fetchNextPartition()#org/apache/spark/sql/execution/window/WindowExec$$anonfun$14$$anon$1/nextRowAvailable()
org/apache/spark/sql/execution/window/WindowExec/anonfun/14/anon/1/fetchNextPartition()#org/apache/spark/sql/execution/window/WindowExec$$anonfun$14$$anon$1/bufferIterator_$eq(scala.collection.Iterator)
org/apache/spark/sql/execution/window/WindowExec/anonfun/14/anon/1/fetchNextPartition()#org/apache/spark/sql/execution/window/WindowExec$$anonfun$14$$anon$1/buffer()
org/apache/spark/sql/execution/window/WindowExec/anonfun/14/anon/1/fetchNextPartition()#org/apache/spark/sql/execution/window/WindowExec$$anonfun$14$$anon$1/nextRow()
org/apache/spark/sql/execution/window/WindowExec/anonfun/14/anon/1/fetchNextPartition()#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/window/WindowExec/anonfun/14/anon/1/fetchNextPartition()#org/apache/spark/sql/execution/window/WindowExec$$anonfun$14$$anon$1/nextGroup()
org/apache/spark/sql/execution/window/WindowExec/anonfun/14/anon/1/fetchNextPartition()#org/apache/spark/sql/execution/window/WindowExec$$anonfun$14$$anon$1/numFrames()
org/apache/spark/sql/execution/window/WindowExec/anonfun/14/anon/1/fetchNextPartition()#org/apache/spark/sql/execution/window/WindowExec$$anonfun$14$$anon$1/frames()
org/apache/spark/sql/execution/window/WindowExec/anonfun/14/anon/1/fetchNextPartition()#org/apache/spark/sql/execution/window/WindowExec$$anonfun$14$$anon$1/fetchNextRow()
org/apache/spark/sql/execution/window/WindowExec/anonfun/14/anon/1/fetchNextPartition()#org/apache/spark/sql/catalyst/expressions/UnsafeRow/copy()
org/apache/spark/sql/execution/window/WindowExec/anonfun/14/anon/1/fetchNextPartition()#org/apache/spark/sql/execution/window/WindowExec$$anonfun$14$$anon$1/rowIndex_$eq(int)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/empty()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/AnalyzeColumnCommand$$anonfun$1/1(org.apache.spark.sql.execution.command.AnalyzeColumnCommand,org.apache.spark.sql.internal.SessionState)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$1()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTableType$/VIEW()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#scala/Tuple2/_1$mcJ$sp()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/stats()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/TableIdentifier/table()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/alterTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/refreshTable(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#scala/math/BigInt$/long2bigInt(long)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#scala/Tuple2/_2()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/tableType()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/AnalyzeColumnCommand$$anonfun$3/3(org.apache.spark.sql.execution.command.AnalyzeColumnCommand)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/immutable/MapLike/$plus$plus(scala.collection.GenTraversableOnce)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogStatistics/CatalogStatistics(scala.math.BigInt,scala.Option,scala.collection.immutable.Map)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$2()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/AnalyzeColumnCommand$$anonfun$2/2(org.apache.spark.sql.execution.command.AnalyzeColumnCommand)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$3()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$10()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$4()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$11()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$5()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$6()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$13()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$7()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$15()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$8()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$16()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/TableIdentifier/database()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$9()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$17()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/TableIdentifier/TableIdentifier(java.lang.String,scala.Option)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/AnalyzeTableCommand$/calculateTotalSize(org.apache.spark.sql.internal.SessionState,org.apache.spark.sql.catalyst.catalog.CatalogTable)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/computeColumnStats(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.TableIdentifier,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/aggregate/Count/toAggregateExpression()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/computeColumnStats(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.TableIdentifier,scala.collection.Seq)#org/apache/spark/sql/execution/command/AnalyzeColumnCommand$$anonfun$8/8(org.apache.spark.sql.execution.command.AnalyzeColumnCommand,org.apache.spark.sql.Row)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/computeColumnStats(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.TableIdentifier,scala.collection.Seq)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/computeColumnStats(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.TableIdentifier,scala.collection.Seq)#scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/computeColumnStats(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.TableIdentifier,scala.collection.Seq)#scala/collection/Seq/zipWithIndex(scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/computeColumnStats(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.TableIdentifier,scala.collection.Seq)#scala/collection/SeqLike/$plus$colon(java.lang.Object,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/computeColumnStats(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.TableIdentifier,scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/computeColumnStats(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.TableIdentifier,scala.collection.Seq)#org/apache/spark/sql/internal/SQLConf/resolver()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/computeColumnStats(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.TableIdentifier,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/aggregate/Count$/apply(org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/computeColumnStats(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.TableIdentifier,scala.collection.Seq)#org/apache/spark/sql/execution/command/AnalyzeColumnCommand$$anonfun$7/7(org.apache.spark.sql.execution.command.AnalyzeColumnCommand)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/computeColumnStats(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.TableIdentifier,scala.collection.Seq)#org/apache/spark/sql/Row/getLong(int)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/computeColumnStats(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.TableIdentifier,scala.collection.Seq)#scala/collection/Seq/foreach(scala.Function1)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/computeColumnStats(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.TableIdentifier,scala.collection.Seq)#org/apache/spark/sql/execution/command/AnalyzeColumnCommand$$anonfun$computeColumnStats$1/1(org.apache.spark.sql.execution.command.AnalyzeColumnCommand,org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/computeColumnStats(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.TableIdentifier,scala.collection.Seq)#org/apache/spark/sql/execution/command/AnalyzeColumnCommand$$anonfun$6/6(org.apache.spark.sql.execution.command.AnalyzeColumnCommand,double)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/computeColumnStats(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.TableIdentifier,scala.collection.Seq)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/computeColumnStats(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.TableIdentifier,scala.collection.Seq)#org/apache/spark/sql/Dataset$/ofRows(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/computeColumnStats(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.TableIdentifier,scala.collection.Seq)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/computeColumnStats(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.TableIdentifier,scala.collection.Seq)#scala/Predef$/$conforms()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/computeColumnStats(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.TableIdentifier,scala.collection.Seq)#org/apache/spark/sql/internal/SQLConf/ndvMaxError()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/computeColumnStats(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.TableIdentifier,scala.collection.Seq)#org/apache/spark/sql/execution/command/AnalyzeColumnCommand$$anonfun$4/4(org.apache.spark.sql.execution.command.AnalyzeColumnCommand,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function2)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/computeColumnStats(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.TableIdentifier,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/Literal$/apply(java.lang.Object)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/computeColumnStats(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.TableIdentifier,scala.collection.Seq)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/computeColumnStats(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.TableIdentifier,scala.collection.Seq)#org/apache/spark/sql/catalyst/plans/logical/Aggregate/Aggregate(scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/computeColumnStats(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.TableIdentifier,scala.collection.Seq)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitExplain/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$ExplainContext/EXTENDED()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitExplain/1/apply()#org/apache/spark/sql/catalyst/parser/ParserUtils$/operationNotAllowed(java.lang.String,org.antlr.v4.runtime.ParserRuleContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitExplain/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$ExplainContext/FORMATTED()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitExplain/1/apply()#org/apache/spark/sql/execution/command/ExplainCommand$/apply$default$2()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitExplain/1/apply()#org/apache/spark/sql/execution/command/ExplainCommand$/apply$default$4()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitExplain/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$ExplainContext/COST()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitExplain/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$ExplainContext/LOGICAL()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitExplain/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$ExplainContext/statement()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitExplain/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder/plan(org.antlr.v4.runtime.ParserRuleContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitExplain/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitExplain$1/apply()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitExplain/1/apply()#org/apache/spark/sql/execution/command/ExplainCommand$/apply$default$3()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitExplain/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$ExplainContext/CODEGEN()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeCatalogTable/1/anonfun/apply/9/apply(java.lang.String)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeCatalogTable/1/anonfun/apply/9/apply(java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeCatalogTable/1/anonfun/apply/9/apply(java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeCatalogTable/1/anonfun/apply/9/apply(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeCatalogTable/1/anonfun/apply/9/apply(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeCatalogTable/1/anonfun/apply/9/apply(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeCatalogTable/1/anonfun/apply/9/apply(java.lang.String)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeCatalogTable/1/anonfun/apply/9/apply(java.lang.String)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeCatalogTable/1/anonfun/apply/9/apply(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeCatalogTable/1/anonfun/apply/9/apply(java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeCatalogTable/1/anonfun/apply/9/apply(java.lang.String)#scala/collection/Seq/mkString(java.lang.String)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeCatalogTable/1/anonfun/apply/9/apply(java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeCatalogTable/1/anonfun/apply/9/apply(java.lang.String)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$/createAggregate(scala.Option,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,int,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/length()
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$/createAggregate$default$5()
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$/createAggregate$default$1()
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$27/27()
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$28/28()
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$29/29()
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$30/30()
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$31/31()
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$32/32()
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$33/33()
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$34/34()
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$35/35()
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$36/36()
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$26/26()
org/apache/spark/sql/execution/aggregate/AggUtils/planStreamingAggregation(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$37/37()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/catalyst/expressions/aggregate/AggregateFunction/children()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/SeqLike/length()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/Tuple2/_2()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/zip(scala.collection.GenIterable,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/generic/GenericTraversableTemplate/unzip(scala.Function1)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$9/9()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/head()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$/createAggregate(scala.Option,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,int,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/length()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$/createAggregate$default$5()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/zipWithIndex(scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$20/20(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$25/25(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/catalyst/expressions/aggregate/AggregateExpression/aggregateFunction()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/Tuple2/_1()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$8/8()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$17/17(scala.collection.immutable.Map)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/Predef$/$conforms()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$/createAggregate$default$1()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$15/15()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$16/16()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$18/18()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$19/19()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$14/14()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$10/10()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$21/21()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$11/11()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$22/22()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$23/23()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$12/12()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$13/13()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$24/24()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithoutDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$2/2()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithoutDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$3/3()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithoutDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$4/4()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithoutDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$5/5()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithoutDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$6/6()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithoutDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$$anonfun$7/7()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithoutDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithoutDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$/planAggregateWithoutDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithoutDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithoutDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithoutDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$/createAggregate(scala.Option,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,int,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithoutDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/length()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithoutDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithoutDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/aggregate/AggUtils/planAggregateWithoutDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)#scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)
org/apache/spark/sql/execution/QueryExecution/anonfun/org/apache/spark/sql/execution/QueryExecution/toHiveStructString/1/1/apply(scala.Tuple2)#org/apache/spark/sql/types/StructField/dataType()
org/apache/spark/sql/execution/QueryExecution/anonfun/org/apache/spark/sql/execution/QueryExecution/toHiveStructString/1/1/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/QueryExecution/anonfun/org/apache/spark/sql/execution/QueryExecution/toHiveStructString/1/1/apply(scala.Tuple2)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/QueryExecution/anonfun/org/apache/spark/sql/execution/QueryExecution/toHiveStructString/1/1/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/QueryExecution/anonfun/org/apache/spark/sql/execution/QueryExecution/toHiveStructString/1/1/apply(scala.Tuple2)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/QueryExecution/anonfun/org/apache/spark/sql/execution/QueryExecution/toHiveStructString/1/1/apply(scala.Tuple2)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/QueryExecution/anonfun/org/apache/spark/sql/execution/QueryExecution/toHiveStructString/1/1/apply(scala.Tuple2)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/QueryExecution/anonfun/org/apache/spark/sql/execution/QueryExecution/toHiveStructString/1/1/apply(scala.Tuple2)#scala/Tuple2/_2()
org/apache/spark/sql/execution/QueryExecution/anonfun/org/apache/spark/sql/execution/QueryExecution/toHiveStructString/1/1/apply(scala.Tuple2)#org/apache/spark/sql/types/StructField/name()
org/apache/spark/sql/execution/QueryExecution/anonfun/org/apache/spark/sql/execution/QueryExecution/toHiveStructString/1/1/apply(scala.Tuple2)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/FindDataSourceTable/org$apache$spark$sql$execution$datasources$FindDataSourceTable$$readDataSourceTable(org.apache.spark.sql.catalyst.catalog.CatalogRelation)#org/apache/spark/sql/catalyst/QualifiedTableName/QualifiedTableName(java.lang.String,java.lang.String)
org/apache/spark/sql/execution/datasources/FindDataSourceTable/org$apache$spark$sql$execution$datasources$FindDataSourceTable$$readDataSourceTable(org.apache.spark.sql.catalyst.catalog.CatalogRelation)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/getCachedPlan(org.apache.spark.sql.catalyst.QualifiedTableName,java.util.concurrent.Callable)
org/apache/spark/sql/execution/datasources/FindDataSourceTable/org$apache$spark$sql$execution$datasources$FindDataSourceTable$$readDataSourceTable(org.apache.spark.sql.catalyst.catalog.CatalogRelation)#org/apache/spark/sql/catalyst/catalog/CatalogTable/database()
org/apache/spark/sql/execution/datasources/FindDataSourceTable/org$apache$spark$sql$execution$datasources$FindDataSourceTable$$readDataSourceTable(org.apache.spark.sql.catalyst.catalog.CatalogRelation)#org/apache/spark/sql/catalyst/catalog/CatalogRelation/tableMeta()
org/apache/spark/sql/execution/datasources/FindDataSourceTable/org$apache$spark$sql$execution$datasources$FindDataSourceTable$$readDataSourceTable(org.apache.spark.sql.catalyst.catalog.CatalogRelation)#org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()
org/apache/spark/sql/execution/datasources/FindDataSourceTable/org$apache$spark$sql$execution$datasources$FindDataSourceTable$$readDataSourceTable(org.apache.spark.sql.catalyst.catalog.CatalogRelation)#scala/collection/Seq/isEmpty()
org/apache/spark/sql/execution/datasources/FindDataSourceTable/org$apache$spark$sql$execution$datasources$FindDataSourceTable$$readDataSourceTable(org.apache.spark.sql.catalyst.catalog.CatalogRelation)#org/apache/spark/sql/catalyst/TableIdentifier/table()
org/apache/spark/sql/execution/datasources/FindDataSourceTable/org$apache$spark$sql$execution$datasources$FindDataSourceTable$$readDataSourceTable(org.apache.spark.sql.catalyst.catalog.CatalogRelation)#org/apache/spark/sql/execution/datasources/FindDataSourceTable$$anon$1/1(org.apache.spark.sql.execution.datasources.FindDataSourceTable,org.apache.spark.sql.catalyst.catalog.CatalogTable)
org/apache/spark/sql/execution/datasources/FindDataSourceTable/org$apache$spark$sql$execution$datasources$FindDataSourceTable$$readDataSourceTable(org.apache.spark.sql.catalyst.catalog.CatalogRelation)#org/apache/spark/sql/catalyst/catalog/CatalogRelation/output()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/anonfun/convertGroupField/2/apply(org.apache.parquet.schema.OriginalType)#org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter$$anonfun$convertGroupField$2$$anonfun$apply$1/1(org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter$$anonfun$convertGroupField$2)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/anonfun/convertGroupField/2/apply(org.apache.parquet.schema.OriginalType)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/anonfun/convertGroupField/2/apply(org.apache.parquet.schema.OriginalType)#org/apache/spark/sql/types/MapType/MapType(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType,boolean)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/anonfun/convertGroupField/2/apply(org.apache.parquet.schema.OriginalType)#org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter$$anonfun$convertGroupField$2$$anonfun$apply$3/3(org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter$$anonfun$convertGroupField$2)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/anonfun/convertGroupField/2/apply(org.apache.parquet.schema.OriginalType)#org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter$$anonfun$convertGroupField$2$$anonfun$apply$5/5(org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter$$anonfun$convertGroupField$2,org.apache.parquet.schema.Type)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/anonfun/convertGroupField/2/apply(org.apache.parquet.schema.OriginalType)#org/apache/parquet/schema/Type/asGroupType()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/anonfun/convertGroupField/2/apply(org.apache.parquet.schema.OriginalType)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/anonfun/convertGroupField/2/apply(org.apache.parquet.schema.OriginalType)#org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter$$anonfun$convertGroupField$2$$anonfun$apply$2/2(org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter$$anonfun$convertGroupField$2,org.apache.parquet.schema.Type)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/anonfun/convertGroupField/2/apply(org.apache.parquet.schema.OriginalType)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/anonfun/convertGroupField/2/apply(org.apache.parquet.schema.OriginalType)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/anonfun/convertGroupField/2/apply(org.apache.parquet.schema.OriginalType)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/anonfun/convertGroupField/2/apply(org.apache.parquet.schema.OriginalType)#org/apache/parquet/schema/Type/isRepetition(org.apache.parquet.schema.Type$Repetition)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/anonfun/convertGroupField/2/apply(org.apache.parquet.schema.OriginalType)#org/apache/spark/sql/types/ArrayType/ArrayType(org.apache.spark.sql.types.DataType,boolean)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/anonfun/convertGroupField/2/apply(org.apache.parquet.schema.OriginalType)#org/apache/parquet/schema/GroupType/getName()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/anonfun/convertGroupField/2/apply(org.apache.parquet.schema.OriginalType)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/anonfun/convertGroupField/2/apply(org.apache.parquet.schema.OriginalType)#org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter$/checkConversionRequirement(scala.Function0,java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/anonfun/convertGroupField/2/apply(org.apache.parquet.schema.OriginalType)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/anonfun/convertGroupField/2/apply(org.apache.parquet.schema.OriginalType)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/anonfun/convertGroupField/2/apply(org.apache.parquet.schema.OriginalType)#org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter$$anonfun$convertGroupField$2$$anonfun$apply$4/4(org.apache.spark.sql.execution.datasources.parquet.ParquetSchemaConverter$$anonfun$convertGroupField$2,org.apache.parquet.schema.GroupType)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/anonfun/convertGroupField/2/apply(org.apache.parquet.schema.OriginalType)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/anonfun/convertGroupField/2/apply(org.apache.parquet.schema.OriginalType)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetSchemaConverter/anonfun/convertGroupField/2/apply(org.apache.parquet.schema.OriginalType)#org/apache/parquet/schema/GroupType/getType(int)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/genEqualsForKeys/1/1/apply(scala.Tuple2)#org/apache/spark/sql/execution/aggregate/HashMapGenerator$Buffer/dataType()
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/genEqualsForKeys/1/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/getValue(java.lang.String,org.apache.spark.sql.types.DataType,java.lang.String)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/genEqualsForKeys/1/1/apply(scala.Tuple2)#java/lang/Object/toString()
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/genEqualsForKeys/1/1/apply(scala.Tuple2)#org/apache/spark/sql/execution/aggregate/HashMapGenerator$Buffer/name()
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/genEqualsForKeys/1/1/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/genEqualsForKeys/1/1/apply(scala.Tuple2)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/genEqualsForKeys/1/1/apply(scala.Tuple2)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/genEqualsForKeys/1/1/apply(scala.Tuple2)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/genEqualsForKeys/1/1/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/genEqualsForKeys/1/1/apply(scala.Tuple2)#scala/Tuple2/_2$mcI$sp()
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/genEqualsForKeys/1/1/apply(scala.Tuple2)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/genEqualsForKeys/1/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/genEqual(org.apache.spark.sql.types.DataType,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/genEqualsForKeys/1/1/apply(scala.Tuple2)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/Dataset/randomSplit(double[],long)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/Dataset/randomSplit(double[],long)#scala/collection/Iterator/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/Dataset/randomSplit(double[],long)#scala/collection/mutable/ArrayOps/sliding(int)
org/apache/spark/sql/Dataset/randomSplit(double[],long)#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/Dataset/randomSplit(double[],long)#scala/runtime/BoxesRunTime/unboxToDouble(java.lang.Object)
org/apache/spark/sql/Dataset/randomSplit(double[],long)#scala/collection/Iterator/map(scala.Function1)
org/apache/spark/sql/Dataset/randomSplit(double[],long)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/Dataset/randomSplit(double[],long)#org/apache/spark/sql/Dataset$$anonfun$randomSplit$3/3(org.apache.spark.sql.Dataset,double[])
org/apache/spark/sql/Dataset/randomSplit(double[],long)#org/apache/spark/sql/Dataset$$anonfun$randomSplit$2/2(org.apache.spark.sql.Dataset,double[])
org/apache/spark/sql/Dataset/randomSplit(double[],long)#scala/collection/mutable/ArrayOps/forall(scala.Function1)
org/apache/spark/sql/Dataset/randomSplit(double[],long)#org/apache/spark/sql/Dataset$$anonfun$randomSplit$4/4(org.apache.spark.sql.Dataset,long,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/Dataset/randomSplit(double[],long)#org/apache/spark/sql/Dataset$$anonfun$22/22(org.apache.spark.sql.Dataset)
org/apache/spark/sql/Dataset/randomSplit(double[],long)#org/apache/spark/sql/Dataset$$anonfun$23/23(org.apache.spark.sql.Dataset)
org/apache/spark/sql/Dataset/randomSplit(double[],long)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/Dataset/randomSplit(double[],long)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/Dataset/randomSplit(double[],long)#scala/Predef$/doubleArrayOps(double[])
org/apache/spark/sql/Dataset/randomSplit(double[],long)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/Dataset/randomSplit(double[],long)#scala/collection/Seq/filter(scala.Function1)
org/apache/spark/sql/Dataset/randomSplit(double[],long)#org/apache/spark/sql/Dataset$$anonfun$3/3(org.apache.spark.sql.Dataset)
org/apache/spark/sql/Dataset/randomSplit(double[],long)#scala/Predef$/require(boolean,scala.Function0)
org/apache/spark/sql/Dataset/randomSplit(double[],long)#scala/collection/mutable/ArrayOps/scanLeft(java.lang.Object,scala.Function2,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/Dataset/randomSplit(double[],long)#org/apache/spark/sql/Dataset$$anonfun$2/2(org.apache.spark.sql.Dataset,double)
org/apache/spark/sql/Dataset/randomSplit(double[],long)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/output()
org/apache/spark/sql/Dataset/randomSplit(double[],long)#scala/runtime/BoxesRunTime/boxToDouble(double)
org/apache/spark/sql/Dataset/randomSplit(double[],long)#scala/reflect/ClassTag$/Double()
org/apache/spark/sql/Dataset/randomSplit(double[],long)#scala/collection/mutable/ArrayOps/sum(scala.math.Numeric)
org/apache/spark/sql/Dataset/randomSplit(double[],long)#org/apache/spark/sql/catalyst/plans/logical/Sort/Sort(scala.collection.Seq,boolean,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/Dataset/randomSplit(double[],long)#org/apache/spark/sql/Dataset$$anonfun$randomSplit$1/1(org.apache.spark.sql.Dataset)
org/apache/spark/sql/Dataset/toDF(scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/Dataset/toDF(scala.collection.Seq)#scala/collection/Seq/zip(scala.collection.GenIterable,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/Dataset/toDF(scala.collection.Seq)#scala/Predef$/require(boolean,scala.Function0)
org/apache/spark/sql/Dataset/toDF(scala.collection.Seq)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/output()
org/apache/spark/sql/Dataset/toDF(scala.collection.Seq)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/Dataset/toDF(scala.collection.Seq)#org/apache/spark/sql/types/StructType/size()
org/apache/spark/sql/Dataset/toDF(scala.collection.Seq)#org/apache/spark/sql/Dataset$$anonfun$11/11(org.apache.spark.sql.Dataset)
org/apache/spark/sql/Dataset/toDF(scala.collection.Seq)#org/apache/spark/sql/Dataset$$anonfun$toDF$1/1(org.apache.spark.sql.Dataset,scala.collection.Seq)
org/apache/spark/sql/Dataset/toDF(scala.collection.Seq)#scala/collection/Seq/size()
org/apache/spark/sql/Dataset/withColumn(java.lang.String,org.apache.spark.sql.Column)#org/apache/spark/sql/catalyst/analysis/Analyzer/resolver()
org/apache/spark/sql/Dataset/withColumn(java.lang.String,org.apache.spark.sql.Column)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/Dataset/withColumn(java.lang.String,org.apache.spark.sql.Column)#scala/collection/Seq/exists(scala.Function1)
org/apache/spark/sql/Dataset/withColumn(java.lang.String,org.apache.spark.sql.Column)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/output()
org/apache/spark/sql/Dataset/withColumn(java.lang.String,org.apache.spark.sql.Column)#org/apache/spark/sql/Column$/apply(java.lang.String)
org/apache/spark/sql/Dataset/withColumn(java.lang.String,org.apache.spark.sql.Column)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/Dataset/withColumn(java.lang.String,org.apache.spark.sql.Column)#org/apache/spark/sql/Dataset$$anonfun$28/28(org.apache.spark.sql.Dataset,java.lang.String,org.apache.spark.sql.Column,scala.Function2)
org/apache/spark/sql/Dataset/withColumn(java.lang.String,org.apache.spark.sql.Column)#org/apache/spark/sql/Dataset$$anonfun$27/27(org.apache.spark.sql.Dataset,java.lang.String,scala.Function2)
org/apache/spark/sql/Dataset/withColumn(java.lang.String,org.apache.spark.sql.Column)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/Dataset/explode(java.lang.String,java.lang.String,scala.Function1,scala.reflect.api.TypeTags$TypeTag)#scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)
org/apache/spark/sql/Dataset/explode(java.lang.String,java.lang.String,scala.Function1,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/Dataset$$anonfun$26/26(org.apache.spark.sql.Dataset,scala.Function1,org.apache.spark.sql.types.DataType)
org/apache/spark/sql/Dataset/explode(java.lang.String,java.lang.String,scala.Function1,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/expressions/AttributeReference$/apply$default$6(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.types.Metadata)
org/apache/spark/sql/Dataset/explode(java.lang.String,java.lang.String,scala.Function1,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/expressions/AttributeReference$/apply$default$7(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.types.Metadata)
org/apache/spark/sql/Dataset/explode(java.lang.String,java.lang.String,scala.Function1,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/expressions/package$AttributeSeq/toStructType()
org/apache/spark/sql/Dataset/explode(java.lang.String,java.lang.String,scala.Function1,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/Dataset/explode(java.lang.String,java.lang.String,scala.Function1,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/expressions/package$/AttributeSeq(scala.collection.Seq)
org/apache/spark/sql/Dataset/explode(java.lang.String,java.lang.String,scala.Function1,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/expressions/AttributeReference$/apply$default$3()
org/apache/spark/sql/Dataset/explode(java.lang.String,java.lang.String,scala.Function1,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/expressions/AttributeReference$/apply$default$4()
org/apache/spark/sql/Dataset/explode(java.lang.String,java.lang.String,scala.Function1,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/expressions/UserDefinedGenerator/UserDefinedGenerator(org.apache.spark.sql.types.StructType,scala.Function1,scala.collection.Seq)
org/apache/spark/sql/Dataset/explode(java.lang.String,java.lang.String,scala.Function1,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/expressions/AttributeReference$/apply$default$5(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.types.Metadata)
org/apache/spark/sql/Dataset/explode(java.lang.String,java.lang.String,scala.Function1,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/expressions/AttributeReference/AttributeReference(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.types.Metadata,org.apache.spark.sql.catalyst.expressions.ExprId,scala.Option,java.lang.Boolean)
org/apache/spark/sql/Dataset/explode(java.lang.String,java.lang.String,scala.Function1,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()
org/apache/spark/sql/Dataset/explode(java.lang.String,java.lang.String,scala.Function1,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/Dataset$$anonfun$explode$2/2(org.apache.spark.sql.Dataset,org.apache.spark.sql.catalyst.expressions.UserDefinedGenerator)
org/apache/spark/sql/Dataset/toString()#scala/util/control/NonFatal$/unapply(java.lang.Throwable)
org/apache/spark/sql/Dataset/toString()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/Dataset/toString()#scala/collection/mutable/StringBuilder/append(java.lang.String)
org/apache/spark/sql/Dataset/toString()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/Dataset/toString()#java/lang/Throwable/getMessage()
org/apache/spark/sql/Dataset/toString()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/Dataset/toString()#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/Dataset/toString()#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/Dataset/toString()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/Dataset/toString()#scala/Option/get()
org/apache/spark/sql/Dataset/toString()#org/apache/spark/sql/types/StructType/length()
org/apache/spark/sql/Dataset/toString()#org/apache/spark/sql/Dataset$$anonfun$10/10(org.apache.spark.sql.Dataset)
org/apache/spark/sql/Dataset/toString()#scala/Option/isEmpty()
org/apache/spark/sql/Dataset/toString()#org/apache/spark/sql/types/StructType/take(int)
org/apache/spark/sql/Dataset/toString()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/Dataset/toString()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/Dataset/toString()#scala/collection/Seq/size()
org/apache/spark/sql/Dataset/toString()#scala/collection/Seq/mkString(java.lang.String)
org/apache/spark/sql/Dataset/toString()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/Dataset/toString()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/Dataset/showString(int,int)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/Dataset/showString(int,int)#scala/collection/mutable/ArrayOps/$plus$colon(java.lang.Object,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/Dataset/showString(int,int)#org/apache/spark/sql/Dataset$$anonfun$1/1(org.apache.spark.sql.Dataset)
org/apache/spark/sql/Dataset/showString(int,int)#org/apache/spark/sql/Dataset$$anonfun$showString$1/1(org.apache.spark.sql.Dataset,int[])
org/apache/spark/sql/Dataset/showString(int,int)#scala/Predef$DummyImplicit$/dummyImplicit()
org/apache/spark/sql/Dataset/showString(int,int)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/Dataset/showString(int,int)#scala/collection/TraversableOnce/addString(scala.collection.mutable.StringBuilder,java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/Dataset/showString(int,int)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/Dataset/showString(int,int)#scala/collection/mutable/ArrayOps/take(int)
org/apache/spark/sql/Dataset/showString(int,int)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/Dataset/showString(int,int)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/Dataset/showString(int,int)#scala/collection/IterableLike/zipWithIndex(scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/Dataset/showString(int,int)#org/apache/spark/sql/Dataset$$anonfun$7/7(org.apache.spark.sql.Dataset,int,scala.runtime.ObjectRef,scala.runtime.VolatileByteRef)
org/apache/spark/sql/Dataset/showString(int,int)#scala/Predef$/intWrapper(int)
org/apache/spark/sql/Dataset/showString(int,int)#scala/reflect/ClassTag$/Int()
org/apache/spark/sql/Dataset/showString(int,int)#scala/collection/Seq/tail()
org/apache/spark/sql/Dataset/showString(int,int)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/Dataset/showString(int,int)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/Dataset/showString(int,int)#scala/Array$/fill(int,scala.Function0,scala.reflect.ClassTag)
org/apache/spark/sql/Dataset/showString(int,int)#scala/collection/mutable/ArrayOps/addString(scala.collection.mutable.StringBuilder,java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/Dataset/showString(int,int)#org/apache/spark/sql/Dataset$$anonfun$showString$2/2(org.apache.spark.sql.Dataset,int,int[])
org/apache/spark/sql/Dataset/showString(int,int)#scala/collection/mutable/StringBuilder/append(java.lang.String)
org/apache/spark/sql/Dataset/showString(int,int)#scala/Predef$/intArrayOps(int[])
org/apache/spark/sql/Dataset/showString(int,int)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/Dataset/showString(int,int)#org/apache/spark/sql/types/StructType/fieldNames()
org/apache/spark/sql/Dataset/showString(int,int)#scala/collection/Seq/foreach(scala.Function1)
org/apache/spark/sql/Dataset/showString(int,int)#scala/Array$/fallbackCanBuildFrom(scala.Predef$DummyImplicit)
org/apache/spark/sql/Dataset/showString(int,int)#scala/runtime/VolatileByteRef/create(byte)
org/apache/spark/sql/Dataset/showString(int,int)#scala/collection/Seq/head()
org/apache/spark/sql/Dataset/showString(int,int)#org/apache/spark/sql/Dataset$$anonfun$9/9(org.apache.spark.sql.Dataset)
org/apache/spark/sql/Dataset/showString(int,int)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/Dataset/showString(int,int)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/Dataset/showString(int,int)#scala/runtime/RichInt$/max$extension(int,int)
org/apache/spark/sql/Dataset/showString(int,int)#scala/collection/mutable/ArrayOps/toSeq()
org/apache/spark/sql/Dataset/showString(int,int)#org/apache/spark/sql/Dataset$$anonfun$showString$3/3(org.apache.spark.sql.Dataset,int,scala.collection.mutable.StringBuilder,int[])
org/apache/spark/sql/Dataset/showString(int,int)#scala/runtime/ObjectRef/zero()
org/apache/spark/sql/Dataset/showString(int,int)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/Dataset/showString(int,int)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/Dataset/showString(int,int)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/Dataset/joinWith(org.apache.spark.sql.Dataset,org.apache.spark.sql.Column,java.lang.String)#org/apache/spark/sql/Dataset$$anonfun$joinWith$1/1(org.apache.spark.sql.Dataset,org.apache.spark.sql.catalyst.plans.logical.Join,org.apache.spark.sql.catalyst.plans.logical.Project,org.apache.spark.sql.catalyst.plans.logical.Project,org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/Dataset/joinWith(org.apache.spark.sql.Dataset,org.apache.spark.sql.Column,java.lang.String)#org/apache/spark/sql/catalyst/expressions/Alias/Alias(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String,org.apache.spark.sql.catalyst.expressions.ExprId,scala.Option,scala.Option,java.lang.Boolean)
org/apache/spark/sql/Dataset/joinWith(org.apache.spark.sql.Dataset,org.apache.spark.sql.Column,java.lang.String)#org/apache/spark/sql/catalyst/encoders/ExpressionEncoder/flat()
org/apache/spark/sql/Dataset/joinWith(org.apache.spark.sql.Dataset,org.apache.spark.sql.Column,java.lang.String)#org/apache/spark/sql/Dataset$$anonfun$4/4(org.apache.spark.sql.Dataset,org.apache.spark.sql.Dataset,org.apache.spark.sql.catalyst.plans.logical.Join,org.apache.spark.sql.catalyst.plans.logical.Project,org.apache.spark.sql.catalyst.plans.logical.Project)
org/apache/spark/sql/Dataset/joinWith(org.apache.spark.sql.Dataset,org.apache.spark.sql.Column,java.lang.String)#org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$5(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)
org/apache/spark/sql/Dataset/joinWith(org.apache.spark.sql.Dataset,org.apache.spark.sql.Column,java.lang.String)#org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$4(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)
org/apache/spark/sql/Dataset/joinWith(org.apache.spark.sql.Dataset,org.apache.spark.sql.Column,java.lang.String)#org/apache/spark/sql/catalyst/trees/TreeNode/transformUp(scala.PartialFunction)
org/apache/spark/sql/Dataset/joinWith(org.apache.spark.sql.Dataset,org.apache.spark.sql.Column,java.lang.String)#scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)
org/apache/spark/sql/Dataset/joinWith(org.apache.spark.sql.Dataset,org.apache.spark.sql.Column,java.lang.String)#org/apache/spark/sql/catalyst/plans/JoinType$/apply(java.lang.String)
org/apache/spark/sql/Dataset/joinWith(org.apache.spark.sql.Dataset,org.apache.spark.sql.Column,java.lang.String)#org/apache/spark/sql/catalyst/plans/logical/Join/condition()
org/apache/spark/sql/Dataset/joinWith(org.apache.spark.sql.Dataset,org.apache.spark.sql.Column,java.lang.String)#scala/Predef$/assert(boolean)
org/apache/spark/sql/Dataset/joinWith(org.apache.spark.sql.Dataset,org.apache.spark.sql.Column,java.lang.String)#org/apache/spark/sql/catalyst/plans/logical/Project/Project(scala.collection.Seq,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/Dataset/joinWith(org.apache.spark.sql.Dataset,org.apache.spark.sql.Column,java.lang.String)#org/apache/spark/sql/catalyst/plans/logical/Join/Join(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,org.apache.spark.sql.catalyst.plans.JoinType,scala.Option)
org/apache/spark/sql/Dataset/joinWith(org.apache.spark.sql.Dataset,org.apache.spark.sql.Column,java.lang.String)#org/apache/spark/sql/catalyst/plans/logical/Join/right()
org/apache/spark/sql/Dataset/joinWith(org.apache.spark.sql.Dataset,org.apache.spark.sql.Column,java.lang.String)#org/apache/spark/sql/catalyst/plans/logical/Join/left()
org/apache/spark/sql/Dataset/joinWith(org.apache.spark.sql.Dataset,org.apache.spark.sql.Column,java.lang.String)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/output()
org/apache/spark/sql/Dataset/joinWith(org.apache.spark.sql.Dataset,org.apache.spark.sql.Column,java.lang.String)#org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$6(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)
org/apache/spark/sql/Dataset/joinWith(org.apache.spark.sql.Dataset,org.apache.spark.sql.Column,java.lang.String)#scala/collection/Seq/head()
org/apache/spark/sql/Dataset/joinWith(org.apache.spark.sql.Dataset,org.apache.spark.sql.Column,java.lang.String)#org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$3(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)
org/apache/spark/sql/Dataset/joinWith(org.apache.spark.sql.Dataset,org.apache.spark.sql.Column,java.lang.String)#org/apache/spark/sql/catalyst/expressions/CreateStruct$/apply(scala.collection.Seq)
org/apache/spark/sql/Dataset/joinWith(org.apache.spark.sql.Dataset,org.apache.spark.sql.Column,java.lang.String)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/Dataset/joinWith(org.apache.spark.sql.Dataset,org.apache.spark.sql.Column,java.lang.String)#org/apache/spark/sql/catalyst/encoders/ExpressionEncoder$/tuple(org.apache.spark.sql.catalyst.encoders.ExpressionEncoder,org.apache.spark.sql.catalyst.encoders.ExpressionEncoder)
org/apache/spark/sql/Dataset/joinWith(org.apache.spark.sql.Dataset,org.apache.spark.sql.Column,java.lang.String)#scala/collection/Seq/length()
org/apache/spark/sql/Dataset/joinWith(org.apache.spark.sql.Dataset,org.apache.spark.sql.Column,java.lang.String)#scala/Option/get()
org/apache/spark/sql/Dataset/org$apache$spark$sql$Dataset$$createTempViewCommand(java.lang.String,boolean,boolean)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/Dataset/org$apache$spark$sql$Dataset$$createTempViewCommand(java.lang.String,boolean,boolean)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/Dataset/org$apache$spark$sql$Dataset$$createTempViewCommand(java.lang.String,boolean,boolean)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/Dataset/org$apache$spark$sql$Dataset$$createTempViewCommand(java.lang.String,boolean,boolean)#org/apache/spark/sql/catalyst/parser/ParserInterface/parseTableIdentifier(java.lang.String)
org/apache/spark/sql/Dataset/org$apache$spark$sql$Dataset$$createTempViewCommand(java.lang.String,boolean,boolean)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/Dataset/org$apache$spark$sql$Dataset$$createTempViewCommand(java.lang.String,boolean,boolean)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/Dataset/org$apache$spark$sql$Dataset$$createTempViewCommand(java.lang.String,boolean,boolean)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/Dataset/org$apache$spark$sql$Dataset$$createTempViewCommand(java.lang.String,boolean,boolean)#scala/Predef$/Map()
org/apache/spark/sql/Dataset/org$apache$spark$sql$Dataset$$createTempViewCommand(java.lang.String,boolean,boolean)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/Dataset/org$apache$spark$sql$Dataset$$createTempViewCommand(java.lang.String,boolean,boolean)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/Dataset/org$apache$spark$sql$Dataset$$createTempViewCommand(java.lang.String,boolean,boolean)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/Dataset/org$apache$spark$sql$Dataset$$createTempViewCommand(java.lang.String,boolean,boolean)#scala/collection/immutable/Map$/empty()
org/apache/spark/sql/Dataset/join(org.apache.spark.sql.Dataset,org.apache.spark.sql.Column,java.lang.String)#org/apache/spark/sql/Dataset$$anonfun$15/15(org.apache.spark.sql.Dataset)
org/apache/spark/sql/Dataset/join(org.apache.spark.sql.Dataset,org.apache.spark.sql.Column,java.lang.String)#org/apache/spark/sql/catalyst/expressions/AttributeSet/intersect(org.apache.spark.sql.catalyst.expressions.AttributeSet)
org/apache/spark/sql/Dataset/join(org.apache.spark.sql.Dataset,org.apache.spark.sql.Column,java.lang.String)#scala/Option/map(scala.Function1)
org/apache/spark/sql/Dataset/join(org.apache.spark.sql.Dataset,org.apache.spark.sql.Column,java.lang.String)#org/apache/spark/sql/Dataset$$anonfun$16/16(org.apache.spark.sql.Dataset,org.apache.spark.sql.Dataset)
org/apache/spark/sql/Dataset/join(org.apache.spark.sql.Dataset,org.apache.spark.sql.Column,java.lang.String)#org/apache/spark/sql/Dataset$$anonfun$join$4/4(org.apache.spark.sql.Dataset,org.apache.spark.sql.catalyst.plans.logical.Join)
org/apache/spark/sql/Dataset/join(org.apache.spark.sql.Dataset,org.apache.spark.sql.Column,java.lang.String)#org/apache/spark/sql/Dataset$$anonfun$join$3/3(org.apache.spark.sql.Dataset,org.apache.spark.sql.catalyst.plans.logical.Join)
org/apache/spark/sql/Dataset/join(org.apache.spark.sql.Dataset,org.apache.spark.sql.Column,java.lang.String)#org/apache/spark/sql/Dataset$$anonfun$14/14(org.apache.spark.sql.Dataset,org.apache.spark.sql.Dataset,org.apache.spark.sql.Column,java.lang.String)
org/apache/spark/sql/Dataset/join(org.apache.spark.sql.Dataset,org.apache.spark.sql.Column,java.lang.String)#org/apache/spark/sql/internal/SQLConf/dataFrameSelfJoinAutoResolveAmbiguity()
org/apache/spark/sql/Dataset/join(org.apache.spark.sql.Dataset,org.apache.spark.sql.Column,java.lang.String)#org/apache/spark/sql/Dataset$$anonfun$17/17(org.apache.spark.sql.Dataset,org.apache.spark.sql.catalyst.plans.logical.Join)
org/apache/spark/sql/Dataset/join(org.apache.spark.sql.Dataset,org.apache.spark.sql.Column,java.lang.String)#org/apache/spark/sql/catalyst/plans/logical/Join/condition()
org/apache/spark/sql/Dataset/join(org.apache.spark.sql.Dataset,org.apache.spark.sql.Column,java.lang.String)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/outputSet()
org/apache/spark/sql/Dataset/join(org.apache.spark.sql.Dataset,org.apache.spark.sql.Column,java.lang.String)#org/apache/spark/sql/Dataset$$anonfun$join$5/5(org.apache.spark.sql.Dataset,org.apache.spark.sql.catalyst.plans.logical.Join,scala.Option)
org/apache/spark/sql/Dataset/join(org.apache.spark.sql.Dataset,org.apache.spark.sql.Column,java.lang.String)#org/apache/spark/sql/catalyst/expressions/AttributeSet/isEmpty()
org/apache/spark/sql/Dataset/drop(org.apache.spark.sql.Column)#org/apache/spark/sql/Dataset$$anonfun$33/33(org.apache.spark.sql.Dataset,org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute)
org/apache/spark/sql/Dataset/drop(org.apache.spark.sql.Column)#org/apache/spark/sql/catalyst/analysis/Analyzer/resolver()
org/apache/spark/sql/Dataset/drop(org.apache.spark.sql.Column)#scala/collection/Seq/filter(scala.Function1)
org/apache/spark/sql/Dataset/drop(org.apache.spark.sql.Column)#org/apache/spark/sql/Dataset$$anonfun$34/34(org.apache.spark.sql.Dataset,org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/Dataset/drop(org.apache.spark.sql.Column)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/Dataset/drop(org.apache.spark.sql.Column)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/Dataset/drop(org.apache.spark.sql.Column)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/Dataset/drop(org.apache.spark.sql.Column)#scala/Option/get()
org/apache/spark/sql/Dataset/drop(org.apache.spark.sql.Column)#org/apache/spark/sql/Dataset$$anonfun$35/35(org.apache.spark.sql.Dataset)
org/apache/spark/sql/Dataset/drop(org.apache.spark.sql.Column)#org/apache/spark/sql/Column$/unapply(org.apache.spark.sql.Column)
org/apache/spark/sql/Dataset/drop(org.apache.spark.sql.Column)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/Dataset/drop(org.apache.spark.sql.Column)#scala/Option/isEmpty()
org/apache/spark/sql/Dataset/drop(org.apache.spark.sql.Column)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/resolveQuoted(java.lang.String,scala.Function2)
org/apache/spark/sql/Dataset/drop(org.apache.spark.sql.Column)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/output()
org/apache/spark/sql/Dataset/drop(org.apache.spark.sql.Column)#org/apache/spark/sql/catalyst/analysis/UnresolvedAttribute/name()
org/apache/spark/sql/Dataset/explode(scala.collection.Seq,scala.Function1,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/Dataset$$anonfun$25/25(org.apache.spark.sql.Dataset)
org/apache/spark/sql/Dataset/explode(scala.collection.Seq,scala.Function1,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()
org/apache/spark/sql/Dataset/explode(scala.collection.Seq,scala.Function1,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/Dataset$$anonfun$explode$1/1(org.apache.spark.sql.Dataset,org.apache.spark.sql.catalyst.expressions.UserDefinedGenerator)
org/apache/spark/sql/Dataset/explode(scala.collection.Seq,scala.Function1,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/Dataset/explode(scala.collection.Seq,scala.Function1,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/CatalystTypeConverters$/createToCatalystConverter(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/Dataset/explode(scala.collection.Seq,scala.Function1,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/expressions/UserDefinedGenerator/UserDefinedGenerator(org.apache.spark.sql.types.StructType,scala.Function1,scala.collection.Seq)
org/apache/spark/sql/Dataset/explode(scala.collection.Seq,scala.Function1,scala.reflect.api.TypeTags$TypeTag)#scala/Function1/andThen(scala.Function1)
org/apache/spark/sql/Dataset/explode(scala.collection.Seq,scala.Function1,scala.reflect.api.TypeTags$TypeTag)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/Dataset/explode(scala.collection.Seq,scala.Function1,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/Dataset$$anonfun$24/24(org.apache.spark.sql.Dataset,scala.Function1)
org/apache/spark/sql/Dataset/explode(scala.collection.Seq,scala.Function1,scala.reflect.api.TypeTags$TypeTag)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/anonfun/fetchFiles/1/apply(org.apache.hadoop.fs.FileStatus)#scala/collection/mutable/ArrayOps/size()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/anonfun/fetchFiles/1/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/Path/getName()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/anonfun/fetchFiles/1/apply(org.apache.hadoop.fs.FileStatus)#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$StoreFile$/apply(long,org.apache.hadoop.fs.Path,boolean)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/anonfun/fetchFiles/1/apply(org.apache.hadoop.fs.FileStatus)#java/lang/String/toLowerCase(java.util.Locale)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/anonfun/fetchFiles/1/apply(org.apache.hadoop.fs.FileStatus)#scala/collection/immutable/StringOps/toLong()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/anonfun/fetchFiles/1/apply(org.apache.hadoop.fs.FileStatus)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/anonfun/fetchFiles/1/apply(org.apache.hadoop.fs.FileStatus)#scala/collection/mutable/HashMap/put(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/anonfun/fetchFiles/1/apply(org.apache.hadoop.fs.FileStatus)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/anonfun/fetchFiles/1/apply(org.apache.hadoop.fs.FileStatus)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/anonfun/fetchFiles/1/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/FileStatus/getPath()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/anonfun/fetchFiles/1/apply(org.apache.hadoop.fs.FileStatus)#java/lang/String/split(java.lang.String)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/anonfun/fetchFiles/1/apply(org.apache.hadoop.fs.FileStatus)#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$$anonfun$fetchFiles$1$$anonfun$apply$1/1(org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$$anonfun$fetchFiles$1,org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/anonfun/fetchFiles/1/apply(org.apache.hadoop.fs.FileStatus)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/anonfun/fetchFiles/1/apply(org.apache.hadoop.fs.FileStatus)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/anonfun/fetchFiles/1/apply(org.apache.hadoop.fs.FileStatus)#scala/collection/mutable/HashMap/contains(java.lang.Object)
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/canBroadcast(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/SparkStrategies$JoinSelection$$anonfun$canBroadcast$1/1(org.apache.spark.sql.execution.SparkStrategies$JoinSelection$)
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/canBroadcast(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/math/BigInt/$greater$eq(scala.math.BigInt)
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/canBroadcast(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/HintInfo/isBroadcastable()
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/canBroadcast(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/math/BigInt/$less$eq(scala.math.BigInt)
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/canBroadcast(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/internal/SQLConf/autoBroadcastJoinThreshold()
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/canBroadcast(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/math/BigInt$/int2bigInt(int)
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/canBroadcast(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Statistics/hints()
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/canBroadcast(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/canBroadcast(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/canBroadcast(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/math/BigInt$/long2bigInt(long)
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/canBroadcast(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Statistics/sizeInBytes()
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/canBroadcast(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/stats(org.apache.spark.sql.internal.SQLConf)
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/SparkStrategies$JoinSelection$/canBuildLeft(org.apache.spark.sql.catalyst.plans.JoinType)
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/SparkStrategies$JoinSelection$/planLater(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Tuple6/_1()
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Tuple6/_2()
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Option/isEmpty()
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/SparkStrategies$JoinSelection$/canBuildLocalHashMap(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Tuple6/_3()
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Join/condition()
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Tuple6/_4()
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/SparkStrategies$JoinSelection$/canBroadcast(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/expressions/RowOrdering$/isOrderable(scala.collection.Seq)
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Join/right()
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/math/BigInt/$less$eq(scala.math.BigInt)
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/internal/SQLConf/preferSortMergeJoin()
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Tuple6/_5()
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Statistics/sizeInBytes()
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Tuple6/_6()
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/SparkStrategies$JoinSelection$/canBuildRight(org.apache.spark.sql.catalyst.plans.JoinType)
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Join/joinType()
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/SparkStrategies$JoinSelection$/muchSmaller(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Join/left()
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/stats(org.apache.spark.sql.internal.SQLConf)
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/planning/ExtractEquiJoinKeys$/unapply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/SparkStrategies/JoinSelection/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Option/get()
org/apache/spark/sql/streaming/StreamingQueryStatus/jsonValue()#org/json4s/JsonAST$JString$/apply(java.lang.String)
org/apache/spark/sql/streaming/StreamingQueryStatus/jsonValue()#org/json4s/JsonDSL$JsonAssoc/$tilde(scala.Tuple2,scala.Function1)
org/apache/spark/sql/streaming/StreamingQueryStatus/jsonValue()#scala/Predef$/ArrowAssoc(java.lang.Object)
org/apache/spark/sql/streaming/StreamingQueryStatus/jsonValue()#org/json4s/JsonDSL$JsonListAssoc/$tilde(scala.Tuple2)
org/apache/spark/sql/streaming/StreamingQueryStatus/jsonValue()#org/json4s/JsonAST$JBool$/apply(boolean)
org/apache/spark/sql/streaming/StreamingQueryStatus/jsonValue()#scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)
org/apache/spark/sql/streaming/StreamingQueryStatus/jsonValue()#java/lang/String/toString()
org/apache/spark/sql/streaming/StreamingQueryStatus/jsonValue()#scala/Predef$/$conforms()
org/apache/spark/sql/streaming/StreamingQueryStatus/jsonValue()#org/json4s/package$/JBool()
org/apache/spark/sql/streaming/StreamingQueryStatus/jsonValue()#org/json4s/JsonDSL$/pair2Assoc(scala.Tuple2,scala.Function1)
org/apache/spark/sql/streaming/StreamingQueryStatus/jsonValue()#org/json4s/JsonDSL$/jobject2assoc(org.json4s.JsonAST$JObject)
org/apache/spark/sql/streaming/StreamingQueryStatus/jsonValue()#org/json4s/package$/JString()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/doExecute()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/doExecute()#org/apache/spark/rdd/RDD/mapPartitionsWithIndexInternal(scala.Function2,boolean,scala.reflect.ClassTag)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/doExecute()#org/apache/spark/sql/execution/columnar/InMemoryTableScanExec$$anonfun$doExecute$1/1(org.apache.spark.sql.execution.columnar.InMemoryTableScanExec,org.apache.spark.sql.execution.metric.SQLMetric,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.package$AttributeSeq)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/doExecute()#org/apache/spark/sql/catalyst/expressions/package$/AttributeSeq(scala.collection.Seq)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/doExecute()#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/doExecute()#scala/collection/Seq/zipWithIndex(scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/doExecute()#org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/longMetric(java.lang.String)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/doExecute()#org/apache/spark/rdd/RDD/mapPartitionsWithIndexInternal$default$2()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/doExecute()#org/apache/spark/util/LongAccumulator/setValue(long)
org/apache/spark/sql/SparkSession/Builder/getOrCreate()#org/apache/spark/sql/SparkSession$Builder$$anonfun$getOrCreate$2/2(org.apache.spark.sql.SparkSession$Builder)
org/apache/spark/sql/SparkSession/Builder/getOrCreate()#org/apache/spark/SparkContext/addSparkListener(org.apache.spark.scheduler.SparkListenerInterface)
org/apache/spark/sql/SparkSession/Builder/getOrCreate()#org/apache/spark/sql/SparkSession$Builder$$anon$1/1(org.apache.spark.sql.SparkSession$Builder)
org/apache/spark/sql/SparkSession/Builder/getOrCreate()#scala/Option/isDefined()
org/apache/spark/sql/SparkSession/Builder/getOrCreate()#org/apache/spark/sql/SparkSession$Builder$$anonfun$6/6(org.apache.spark.sql.SparkSession$Builder)
org/apache/spark/sql/SparkSession/Builder/getOrCreate()#org/apache/spark/SparkContext/isStopped()
org/apache/spark/sql/SparkSession/Builder/getOrCreate()#org/apache/spark/SparkContext/conf()
org/apache/spark/sql/SparkSession/Builder/getOrCreate()#org/apache/spark/sql/SparkSession$Builder$$anonfun$getOrCreate$3/3(org.apache.spark.sql.SparkSession$Builder,scala.runtime.ObjectRef)
org/apache/spark/sql/SparkSession/Builder/getOrCreate()#org/apache/spark/sql/SparkSession$Builder$$anonfun$getOrCreate$1/1(org.apache.spark.sql.SparkSession$Builder,scala.runtime.ObjectRef)
org/apache/spark/sql/SparkSession/Builder/getOrCreate()#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/SparkSession/Builder/getOrCreate()#org/apache/spark/sql/SparkSession$Builder$$anonfun$getOrCreate$4/4(org.apache.spark.sql.SparkSession$Builder)
org/apache/spark/sql/SparkSession/Builder/getOrCreate()#scala/runtime/ObjectRef/create(java.lang.Object)
org/apache/spark/sql/SparkSession/Builder/getOrCreate()#scala/Option/get()
org/apache/spark/sql/SparkSession/Builder/getOrCreate()#org/apache/spark/sql/internal/StaticSQLConf$/SPARK_SESSION_EXTENSIONS()
org/apache/spark/sql/SparkSession/Builder/getOrCreate()#java/lang/InheritableThreadLocal/get()
org/apache/spark/sql/SparkSession/Builder/getOrCreate()#org/apache/spark/sql/SparkSession$/org$apache$spark$sql$SparkSession$$activeThreadSession()
org/apache/spark/sql/SparkSession/Builder/getOrCreate()#scala/collection/mutable/HashMap/foreach(scala.Function1)
org/apache/spark/sql/SparkSession/Builder/getOrCreate()#org/apache/spark/SparkConf/get(org.apache.spark.internal.config.ConfigEntry)
org/apache/spark/sql/SparkSession/Builder/getOrCreate()#org/apache/spark/sql/SparkSession$Builder/logWarning(scala.Function0)
org/apache/spark/sql/SparkSession/Builder/getOrCreate()#org/apache/spark/sql/SparkSession$/org$apache$spark$sql$SparkSession$$defaultSession()
org/apache/spark/sql/SparkSession/Builder/getOrCreate()#scala/collection/mutable/HashMap/nonEmpty()
org/apache/spark/sql/SparkSession/Builder/getOrCreate()#org/apache/spark/sql/SparkSession$Builder$$anonfun$getOrCreate$5/5(org.apache.spark.sql.SparkSession$Builder,scala.runtime.ObjectRef)
org/apache/spark/sql/SparkSession/Builder/getOrCreate()#java/util/concurrent/atomic/AtomicReference/set(java.lang.Object)
org/apache/spark/sql/SparkSession/Builder/getOrCreate()#org/apache/spark/sql/SparkSession$Builder/liftedTree1$1(java.lang.String)
org/apache/spark/sql/SparkSession/Builder/getOrCreate()#java/util/concurrent/atomic/AtomicReference/get()
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/1/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/types/DecimalType/scale()
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/1/apply(org.apache.spark.sql.types.StructField)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/1/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addReferenceMinorObj(java.lang.Object,java.lang.String)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/1/apply(org.apache.spark.sql.types.StructField)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/1/apply(org.apache.spark.sql.types.StructField)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/1/apply(org.apache.spark.sql.types.StructField)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/1/apply(org.apache.spark.sql.types.StructField)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/1/apply(org.apache.spark.sql.types.StructField)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/1/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/types/StructField/dataType()
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/1/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addReferenceMinorObj$default$2()
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/1/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/types/StructField/name()
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/1/apply(org.apache.spark.sql.types.StructField)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/1/apply(org.apache.spark.sql.types.StructField)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/1/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/types/DecimalType/precision()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/doExecute/1/apply(scala.collection.Iterator,scala.collection.Iterator)#org/apache/spark/sql/execution/joins/SortMergeJoinExec$$anonfun$doExecute$1$$anon$4/4(org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1,scala.Function1,scala.math.Ordering,scala.Function1,scala.collection.Iterator,scala.collection.Iterator)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/doExecute/1/apply(scala.collection.Iterator,scala.collection.Iterator)#org/apache/spark/sql/execution/joins/SortMergeJoinExec$$anonfun$doExecute$1$$anonfun$3/3(org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/doExecute/1/apply(scala.collection.Iterator,scala.collection.Iterator)#org/apache/spark/sql/execution/joins/LeftOuterIterator/toScala()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/doExecute/1/apply(scala.collection.Iterator,scala.collection.Iterator)#org/apache/spark/sql/catalyst/expressions/GenericInternalRow/GenericInternalRow(int)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/doExecute/1/apply(scala.collection.Iterator,scala.collection.Iterator)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/doExecute/1/apply(scala.collection.Iterator,scala.collection.Iterator)#scala/collection/Seq/length()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/doExecute/1/apply(scala.collection.Iterator,scala.collection.Iterator)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/doExecute/1/apply(scala.collection.Iterator,scala.collection.Iterator)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/doExecute/1/apply(scala.collection.Iterator,scala.collection.Iterator)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/doExecute/1/apply(scala.collection.Iterator,scala.collection.Iterator)#org/apache/spark/sql/catalyst/expressions/UnsafeProjection$/create(scala.collection.Seq,scala.collection.Seq)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/doExecute/1/apply(scala.collection.Iterator,scala.collection.Iterator)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/doExecute/1/apply(scala.collection.Iterator,scala.collection.Iterator)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/doExecute/1/apply(scala.collection.Iterator,scala.collection.Iterator)#org/apache/spark/sql/execution/joins/SortMergeJoinExec/newNaturalAscendingOrdering(scala.collection.Seq)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/doExecute/1/apply(scala.collection.Iterator,scala.collection.Iterator)#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/doExecute/1/apply(scala.collection.Iterator,scala.collection.Iterator)#org/apache/spark/sql/execution/joins/SortMergeJoinExec$$anonfun$doExecute$1$$anon$1/toScala()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/doExecute/1/apply(scala.collection.Iterator,scala.collection.Iterator)#org/apache/spark/sql/execution/joins/SortMergeJoinExec$$anonfun$doExecute$1$$anon$1/1(org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1,scala.Function1,scala.math.Ordering,scala.Function1,scala.collection.Iterator,scala.collection.Iterator)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/doExecute/1/apply(scala.collection.Iterator,scala.collection.Iterator)#org/apache/spark/sql/execution/joins/SortMergeJoinExec$$anonfun$doExecute$1$$anon$2/toScala()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/doExecute/1/apply(scala.collection.Iterator,scala.collection.Iterator)#org/apache/spark/sql/execution/joins/SortMergeJoinExec$$anonfun$doExecute$1$$anon$3/3(org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1,scala.Function1,scala.math.Ordering,scala.collection.Iterator,scala.collection.Iterator)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/doExecute/1/apply(scala.collection.Iterator,scala.collection.Iterator)#org/apache/spark/sql/execution/joins/SortMergeJoinExec$$anonfun$doExecute$1$$anon$3/toScala()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/doExecute/1/apply(scala.collection.Iterator,scala.collection.Iterator)#org/apache/spark/sql/execution/joins/FullOuterIterator/toScala()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/doExecute/1/apply(scala.collection.Iterator,scala.collection.Iterator)#org/apache/spark/sql/execution/RowIterator$/fromScala(scala.collection.Iterator)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/doExecute/1/apply(scala.collection.Iterator,scala.collection.Iterator)#org/apache/spark/sql/execution/joins/SortMergeJoinExec$$anonfun$doExecute$1$$anon$4/toScala()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/doExecute/1/apply(scala.collection.Iterator,scala.collection.Iterator)#org/apache/spark/sql/execution/joins/RightOuterIterator/toScala()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/doExecute/1/apply(scala.collection.Iterator,scala.collection.Iterator)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/doExecute/1/apply(scala.collection.Iterator,scala.collection.Iterator)#org/apache/spark/sql/execution/joins/SortMergeJoinExec$$anonfun$doExecute$1$$anon$2/2(org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1,scala.Function1,scala.math.Ordering,scala.collection.Iterator,scala.collection.Iterator)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/doExecute/1/apply(scala.collection.Iterator,scala.collection.Iterator)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/doExecute/1/apply(scala.collection.Iterator,scala.collection.Iterator)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/doExecute/1/apply(scala.collection.Iterator,scala.collection.Iterator)#org/apache/spark/sql/execution/joins/SortMergeJoinExec$$anonfun$doExecute$1$$anonfun$2/2(org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/doExecute/1/apply(scala.collection.Iterator,scala.collection.Iterator)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/doExecute/1/apply(scala.collection.Iterator,scala.collection.Iterator)#org/apache/spark/sql/execution/joins/SortMergeJoinExec$$anonfun$doExecute$1$$anonfun$1/1(org.apache.spark.sql.execution.joins.SortMergeJoinExec$$anonfun$doExecute$1)
org/apache/spark/sql/execution/SparkStrategies/Aggregation/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/Seq/isEmpty()
org/apache/spark/sql/execution/SparkStrategies/Aggregation/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/SparkStrategies$Aggregation$$anonfun$1/1(org.apache.spark.sql.execution.SparkStrategies$Aggregation$)
org/apache/spark/sql/execution/SparkStrategies/Aggregation/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Tuple4/_1()
org/apache/spark/sql/execution/SparkStrategies/Aggregation/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Option/isEmpty()
org/apache/spark/sql/execution/SparkStrategies/Aggregation/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/planning/PhysicalAggregation$/unapply(java.lang.Object)
org/apache/spark/sql/execution/SparkStrategies/Aggregation/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$/planAggregateWithoutDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/SparkStrategies/Aggregation/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/SparkStrategies/Aggregation/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/sys/package$/error(java.lang.String)
org/apache/spark/sql/execution/SparkStrategies/Aggregation/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/SeqLike/distinct()
org/apache/spark/sql/execution/SparkStrategies/Aggregation/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/SeqLike/length()
org/apache/spark/sql/execution/SparkStrategies/Aggregation/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/SparkStrategies$Aggregation$/planLater(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/SparkStrategies/Aggregation/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/SparkStrategies/Aggregation/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/aggregate/AggUtils$/planAggregateWithOneDistinct(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/SparkStrategies/Aggregation/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/Seq/partition(scala.Function1)
org/apache/spark/sql/execution/SparkStrategies/Aggregation/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Tuple4/_2()
org/apache/spark/sql/execution/SparkStrategies/Aggregation/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Tuple2/_1()
org/apache/spark/sql/execution/SparkStrategies/Aggregation/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Tuple4/_3()
org/apache/spark/sql/execution/SparkStrategies/Aggregation/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Tuple2/_2()
org/apache/spark/sql/execution/SparkStrategies/Aggregation/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Tuple4/_4()
org/apache/spark/sql/execution/SparkStrategies/Aggregation/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/SparkStrategies$Aggregation$$anonfun$apply$1/1(org.apache.spark.sql.execution.SparkStrategies$Aggregation$)
org/apache/spark/sql/execution/SparkStrategies/Aggregation/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/SparkStrategies/Aggregation/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/SparkStrategies/Aggregation/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Option/get()
org/apache/spark/sql/SQLImplicits/typecreator11/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ModuleSymbolApi/asModule()
org/apache/spark/sql/SQLImplicits/typecreator11/1/apply(scala.reflect.api.Mirror)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/SQLImplicits/typecreator11/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticPackage(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator11/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$InternalApi/reificationSupport()
org/apache/spark/sql/SQLImplicits/typecreator11/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/ThisType(scala.reflect.api.Symbols$SymbolApi)
org/apache/spark/sql/SQLImplicits/typecreator11/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ClassSymbolApi/asType()
org/apache/spark/sql/SQLImplicits/typecreator11/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ModuleSymbolApi/moduleClass()
org/apache/spark/sql/SQLImplicits/typecreator11/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/TypeRef(scala.reflect.api.Types$TypeApi,scala.reflect.api.Symbols$SymbolApi,scala.collection.immutable.List)
org/apache/spark/sql/SQLImplicits/typecreator11/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticClass(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator11/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/universe()
org/apache/spark/sql/SQLImplicits/typecreator11/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$TypeSymbolApi/toTypeConstructor()
org/apache/spark/sql/SQLImplicits/typecreator11/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Universe/internal()
org/apache/spark/sql/SQLImplicits/typecreator11/1/apply(scala.reflect.api.Mirror)#scala/collection/immutable/List$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/anonfun/1/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/anonfun/1/apply()#org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation$$anonfun$1/apply()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/anonfun/1/apply()#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/anonfun/1/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/anonfun/1/apply()#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/anonfun/1/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/anonfun/1/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/anonfun/1/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/anonfun/1/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/anonfun/1/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/3/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/3/apply(scala.Tuple2)#scala/Tuple2/_2()
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/3/apply(scala.Tuple2)#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$populateStartOffsets$3$$anonfun$apply$4/4(org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$populateStartOffsets$3,org.apache.spark.sql.execution.streaming.Offset)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/3/apply(scala.Tuple2)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/3/apply(scala.Tuple2)#scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/3/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/3/apply(scala.Tuple2)#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$populateStartOffsets$3$$anonfun$apply$1/1(org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$populateStartOffsets$3)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/3/apply(scala.Tuple2)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/3/apply(scala.Tuple2)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.catalyst.expressions.SortOrder)#org/apache/spark/sql/types/DecimalType/scale()
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.catalyst.expressions.SortOrder)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.catalyst.expressions.SortOrder)#org/apache/spark/sql/execution/SortPrefixUtils$/binaryPrefixComparator(org.apache.spark.sql.catalyst.expressions.SortOrder)
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.catalyst.expressions.SortOrder)#org/apache/spark/sql/execution/SortPrefixUtils$/getPrefixComparator(org.apache.spark.sql.catalyst.expressions.SortOrder)
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.catalyst.expressions.SortOrder)#org/apache/spark/sql/types/DecimalType/precision()
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.catalyst.expressions.SortOrder)#org/apache/spark/sql/catalyst/expressions/SortOrder/dataType()
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.catalyst.expressions.SortOrder)#org/apache/spark/sql/execution/SortPrefixUtils$/longPrefixComparator(org.apache.spark.sql.catalyst.expressions.SortOrder)
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.catalyst.expressions.SortOrder)#org/apache/spark/sql/types/Decimal$/MAX_LONG_DIGITS()
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.catalyst.expressions.SortOrder)#org/apache/spark/sql/execution/SortPrefixUtils$/stringPrefixComparator(org.apache.spark.sql.catalyst.expressions.SortOrder)
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.catalyst.expressions.SortOrder)#org/apache/spark/sql/execution/SortPrefixUtils$/doublePrefixComparator(org.apache.spark.sql.catalyst.expressions.SortOrder)
org/apache/spark/sql/execution/SortPrefixUtils/createPrefixGenerator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/SortPrefixUtils$$anon$3/3()
org/apache/spark/sql/execution/SortPrefixUtils/createPrefixGenerator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/SortPrefixUtils$/createPrefixGenerator(org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/SortPrefixUtils/createPrefixGenerator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/types/StructField/dataType()
org/apache/spark/sql/execution/SortPrefixUtils/createPrefixGenerator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/catalyst/expressions/SortOrder$/apply(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.SortDirection,scala.collection.immutable.Set)
org/apache/spark/sql/execution/SortPrefixUtils/createPrefixGenerator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/catalyst/expressions/SortPrefix/SortPrefix(org.apache.spark.sql.catalyst.expressions.SortOrder)
org/apache/spark/sql/execution/SortPrefixUtils/createPrefixGenerator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/catalyst/expressions/SortOrder$/apply$default$3()
org/apache/spark/sql/execution/SortPrefixUtils/createPrefixGenerator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/catalyst/expressions/UnsafeProjection$/create(org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/execution/SortPrefixUtils/createPrefixGenerator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/catalyst/expressions/BoundReference/BoundReference(int,org.apache.spark.sql.types.DataType,boolean)
org/apache/spark/sql/execution/SortPrefixUtils/createPrefixGenerator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/SortPrefixUtils$$anon$2/2(org.apache.spark.sql.catalyst.expressions.SortPrefix,org.apache.spark.sql.catalyst.expressions.UnsafeProjection)
org/apache/spark/sql/execution/SortPrefixUtils/createPrefixGenerator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/types/StructType/head()
org/apache/spark/sql/execution/SortPrefixUtils/createPrefixGenerator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/types/StructType/nonEmpty()
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/catalyst/expressions/BoundReference/BoundReference(int,org.apache.spark.sql.types.DataType,boolean)
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/types/StructField/nullable()
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/catalyst/expressions/SortOrder$/apply(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.SortDirection,scala.collection.immutable.Set)
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/SortPrefixUtils$/getPrefixComparator(org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/catalyst/expressions/SortOrder$/apply$default$3()
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/SortPrefixUtils$$anon$1/1()
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/types/StructField/dataType()
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/SortPrefixUtils$/getPrefixComparator(org.apache.spark.sql.catalyst.expressions.SortOrder)
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/types/StructType/head()
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/types/StructType/nonEmpty()
org/apache/spark/sql/execution/streaming/MemoryStream/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/collection/mutable/ListBuffer/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/streaming/MemoryStream/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/streaming/LongOffset$/convert(org.apache.spark.sql.execution.streaming.Offset)
org/apache/spark/sql/execution/streaming/MemoryStream/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/collection/mutable/ListBuffer/slice(int,int)
org/apache/spark/sql/execution/streaming/MemoryStream/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/Option/flatMap(scala.Function1)
org/apache/spark/sql/execution/streaming/MemoryStream/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/streaming/MemoryStream$$anonfun$getBatch$1/1(org.apache.spark.sql.execution.streaming.MemoryStream,int,int,scala.collection.mutable.ListBuffer)
org/apache/spark/sql/execution/streaming/MemoryStream/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/streaming/MemoryStream$$anonfun$getBatch$4/4(org.apache.spark.sql.execution.streaming.MemoryStream)
org/apache/spark/sql/execution/streaming/MemoryStream/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/collection/TraversableOnce/reduceOption(scala.Function2)
org/apache/spark/sql/execution/streaming/MemoryStream/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/streaming/MemoryStream$$anonfun$1/1(org.apache.spark.sql.execution.streaming.MemoryStream)
org/apache/spark/sql/execution/streaming/MemoryStream/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/streaming/MemoryStream/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#scala/collection/mutable/ListBuffer$/canBuildFrom()
org/apache/spark/sql/execution/streaming/MemoryStream/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/streaming/MemoryStream$$anonfun$getBatch$3/3(org.apache.spark.sql.execution.streaming.MemoryStream)
org/apache/spark/sql/execution/streaming/MemoryStream/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/streaming/MemoryStream$$anonfun$3/3(org.apache.spark.sql.execution.streaming.MemoryStream)
org/apache/spark/sql/execution/streaming/MemoryStream/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/streaming/MemoryStream$$anonfun$getBatch$2/2(org.apache.spark.sql.execution.streaming.MemoryStream)
org/apache/spark/sql/execution/streaming/MemoryStream/getBatch(scala.Option,org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/streaming/MemoryStream$$anonfun$2/2(org.apache.spark.sql.execution.streaming.MemoryStream)
org/apache/spark/sql/execution/streaming/MemoryStream/commit(org.apache.spark.sql.execution.streaming.Offset)#scala/Some/x()
org/apache/spark/sql/execution/streaming/MemoryStream/commit(org.apache.spark.sql.execution.streaming.Offset)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/MemoryStream/commit(org.apache.spark.sql.execution.streaming.Offset)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/MemoryStream/commit(org.apache.spark.sql.execution.streaming.Offset)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/streaming/MemoryStream/commit(org.apache.spark.sql.execution.streaming.Offset)#scala/sys/package$/error(java.lang.String)
org/apache/spark/sql/execution/streaming/MemoryStream/commit(org.apache.spark.sql.execution.streaming.Offset)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/streaming/MemoryStream/commit(org.apache.spark.sql.execution.streaming.Offset)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/streaming/MemoryStream/commit(org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/streaming/LongOffset$/convert(org.apache.spark.sql.execution.streaming.Offset)
org/apache/spark/sql/execution/streaming/MemoryStream/commit(org.apache.spark.sql.execution.streaming.Offset)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/streaming/MemoryStream/commit(org.apache.spark.sql.execution.streaming.Offset)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/MemoryStream/commit(org.apache.spark.sql.execution.streaming.Offset)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/MemoryStream/commit(org.apache.spark.sql.execution.streaming.Offset)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#scala/collection/immutable/Map/size()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#org/apache/spark/sql/internal/SQLConf/resolver()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/copy$default$3()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/copy$default$4()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/copy$default$5()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/query()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/partition()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/copy(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.immutable.Map,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,boolean,boolean)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#org/apache/spark/sql/execution/datasources/PreprocessTableInsertion$$anonfun$22/22(org.apache.spark.sql.execution.datasources.PreprocessTableInsertion,scala.collection.immutable.Set)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#org/apache/spark/sql/types/StructType/length()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#scala/collection/immutable/Map/nonEmpty()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/copy$default$1()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/schema()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#scala/collection/immutable/MapLike/keySet()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/output()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#scala/collection/Seq/size()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#scala/Predef$/$conforms()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/table()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#scala/collection/immutable/Map/keys()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#scala/collection/Seq/filterNot(scala.Function1)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#scala/collection/Iterable/mkString(java.lang.String)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#org/apache/spark/sql/execution/datasources/PartitioningUtils$/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#org/apache/spark/sql/execution/datasources/PreprocessTableInsertion$$anonfun$23/23(org.apache.spark.sql.execution.datasources.PreprocessTableInsertion)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#org/apache/spark/sql/execution/datasources/PreprocessTableInsertion$$anonfun$21/21(org.apache.spark.sql.execution.datasources.PreprocessTableInsertion)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#scala/collection/immutable/Map/filter(scala.Function1)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#scala/collection/Seq/length()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#scala/collection/Seq/mkString(java.lang.String)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/org$apache$spark$sql$execution$datasources$PreprocessTableInsertion$$preprocess(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,java.lang.String,scala.collection.Seq)#scala/collection/immutable/Set/size()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/castAndRenameChildOutput(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,scala.collection.Seq)#org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/copy(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.immutable.Map,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,boolean,boolean)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/castAndRenameChildOutput(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,scala.collection.Seq)#org/apache/spark/sql/catalyst/plans/logical/Project/Project(scala.collection.Seq,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/castAndRenameChildOutput(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,scala.collection.Seq)#org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/copy$default$1()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/castAndRenameChildOutput(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,scala.collection.Seq)#org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/copy$default$2()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/castAndRenameChildOutput(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,scala.collection.Seq)#org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/copy$default$5()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/castAndRenameChildOutput(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,scala.collection.Seq)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/output()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/castAndRenameChildOutput(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,scala.collection.Seq)#org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/query()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/castAndRenameChildOutput(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,scala.collection.Seq)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/castAndRenameChildOutput(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,scala.collection.Seq)#org/apache/spark/sql/execution/datasources/PreprocessTableInsertion$$anonfun$24/24(org.apache.spark.sql.execution.datasources.PreprocessTableInsertion)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/castAndRenameChildOutput(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,scala.collection.Seq)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/castAndRenameChildOutput(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/castAndRenameChildOutput(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,scala.collection.Seq)#scala/collection/Seq/zip(scala.collection.GenIterable,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/castAndRenameChildOutput(org.apache.spark.sql.catalyst.plans.logical.InsertIntoTable,scala.collection.Seq)#org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/copy$default$4()
org/apache/spark/sql/execution/streaming/SinkFileStatus/apply(org.apache.hadoop.fs.FileStatus)#org/apache/spark/sql/execution/streaming/FileStreamSinkLog$/ADD_ACTION()
org/apache/spark/sql/execution/streaming/SinkFileStatus/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/FileStatus/getReplication()
org/apache/spark/sql/execution/streaming/SinkFileStatus/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/FileStatus/getPath()
org/apache/spark/sql/execution/streaming/SinkFileStatus/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/FileStatus/getBlockSize()
org/apache/spark/sql/execution/streaming/SinkFileStatus/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/FileStatus/getModificationTime()
org/apache/spark/sql/execution/streaming/SinkFileStatus/apply(org.apache.hadoop.fs.FileStatus)#java/net/URI/toString()
org/apache/spark/sql/execution/streaming/SinkFileStatus/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/Path/toUri()
org/apache/spark/sql/execution/streaming/SinkFileStatus/apply(org.apache.hadoop.fs.FileStatus)#org/apache/spark/sql/execution/streaming/SinkFileStatus$/apply(org.apache.hadoop.fs.FileStatus)
org/apache/spark/sql/execution/streaming/SinkFileStatus/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/FileStatus/isDirectory()
org/apache/spark/sql/execution/streaming/SinkFileStatus/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/FileStatus/getLen()
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/verifyTemporaryObjectsNotExists/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/verifyTemporaryObjectsNotExists/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/resolved()
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/verifyTemporaryObjectsNotExists/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/verifyTemporaryObjectsNotExists/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/verifyTemporaryObjectsNotExists/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/command/CreateViewCommand$$anonfun$verifyTemporaryObjectsNotExists$1$$anonfun$applyOrElse$1/1(org.apache.spark.sql.execution.command.CreateViewCommand$$anonfun$verifyTemporaryObjectsNotExists$1)
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/verifyTemporaryObjectsNotExists/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/isTemporaryTable(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/verifyTemporaryObjectsNotExists/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/verifyTemporaryObjectsNotExists/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/verifyTemporaryObjectsNotExists/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/expressions()
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/verifyTemporaryObjectsNotExists/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/verifyTemporaryObjectsNotExists/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/verifyTemporaryObjectsNotExists/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/verifyTemporaryObjectsNotExists/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/verifyTemporaryObjectsNotExists/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/verifyTemporaryObjectsNotExists/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/verifyTemporaryObjectsNotExists/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/analysis/UnresolvedRelation/tableIdentifier()
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/verifyTemporaryObjectsNotExists/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/verifyTemporaryObjectsNotExists/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/verifyTemporaryObjectsNotExists/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/verifyTemporaryObjectsNotExists/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/streaming/StateStoreSaveExec/anonfun/doExecute/3/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/streaming/StateStoreSaveExec/anonfun/doExecute/3/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#scala/collection/Iterator/filter(scala.Function1)
org/apache/spark/sql/execution/streaming/StateStoreSaveExec/anonfun/doExecute/3/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#org/apache/spark/sql/execution/streaming/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$8/8(org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anonfun$doExecute$3,org.apache.spark.sql.execution.metric.SQLMetric)
org/apache/spark/sql/execution/streaming/StateStoreSaveExec/anonfun/doExecute/3/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#org/apache/spark/sql/execution/streaming/StateStoreSaveExec$$anonfun$doExecute$3$$anon$1/1(org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anonfun$doExecute$3,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,org.apache.spark.sql.execution.metric.SQLMetric,org.apache.spark.sql.execution.metric.SQLMetric,org.apache.spark.sql.execution.metric.SQLMetric,org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)
org/apache/spark/sql/execution/streaming/StateStoreSaveExec/anonfun/doExecute/3/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#scala/collection/Iterator/map(scala.Function1)
org/apache/spark/sql/execution/streaming/StateStoreSaveExec/anonfun/doExecute/3/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#org/apache/spark/sql/execution/streaming/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$7/7(org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anonfun$doExecute$3)
org/apache/spark/sql/execution/streaming/StateStoreSaveExec/anonfun/doExecute/3/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/StateStoreSaveExec/anonfun/doExecute/3/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#scala/Option/get()
org/apache/spark/sql/execution/streaming/StateStoreSaveExec/anonfun/doExecute/3/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#scala/Some/x()
org/apache/spark/sql/execution/streaming/StateStoreSaveExec/anonfun/doExecute/3/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#org/apache/spark/sql/catalyst/expressions/codegen/GenerateUnsafeProjection$/generate(java.lang.Object,scala.collection.Seq)
org/apache/spark/sql/execution/streaming/StateStoreSaveExec/anonfun/doExecute/3/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#org/apache/spark/sql/execution/streaming/StateStoreSaveExec/longMetric(java.lang.String)
org/apache/spark/sql/execution/streaming/StateStoreSaveExec/anonfun/doExecute/3/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/streaming/StateStoreSaveExec/anonfun/doExecute/3/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#scala/collection/Iterator/next()
org/apache/spark/sql/execution/streaming/StateStoreSaveExec/anonfun/doExecute/3/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#org/apache/spark/sql/catalyst/expressions/UnsafeProjection/apply(org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/streaming/StateStoreSaveExec/anonfun/doExecute/3/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#org/apache/spark/sql/execution/streaming/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$5/5(org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anonfun$doExecute$3,org.apache.spark.sql.execution.metric.SQLMetric)
org/apache/spark/sql/execution/streaming/StateStoreSaveExec/anonfun/doExecute/3/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/StateStoreSaveExec/anonfun/doExecute/3/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/copy()
org/apache/spark/sql/execution/streaming/StateStoreSaveExec/anonfun/doExecute/3/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#scala/collection/Iterator/hasNext()
org/apache/spark/sql/execution/streaming/StateStoreSaveExec/anonfun/doExecute/3/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/StateStoreSaveExec/anonfun/doExecute/3/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/StateStoreSaveExec/anonfun/doExecute/3/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#java/lang/UnsupportedOperationException/UnsupportedOperationException(java.lang.String)
org/apache/spark/sql/execution/streaming/StateStoreSaveExec/anonfun/doExecute/3/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#org/apache/spark/sql/execution/streaming/StateStoreSaveExec$$anonfun$doExecute$3$$anonfun$apply$6/6(org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anonfun$doExecute$3,org.apache.spark.sql.catalyst.expressions.codegen.Predicate)
org/apache/spark/sql/execution/datasources/FindDataSourceTable/anon/1/call()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/properties()
org/apache/spark/sql/execution/datasources/FindDataSourceTable/anon/1/call()#org/apache/spark/sql/catalyst/catalog/CatalogTable/provider()
org/apache/spark/sql/execution/datasources/FindDataSourceTable/anon/1/call()#scala/Option$/option2Iterable(scala.Option)
org/apache/spark/sql/execution/datasources/FindDataSourceTable/anon/1/call()#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/datasources/FindDataSourceTable/anon/1/call()#org/apache/spark/sql/catalyst/catalog/CatalogTable/schema()
org/apache/spark/sql/execution/datasources/FindDataSourceTable/anon/1/call()#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/datasources/FindDataSourceTable/anon/1/call()#org/apache/spark/sql/catalyst/catalog/CatalogTable/bucketSpec()
org/apache/spark/sql/execution/datasources/FindDataSourceTable/anon/1/call()#org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()
org/apache/spark/sql/execution/datasources/FindDataSourceTable/anon/1/call()#scala/collection/immutable/Map/$plus$plus(scala.collection.GenTraversableOnce)
org/apache/spark/sql/execution/datasources/FindDataSourceTable/anon/1/call()#scala/Option/get()
org/apache/spark/sql/execution/datasources/FindDataSourceTable/anon/1/call()#org/apache/spark/sql/execution/datasources/LogicalRelation$/apply(org.apache.spark.sql.sources.BaseRelation,org.apache.spark.sql.catalyst.catalog.CatalogTable)
org/apache/spark/sql/execution/datasources/FindDataSourceTable/anon/1/call()#org/apache/spark/sql/execution/datasources/FindDataSourceTable$$anon$1$$anonfun$9/9(org.apache.spark.sql.execution.datasources.FindDataSourceTable$$anon$1)
org/apache/spark/sql/execution/datasources/FindDataSourceTable/anon/1/call()#org/apache/spark/sql/execution/datasources/FindDataSourceTable$$anon$1/call()
org/apache/spark/sql/execution/datasources/FindDataSourceTable/anon/1/call()#org/apache/spark/sql/execution/datasources/DataSource$/apply$default$3()
org/apache/spark/sql/execution/datasources/FindDataSourceTable/anon/1/call()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/locationUri()
org/apache/spark/sql/execution/datasources/FindDataSourceTable/anon/1/call()#org/apache/spark/sql/types/StructType/isEmpty()
org/apache/spark/sql/execution/datasources/FindDataSourceTable/anon/1/call()#org/apache/spark/sql/catalyst/catalog/CatalogTable/storage()
org/apache/spark/sql/DataFrameNaFunctions/org$apache$spark$sql$DataFrameNaFunctions$$convertToDouble(java.lang.Object)#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/DataFrameNaFunctions/org$apache$spark$sql$DataFrameNaFunctions$$convertToDouble(java.lang.Object)#scala/runtime/BoxesRunTime/unboxToFloat(java.lang.Object)
org/apache/spark/sql/DataFrameNaFunctions/org$apache$spark$sql$DataFrameNaFunctions$$convertToDouble(java.lang.Object)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/DataFrameNaFunctions/org$apache$spark$sql$DataFrameNaFunctions$$convertToDouble(java.lang.Object)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/DataFrameNaFunctions/org$apache$spark$sql$DataFrameNaFunctions$$convertToDouble(java.lang.Object)#java/lang/Class/getName()
org/apache/spark/sql/DataFrameNaFunctions/org$apache$spark$sql$DataFrameNaFunctions$$convertToDouble(java.lang.Object)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/DataFrameNaFunctions/org$apache$spark$sql$DataFrameNaFunctions$$convertToDouble(java.lang.Object)#java/lang/Object/getClass()
org/apache/spark/sql/DataFrameNaFunctions/org$apache$spark$sql$DataFrameNaFunctions$$convertToDouble(java.lang.Object)#scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)
org/apache/spark/sql/DataFrameNaFunctions/org$apache$spark$sql$DataFrameNaFunctions$$convertToDouble(java.lang.Object)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/DataFrameNaFunctions/org$apache$spark$sql$DataFrameNaFunctions$$convertToDouble(java.lang.Object)#scala/runtime/BoxesRunTime/unboxToDouble(java.lang.Object)
org/apache/spark/sql/DataFrameNaFunctions/org$apache$spark$sql$DataFrameNaFunctions$$convertToDouble(java.lang.Object)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/DataFrameNaFunctions/org$apache$spark$sql$DataFrameNaFunctions$$fillCol(org.apache.spark.sql.types.StructField,java.lang.Object)#org/apache/spark/sql/functions$/lit(java.lang.Object)
org/apache/spark/sql/DataFrameNaFunctions/org$apache$spark$sql$DataFrameNaFunctions$$fillCol(org.apache.spark.sql.types.StructField,java.lang.Object)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/DataFrameNaFunctions/org$apache$spark$sql$DataFrameNaFunctions$$fillCol(org.apache.spark.sql.types.StructField,java.lang.Object)#org/apache/spark/sql/functions$/coalesce(scala.collection.Seq)
org/apache/spark/sql/DataFrameNaFunctions/org$apache$spark$sql$DataFrameNaFunctions$$fillCol(org.apache.spark.sql.types.StructField,java.lang.Object)#org/apache/spark/sql/functions$/nanvl(org.apache.spark.sql.Column,org.apache.spark.sql.Column)
org/apache/spark/sql/DataFrameNaFunctions/org$apache$spark$sql$DataFrameNaFunctions$$fillCol(org.apache.spark.sql.types.StructField,java.lang.Object)#org/apache/spark/sql/types/StructField/dataType()
org/apache/spark/sql/DataFrameNaFunctions/org$apache$spark$sql$DataFrameNaFunctions$$fillCol(org.apache.spark.sql.types.StructField,java.lang.Object)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/DataFrameNaFunctions/org$apache$spark$sql$DataFrameNaFunctions$$fillCol(org.apache.spark.sql.types.StructField,java.lang.Object)#org/apache/spark/sql/types/StructField/name()
org/apache/spark/sql/DataFrameNaFunctions/org$apache$spark$sql$DataFrameNaFunctions$$fillCol(org.apache.spark.sql.types.StructField,java.lang.Object)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/DataFrameNaFunctions/org$apache$spark$sql$DataFrameNaFunctions$$fillCol(org.apache.spark.sql.types.StructField,java.lang.Object)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/DataFrameNaFunctions/org$apache$spark$sql$DataFrameNaFunctions$$fillCol(org.apache.spark.sql.types.StructField,java.lang.Object)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/DataFrameNaFunctions/replace0(scala.collection.Seq,scala.collection.immutable.Map)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/DataFrameNaFunctions/replace0(scala.collection.Seq,scala.collection.immutable.Map)#scala/collection/Seq/isEmpty()
org/apache/spark/sql/DataFrameNaFunctions/replace0(scala.collection.Seq,scala.collection.immutable.Map)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/DataFrameNaFunctions/replace0(scala.collection.Seq,scala.collection.immutable.Map)#scala/Tuple2/_1()
org/apache/spark/sql/DataFrameNaFunctions/replace0(scala.collection.Seq,scala.collection.immutable.Map)#scala/collection/immutable/Map/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/DataFrameNaFunctions/replace0(scala.collection.Seq,scala.collection.immutable.Map)#org/apache/spark/sql/DataFrameNaFunctions$$anonfun$2/2(org.apache.spark.sql.DataFrameNaFunctions)
org/apache/spark/sql/DataFrameNaFunctions/replace0(scala.collection.Seq,scala.collection.immutable.Map)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/DataFrameNaFunctions/replace0(scala.collection.Seq,scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/analysis/Analyzer/resolver()
org/apache/spark/sql/DataFrameNaFunctions/replace0(scala.collection.Seq,scala.collection.immutable.Map)#org/apache/spark/sql/DataFrameNaFunctions$$anonfun$3/3(org.apache.spark.sql.DataFrameNaFunctions,scala.collection.Seq,scala.collection.immutable.Map,org.apache.spark.sql.types.AtomicType,scala.Function2)
org/apache/spark/sql/DataFrameNaFunctions/replace0(scala.collection.Seq,scala.collection.immutable.Map)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/DataFrameNaFunctions/replace0(scala.collection.Seq,scala.collection.immutable.Map)#scala/collection/immutable/Map/head()
org/apache/spark/sql/DataFrameNaFunctions/replace0(scala.collection.Seq,scala.collection.immutable.Map)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/DataFrameNaFunctions/replace0(scala.collection.Seq,scala.collection.immutable.Map)#scala/Tuple2/_2()
org/apache/spark/sql/DataFrameNaFunctions/replace0(scala.collection.Seq,scala.collection.immutable.Map)#scala/collection/immutable/Map$/canBuildFrom()
org/apache/spark/sql/DataFrameNaFunctions/replace0(scala.collection.Seq,scala.collection.immutable.Map)#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/DataFrameNaFunctions/replace0(scala.collection.Seq,scala.collection.immutable.Map)#scala/collection/immutable/Map/isEmpty()
org/apache/spark/sql/DataFrameNaFunctions/replace0(scala.collection.Seq,scala.collection.immutable.Map)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/DataFrameNaFunctions/fillMap(scala.collection.Seq)#org/apache/spark/sql/catalyst/analysis/Analyzer/resolver()
org/apache/spark/sql/DataFrameNaFunctions/fillMap(scala.collection.Seq)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/DataFrameNaFunctions/fillMap(scala.collection.Seq)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/DataFrameNaFunctions/fillMap(scala.collection.Seq)#scala/collection/Seq/foreach(scala.Function1)
org/apache/spark/sql/DataFrameNaFunctions/fillMap(scala.collection.Seq)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/DataFrameNaFunctions/fillMap(scala.collection.Seq)#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/DataFrameNaFunctions/fillMap(scala.collection.Seq)#org/apache/spark/sql/DataFrameNaFunctions$$anonfun$fillMap$1/1(org.apache.spark.sql.DataFrameNaFunctions)
org/apache/spark/sql/DataFrameNaFunctions/fillMap(scala.collection.Seq)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/DataFrameNaFunctions/fillMap(scala.collection.Seq)#org/apache/spark/sql/DataFrameNaFunctions$$anonfun$5/5(org.apache.spark.sql.DataFrameNaFunctions,scala.collection.Seq,scala.Function2)
org/apache/spark/sql/DataFrameNaFunctions/fillMap(scala.collection.Seq)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/DataFrameNaFunctions/fillValue(java.lang.Object,scala.collection.Seq)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/DataFrameNaFunctions/fillValue(java.lang.Object,scala.collection.Seq)#org/apache/spark/sql/catalyst/analysis/Analyzer/resolver()
org/apache/spark/sql/DataFrameNaFunctions/fillValue(java.lang.Object,scala.collection.Seq)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/DataFrameNaFunctions/fillValue(java.lang.Object,scala.collection.Seq)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/DataFrameNaFunctions/fillValue(java.lang.Object,scala.collection.Seq)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/DataFrameNaFunctions/fillValue(java.lang.Object,scala.collection.Seq)#java/lang/Class/getName()
org/apache/spark/sql/DataFrameNaFunctions/fillValue(java.lang.Object,scala.collection.Seq)#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/DataFrameNaFunctions/fillValue(java.lang.Object,scala.collection.Seq)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/DataFrameNaFunctions/fillValue(java.lang.Object,scala.collection.Seq)#java/lang/Object/getClass()
org/apache/spark/sql/DataFrameNaFunctions/fillValue(java.lang.Object,scala.collection.Seq)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/DataFrameNaFunctions/fillValue(java.lang.Object,scala.collection.Seq)#org/apache/spark/sql/DataFrameNaFunctions$$anonfun$7/7(org.apache.spark.sql.DataFrameNaFunctions,java.lang.Object,scala.collection.Seq,org.apache.spark.sql.types.AbstractDataType,scala.Function2)
org/apache/spark/sql/DataFrameNaFunctions/fillValue(java.lang.Object,scala.collection.Seq)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/DataFrameNaFunctions/fillValue(java.lang.Object,scala.collection.Seq)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/DataFrameNaFunctions/fillValue(java.lang.Object,scala.collection.Seq)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/internal/SessionResourceLoader/loadResource(org.apache.spark.sql.catalyst.catalog.FunctionResource)#org/apache/spark/sql/catalyst/catalog/FunctionResource/uri()
org/apache/spark/sql/internal/SessionResourceLoader/loadResource(org.apache.spark.sql.catalyst.catalog.FunctionResource)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/internal/SessionResourceLoader/loadResource(org.apache.spark.sql.catalyst.catalog.FunctionResource)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/internal/SessionResourceLoader/loadResource(org.apache.spark.sql.catalyst.catalog.FunctionResource)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/internal/SessionResourceLoader/loadResource(org.apache.spark.sql.catalyst.catalog.FunctionResource)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/internal/SessionResourceLoader/loadResource(org.apache.spark.sql.catalyst.catalog.FunctionResource)#org/apache/spark/SparkContext/addFile(java.lang.String)
org/apache/spark/sql/internal/SessionResourceLoader/loadResource(org.apache.spark.sql.catalyst.catalog.FunctionResource)#org/apache/spark/sql/catalyst/catalog/FunctionResource/resourceType()
org/apache/spark/sql/internal/SessionResourceLoader/loadResource(org.apache.spark.sql.catalyst.catalog.FunctionResource)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/internal/SessionResourceLoader/loadResource(org.apache.spark.sql.catalyst.catalog.FunctionResource)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/internal/SessionResourceLoader/loadResource(org.apache.spark.sql.catalyst.catalog.FunctionResource)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/internal/SessionResourceLoader/addJar(java.lang.String)#org/apache/spark/SparkContext/addJar(java.lang.String)
org/apache/spark/sql/internal/SessionResourceLoader/addJar(java.lang.String)#java/lang/Thread/setContextClassLoader(java.lang.ClassLoader)
org/apache/spark/sql/internal/SessionResourceLoader/addJar(java.lang.String)#org/apache/spark/sql/internal/NonClosableMutableURLClassLoader/addURL(java.net.URL)
org/apache/spark/sql/internal/SessionResourceLoader/addJar(java.lang.String)#java/net/URI/getScheme()
org/apache/spark/sql/internal/SessionResourceLoader/addJar(java.lang.String)#java/io/File/toURI()
org/apache/spark/sql/internal/SessionResourceLoader/addJar(java.lang.String)#org/apache/hadoop/fs/Path/Path(java.lang.String)
org/apache/spark/sql/internal/SessionResourceLoader/addJar(java.lang.String)#java/lang/Thread/currentThread()
org/apache/spark/sql/internal/SessionResourceLoader/addJar(java.lang.String)#java/net/URI/toURL()
org/apache/spark/sql/internal/SessionResourceLoader/addJar(java.lang.String)#org/apache/hadoop/fs/Path/toUri()
org/apache/spark/sql/internal/SessionResourceLoader/addJar(java.lang.String)#java/io/File/File(java.lang.String)
org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/anonfun/processInputs/1/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/anonfun/processInputs/1/apply()#org/apache/spark/sql/internal/SQLConf$/OBJECT_AGG_SORT_BASED_FALLBACK_THRESHOLD()
org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/anonfun/processInputs/1/apply()#org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator$$anonfun$processInputs$1/apply()
org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/anonfun/processInputs/1/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/anonfun/processInputs/1/apply()#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/anonfun/processInputs/1/apply()#org/apache/spark/internal/config/ConfigEntry/key()
org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/anonfun/processInputs/1/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/anonfun/processInputs/1/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/anonfun/processInputs/1/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/anonfun/processInputs/1/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/anonfun/processInputs/1/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/generateFindOrInsert()#scala/collection/Seq/zipWithIndex(scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/generateFindOrInsert()#org/apache/spark/sql/types/StructType/length()
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/generateFindOrInsert()#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/generateFindOrInsert()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/generateFindOrInsert()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/generateFindOrInsert()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/generateFindOrInsert()#org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator$$anonfun$3/3(org.apache.spark.sql.execution.aggregate.RowBasedHashMapGenerator)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/generateFindOrInsert()#org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/groupingKeys()
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/generateFindOrInsert()#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/generateFindOrInsert()#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/generateFindOrInsert()#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/generateFindOrInsert()#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/generateFindOrInsert()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/generateFindOrInsert()#org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator$$anonfun$generateFindOrInsert$2/2(org.apache.spark.sql.execution.aggregate.RowBasedHashMapGenerator)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/generateFindOrInsert()#org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator$$anonfun$5/5(org.apache.spark.sql.execution.aggregate.RowBasedHashMapGenerator)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/generateFindOrInsert()#scala/collection/TraversableOnce/mkString(java.lang.String)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/generateFindOrInsert()#org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator$$anonfun$generateFindOrInsert$1/1(org.apache.spark.sql.execution.aggregate.RowBasedHashMapGenerator)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/generateFindOrInsert()#org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator$$anonfun$4/4(org.apache.spark.sql.execution.aggregate.RowBasedHashMapGenerator)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/generateFindOrInsert()#scala/collection/TraversableOnce/count(scala.Function1)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/generateFindOrInsert()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/generateFindOrInsert()#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/generateFindOrInsert()#org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/groupingKeySignature()
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/generateEquals()#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/generateEquals()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/generateEquals()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/generateEquals()#org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/groupingKeys()
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/generateEquals()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/generateEquals()#org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/groupingKeySignature()
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/generateEquals()#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/generateEquals()#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/generateEquals()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/initializeAggregateHashMap()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/initializeAggregateHashMap()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/initializeAggregateHashMap()#scala/collection/TraversableOnce/mkString(java.lang.String)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/initializeAggregateHashMap()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/initializeAggregateHashMap()#org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator$$anonfun$2/2(org.apache.spark.sql.execution.aggregate.RowBasedHashMapGenerator)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/initializeAggregateHashMap()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/initializeAggregateHashMap()#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/initializeAggregateHashMap()#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/initializeAggregateHashMap()#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/initializeAggregateHashMap()#org/apache/spark/sql/types/StructType/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/initializeAggregateHashMap()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/initializeAggregateHashMap()#java/lang/String/concat(java.lang.String)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/initializeAggregateHashMap()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/initializeAggregateHashMap()#org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator$$anonfun$1/1(org.apache.spark.sql.execution.aggregate.RowBasedHashMapGenerator)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/initializeAggregateHashMap()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/initializeAggregateHashMap()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/anonfun/add/1/apply()#org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray$$anonfun$add$1/apply()
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/anonfun/add/1/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/anonfun/add/1/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/anonfun/add/1/apply()#java/lang/Class/getName()
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/anonfun/add/1/apply()#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/anonfun/add/1/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/anonfun/add/1/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/anonfun/add/1/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/anonfun/add/1/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/anonfun/add/1/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/anonfun/partitionPathExpression/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/ScalaUDF/ScalaUDF(java.lang.Object,org.apache.spark.sql.types.DataType,scala.collection.Seq,scala.collection.Seq,scala.Option)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/anonfun/partitionPathExpression/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/Cast/Cast(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.types.DataType,scala.Option)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/anonfun/partitionPathExpression/1/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/anonfun/partitionPathExpression/1/apply(scala.Tuple2)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/anonfun/partitionPathExpression/1/apply(scala.Tuple2)#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/anonfun/partitionPathExpression/1/apply(scala.Tuple2)#org/apache/spark/sql/execution/datasources/FileFormatWriter$DynamicPartitionWriteTask$$anonfun$partitionPathExpression$1$$anonfun$16/16(org.apache.spark.sql.execution.datasources.FileFormatWriter$DynamicPartitionWriteTask$$anonfun$partitionPathExpression$1)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/anonfun/partitionPathExpression/1/apply(scala.Tuple2)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/anonfun/partitionPathExpression/1/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/anonfun/partitionPathExpression/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/Attribute/name()
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/anonfun/partitionPathExpression/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$4()
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/anonfun/partitionPathExpression/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/ScalaUDF$/apply$default$5()
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/anonfun/partitionPathExpression/1/apply(scala.Tuple2)#scala/Tuple2/_2$mcI$sp()
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/anonfun/partitionPathExpression/1/apply(scala.Tuple2)#org/apache/spark/sql/execution/datasources/FileFormatWriter$WriteJobDescription/timeZoneId()
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/anonfun/partitionPathExpression/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/Literal$/apply(java.lang.Object)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#org/apache/spark/sql/types/MapType$/apply(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#scala/util/matching/Regex/unapplySeq(java.lang.CharSequence)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#scala/collection/LinearSeqOptimized/lengthCompare(int)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#org/apache/spark/sql/types/ArrayType$/apply(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#org/apache/spark/sql/api/r/SQLUtils$/getSQLDataType(java.lang.String)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#scala/Option/get()
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#java/lang/String/split(java.lang.String)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#org/apache/spark/sql/api/r/SQLUtils$/org$apache$spark$sql$api$r$SQLUtils$$RegexContext(scala.StringContext)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#org/apache/spark/sql/api/r/SQLUtils$$anonfun$4/4(java.lang.String)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#scala/collection/immutable/StringOps$/apply$extension(java.lang.String,int)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#scala/collection/LinearSeqOptimized/apply(int)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#org/apache/spark/sql/api/r/SQLUtils$RegexContext/r()
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#java/lang/String/length()
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#scala/Option/isEmpty()
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#org/apache/spark/sql/api/r/SQLUtils$/createStructType(scala.collection.Seq)
org/apache/spark/sql/api/r/SQLUtils/setSparkContextSessionConf(org.apache.spark.sql.SparkSession,java.util.Map)#scala/collection/TraversableLike/withFilter(scala.Function1)
org/apache/spark/sql/api/r/SQLUtils/setSparkContextSessionConf(org.apache.spark.sql.SparkSession,java.util.Map)#scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)
org/apache/spark/sql/api/r/SQLUtils/setSparkContextSessionConf(org.apache.spark.sql.SparkSession,java.util.Map)#scala/collection/convert/Decorators$AsScala/asScala()
org/apache/spark/sql/api/r/SQLUtils/setSparkContextSessionConf(org.apache.spark.sql.SparkSession,java.util.Map)#scala/collection/generic/FilterMonadic/foreach(scala.Function1)
org/apache/spark/sql/api/r/SQLUtils/setSparkContextSessionConf(org.apache.spark.sql.SparkSession,java.util.Map)#org/apache/spark/sql/api/r/SQLUtils$$anonfun$setSparkContextSessionConf$1/1()
org/apache/spark/sql/api/r/SQLUtils/setSparkContextSessionConf(org.apache.spark.sql.SparkSession,java.util.Map)#org/apache/spark/sql/api/r/SQLUtils$$anonfun$setSparkContextSessionConf$4/4(org.apache.spark.sql.SparkSession)
org/apache/spark/sql/api/r/SQLUtils/setSparkContextSessionConf(org.apache.spark.sql.SparkSession,java.util.Map)#org/apache/spark/sql/api/r/SQLUtils$$anonfun$setSparkContextSessionConf$3/3()
org/apache/spark/sql/api/r/SQLUtils/setSparkContextSessionConf(org.apache.spark.sql.SparkSession,java.util.Map)#org/apache/spark/sql/api/r/SQLUtils$/setSparkContextSessionConf(org.apache.spark.sql.SparkSession,java.util.Map)
org/apache/spark/sql/api/r/SQLUtils/setSparkContextSessionConf(org.apache.spark.sql.SparkSession,java.util.Map)#org/apache/spark/sql/api/r/SQLUtils$$anonfun$setSparkContextSessionConf$2/2(org.apache.spark.sql.SparkSession)
org/apache/spark/sql/api/r/SQLUtils/rowToRBytes(org.apache.spark.sql.Row)#org/apache/spark/sql/Row/length()
org/apache/spark/sql/api/r/SQLUtils/rowToRBytes(org.apache.spark.sql.Row)#scala/reflect/ClassTag$/Object()
org/apache/spark/sql/api/r/SQLUtils/rowToRBytes(org.apache.spark.sql.Row)#scala/collection/immutable/Range/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/api/r/SQLUtils/rowToRBytes(org.apache.spark.sql.Row)#scala/collection/immutable/IndexedSeq$/canBuildFrom()
org/apache/spark/sql/api/r/SQLUtils/rowToRBytes(org.apache.spark.sql.Row)#scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/api/r/SQLUtils/rowToRBytes(org.apache.spark.sql.Row)#scala/runtime/RichInt$/until$extension0(int,int)
org/apache/spark/sql/api/r/SQLUtils/rowToRBytes(org.apache.spark.sql.Row)#org/apache/spark/api/r/SerDe$/writeObject(java.io.DataOutputStream,java.lang.Object,org.apache.spark.api.r.JVMObjectTracker)
org/apache/spark/sql/api/r/SQLUtils/rowToRBytes(org.apache.spark.sql.Row)#scala/Predef$/intWrapper(int)
org/apache/spark/sql/api/r/SQLUtils/rowToRBytes(org.apache.spark.sql.Row)#java/io/DataOutputStream/DataOutputStream(java.io.OutputStream)
org/apache/spark/sql/api/r/SQLUtils/rowToRBytes(org.apache.spark.sql.Row)#java/io/ByteArrayOutputStream/toByteArray()
org/apache/spark/sql/api/r/SQLUtils/rowToRBytes(org.apache.spark.sql.Row)#org/apache/spark/sql/api/r/SQLUtils$$anonfun$6/6(org.apache.spark.sql.Row)
org/apache/spark/sql/api/r/SQLUtils/rowToRBytes(org.apache.spark.sql.Row)#java/io/ByteArrayOutputStream/ByteArrayOutputStream()
org/apache/spark/sql/api/r/SQLUtils/getOrCreateSparkSession(org.apache.spark.api.java.JavaSparkContext,java.util.Map,boolean)#org/apache/spark/SparkConf/get(java.lang.String,java.lang.String)
org/apache/spark/sql/api/r/SQLUtils/getOrCreateSparkSession(org.apache.spark.api.java.JavaSparkContext,java.util.Map,boolean)#org/apache/spark/sql/SparkSession$/builder()
org/apache/spark/sql/api/r/SQLUtils/getOrCreateSparkSession(org.apache.spark.api.java.JavaSparkContext,java.util.Map,boolean)#org/apache/spark/sql/api/r/SQLUtils$$anonfun$3/3()
org/apache/spark/sql/api/r/SQLUtils/getOrCreateSparkSession(org.apache.spark.api.java.JavaSparkContext,java.util.Map,boolean)#org/apache/spark/sql/SparkSession$/hiveClassesArePresent()
org/apache/spark/sql/api/r/SQLUtils/getOrCreateSparkSession(org.apache.spark.api.java.JavaSparkContext,java.util.Map,boolean)#java/lang/String/toLowerCase(java.util.Locale)
org/apache/spark/sql/api/r/SQLUtils/getOrCreateSparkSession(org.apache.spark.api.java.JavaSparkContext,java.util.Map,boolean)#org/apache/spark/sql/api/r/SQLUtils$/getOrCreateSparkSession(org.apache.spark.api.java.JavaSparkContext,java.util.Map,boolean)
org/apache/spark/sql/api/r/SQLUtils/getOrCreateSparkSession(org.apache.spark.api.java.JavaSparkContext,java.util.Map,boolean)#org/apache/spark/SparkContext/conf()
org/apache/spark/sql/api/r/SQLUtils/getOrCreateSparkSession(org.apache.spark.api.java.JavaSparkContext,java.util.Map,boolean)#org/apache/spark/sql/api/r/SQLUtils$/logWarning(scala.Function0)
org/apache/spark/sql/api/r/SQLUtils/getOrCreateSparkSession(org.apache.spark.api.java.JavaSparkContext,java.util.Map,boolean)#org/apache/spark/sql/internal/StaticSQLConf$/CATALOG_IMPLEMENTATION()
org/apache/spark/sql/api/r/SQLUtils/getOrCreateSparkSession(org.apache.spark.api.java.JavaSparkContext,java.util.Map,boolean)#org/apache/spark/sql/SparkSession$Builder/getOrCreate()
org/apache/spark/sql/api/r/SQLUtils/getOrCreateSparkSession(org.apache.spark.api.java.JavaSparkContext,java.util.Map,boolean)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/api/r/SQLUtils/getOrCreateSparkSession(org.apache.spark.api.java.JavaSparkContext,java.util.Map,boolean)#org/apache/spark/sql/api/r/SQLUtils$/withHiveExternalCatalog(org.apache.spark.SparkContext)
org/apache/spark/sql/api/r/SQLUtils/getOrCreateSparkSession(org.apache.spark.api.java.JavaSparkContext,java.util.Map,boolean)#org/apache/spark/sql/api/r/SQLUtils$/setSparkContextSessionConf(org.apache.spark.sql.SparkSession,java.util.Map)
org/apache/spark/sql/api/r/SQLUtils/getOrCreateSparkSession(org.apache.spark.api.java.JavaSparkContext,java.util.Map,boolean)#org/apache/spark/sql/SparkSession$Builder/sparkContext(org.apache.spark.SparkContext)
org/apache/spark/sql/api/r/SQLUtils/getOrCreateSparkSession(org.apache.spark.api.java.JavaSparkContext,java.util.Map,boolean)#org/apache/spark/api/java/JavaSparkContext/sc()
org/apache/spark/sql/api/r/SQLUtils/getOrCreateSparkSession(org.apache.spark.api.java.JavaSparkContext,java.util.Map,boolean)#org/apache/spark/internal/config/ConfigEntry/key()
org/apache/spark/sql/api/r/SQLUtils/getTableNames(org.apache.spark.sql.SparkSession,java.lang.String)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/api/r/SQLUtils/getTableNames(org.apache.spark.sql.SparkSession,java.lang.String)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/api/r/SQLUtils/getTableNames(org.apache.spark.sql.SparkSession,java.lang.String)#org/apache/spark/sql/api/r/SQLUtils$/getTableNames(org.apache.spark.sql.SparkSession,java.lang.String)
org/apache/spark/sql/api/r/SQLUtils/getTableNames(org.apache.spark.sql.SparkSession,java.lang.String)#org/apache/spark/sql/api/r/SQLUtils$$anonfun$getTableNames$1/1()
org/apache/spark/sql/api/r/SQLUtils/getTableNames(org.apache.spark.sql.SparkSession,java.lang.String)#scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/api/r/SQLUtils/getTableNames(org.apache.spark.sql.SparkSession,java.lang.String)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/api/r/SQLUtils/getTableNames(org.apache.spark.sql.SparkSession,java.lang.String)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/api/r/SQLUtils/getTableNames(org.apache.spark.sql.SparkSession,java.lang.String)#scala/collection/immutable/StringOps/nonEmpty()
org/apache/spark/sql/api/r/SQLUtils/getTableNames(org.apache.spark.sql.SparkSession,java.lang.String)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/api/r/SQLUtils/getTableNames(org.apache.spark.sql.SparkSession,java.lang.String)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/listTables(java.lang.String)
org/apache/spark/sql/api/r/SQLUtils/getTableNames(org.apache.spark.sql.SparkSession,java.lang.String)#java/lang/String/trim()
org/apache/spark/sql/api/r/SQLUtils/getTables(org.apache.spark.sql.SparkSession,java.lang.String)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/api/r/SQLUtils/getTables(org.apache.spark.sql.SparkSession,java.lang.String)#org/apache/spark/sql/execution/command/ShowTablesCommand$/apply$default$3()
org/apache/spark/sql/api/r/SQLUtils/getTables(org.apache.spark.sql.SparkSession,java.lang.String)#org/apache/spark/sql/execution/command/ShowTablesCommand$/apply$default$4()
org/apache/spark/sql/api/r/SQLUtils/getTables(org.apache.spark.sql.SparkSession,java.lang.String)#java/lang/String/trim()
org/apache/spark/sql/api/r/SQLUtils/getTables(org.apache.spark.sql.SparkSession,java.lang.String)#org/apache/spark/sql/Dataset$/ofRows(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/api/r/SQLUtils/getTables(org.apache.spark.sql.SparkSession,java.lang.String)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/api/r/SQLUtils/getTables(org.apache.spark.sql.SparkSession,java.lang.String)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/api/r/SQLUtils/getTables(org.apache.spark.sql.SparkSession,java.lang.String)#org/apache/spark/sql/api/r/SQLUtils$/getTables(org.apache.spark.sql.SparkSession,java.lang.String)
org/apache/spark/sql/api/r/SQLUtils/getTables(org.apache.spark.sql.SparkSession,java.lang.String)#scala/collection/immutable/StringOps/nonEmpty()
org/apache/spark/sql/api/r/SQLUtils/bytesToRow(byte[],org.apache.spark.sql.types.StructType)#scala/collection/immutable/Range/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/api/r/SQLUtils/bytesToRow(byte[],org.apache.spark.sql.types.StructType)#java/io/ByteArrayInputStream/ByteArrayInputStream(byte[])
org/apache/spark/sql/api/r/SQLUtils/bytesToRow(byte[],org.apache.spark.sql.types.StructType)#java/io/DataInputStream/DataInputStream(java.io.InputStream)
org/apache/spark/sql/api/r/SQLUtils/bytesToRow(byte[],org.apache.spark.sql.types.StructType)#scala/collection/immutable/IndexedSeq$/canBuildFrom()
org/apache/spark/sql/api/r/SQLUtils/bytesToRow(byte[],org.apache.spark.sql.types.StructType)#org/apache/spark/sql/api/r/SQLUtils$$anonfun$bytesToRow$1/1(org.apache.spark.sql.types.StructType,java.io.DataInputStream)
org/apache/spark/sql/api/r/SQLUtils/bytesToRow(byte[],org.apache.spark.sql.types.StructType)#org/apache/spark/sql/Row$/fromSeq(scala.collection.Seq)
org/apache/spark/sql/api/r/SQLUtils/bytesToRow(byte[],org.apache.spark.sql.types.StructType)#org/apache/spark/api/r/SerDe$/readInt(java.io.DataInputStream)
org/apache/spark/sql/api/r/SQLUtils/bytesToRow(byte[],org.apache.spark.sql.types.StructType)#scala/runtime/RichInt$/until$extension0(int,int)
org/apache/spark/sql/api/r/SQLUtils/bytesToRow(byte[],org.apache.spark.sql.types.StructType)#scala/Predef$/intWrapper(int)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/2/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/types/DecimalType/scale()
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/2/apply(org.apache.spark.sql.types.StructField)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/2/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addReferenceMinorObj(java.lang.Object,java.lang.String)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/2/apply(org.apache.spark.sql.types.StructField)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/2/apply(org.apache.spark.sql.types.StructField)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/2/apply(org.apache.spark.sql.types.StructField)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/2/apply(org.apache.spark.sql.types.StructField)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/2/apply(org.apache.spark.sql.types.StructField)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/2/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/types/StructField/dataType()
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/2/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addReferenceMinorObj$default$2()
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/2/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/types/StructField/name()
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/2/apply(org.apache.spark.sql.types.StructField)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/2/apply(org.apache.spark.sql.types.StructField)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/2/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/types/DecimalType/precision()
org/apache/spark/sql/execution/aggregate/SortBasedAggregator/anon/1/findNextSortedGroup()#org/apache/spark/unsafe/KVIterator/next()
org/apache/spark/sql/execution/aggregate/SortBasedAggregator/anon/1/findNextSortedGroup()#org/apache/spark/sql/execution/UnsafeKVExternalSorter$KVSorterIterator/getValue()
org/apache/spark/sql/execution/aggregate/SortBasedAggregator/anon/1/findNextSortedGroup()#org/apache/spark/sql/catalyst/expressions/UnsafeRow/copy()
org/apache/spark/sql/execution/aggregate/SortBasedAggregator/anon/1/findNextSortedGroup()#org/apache/spark/sql/catalyst/expressions/codegen/BaseOrdering/compare(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/aggregate/SortBasedAggregator/anon/1/findNextSortedGroup()#org/apache/spark/sql/execution/UnsafeKVExternalSorter$KVSorterIterator/getKey()
org/apache/spark/sql/execution/aggregate/SortBasedAggregator/anon/1/findNextSortedGroup()#org/apache/spark/sql/execution/aggregate/SortBasedAggregator$$anon$1/groupingKey_$eq(org.apache.spark.sql.catalyst.expressions.UnsafeRow)
org/apache/spark/sql/execution/aggregate/SortBasedAggregator/anon/1/findNextSortedGroup()#org/apache/spark/unsafe/KVIterator/getKey()
org/apache/spark/sql/execution/aggregate/SortBasedAggregator/anon/1/findNextSortedGroup()#org/apache/spark/sql/execution/UnsafeKVExternalSorter$KVSorterIterator/next()
org/apache/spark/sql/execution/aggregate/SortBasedAggregator/anon/1/findNextSortedGroup()#org/apache/spark/sql/execution/aggregate/SortBasedAggregator$$anon$1/result()
org/apache/spark/sql/execution/aggregate/SortBasedAggregator/anon/1/findNextSortedGroup()#org/apache/spark/sql/execution/aggregate/SortBasedAggregator$$anon$1/hasNextInput()
org/apache/spark/sql/execution/aggregate/SortBasedAggregator/anon/1/findNextSortedGroup()#org/apache/spark/sql/execution/aggregate/SortBasedAggregator$$anon$1/inputIterator()
org/apache/spark/sql/execution/aggregate/SortBasedAggregator/anon/1/findNextSortedGroup()#scala/Function0/apply()
org/apache/spark/sql/execution/aggregate/SortBasedAggregator/anon/1/findNextSortedGroup()#org/apache/spark/sql/execution/aggregate/SortBasedAggregator$$anon$1/hasNextInput_$eq(boolean)
org/apache/spark/sql/execution/aggregate/SortBasedAggregator/anon/1/findNextSortedGroup()#org/apache/spark/unsafe/KVIterator/getValue()
org/apache/spark/sql/execution/aggregate/SortBasedAggregator/anon/1/findNextSortedGroup()#org/apache/spark/sql/execution/aggregate/SortBasedAggregator$$anon$1/groupingKey()
org/apache/spark/sql/execution/aggregate/SortBasedAggregator/anon/1/findNextSortedGroup()#scala/Function2/apply(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/aggregate/SortBasedAggregator/anon/1/findNextSortedGroup()#org/apache/spark/sql/execution/aggregate/SortBasedAggregator$$anon$1/findGroupingKey()
org/apache/spark/sql/execution/aggregate/SortBasedAggregator/anon/1/findNextSortedGroup()#org/apache/spark/sql/execution/aggregate/SortBasedAggregator$$anon$1/hasNextAggBuffer_$eq(boolean)
org/apache/spark/sql/execution/aggregate/SortBasedAggregator/anon/1/findNextSortedGroup()#org/apache/spark/sql/execution/aggregate/SortBasedAggregator$$anon$1/hasNextAggBuffer()
org/apache/spark/sql/execution/aggregate/SortBasedAggregator/anon/1/findNextSortedGroup()#org/apache/spark/sql/execution/aggregate/SortBasedAggregator$$anon$1/result_$eq(org.apache.spark.sql.execution.aggregate.AggregationBufferEntry)
org/apache/spark/sql/execution/aggregate/SortBasedAggregator/anon/1/findGroupingKey()#org/apache/spark/sql/execution/aggregate/SortBasedAggregator$$anon$1/hasNextInput()
org/apache/spark/sql/execution/aggregate/SortBasedAggregator/anon/1/findGroupingKey()#org/apache/spark/sql/execution/aggregate/SortBasedAggregator$$anon$1/inputIterator()
org/apache/spark/sql/execution/aggregate/SortBasedAggregator/anon/1/findGroupingKey()#org/apache/spark/sql/catalyst/expressions/UnsafeRow/copy()
org/apache/spark/sql/execution/aggregate/SortBasedAggregator/anon/1/findGroupingKey()#org/apache/spark/sql/execution/aggregate/SortBasedAggregator$$anon$1/groupingKey()
org/apache/spark/sql/execution/aggregate/SortBasedAggregator/anon/1/findGroupingKey()#org/apache/spark/sql/catalyst/expressions/codegen/BaseOrdering/compare(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/aggregate/SortBasedAggregator/anon/1/findGroupingKey()#org/apache/spark/sql/execution/UnsafeKVExternalSorter$KVSorterIterator/getKey()
org/apache/spark/sql/execution/aggregate/SortBasedAggregator/anon/1/findGroupingKey()#org/apache/spark/sql/execution/aggregate/SortBasedAggregator$$anon$1/groupingKey_$eq(org.apache.spark.sql.catalyst.expressions.UnsafeRow)
org/apache/spark/sql/execution/aggregate/SortBasedAggregator/anon/1/findGroupingKey()#org/apache/spark/sql/execution/aggregate/SortBasedAggregator$$anon$1/hasNextAggBuffer()
org/apache/spark/sql/execution/aggregate/SortBasedAggregator/anon/1/findGroupingKey()#org/apache/spark/unsafe/KVIterator/getKey()
org/apache/spark/sql/execution/aggregate/SortBasedAggregator/anon/1/findGroupingKey()#org/apache/spark/sql/catalyst/expressions/UnsafeRow/copyFrom(org.apache.spark.sql.catalyst.expressions.UnsafeRow)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#scala/reflect/ClassTag$/Int()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#org/apache/spark/sql/catalyst/expressions/codegen/Predicate/initialize(int)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#scala/collection/generic/GenericTraversableTemplate/unzip(scala.Function1)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#org/apache/spark/sql/execution/columnar/ColumnarIterator/hasNext()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#org/apache/spark/sql/execution/columnar/InMemoryTableScanExec$$anonfun$doExecute$1$$anonfun$4/4(org.apache.spark.sql.execution.columnar.InMemoryTableScanExec$$anonfun$doExecute$1)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#org/apache/spark/sql/execution/columnar/InMemoryTableScanExec$$anonfun$doExecute$1$$anonfun$6/6(org.apache.spark.sql.execution.columnar.InMemoryTableScanExec$$anonfun$doExecute$1,org.apache.spark.sql.catalyst.expressions.codegen.Predicate)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#org/apache/spark/sql/execution/columnar/GenerateColumnAccessor$/generate(java.lang.Object)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#org/apache/spark/sql/execution/columnar/InMemoryTableScanExec$$anonfun$doExecute$1$$anonfun$8/8(org.apache.spark.sql.execution.columnar.InMemoryTableScanExec$$anonfun$doExecute$1)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#scala/collection/Seq/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#scala/collection/Seq/reduceOption(scala.Function2)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#scala/collection/Iterator/filter(scala.Function1)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/newPredicate(org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#scala/Tuple2/_1()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#scala/Tuple2/_2()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#scala/collection/Iterator/map(scala.Function1)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#scala/Predef$/$conforms()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#org/apache/spark/util/LongAccumulator/add(long)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#org/apache/spark/sql/execution/columnar/InMemoryTableScanExec$$anonfun$doExecute$1$$anonfun$5/5(org.apache.spark.sql.execution.columnar.InMemoryTableScanExec$$anonfun$doExecute$1)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#org/apache/spark/sql/execution/columnar/InMemoryTableScanExec$$anonfun$doExecute$1$$anonfun$7/7(org.apache.spark.sql.execution.columnar.InMemoryTableScanExec$$anonfun$doExecute$1)
org/apache/spark/sql/SQLImplicits/typecreator12/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ModuleSymbolApi/asModule()
org/apache/spark/sql/SQLImplicits/typecreator12/1/apply(scala.reflect.api.Mirror)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/SQLImplicits/typecreator12/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticPackage(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator12/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$InternalApi/reificationSupport()
org/apache/spark/sql/SQLImplicits/typecreator12/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/ThisType(scala.reflect.api.Symbols$SymbolApi)
org/apache/spark/sql/SQLImplicits/typecreator12/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ClassSymbolApi/asType()
org/apache/spark/sql/SQLImplicits/typecreator12/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ModuleSymbolApi/moduleClass()
org/apache/spark/sql/SQLImplicits/typecreator12/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/TypeRef(scala.reflect.api.Types$TypeApi,scala.reflect.api.Symbols$SymbolApi,scala.collection.immutable.List)
org/apache/spark/sql/SQLImplicits/typecreator12/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticClass(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator12/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/universe()
org/apache/spark/sql/SQLImplicits/typecreator12/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$TypeSymbolApi/toTypeConstructor()
org/apache/spark/sql/SQLImplicits/typecreator12/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Universe/internal()
org/apache/spark/sql/SQLImplicits/typecreator12/1/apply(scala.reflect.api.Mirror)#scala/collection/immutable/List$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/ui/ExecutionTable/toNodeSeq()#org/apache/spark/ui/UIUtils$/listingTable(scala.collection.Seq,scala.Function1,scala.collection.Iterable,boolean,scala.Option,scala.collection.Seq,boolean,boolean)
org/apache/spark/sql/execution/ui/ExecutionTable/toNodeSeq()#org/apache/spark/ui/UIUtils$/listingTable$default$4()
org/apache/spark/sql/execution/ui/ExecutionTable/toNodeSeq()#org/apache/spark/ui/UIUtils$/listingTable$default$6()
org/apache/spark/sql/execution/ui/ExecutionTable/toNodeSeq()#org/apache/spark/ui/UIUtils$/listingTable$default$7()
org/apache/spark/sql/execution/ui/ExecutionTable/toNodeSeq()#org/apache/spark/sql/execution/ui/ExecutionTable$$anonfun$8/8(org.apache.spark.sql.execution.ui.ExecutionTable)
org/apache/spark/sql/execution/ui/ExecutionTable/toNodeSeq()#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/ui/ExecutionTable/toNodeSeq()#scala/xml/NodeBuffer/$amp$plus(java.lang.Object)
org/apache/spark/sql/execution/ui/ExecutionTable/toNodeSeq()#scala/xml/Text/Text(java.lang.String)
org/apache/spark/sql/execution/ui/ExecutionTable/toNodeSeq()#org/apache/spark/ui/UIUtils$/listingTable$default$8()
org/apache/spark/sql/execution/ui/ExecutionTable/toNodeSeq()#scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)
org/apache/spark/sql/execution/ui/ExecutionTable/toNodeSeq()#scala/xml/NodeBuffer/NodeBuffer()
org/apache/spark/sql/execution/ui/ExecutionTable/descriptionCell(org.apache.spark.sql.execution.ui.SQLExecutionUIData)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/ui/ExecutionTable/descriptionCell(org.apache.spark.sql.execution.ui.SQLExecutionUIData)#scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,java.lang.String,scala.xml.MetaData)
org/apache/spark/sql/execution/ui/ExecutionTable/descriptionCell(org.apache.spark.sql.execution.ui.SQLExecutionUIData)#scala/xml/NodeSeq$/canBuildFrom()
org/apache/spark/sql/execution/ui/ExecutionTable/descriptionCell(org.apache.spark.sql.execution.ui.SQLExecutionUIData)#scala/xml/Text/Text(java.lang.String)
org/apache/spark/sql/execution/ui/ExecutionTable/descriptionCell(org.apache.spark.sql.execution.ui.SQLExecutionUIData)#scala/xml/NodeBuffer/$amp$plus(java.lang.Object)
org/apache/spark/sql/execution/ui/ExecutionTable/descriptionCell(org.apache.spark.sql.execution.ui.SQLExecutionUIData)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/ui/ExecutionTable/descriptionCell(org.apache.spark.sql.execution.ui.SQLExecutionUIData)#scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,scala.collection.Seq,scala.xml.MetaData)
org/apache/spark/sql/execution/ui/ExecutionTable/descriptionCell(org.apache.spark.sql.execution.ui.SQLExecutionUIData)#scala/collection/immutable/StringOps/nonEmpty()
org/apache/spark/sql/execution/ui/ExecutionTable/descriptionCell(org.apache.spark.sql.execution.ui.SQLExecutionUIData)#scala/xml/Elem/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/ui/ExecutionTable/descriptionCell(org.apache.spark.sql.execution.ui.SQLExecutionUIData)#scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)
org/apache/spark/sql/execution/ui/ExecutionTable/descriptionCell(org.apache.spark.sql.execution.ui.SQLExecutionUIData)#scala/xml/NodeBuffer/NodeBuffer()
org/apache/spark/sql/execution/ui/ExecutionTable/executionURL(long)#org/apache/spark/ui/UIUtils$/prependBaseUri$default$2()
org/apache/spark/sql/execution/ui/ExecutionTable/executionURL(long)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/ui/ExecutionTable/executionURL(long)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/ui/ExecutionTable/executionURL(long)#org/apache/spark/sql/execution/ui/SQLTab/prefix()
org/apache/spark/sql/execution/ui/ExecutionTable/executionURL(long)#org/apache/spark/ui/UIUtils$/prependBaseUri(java.lang.String,java.lang.String)
org/apache/spark/sql/execution/ui/ExecutionTable/executionURL(long)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/ui/ExecutionTable/executionURL(long)#org/apache/spark/sql/execution/ui/SQLTab/basePath()
org/apache/spark/sql/execution/ui/ExecutionTable/executionURL(long)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/ui/ExecutionTable/executionURL(long)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/ui/ExecutionTable/row(long,org.apache.spark.sql.execution.ui.SQLExecutionUIData)#scala/xml/Text/Text(java.lang.String)
org/apache/spark/sql/execution/ui/ExecutionTable/row(long,org.apache.spark.sql.execution.ui.SQLExecutionUIData)#org/apache/spark/sql/execution/ui/ExecutionTable$$anonfun$7/7(org.apache.spark.sql.execution.ui.ExecutionTable)
org/apache/spark/sql/execution/ui/ExecutionTable/row(long,org.apache.spark.sql.execution.ui.SQLExecutionUIData)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/ui/ExecutionTable/row(long,org.apache.spark.sql.execution.ui.SQLExecutionUIData)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/ui/ExecutionTable/row(long,org.apache.spark.sql.execution.ui.SQLExecutionUIData)#scala/xml/NodeBuffer/$amp$plus(java.lang.Object)
org/apache/spark/sql/execution/ui/ExecutionTable/row(long,org.apache.spark.sql.execution.ui.SQLExecutionUIData)#org/apache/spark/ui/UIUtils$/formatDate(long)
org/apache/spark/sql/execution/ui/ExecutionTable/row(long,org.apache.spark.sql.execution.ui.SQLExecutionUIData)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/ui/ExecutionTable/row(long,org.apache.spark.sql.execution.ui.SQLExecutionUIData)#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/ui/ExecutionTable/row(long,org.apache.spark.sql.execution.ui.SQLExecutionUIData)#org/apache/spark/ui/UIUtils$/formatDuration(long)
org/apache/spark/sql/execution/ui/ExecutionTable/row(long,org.apache.spark.sql.execution.ui.SQLExecutionUIData)#scala/collection/Seq/sorted(scala.math.Ordering)
org/apache/spark/sql/execution/ui/ExecutionTable/row(long,org.apache.spark.sql.execution.ui.SQLExecutionUIData)#java/lang/Object/toString()
org/apache/spark/sql/execution/ui/ExecutionTable/row(long,org.apache.spark.sql.execution.ui.SQLExecutionUIData)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/ui/ExecutionTable/row(long,org.apache.spark.sql.execution.ui.SQLExecutionUIData)#scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,java.lang.String,scala.xml.MetaData)
org/apache/spark/sql/execution/ui/ExecutionTable/row(long,org.apache.spark.sql.execution.ui.SQLExecutionUIData)#org/apache/spark/sql/execution/ui/ExecutionTable$$anonfun$5/5(org.apache.spark.sql.execution.ui.ExecutionTable)
org/apache/spark/sql/execution/ui/ExecutionTable/row(long,org.apache.spark.sql.execution.ui.SQLExecutionUIData)#org/apache/spark/sql/execution/ui/ExecutionTable$$anonfun$1/1(org.apache.spark.sql.execution.ui.ExecutionTable,long)
org/apache/spark/sql/execution/ui/ExecutionTable/row(long,org.apache.spark.sql.execution.ui.SQLExecutionUIData)#org/apache/spark/sql/execution/ui/ExecutionTable$$anonfun$6/6(org.apache.spark.sql.execution.ui.ExecutionTable)
org/apache/spark/sql/execution/ui/ExecutionTable/row(long,org.apache.spark.sql.execution.ui.SQLExecutionUIData)#scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)
org/apache/spark/sql/execution/ui/ExecutionTable/row(long,org.apache.spark.sql.execution.ui.SQLExecutionUIData)#scala/xml/NodeBuffer/NodeBuffer()
org/apache/spark/sql/execution/ui/ExecutionTable/row(long,org.apache.spark.sql.execution.ui.SQLExecutionUIData)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/4/apply()#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$populateStartOffsets$4/apply()
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/4/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/4/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/4/apply()#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/4/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/4/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/4/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/4/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/4/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/CodecStreams/createOutputStream(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)#org/apache/spark/sql/execution/datasources/CodecStreams$$anonfun$createOutputStream$1/1(java.io.OutputStream)
org/apache/spark/sql/execution/datasources/CodecStreams/createOutputStream(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)#org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/datasources/CodecStreams/createOutputStream(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)#org/apache/spark/sql/execution/datasources/CodecStreams$$anonfun$createOutputStream$2/2(java.io.OutputStream)
org/apache/spark/sql/execution/datasources/CodecStreams/createOutputStream(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)#org/apache/spark/sql/execution/datasources/CodecStreams$/createOutputStream(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/datasources/CodecStreams/createOutputStream(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)#org/apache/hadoop/fs/FileSystem/create(org.apache.hadoop.fs.Path,boolean)
org/apache/spark/sql/execution/datasources/CodecStreams/createOutputStream(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/datasources/CodecStreams/createOutputStream(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/datasources/CodecStreams/createOutputStream(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)#org/apache/spark/sql/execution/datasources/CodecStreams$/getCompressionCodec(org.apache.hadoop.mapreduce.JobContext,scala.Option)
org/apache/spark/sql/execution/datasources/CodecStreams/createOutputStream(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)#org/apache/hadoop/mapreduce/JobContext/getConfiguration()
org/apache/spark/sql/execution/datasources/CodecStreams/createOutputStream(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDescribeFunction/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$DescribeFunctionContext/EXTENDED()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDescribeFunction/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$DescribeFunctionContext/describeFuncName()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDescribeFunction/1/apply()#org/apache/spark/sql/catalyst/FunctionIdentifier/FunctionIdentifier(java.lang.String,scala.Option)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDescribeFunction/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$DescribeFuncNameContext/STRING()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDescribeFunction/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitDescribeFunction$1/apply()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDescribeFunction/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$DescribeFuncNameContext/getText()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDescribeFunction/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$DescribeFuncNameContext/qualifiedName()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDescribeFunction/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder/visitFunctionName(org.apache.spark.sql.catalyst.parser.SqlBaseParser$QualifiedNameContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDescribeFunction/1/apply()#org/apache/spark/sql/catalyst/parser/ParserUtils$/string(org.antlr.v4.runtime.tree.TerminalNode)
org/apache/spark/sql/execution/joins/HashJoin/rewriteKeyExpr(scala.collection.Seq)#scala/collection/Seq/exists(scala.Function1)
org/apache/spark/sql/execution/joins/HashJoin/rewriteKeyExpr(scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/Cast/Cast(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.types.DataType,scala.Option)
org/apache/spark/sql/execution/joins/HashJoin/rewriteKeyExpr(scala.collection.Seq)#scala/Predef$/assert(boolean)
org/apache/spark/sql/execution/joins/HashJoin/rewriteKeyExpr(scala.collection.Seq)#org/apache/spark/sql/execution/joins/HashJoin$$anonfun$rewriteKeyExpr$1/1()
org/apache/spark/sql/execution/joins/HashJoin/rewriteKeyExpr(scala.collection.Seq)#org/apache/spark/sql/execution/joins/HashJoin$$anonfun$rewriteKeyExpr$3/3(scala.runtime.ObjectRef)
org/apache/spark/sql/execution/joins/HashJoin/rewriteKeyExpr(scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/Expression/dataType()
org/apache/spark/sql/execution/joins/HashJoin/rewriteKeyExpr(scala.collection.Seq)#scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)
org/apache/spark/sql/execution/joins/HashJoin/rewriteKeyExpr(scala.collection.Seq)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/joins/HashJoin/rewriteKeyExpr(scala.collection.Seq)#scala/runtime/ObjectRef/create(java.lang.Object)
org/apache/spark/sql/execution/joins/HashJoin/rewriteKeyExpr(scala.collection.Seq)#scala/collection/Seq/tail()
org/apache/spark/sql/execution/joins/HashJoin/rewriteKeyExpr(scala.collection.Seq)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/joins/HashJoin/rewriteKeyExpr(scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/joins/HashJoin/rewriteKeyExpr(scala.collection.Seq)#scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)
org/apache/spark/sql/execution/joins/HashJoin/rewriteKeyExpr(scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/Cast$/apply$default$3()
org/apache/spark/sql/execution/joins/HashJoin/rewriteKeyExpr(scala.collection.Seq)#org/apache/spark/sql/execution/joins/HashJoin$$anonfun$rewriteKeyExpr$2/2()
org/apache/spark/sql/execution/joins/HashJoin/rewriteKeyExpr(scala.collection.Seq)#scala/collection/TraversableOnce/sum(scala.math.Numeric)
org/apache/spark/sql/execution/joins/HashJoin/rewriteKeyExpr(scala.collection.Seq)#scala/collection/IterableLike/foreach(scala.Function1)
org/apache/spark/sql/execution/joins/HashJoin/rewriteKeyExpr(scala.collection.Seq)#scala/collection/Seq/head()
org/apache/spark/sql/execution/joins/HashJoin/rewriteKeyExpr(scala.collection.Seq)#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/streaming/StreamingQueryManager/createQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#org/apache/spark/sql/streaming/StreamingQueryManager$$anonfun$1/1(org.apache.spark.sql.streaming.StreamingQueryManager)
org/apache/spark/sql/streaming/StreamingQueryManager/createQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#org/apache/hadoop/fs/Path/Path(java.lang.String,java.lang.String)
org/apache/spark/sql/streaming/StreamingQueryManager/createQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#org/apache/hadoop/fs/FileSystem/exists(org.apache.hadoop.fs.Path)
org/apache/spark/sql/streaming/StreamingQueryManager/createQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#org/apache/spark/sql/streaming/StreamingQueryManager$$anonfun$createQuery$1/1(org.apache.spark.sql.streaming.StreamingQueryManager)
org/apache/spark/sql/streaming/StreamingQueryManager/createQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/streaming/StreamingQueryManager/createQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#scala/Option/orElse(scala.Function0)
org/apache/spark/sql/streaming/StreamingQueryManager/createQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#org/apache/spark/sql/streaming/StreamingQueryManager$$anonfun$2/2(org.apache.spark.sql.streaming.StreamingQueryManager,scala.Option,org.apache.spark.sql.Dataset)
org/apache/spark/sql/streaming/StreamingQueryManager/createQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/streaming/StreamingQueryManager/createQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/streaming/StreamingQueryManager/createQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/streaming/StreamingQueryManager/createQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/streaming/StreamingQueryManager/createQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#scala/runtime/BooleanRef/create(boolean)
org/apache/spark/sql/streaming/StreamingQueryManager/createQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#org/apache/spark/sql/catalyst/analysis/UnsupportedOperationChecker$/checkForStreaming(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,org.apache.spark.sql.streaming.OutputMode)
org/apache/spark/sql/streaming/StreamingQueryManager/createQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/streaming/StreamingQueryManager/createQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#scala/Option/orNull(scala.Predef$$less$colon$less)
org/apache/spark/sql/streaming/StreamingQueryManager/createQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#scala/Option/map(scala.Function1)
org/apache/spark/sql/streaming/StreamingQueryManager/createQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/streaming/StreamingQueryManager/createQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/streaming/StreamingQueryManager/createQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/streaming/StreamingQueryManager/createQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/streaming/StreamingQueryManager/createQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#org/apache/spark/sql/internal/SQLConf/isUnsupportedOperationCheckEnabled()
org/apache/spark/sql/streaming/StreamingQueryManager/createQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#org/apache/spark/sql/streaming/StreamingQueryManager$$anonfun$3/3(org.apache.spark.sql.streaming.StreamingQueryManager,boolean,scala.runtime.BooleanRef)
org/apache/spark/sql/streaming/StreamingQueryManager/createQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#scala/Predef$/$conforms()
org/apache/spark/sql/streaming/StreamingQueryManager/createQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/streaming/StreamingQueryManager/createQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/streaming/StreamingQueryManager/createQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/streaming/StreamingQueryManager/createQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/streaming/StreamingQueryManager/createQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#org/apache/spark/sql/internal/SQLConf/adaptiveExecutionEnabled()
org/apache/spark/sql/streaming/StreamingQueryManager/startQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#scala/Option/foreach(scala.Function1)
org/apache/spark/sql/streaming/StreamingQueryManager/startQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#org/apache/spark/sql/streaming/StreamingQueryManager$$anonfun$startQuery$1/1(org.apache.spark.sql.streaming.StreamingQueryManager)
org/apache/spark/sql/streaming/StreamingQueryManager/startQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/streaming/StreamingQueryManager/startQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#java/lang/IllegalStateException/IllegalStateException(java.lang.String)
org/apache/spark/sql/streaming/StreamingQueryManager/startQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#scala/collection/mutable/HashMap/put(java.lang.Object,java.lang.Object)
org/apache/spark/sql/streaming/StreamingQueryManager/startQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#scala/collection/Iterable/exists(scala.Function1)
org/apache/spark/sql/streaming/StreamingQueryManager/startQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#scala/collection/mutable/HashMap/values()
org/apache/spark/sql/streaming/StreamingQueryManager/startQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/streaming/StreamingQueryManager/startQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/streaming/StreamingQueryManager/startQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/streaming/StreamingQueryManager/startQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/streaming/StreamingQueryManager/startQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#scala/collection/mutable/HashMap/$minus$eq(java.lang.Object)
org/apache/spark/sql/streaming/StreamingQueryManager/startQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/streaming/StreamingQueryManager/startQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#org/apache/spark/sql/streaming/StreamingQueryManager$$anonfun$startQuery$2/2(org.apache.spark.sql.streaming.StreamingQueryManager,org.apache.spark.sql.execution.streaming.StreamingQueryWrapper)
org/apache/spark/sql/streaming/StreamingQueryManager/startQuery(scala.Option,scala.Option,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.streaming.Sink,org.apache.spark.sql.streaming.OutputMode,boolean,boolean,org.apache.spark.sql.streaming.Trigger,org.apache.spark.util.Clock)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinatorRef/forDriver(org.apache.spark.SparkEnv)#org/apache/spark/sql/execution/streaming/state/StateStoreCoordinatorRef$/logDebug(scala.Function0)
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinatorRef/forDriver(org.apache.spark.SparkEnv)#org/apache/spark/sql/execution/streaming/state/StateStoreCoordinatorRef$/endpointName()
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinatorRef/forDriver(org.apache.spark.SparkEnv)#org/apache/spark/sql/execution/streaming/state/StateStoreCoordinatorRef$/forDriver(org.apache.spark.SparkEnv)
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinatorRef/forDriver(org.apache.spark.SparkEnv)#org/apache/spark/sql/execution/streaming/state/StateStoreCoordinatorRef$$anonfun$forDriver$1/1()
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinatorRef/forDriver(org.apache.spark.SparkEnv)#org/apache/spark/sql/execution/streaming/state/StateStoreCoordinatorRef$/logInfo(scala.Function0)
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinatorRef/forDriver(org.apache.spark.SparkEnv)#org/apache/spark/sql/execution/streaming/state/StateStoreCoordinatorRef$$anonfun$forDriver$2/2()
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinatorRef/forDriver(org.apache.spark.SparkEnv)#org/apache/spark/SparkEnv/rpcEnv()
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinatorRef/forDriver(org.apache.spark.SparkEnv)#org/apache/spark/SparkEnv/conf()
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinatorRef/forDriver(org.apache.spark.SparkEnv)#org/apache/spark/rpc/RpcEnv/setupEndpoint(java.lang.String,org.apache.spark.rpc.RpcEndpoint)
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinatorRef/forDriver(org.apache.spark.SparkEnv)#org/apache/spark/util/RpcUtils$/makeDriverRef(java.lang.String,org.apache.spark.SparkConf,org.apache.spark.rpc.RpcEnv)
org/apache/spark/sql/execution/aggregate/SimpleTypedAggregateExpression/evaluateExpression$lzycompute()#org/apache/spark/sql/execution/aggregate/SimpleTypedAggregateExpression$$anonfun$4/4(org.apache.spark.sql.execution.aggregate.SimpleTypedAggregateExpression,org.apache.spark.sql.catalyst.expressions.objects.Invoke)
org/apache/spark/sql/execution/aggregate/SimpleTypedAggregateExpression/evaluateExpression$lzycompute()#org/apache/spark/sql/catalyst/expressions/objects/Invoke/Invoke(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String,org.apache.spark.sql.types.DataType,scala.collection.Seq,boolean,boolean)
org/apache/spark/sql/execution/aggregate/SimpleTypedAggregateExpression/evaluateExpression$lzycompute()#scala/Predef$/assert(boolean)
org/apache/spark/sql/execution/aggregate/SimpleTypedAggregateExpression/evaluateExpression$lzycompute()#org/apache/spark/sql/catalyst/expressions/Literal$/create(java.lang.Object,org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/aggregate/SimpleTypedAggregateExpression/evaluateExpression$lzycompute()#scala/collection/Seq/head()
org/apache/spark/sql/execution/aggregate/SimpleTypedAggregateExpression/evaluateExpression$lzycompute()#org/apache/spark/sql/catalyst/expressions/CreateStruct$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/SimpleTypedAggregateExpression/evaluateExpression$lzycompute()#org/apache/spark/sql/catalyst/expressions/objects/Invoke$/apply$default$5()
org/apache/spark/sql/execution/aggregate/SimpleTypedAggregateExpression/evaluateExpression$lzycompute()#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/SimpleTypedAggregateExpression/evaluateExpression$lzycompute()#scala/Option/get()
org/apache/spark/sql/execution/aggregate/SimpleTypedAggregateExpression/evaluateExpression$lzycompute()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/aggregate/SimpleTypedAggregateExpression/evaluateExpression$lzycompute()#scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)
org/apache/spark/sql/execution/aggregate/SimpleTypedAggregateExpression/evaluateExpression$lzycompute()#org/apache/spark/sql/execution/aggregate/SimpleTypedAggregateExpression$$anonfun$5/5(org.apache.spark.sql.execution.aggregate.SimpleTypedAggregateExpression)
org/apache/spark/sql/execution/aggregate/SimpleTypedAggregateExpression/evaluateExpression$lzycompute()#org/apache/spark/sql/catalyst/trees/TreeNode/find(scala.Function1)
org/apache/spark/sql/execution/aggregate/SimpleTypedAggregateExpression/evaluateExpression$lzycompute()#org/apache/spark/sql/catalyst/expressions/IsNull/IsNull(org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/execution/aggregate/SimpleTypedAggregateExpression/evaluateExpression$lzycompute()#org/apache/spark/sql/catalyst/expressions/If/If(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/execution/aggregate/SimpleTypedAggregateExpression/evaluateExpression$lzycompute()#org/apache/spark/sql/catalyst/expressions/objects/Invoke$/apply$default$6()
org/apache/spark/sql/execution/aggregate/SimpleTypedAggregateExpression/evaluateExpression$lzycompute()#scala/collection/Seq/length()
org/apache/spark/sql/execution/SortExec/createSorter()#org/apache/spark/sql/catalyst/expressions/UnsafeProjection$/create(scala.collection.Seq)
org/apache/spark/sql/execution/SortExec/createSorter()#org/apache/spark/sql/execution/UnsafeExternalRowSorter/UnsafeExternalRowSorter(org.apache.spark.sql.types.StructType,scala.math.Ordering,org.apache.spark.util.collection.unsafe.sort.PrefixComparator,org.apache.spark.sql.execution.UnsafeExternalRowSorter$PrefixComputer,long,boolean)
org/apache/spark/sql/execution/SortExec/createSorter()#org/apache/spark/sql/catalyst/expressions/BindReferences$/bindReference$default$3()
org/apache/spark/sql/execution/SortExec/createSorter()#org/apache/spark/sql/execution/SortPrefixUtils$/getPrefixComparator(org.apache.spark.sql.catalyst.expressions.SortOrder)
org/apache/spark/sql/execution/SortExec/createSorter()#org/apache/spark/sql/execution/SortExec/newOrdering(scala.collection.Seq,scala.collection.Seq)
org/apache/spark/sql/execution/SortExec/createSorter()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/SortExec/createSorter()#org/apache/spark/SparkEnv$/get()
org/apache/spark/sql/execution/SortExec/createSorter()#org/apache/spark/memory/MemoryManager/pageSizeBytes()
org/apache/spark/sql/execution/SortExec/createSorter()#org/apache/spark/sql/execution/SortExec$$anon$1/1(org.apache.spark.sql.execution.SortExec,org.apache.spark.sql.catalyst.expressions.SortPrefix,org.apache.spark.sql.catalyst.expressions.UnsafeProjection)
org/apache/spark/sql/execution/SortExec/createSorter()#org/apache/spark/sql/catalyst/expressions/package$/AttributeSeq(scala.collection.Seq)
org/apache/spark/sql/execution/SortExec/createSorter()#scala/collection/Seq/length()
org/apache/spark/sql/execution/SortExec/createSorter()#org/apache/spark/sql/execution/SortPrefixUtils$/canSortFullyWithPrefix(org.apache.spark.sql.catalyst.expressions.SortOrder)
org/apache/spark/sql/execution/SortExec/createSorter()#org/apache/spark/SparkEnv/memoryManager()
org/apache/spark/sql/execution/SortExec/createSorter()#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/SortExec/createSorter()#org/apache/spark/sql/execution/UnsafeExternalRowSorter/setTestSpillFrequency(int)
org/apache/spark/sql/execution/SortExec/createSorter()#scala/collection/Seq/head()
org/apache/spark/sql/execution/SortExec/createSorter()#org/apache/spark/sql/catalyst/expressions/SortPrefix/SortPrefix(org.apache.spark.sql.catalyst.expressions.SortOrder)
org/apache/spark/sql/execution/SortExec/createSorter()#org/apache/spark/sql/execution/SortExec/schema()
org/apache/spark/sql/execution/SortExec/createSorter()#org/apache/spark/sql/catalyst/expressions/BindReferences$/bindReference(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.package$AttributeSeq,boolean)
org/apache/spark/sql/execution/SortExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/SortExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#java/lang/String/trim()
org/apache/spark/sql/execution/SortExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/copyResult_$eq(boolean)
org/apache/spark/sql/execution/SortExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/SortExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addNewFunction(java.lang.String,java.lang.String)
org/apache/spark/sql/execution/SortExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/SortExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/SortExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addMutableState(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/SortExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/SortExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#java/lang/Class/getName()
org/apache/spark/sql/execution/SortExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/freshName(java.lang.String)
org/apache/spark/sql/execution/SortExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/SortExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addReferenceObj(java.lang.String,java.lang.Object,java.lang.String)
org/apache/spark/sql/execution/SortExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/SortExec/doProduce(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addReferenceObj$default$3()
org/apache/spark/sql/execution/SortExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/SortExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/code()
org/apache/spark/sql/execution/SortExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/SortExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/SortExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/SortExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/SortExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/SortExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/value()
org/apache/spark/sql/execution/SortExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/doExecute()#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/doExecute()#org/apache/spark/rdd/RDD/mapPartitionsWithIndexInternal(scala.Function2,boolean,scala.reflect.ClassTag)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/doExecute()#scala/Tuple2/_1()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/doExecute()#scala/Tuple2/_2()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/doExecute()#org/apache/spark/rdd/RDD/mapPartitionsWithIndexInternal$default$2()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/doExecute()#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/doExecute()#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/doExecute()#org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/longMetric(java.lang.String)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/doExecute()#org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec$$anonfun$doExecute$1/1(org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec,org.apache.spark.sql.execution.metric.SQLMetric)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/defaultJoin(org.apache.spark.broadcast.Broadcast)#org/apache/spark/SparkContext/union(org.apache.spark.rdd.RDD,scala.collection.Seq,scala.reflect.ClassTag)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/defaultJoin(org.apache.spark.broadcast.Broadcast)#org/apache/spark/rdd/RDD/mapPartitionsInternal$default$2()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/defaultJoin(org.apache.spark.broadcast.Broadcast)#org/apache/spark/broadcast/Broadcast/value()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/defaultJoin(org.apache.spark.broadcast.Broadcast)#scala/Predef$/assert(boolean)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/defaultJoin(org.apache.spark.broadcast.Broadcast)#scala/Array$/apply(scala.collection.Seq,scala.reflect.ClassTag)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/defaultJoin(org.apache.spark.broadcast.Broadcast)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/defaultJoin(org.apache.spark.broadcast.Broadcast)#org/apache/spark/sql/catalyst/expressions/GenericInternalRow/GenericInternalRow(int)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/defaultJoin(org.apache.spark.broadcast.Broadcast)#org/apache/spark/SparkContext/makeRDD$default$2()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/defaultJoin(org.apache.spark.broadcast.Broadcast)#org/apache/spark/sql/catalyst/expressions/GenericInternalRow/GenericInternalRow(java.lang.Object[])
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/defaultJoin(org.apache.spark.broadcast.Broadcast)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/defaultJoin(org.apache.spark.broadcast.Broadcast)#scala/reflect/ClassTag$/Any()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/defaultJoin(org.apache.spark.broadcast.Broadcast)#org/apache/spark/rdd/RDD/mapPartitionsInternal(scala.Function1,boolean,scala.reflect.ClassTag)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/defaultJoin(org.apache.spark.broadcast.Broadcast)#org/apache/spark/util/collection/BitSet/BitSet(int)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/defaultJoin(org.apache.spark.broadcast.Broadcast)#org/apache/spark/sql/catalyst/expressions/JoinedRow/withRight(org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/defaultJoin(org.apache.spark.broadcast.Broadcast)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/defaultJoin(org.apache.spark.broadcast.Broadcast)#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/defaultJoin(org.apache.spark.broadcast.Broadcast)#org/apache/spark/rdd/RDD/fold(java.lang.Object,scala.Function2)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/defaultJoin(org.apache.spark.broadcast.Broadcast)#org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/sparkContext()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/defaultJoin(org.apache.spark.broadcast.Broadcast)#org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec$$anonfun$9/9(org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec,org.apache.spark.broadcast.Broadcast)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/defaultJoin(org.apache.spark.broadcast.Broadcast)#org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec$$anonfun$8/8(org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/defaultJoin(org.apache.spark.broadcast.Broadcast)#org/apache/spark/sql/catalyst/expressions/JoinedRow/JoinedRow(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/defaultJoin(org.apache.spark.broadcast.Broadcast)#org/apache/spark/SparkContext/makeRDD(scala.collection.Seq,int,scala.reflect.ClassTag)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/defaultJoin(org.apache.spark.broadcast.Broadcast)#org/apache/spark/sql/catalyst/expressions/JoinedRow/copy()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/defaultJoin(org.apache.spark.broadcast.Broadcast)#org/apache/spark/sql/catalyst/InternalRow/copy()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/defaultJoin(org.apache.spark.broadcast.Broadcast)#scala/runtime/BoxesRunTime/boxToBoolean(boolean)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/defaultJoin(org.apache.spark.broadcast.Broadcast)#org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec$$anonfun$7/7(org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec,org.apache.spark.broadcast.Broadcast)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/defaultJoin(org.apache.spark.broadcast.Broadcast)#org/apache/spark/util/collection/CompactBuffer/CompactBuffer(scala.reflect.ClassTag)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/defaultJoin(org.apache.spark.broadcast.Broadcast)#org/apache/spark/sql/catalyst/expressions/JoinedRow/withLeft(org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/defaultJoin(org.apache.spark.broadcast.Broadcast)#org/apache/spark/util/collection/CompactBuffer/$plus$eq(java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/defaultJoin(org.apache.spark.broadcast.Broadcast)#org/apache/spark/util/collection/BitSet/get(int)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/defaultJoin(org.apache.spark.broadcast.Broadcast)#org/apache/spark/sql/catalyst/expressions/JoinedRow/JoinedRow()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/defaultJoin(org.apache.spark.broadcast.Broadcast)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/defaultJoin(org.apache.spark.broadcast.Broadcast)#scala/collection/Seq/size()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/output()#org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec$$anonfun$output$2/2(org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/output()#org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec$$anonfun$output$3/3(org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/output()#org/apache/spark/sql/catalyst/plans/LeftExistence$/unapply(org.apache.spark.sql.catalyst.plans.JoinType)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/output()#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/output()#scala/Option/isEmpty()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/output()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/output()#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/output()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/output()#scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/output()#scala/collection/Seq/$colon$plus(java.lang.Object,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/output()#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/output()#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/output()#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/output()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/output()#org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec$$anonfun$output$4/4(org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/output()#org/apache/spark/sql/catalyst/plans/ExistenceJoin/exists()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/output()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/output()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/output()#org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec$$anonfun$output$1/1(org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec)
org/apache/spark/sql/execution/command/AlterTableRenameCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/AlterTableRenameCommand/log()
org/apache/spark/sql/execution/command/AlterTableRenameCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/AlterTableRenameCommand$$anonfun$2/2(org.apache.spark.sql.execution.command.AlterTableRenameCommand)
org/apache/spark/sql/execution/command/AlterTableRenameCommand/run(org.apache.spark.sql.SparkSession)#scala/util/control/NonFatal$/unapply(java.lang.Throwable)
org/apache/spark/sql/execution/command/AlterTableRenameCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/AlterTableRenameCommand$$anonfun$1/1(org.apache.spark.sql.execution.command.AlterTableRenameCommand,org.apache.spark.sql.SparkSession)
org/apache/spark/sql/execution/command/AlterTableRenameCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/isEmpty()
org/apache/spark/sql/execution/command/AlterTableRenameCommand/run(org.apache.spark.sql.SparkSession)#scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)
org/apache/spark/sql/execution/command/AlterTableRenameCommand/run(org.apache.spark.sql.SparkSession)#scala/util/Try/getOrElse(scala.Function0)
org/apache/spark/sql/execution/command/AlterTableRenameCommand/run(org.apache.spark.sql.SparkSession)#org/slf4j/Logger/warn(java.lang.String,java.lang.Throwable)
org/apache/spark/sql/execution/command/AlterTableRenameCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/refreshTable(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/AlterTableRenameCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/empty()
org/apache/spark/sql/execution/command/AlterTableRenameCommand/run(org.apache.spark.sql.SparkSession)#java/lang/Throwable/toString()
org/apache/spark/sql/execution/command/AlterTableRenameCommand/run(org.apache.spark.sql.SparkSession)#scala/util/Try$/apply(scala.Function0)
org/apache/spark/sql/execution/command/AlterTableRenameCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/isTemporaryTable(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/AlterTableRenameCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/AlterTableRenameCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/DDLUtils$/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)
org/apache/spark/sql/execution/command/AlterTableRenameCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/TableIdentifier/unquotedString()
org/apache/spark/sql/execution/command/AlterTableRenameCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/renameTable(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/AlterTableRenameCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/get()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/addPartitions/1/anonfun/12/apply(scala.Tuple2)#org/apache/hadoop/fs/Path/toString()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/addPartitions/1/anonfun/12/apply(scala.Tuple2)#scala/collection/GenMap/get(java.lang.Object)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/addPartitions/1/anonfun/12/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/catalog/CatalogTablePartition/CatalogTablePartition(scala.collection.immutable.Map,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,scala.collection.immutable.Map)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/addPartitions/1/anonfun/12/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/addPartitions/1/anonfun/12/apply(scala.Tuple2)#scala/Tuple2/_2()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/addPartitions/1/anonfun/12/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$2()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/addPartitions/1/anonfun/12/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$5()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/addPartitions/1/anonfun/12/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$6()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/addPartitions/1/anonfun/12/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy(scala.Option,scala.Option,scala.Option,scala.Option,boolean,scala.collection.immutable.Map)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/addPartitions/1/anonfun/12/apply(scala.Tuple2)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/addPartitions/1/anonfun/12/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/catalog/CatalogTable/storage()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/addPartitions/1/anonfun/12/apply(scala.Tuple2)#org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand$$anonfun$addPartitions$1$$anonfun$12$$anonfun$14/14(org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsCommand$$anonfun$addPartitions$1$$anonfun$12)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/addPartitions/1/anonfun/12/apply(scala.Tuple2)#org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand$$anonfun$addPartitions$1$$anonfun$12$$anonfun$13/13(org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsCommand$$anonfun$addPartitions$1$$anonfun$12)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/addPartitions/1/anonfun/12/apply(scala.Tuple2)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/addPartitions/1/anonfun/12/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$3()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/addPartitions/1/anonfun/12/apply(scala.Tuple2)#org/apache/hadoop/fs/Path/toUri()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/addPartitions/1/anonfun/12/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$4()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/addPartitions/1/anonfun/12/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/addPartitions/1/anonfun/12/apply(scala.Tuple2)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/command/ShowColumnsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/types/StructType/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/ShowColumnsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/command/ShowColumnsCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/exists(scala.Function1)
org/apache/spark/sql/execution/command/ShowColumnsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/schema()
org/apache/spark/sql/execution/command/ShowColumnsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/command/ShowColumnsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/command/ShowColumnsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/TableIdentifier/database()
org/apache/spark/sql/execution/command/ShowColumnsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/TableIdentifier/TableIdentifier(java.lang.String,scala.Option)
org/apache/spark/sql/execution/command/ShowColumnsCommand/run(org.apache.spark.sql.SparkSession)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/ShowColumnsCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/get()
org/apache/spark/sql/execution/command/ShowColumnsCommand/run(org.apache.spark.sql.SparkSession)#scala/Some/x()
org/apache/spark/sql/execution/command/ShowColumnsCommand/run(org.apache.spark.sql.SparkSession)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/command/ShowColumnsCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/command/ShowColumnsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/internal/SQLConf/resolver()
org/apache/spark/sql/execution/command/ShowColumnsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/ShowColumnsCommand$$anonfun$run$5/5(org.apache.spark.sql.execution.command.ShowColumnsCommand)
org/apache/spark/sql/execution/command/ShowColumnsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/TableIdentifier/identifier()
org/apache/spark/sql/execution/command/ShowColumnsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTempViewOrPermanentTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/ShowColumnsCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/ShowColumnsCommand/run(org.apache.spark.sql.SparkSession)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/ShowColumnsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/ShowColumnsCommand$$anonfun$15/15(org.apache.spark.sql.execution.command.ShowColumnsCommand,scala.Function2,java.lang.String)
org/apache/spark/sql/execution/command/ShowColumnsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/command/ShowColumnsCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/ShowColumnsCommand/run(org.apache.spark.sql.SparkSession)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/command/ShowColumnsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/command/ShowColumnsCommand/run(org.apache.spark.sql.SparkSession)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/RuntimeConfig/requireNonStaticConf(java.lang.String)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/RuntimeConfig/requireNonStaticConf(java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/RuntimeConfig/requireNonStaticConf(java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/RuntimeConfig/requireNonStaticConf(java.lang.String)#java/util/Set/contains(java.lang.Object)
org/apache/spark/sql/RuntimeConfig/requireNonStaticConf(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/RuntimeConfig/requireNonStaticConf(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/RuntimeConfig/requireNonStaticConf(java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/RuntimeConfig/requireNonStaticConf(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/RuntimeConfig/requireNonStaticConf(java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/RuntimeConfig/requireNonStaticConf(java.lang.String)#org/apache/spark/sql/internal/SQLConf$/staticConfKeys()
org/apache/spark/sql/RuntimeConfig/requireNonStaticConf(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/command/DropTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/DropTableCommand/log()
org/apache/spark/sql/execution/command/DropTableCommand/run(org.apache.spark.sql.SparkSession)#scala/util/control/NonFatal$/unapply(java.lang.Throwable)
org/apache/spark/sql/execution/command/DropTableCommand/run(org.apache.spark.sql.SparkSession)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/command/DropTableCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/empty()
org/apache/spark/sql/execution/command/DropTableCommand/run(org.apache.spark.sql.SparkSession)#java/lang/Throwable/toString()
org/apache/spark/sql/execution/command/DropTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/command/DropTableCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/isEmpty()
org/apache/spark/sql/execution/command/DropTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/command/DropTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/command/DropTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTableType$/VIEW()
org/apache/spark/sql/execution/command/DropTableCommand/run(org.apache.spark.sql.SparkSession)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/DropTableCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/get()
org/apache/spark/sql/execution/command/DropTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/tableExists(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/DropTableCommand/run(org.apache.spark.sql.SparkSession)#org/slf4j/Logger/warn(java.lang.String,java.lang.Throwable)
org/apache/spark/sql/execution/command/DropTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/refreshTable(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/DropTableCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/DropTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/tableType()
org/apache/spark/sql/execution/command/DropTableCommand/run(org.apache.spark.sql.SparkSession)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/DropTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/isTemporaryTable(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/DropTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/DropTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/command/DropTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/dropTable(org.apache.spark.sql.catalyst.TableIdentifier,boolean,boolean)
org/apache/spark/sql/execution/command/DropTableCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/13/apply(scala.Tuple2)#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/13/apply(scala.Tuple2)#scala/Predef$/ArrowAssoc(java.lang.Object)
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/13/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/13/apply(scala.Tuple2)#scala/Tuple2/_2()
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/13/apply(scala.Tuple2)#scala/collection/immutable/Map/get(java.lang.Object)
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/13/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/13/apply(scala.Tuple2)#org/apache/spark/sql/execution/streaming/ProgressReporter$$anonfun$13$$anonfun$14/14(org.apache.spark.sql.execution.streaming.ProgressReporter$$anonfun$13)
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/13/apply(scala.Tuple2)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/13/apply(scala.Tuple2)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/13/apply(scala.Tuple2)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/13/apply(scala.Tuple2)#org/apache/spark/sql/execution/streaming/ProgressReporter$$anonfun$13$$anonfun$3/3(org.apache.spark.sql.execution.streaming.ProgressReporter$$anonfun$13)
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/13/apply(scala.Tuple2)#scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#scala/collection/mutable/Buffer/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#scala/Option/foreach(scala.Function1)
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#scala/Option$/option2Iterable(scala.Option)
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#scala/Option/isDefined()
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#scala/collection/mutable/Buffer/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#org/apache/spark/sql/execution/window/AggregateProcessor$$anonfun$1/1()
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#org/apache/spark/sql/execution/window/AggregateProcessor$$anonfun$2/2()
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#org/apache/spark/sql/execution/window/AggregateProcessor$$anonfun$apply$3/3(scala.collection.Seq,scala.collection.mutable.Buffer,scala.collection.mutable.Buffer,scala.collection.mutable.Buffer,scala.collection.mutable.Buffer,scala.collection.mutable.Buffer)
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#org/apache/spark/sql/execution/window/AggregateProcessor$$anonfun$3/3()
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#scala/collection/mutable/Buffer$/fill(int,scala.Function0)
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#scala/collection/mutable/ArrayOps/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#scala/collection/mutable/Buffer$/canBuildFrom()
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#scala/collection/mutable/Buffer$/empty()
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#scala/collection/Iterable/toSeq()
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#org/apache/spark/sql/execution/window/AggregateProcessor$/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#scala/Function2/apply(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#scala/collection/mutable/ArrayOps/headOption()
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#scala/collection/mutable/ArrayOps/foreach(scala.Function1)
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#org/apache/spark/sql/execution/window/AggregateProcessor$$anonfun$apply$2/2(scala.collection.mutable.Buffer,scala.collection.mutable.Buffer,scala.collection.mutable.Buffer)
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/anonfun/computeColumnStats/1/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#org/apache/spark/sql/catalyst/plans/logical/ColumnStat$/supportsType(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/anonfun/computeColumnStats/1/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/anonfun/computeColumnStats/1/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/anonfun/computeColumnStats/1/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/anonfun/computeColumnStats/1/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#org/apache/spark/sql/catalyst/expressions/Attribute/dataType()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/anonfun/computeColumnStats/1/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/anonfun/computeColumnStats/1/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/anonfun/computeColumnStats/1/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/anonfun/computeColumnStats/1/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/anonfun/computeColumnStats/1/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/anonfun/computeColumnStats/1/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/anonfun/computeColumnStats/1/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/anonfun/computeColumnStats/1/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#org/apache/spark/sql/catalyst/expressions/Attribute/name()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/anonfun/computeColumnStats/1/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/anonfun/computeColumnStats/1/apply(org.apache.spark.sql.catalyst.expressions.Attribute)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/columnar/ColumnBuilder/apply(org.apache.spark.sql.types.DataType,int,java.lang.String,boolean)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/columnar/ColumnBuilder/apply(org.apache.spark.sql.types.DataType,int,java.lang.String,boolean)#org/apache/spark/sql/types/UserDefinedType/sqlType()
org/apache/spark/sql/execution/columnar/ColumnBuilder/apply(org.apache.spark.sql.types.DataType,int,java.lang.String,boolean)#org/apache/spark/sql/types/Decimal$/MAX_LONG_DIGITS()
org/apache/spark/sql/execution/columnar/ColumnBuilder/apply(org.apache.spark.sql.types.DataType,int,java.lang.String,boolean)#org/apache/spark/sql/execution/columnar/ColumnBuilder$/apply(org.apache.spark.sql.types.DataType,int,java.lang.String,boolean)
org/apache/spark/sql/execution/columnar/ColumnBuilder/apply(org.apache.spark.sql.types.DataType,int,java.lang.String,boolean)#java/lang/Exception/Exception(java.lang.String)
org/apache/spark/sql/execution/columnar/ColumnBuilder/apply(org.apache.spark.sql.types.DataType,int,java.lang.String,boolean)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/columnar/ColumnBuilder/apply(org.apache.spark.sql.types.DataType,int,java.lang.String,boolean)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/columnar/ColumnBuilder/apply(org.apache.spark.sql.types.DataType,int,java.lang.String,boolean)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/columnar/ColumnBuilder/apply(org.apache.spark.sql.types.DataType,int,java.lang.String,boolean)#org/apache/spark/sql/types/DecimalType/precision()
org/apache/spark/sql/execution/columnar/ColumnBuilder/apply(org.apache.spark.sql.types.DataType,int,java.lang.String,boolean)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/columnar/ColumnBuilder/ensureFreeSpace(java.nio.ByteBuffer,int)#java/nio/ByteOrder/nativeOrder()
org/apache/spark/sql/execution/columnar/ColumnBuilder/ensureFreeSpace(java.nio.ByteBuffer,int)#java/nio/ByteBuffer/allocate(int)
org/apache/spark/sql/execution/columnar/ColumnBuilder/ensureFreeSpace(java.nio.ByteBuffer,int)#java/nio/ByteBuffer/position()
org/apache/spark/sql/execution/columnar/ColumnBuilder/ensureFreeSpace(java.nio.ByteBuffer,int)#scala/Predef$/intWrapper(int)
org/apache/spark/sql/execution/columnar/ColumnBuilder/ensureFreeSpace(java.nio.ByteBuffer,int)#java/nio/ByteBuffer/order(java.nio.ByteOrder)
org/apache/spark/sql/execution/columnar/ColumnBuilder/ensureFreeSpace(java.nio.ByteBuffer,int)#java/nio/ByteBuffer/array()
org/apache/spark/sql/execution/columnar/ColumnBuilder/ensureFreeSpace(java.nio.ByteBuffer,int)#scala/runtime/RichInt$/max$extension(int,int)
org/apache/spark/sql/execution/columnar/ColumnBuilder/ensureFreeSpace(java.nio.ByteBuffer,int)#java/nio/ByteBuffer/put(byte[],int,int)
org/apache/spark/sql/execution/columnar/ColumnBuilder/ensureFreeSpace(java.nio.ByteBuffer,int)#java/nio/ByteBuffer/capacity()
org/apache/spark/sql/execution/columnar/ColumnBuilder/ensureFreeSpace(java.nio.ByteBuffer,int)#java/nio/ByteBuffer/remaining()
org/apache/spark/sql/Dataset/anonfun/resolve/1/apply()#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/Dataset/anonfun/resolve/1/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/Dataset/anonfun/resolve/1/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/Dataset/anonfun/resolve/1/apply()#org/apache/spark/sql/Dataset$$anonfun$resolve$1/apply()
org/apache/spark/sql/Dataset/anonfun/resolve/1/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/Dataset/anonfun/resolve/1/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/Dataset/anonfun/resolve/1/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/Dataset/anonfun/resolve/1/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/Dataset/anonfun/resolve/1/apply()#org/apache/spark/sql/types/StructType/fieldNames()
org/apache/spark/sql/Dataset/anonfun/resolve/1/apply()#scala/collection/mutable/ArrayOps/mkString(java.lang.String)
org/apache/spark/sql/Dataset/anonfun/resolve/1/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/Dataset/anonfun/resolve/1/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/Dataset/anonfun/resolve/1/apply()#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/ui/AllExecutionsPage/render(javax.servlet.http.HttpServletRequest)#scala/xml/Text/Text(java.lang.String)
org/apache/spark/sql/execution/ui/AllExecutionsPage/render(javax.servlet.http.HttpServletRequest)#org/apache/spark/sql/execution/ui/AllExecutionsPage$$anonfun$3/3(org.apache.spark.sql.execution.ui.AllExecutionsPage)
org/apache/spark/sql/execution/ui/AllExecutionsPage/render(javax.servlet.http.HttpServletRequest)#org/apache/spark/ui/UIUtils$/headerSparkPage(java.lang.String,scala.Function0,org.apache.spark.ui.SparkUITab,scala.Option,scala.Option,boolean,boolean)
org/apache/spark/sql/execution/ui/AllExecutionsPage/render(javax.servlet.http.HttpServletRequest)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/ui/AllExecutionsPage/render(javax.servlet.http.HttpServletRequest)#scala/collection/mutable/ListBuffer$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/ui/AllExecutionsPage/render(javax.servlet.http.HttpServletRequest)#scala/xml/NodeBuffer/$amp$plus(java.lang.Object)
org/apache/spark/sql/execution/ui/AllExecutionsPage/render(javax.servlet.http.HttpServletRequest)#java/lang/System/currentTimeMillis()
org/apache/spark/sql/execution/ui/AllExecutionsPage/render(javax.servlet.http.HttpServletRequest)#scala/collection/mutable/ListBuffer/$plus$plus$eq(scala.collection.TraversableOnce)
org/apache/spark/sql/execution/ui/AllExecutionsPage/render(javax.servlet.http.HttpServletRequest)#scala/collection/Seq/sortBy(scala.Function1,scala.math.Ordering)
org/apache/spark/sql/execution/ui/AllExecutionsPage/render(javax.servlet.http.HttpServletRequest)#org/apache/spark/sql/execution/ui/RunningExecutionTable/toNodeSeq()
org/apache/spark/sql/execution/ui/AllExecutionsPage/render(javax.servlet.http.HttpServletRequest)#org/apache/spark/sql/execution/ui/AllExecutionsPage$$anonfun$4/4(org.apache.spark.sql.execution.ui.AllExecutionsPage)
org/apache/spark/sql/execution/ui/AllExecutionsPage/render(javax.servlet.http.HttpServletRequest)#scala/xml/NodeBuffer/NodeBuffer()
org/apache/spark/sql/execution/ui/AllExecutionsPage/render(javax.servlet.http.HttpServletRequest)#org/apache/spark/sql/execution/ui/AllExecutionsPage$$anonfun$2/2(org.apache.spark.sql.execution.ui.AllExecutionsPage)
org/apache/spark/sql/execution/ui/AllExecutionsPage/render(javax.servlet.http.HttpServletRequest)#scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)
org/apache/spark/sql/execution/ui/AllExecutionsPage/render(javax.servlet.http.HttpServletRequest)#org/apache/spark/ui/UIUtils$/headerSparkPage$default$5()
org/apache/spark/sql/execution/ui/AllExecutionsPage/render(javax.servlet.http.HttpServletRequest)#org/apache/spark/ui/UIUtils$/headerSparkPage$default$6()
org/apache/spark/sql/execution/ui/AllExecutionsPage/render(javax.servlet.http.HttpServletRequest)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/ui/AllExecutionsPage/render(javax.servlet.http.HttpServletRequest)#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/execution/ui/AllExecutionsPage/render(javax.servlet.http.HttpServletRequest)#org/apache/spark/ui/UIUtils$/headerSparkPage$default$7()
org/apache/spark/sql/execution/ui/AllExecutionsPage/render(javax.servlet.http.HttpServletRequest)#org/apache/spark/sql/execution/ui/CompletedExecutionTable/toNodeSeq()
org/apache/spark/sql/execution/ui/AllExecutionsPage/render(javax.servlet.http.HttpServletRequest)#org/apache/spark/sql/execution/ui/AllExecutionsPage$$anonfun$render$1/1(org.apache.spark.sql.execution.ui.AllExecutionsPage,scala.collection.mutable.ListBuffer)
org/apache/spark/sql/execution/ui/AllExecutionsPage/render(javax.servlet.http.HttpServletRequest)#scala/collection/SeqLike/reverse()
org/apache/spark/sql/execution/ui/AllExecutionsPage/render(javax.servlet.http.HttpServletRequest)#org/apache/spark/sql/execution/ui/FailedExecutionTable/toNodeSeq()
org/apache/spark/sql/execution/command/AlterTableSetLocationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/withNewStorage$default$4()
org/apache/spark/sql/execution/command/AlterTableSetLocationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/withNewStorage$default$6()
org/apache/spark/sql/execution/command/AlterTableSetLocationCommand/run(org.apache.spark.sql.SparkSession)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/command/AlterTableSetLocationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/alterTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)
org/apache/spark/sql/execution/command/AlterTableSetLocationCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/empty()
org/apache/spark/sql/execution/command/AlterTableSetLocationCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/AlterTableSetLocationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$2()
org/apache/spark/sql/execution/command/AlterTableSetLocationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$5()
org/apache/spark/sql/execution/command/AlterTableSetLocationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$6()
org/apache/spark/sql/execution/command/AlterTableSetLocationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/DDLUtils$/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)
org/apache/spark/sql/execution/command/AlterTableSetLocationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTablePartition/storage()
org/apache/spark/sql/execution/command/AlterTableSetLocationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()
org/apache/spark/sql/execution/command/AlterTableSetLocationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/withNewStorage$default$2()
org/apache/spark/sql/execution/command/AlterTableSetLocationCommand/run(org.apache.spark.sql.SparkSession)#scala/Some/x()
org/apache/spark/sql/execution/command/AlterTableSetLocationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/withNewStorage$default$3()
org/apache/spark/sql/execution/command/AlterTableSetLocationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/withNewStorage$default$5()
org/apache/spark/sql/execution/command/AlterTableSetLocationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/getPartition(org.apache.spark.sql.catalyst.TableIdentifier,scala.collection.immutable.Map)
org/apache/spark/sql/execution/command/AlterTableSetLocationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/withNewStorage(scala.Option,scala.Option,scala.Option,boolean,scala.Option,scala.collection.immutable.Map)
org/apache/spark/sql/execution/command/AlterTableSetLocationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$3()
org/apache/spark/sql/execution/command/AlterTableSetLocationCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/command/AlterTableSetLocationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/AlterTableSetLocationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogUtils$/stringToURI(java.lang.String)
org/apache/spark/sql/execution/command/AlterTableSetLocationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/DDLUtils$/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)
org/apache/spark/sql/execution/command/AlterTableSetLocationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$4()
org/apache/spark/sql/execution/command/AlterTableSetLocationCommand/run(org.apache.spark.sql.SparkSession)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/command/AlterTableSetLocationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/alterPartitions(org.apache.spark.sql.catalyst.TableIdentifier,scala.collection.Seq)
org/apache/spark/sql/execution/command/AlterTableSetLocationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTablePartition/copy$default$1()
org/apache/spark/sql/execution/command/AlterTableSetLocationCommand/run(org.apache.spark.sql.SparkSession)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/command/AlterTableSetLocationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTablePartition/copy$default$3()
org/apache/spark/sql/execution/command/AlterTableSetLocationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTablePartition/copy(scala.collection.immutable.Map,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,scala.collection.immutable.Map)
org/apache/spark/sql/execution/command/AlterTableSetLocationCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy(scala.Option,scala.Option,scala.Option,scala.Option,boolean,scala.collection.immutable.Map)
org/apache/spark/sql/SQLImplicits/typecreator13/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ModuleSymbolApi/asModule()
org/apache/spark/sql/SQLImplicits/typecreator13/1/apply(scala.reflect.api.Mirror)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/SQLImplicits/typecreator13/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticPackage(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator13/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$InternalApi/reificationSupport()
org/apache/spark/sql/SQLImplicits/typecreator13/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/ThisType(scala.reflect.api.Symbols$SymbolApi)
org/apache/spark/sql/SQLImplicits/typecreator13/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ClassSymbolApi/asType()
org/apache/spark/sql/SQLImplicits/typecreator13/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ModuleSymbolApi/moduleClass()
org/apache/spark/sql/SQLImplicits/typecreator13/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/TypeRef(scala.reflect.api.Types$TypeApi,scala.reflect.api.Symbols$SymbolApi,scala.collection.immutable.List)
org/apache/spark/sql/SQLImplicits/typecreator13/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticClass(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator13/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/universe()
org/apache/spark/sql/SQLImplicits/typecreator13/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$TypeSymbolApi/toTypeConstructor()
org/apache/spark/sql/SQLImplicits/typecreator13/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Universe/internal()
org/apache/spark/sql/SQLImplicits/typecreator13/1/apply(scala.reflect.api.Mirror)#scala/collection/immutable/List$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/anonfun/5/apply(org.apache.spark.sql.execution.SparkPlan)#scala/collection/mutable/ArrayOps/zip(scala.collection.GenIterable,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/anonfun/5/apply(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/python/ExtractPythonUDFs/anonfun/5/apply(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/python/ExtractPythonUDFs/anonfun/5/apply(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/filter(scala.Function1)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/anonfun/5/apply(org.apache.spark.sql.execution.SparkPlan)#scala/Predef$DummyImplicit$/dummyImplicit()
org/apache/spark/sql/execution/python/ExtractPythonUDFs/anonfun/5/apply(org.apache.spark.sql.execution.SparkPlan)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/python/ExtractPythonUDFs/anonfun/5/apply(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/anonfun/5/apply(org.apache.spark.sql.execution.SparkPlan)#scala/collection/mutable/HashMap/$plus$plus$eq(scala.collection.TraversableOnce)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/anonfun/5/apply(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/python/ExtractPythonUDFs$$anonfun$5$$anonfun$7/7(org.apache.spark.sql.execution.python.ExtractPythonUDFs$$anonfun$5)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/anonfun/5/apply(org.apache.spark.sql.execution.SparkPlan)#scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/anonfun/5/apply(org.apache.spark.sql.execution.SparkPlan)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/anonfun/5/apply(org.apache.spark.sql.execution.SparkPlan)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/python/ExtractPythonUDFs/anonfun/5/apply(org.apache.spark.sql.execution.SparkPlan)#scala/collection/mutable/ArrayOps/nonEmpty()
org/apache/spark/sql/execution/python/ExtractPythonUDFs/anonfun/5/apply(org.apache.spark.sql.execution.SparkPlan)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/anonfun/5/apply(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/zipWithIndex(scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/anonfun/5/apply(org.apache.spark.sql.execution.SparkPlan)#scala/Array$/fallbackCanBuildFrom(scala.Predef$DummyImplicit)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/anonfun/5/apply(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/python/ExtractPythonUDFs$$anonfun$5$$anonfun$6/6(org.apache.spark.sql.execution.python.ExtractPythonUDFs$$anonfun$5,org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/internal/CatalogImpl/listFunctions(java.lang.String)#org/apache/spark/sql/internal/CatalogImpl$$typecreator3$1/1(org.apache.spark.sql.internal.CatalogImpl)
org/apache/spark/sql/internal/CatalogImpl/listFunctions(java.lang.String)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/internal/CatalogImpl/listFunctions(java.lang.String)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/listFunctions(java.lang.String)
org/apache/spark/sql/internal/CatalogImpl/listFunctions(java.lang.String)#scala/reflect/api/JavaUniverse/runtimeMirror(java.lang.ClassLoader)
org/apache/spark/sql/internal/CatalogImpl/listFunctions(java.lang.String)#java/lang/Class/getClassLoader()
org/apache/spark/sql/internal/CatalogImpl/listFunctions(java.lang.String)#scala/reflect/api/TypeTags$TypeTag$/apply(scala.reflect.api.Mirror,scala.reflect.api.TypeCreator)
org/apache/spark/sql/internal/CatalogImpl/listFunctions(java.lang.String)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/internal/CatalogImpl/listFunctions(java.lang.String)#org/apache/spark/sql/internal/CatalogImpl$/makeDataset(scala.collection.Seq,org.apache.spark.sql.SparkSession,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/internal/CatalogImpl/listFunctions(java.lang.String)#scala/reflect/runtime/package$/universe()
org/apache/spark/sql/internal/CatalogImpl/listFunctions(java.lang.String)#org/apache/spark/sql/internal/CatalogImpl$$anonfun$3/3(org.apache.spark.sql.internal.CatalogImpl)
org/apache/spark/sql/internal/CatalogImpl/listFunctions(java.lang.String)#scala/reflect/api/TypeTags/TypeTag()
org/apache/spark/sql/internal/CatalogImpl/listColumns(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/internal/CatalogImpl$$typecreator4$1/1(org.apache.spark.sql.internal.CatalogImpl)
org/apache/spark/sql/internal/CatalogImpl/listColumns(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/types/StructType/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/internal/CatalogImpl/listColumns(org.apache.spark.sql.catalyst.TableIdentifier)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/internal/CatalogImpl/listColumns(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/internal/CatalogImpl$$anonfun$6/6(org.apache.spark.sql.internal.CatalogImpl,scala.collection.immutable.Set,scala.collection.immutable.Set)
org/apache/spark/sql/internal/CatalogImpl/listColumns(org.apache.spark.sql.catalyst.TableIdentifier)#scala/collection/Seq/toSet()
org/apache/spark/sql/internal/CatalogImpl/listColumns(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/internal/CatalogImpl$$anonfun$5/5(org.apache.spark.sql.internal.CatalogImpl)
org/apache/spark/sql/internal/CatalogImpl/listColumns(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTempViewOrPermanentTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/internal/CatalogImpl/listColumns(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/internal/CatalogImpl$$anonfun$4/4(org.apache.spark.sql.internal.CatalogImpl)
org/apache/spark/sql/internal/CatalogImpl/listColumns(org.apache.spark.sql.catalyst.TableIdentifier)#scala/reflect/api/TypeTags/TypeTag()
org/apache/spark/sql/internal/CatalogImpl/listColumns(org.apache.spark.sql.catalyst.TableIdentifier)#scala/Option/map(scala.Function1)
org/apache/spark/sql/internal/CatalogImpl/listColumns(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/catalog/CatalogTable/schema()
org/apache/spark/sql/internal/CatalogImpl/listColumns(org.apache.spark.sql.catalyst.TableIdentifier)#scala/reflect/api/JavaUniverse/runtimeMirror(java.lang.ClassLoader)
org/apache/spark/sql/internal/CatalogImpl/listColumns(org.apache.spark.sql.catalyst.TableIdentifier)#java/lang/Class/getClassLoader()
org/apache/spark/sql/internal/CatalogImpl/listColumns(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/internal/CatalogImpl$/makeDataset(scala.collection.Seq,org.apache.spark.sql.SparkSession,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/internal/CatalogImpl/listColumns(org.apache.spark.sql.catalyst.TableIdentifier)#scala/reflect/runtime/package$/universe()
org/apache/spark/sql/internal/CatalogImpl/listColumns(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/catalog/CatalogTable/bucketSpec()
org/apache/spark/sql/internal/CatalogImpl/listColumns(org.apache.spark.sql.catalyst.TableIdentifier)#scala/collection/TraversableOnce/toSet()
org/apache/spark/sql/internal/CatalogImpl/listColumns(org.apache.spark.sql.catalyst.TableIdentifier)#scala/reflect/api/TypeTags$TypeTag$/apply(scala.reflect.api.Mirror,scala.reflect.api.TypeCreator)
org/apache/spark/sql/internal/CatalogImpl/listColumns(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()
org/apache/spark/sql/internal/CatalogImpl/listColumns(org.apache.spark.sql.catalyst.TableIdentifier)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/internal/CatalogImpl/createTable(java.lang.String,java.lang.String,org.apache.spark.sql.types.StructType,scala.collection.immutable.Map)#scala/Option/isDefined()
org/apache/spark/sql/internal/CatalogImpl/createTable(java.lang.String,java.lang.String,org.apache.spark.sql.types.StructType,scala.collection.immutable.Map)#org/apache/spark/sql/execution/datasources/DataSource$/buildStorageFormatFromOptions(scala.collection.immutable.Map)
org/apache/spark/sql/internal/CatalogImpl/createTable(java.lang.String,java.lang.String,org.apache.spark.sql.types.StructType,scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/parser/ParserInterface/parseTableIdentifier(java.lang.String)
org/apache/spark/sql/internal/CatalogImpl/createTable(java.lang.String,java.lang.String,org.apache.spark.sql.types.StructType,scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$10()
org/apache/spark/sql/internal/CatalogImpl/createTable(java.lang.String,java.lang.String,org.apache.spark.sql.types.StructType,scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$15()
org/apache/spark/sql/internal/CatalogImpl/createTable(java.lang.String,java.lang.String,org.apache.spark.sql.types.StructType,scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$16()
org/apache/spark/sql/internal/CatalogImpl/createTable(java.lang.String,java.lang.String,org.apache.spark.sql.types.StructType,scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/catalog/CatalogTable/CatalogTable(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)
org/apache/spark/sql/internal/CatalogImpl/createTable(java.lang.String,java.lang.String,org.apache.spark.sql.types.StructType,scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/catalog/CatalogTableType$/EXTERNAL()
org/apache/spark/sql/internal/CatalogImpl/createTable(java.lang.String,java.lang.String,org.apache.spark.sql.types.StructType,scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$6()
org/apache/spark/sql/internal/CatalogImpl/createTable(java.lang.String,java.lang.String,org.apache.spark.sql.types.StructType,scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$11()
org/apache/spark/sql/internal/CatalogImpl/createTable(java.lang.String,java.lang.String,org.apache.spark.sql.types.StructType,scala.collection.immutable.Map)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/internal/CatalogImpl/createTable(java.lang.String,java.lang.String,org.apache.spark.sql.types.StructType,scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$7()
org/apache/spark/sql/internal/CatalogImpl/createTable(java.lang.String,java.lang.String,org.apache.spark.sql.types.StructType,scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$12()
org/apache/spark/sql/internal/CatalogImpl/createTable(java.lang.String,java.lang.String,org.apache.spark.sql.types.StructType,scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$8()
org/apache/spark/sql/internal/CatalogImpl/createTable(java.lang.String,java.lang.String,org.apache.spark.sql.types.StructType,scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$13()
org/apache/spark/sql/internal/CatalogImpl/createTable(java.lang.String,java.lang.String,org.apache.spark.sql.types.StructType,scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$9()
org/apache/spark/sql/internal/CatalogImpl/createTable(java.lang.String,java.lang.String,org.apache.spark.sql.types.StructType,scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$14()
org/apache/spark/sql/internal/CatalogImpl/createTable(java.lang.String,java.lang.String,org.apache.spark.sql.types.StructType,scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$17()
org/apache/spark/sql/internal/CatalogImpl/createTable(java.lang.String,java.lang.String,org.apache.spark.sql.types.StructType,scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/locationUri()
org/apache/spark/sql/internal/CatalogImpl/createTable(java.lang.String,java.lang.String,org.apache.spark.sql.types.StructType,scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/catalog/CatalogTableType$/MANAGED()
org/apache/spark/sql/internal/CatalogImpl/org$apache$spark$sql$internal$CatalogImpl$$makeTable(org.apache.spark.sql.catalyst.TableIdentifier)#scala/util/control/NonFatal$/unapply(java.lang.Throwable)
org/apache/spark/sql/internal/CatalogImpl/org$apache$spark$sql$internal$CatalogImpl$$makeTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTempViewOrPermanentTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/internal/CatalogImpl/org$apache$spark$sql$internal$CatalogImpl$$makeTable(org.apache.spark.sql.catalyst.TableIdentifier)#scala/Option/isEmpty()
org/apache/spark/sql/internal/CatalogImpl/org$apache$spark$sql$internal$CatalogImpl$$makeTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/internal/CatalogImpl$$anonfun$org$apache$spark$sql$internal$CatalogImpl$$makeTable$3/3(org.apache.spark.sql.internal.CatalogImpl)
org/apache/spark/sql/internal/CatalogImpl/org$apache$spark$sql$internal$CatalogImpl$$makeTable(org.apache.spark.sql.catalyst.TableIdentifier)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/internal/CatalogImpl/org$apache$spark$sql$internal$CatalogImpl$$makeTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/internal/CatalogImpl$$anonfun$org$apache$spark$sql$internal$CatalogImpl$$makeTable$2/2(org.apache.spark.sql.internal.CatalogImpl,org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/internal/CatalogImpl/org$apache$spark$sql$internal$CatalogImpl$$makeTable(org.apache.spark.sql.catalyst.TableIdentifier)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/internal/CatalogImpl/org$apache$spark$sql$internal$CatalogImpl$$makeTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/TableIdentifier/table()
org/apache/spark/sql/internal/CatalogImpl/org$apache$spark$sql$internal$CatalogImpl$$makeTable(org.apache.spark.sql.catalyst.TableIdentifier)#scala/Option/orNull(scala.Predef$$less$colon$less)
org/apache/spark/sql/internal/CatalogImpl/org$apache$spark$sql$internal$CatalogImpl$$makeTable(org.apache.spark.sql.catalyst.TableIdentifier)#scala/Option/map(scala.Function1)
org/apache/spark/sql/internal/CatalogImpl/org$apache$spark$sql$internal$CatalogImpl$$makeTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/isTemporaryTable(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/internal/CatalogImpl/org$apache$spark$sql$internal$CatalogImpl$$makeTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/internal/CatalogImpl$$anonfun$org$apache$spark$sql$internal$CatalogImpl$$makeTable$4/4(org.apache.spark.sql.internal.CatalogImpl)
org/apache/spark/sql/internal/CatalogImpl/org$apache$spark$sql$internal$CatalogImpl$$makeTable(org.apache.spark.sql.catalyst.TableIdentifier)#scala/Predef$/$conforms()
org/apache/spark/sql/internal/CatalogImpl/org$apache$spark$sql$internal$CatalogImpl$$makeTable(org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/internal/CatalogImpl$$anonfun$org$apache$spark$sql$internal$CatalogImpl$$makeTable$1/1(org.apache.spark.sql.internal.CatalogImpl)
org/apache/spark/sql/internal/CatalogImpl/getTable(java.lang.String,java.lang.String)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/internal/CatalogImpl/getTable(java.lang.String,java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/internal/CatalogImpl/getTable(java.lang.String,java.lang.String)#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/internal/CatalogImpl/getTable(java.lang.String,java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/internal/CatalogImpl/getTable(java.lang.String,java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/internal/CatalogImpl/getTable(java.lang.String,java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/internal/CatalogImpl/getTable(java.lang.String,java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/internal/CatalogImpl/getTable(java.lang.String,java.lang.String)#org/apache/spark/sql/catalyst/TableIdentifier/TableIdentifier(java.lang.String,scala.Option)
org/apache/spark/sql/internal/CatalogImpl/getTable(java.lang.String,java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/internal/CatalogImpl/getTable(java.lang.String,java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/internal/CatalogImpl/getTable(java.lang.String,java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/internal/CatalogImpl/requireDatabaseExists(java.lang.String)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/internal/CatalogImpl/requireDatabaseExists(java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/internal/CatalogImpl/requireDatabaseExists(java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/internal/CatalogImpl/requireDatabaseExists(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/internal/CatalogImpl/requireDatabaseExists(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/internal/CatalogImpl/requireDatabaseExists(java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/internal/CatalogImpl/requireDatabaseExists(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/internal/CatalogImpl/requireDatabaseExists(java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/internal/CatalogImpl/requireDatabaseExists(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/internal/CatalogImpl/requireDatabaseExists(java.lang.String)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/databaseExists(java.lang.String)
org/apache/spark/sql/internal/CatalogImpl/listTables(java.lang.String)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/internal/CatalogImpl/listTables(java.lang.String)#scala/reflect/api/TypeTags/TypeTag()
org/apache/spark/sql/internal/CatalogImpl/listTables(java.lang.String)#org/apache/spark/sql/internal/CatalogImpl$$anonfun$2/2(org.apache.spark.sql.internal.CatalogImpl)
org/apache/spark/sql/internal/CatalogImpl/listTables(java.lang.String)#scala/reflect/api/JavaUniverse/runtimeMirror(java.lang.ClassLoader)
org/apache/spark/sql/internal/CatalogImpl/listTables(java.lang.String)#java/lang/Class/getClassLoader()
org/apache/spark/sql/internal/CatalogImpl/listTables(java.lang.String)#scala/reflect/api/TypeTags$TypeTag$/apply(scala.reflect.api.Mirror,scala.reflect.api.TypeCreator)
org/apache/spark/sql/internal/CatalogImpl/listTables(java.lang.String)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/internal/CatalogImpl/listTables(java.lang.String)#org/apache/spark/sql/internal/CatalogImpl$/makeDataset(scala.collection.Seq,org.apache.spark.sql.SparkSession,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/internal/CatalogImpl/listTables(java.lang.String)#scala/reflect/runtime/package$/universe()
org/apache/spark/sql/internal/CatalogImpl/listTables(java.lang.String)#org/apache/spark/sql/internal/CatalogImpl$$typecreator2$1/1(org.apache.spark.sql.internal.CatalogImpl)
org/apache/spark/sql/internal/CatalogImpl/listTables(java.lang.String)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/listTables(java.lang.String)
org/apache/spark/sql/internal/CatalogImpl/listDatabases()#org/apache/spark/sql/catalyst/catalog/SessionCatalog/listDatabases()
org/apache/spark/sql/internal/CatalogImpl/listDatabases()#scala/reflect/api/TypeTags/TypeTag()
org/apache/spark/sql/internal/CatalogImpl/listDatabases()#org/apache/spark/sql/internal/CatalogImpl$$anonfun$1/1(org.apache.spark.sql.internal.CatalogImpl)
org/apache/spark/sql/internal/CatalogImpl/listDatabases()#scala/reflect/api/JavaUniverse/runtimeMirror(java.lang.ClassLoader)
org/apache/spark/sql/internal/CatalogImpl/listDatabases()#java/lang/Class/getClassLoader()
org/apache/spark/sql/internal/CatalogImpl/listDatabases()#scala/reflect/api/TypeTags$TypeTag$/apply(scala.reflect.api.Mirror,scala.reflect.api.TypeCreator)
org/apache/spark/sql/internal/CatalogImpl/listDatabases()#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/internal/CatalogImpl/listDatabases()#org/apache/spark/sql/internal/CatalogImpl$/makeDataset(scala.collection.Seq,org.apache.spark.sql.SparkSession,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/internal/CatalogImpl/listDatabases()#scala/reflect/runtime/package$/universe()
org/apache/spark/sql/internal/CatalogImpl/listDatabases()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/internal/CatalogImpl/listDatabases()#org/apache/spark/sql/internal/CatalogImpl$$typecreator1$1/1(org.apache.spark.sql.internal.CatalogImpl)
org/apache/spark/sql/internal/CatalogImpl/requireTableExists(java.lang.String,java.lang.String)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/tableExists(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/internal/CatalogImpl/requireTableExists(java.lang.String,java.lang.String)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/internal/CatalogImpl/requireTableExists(java.lang.String,java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/internal/CatalogImpl/requireTableExists(java.lang.String,java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/internal/CatalogImpl/requireTableExists(java.lang.String,java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/internal/CatalogImpl/requireTableExists(java.lang.String,java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/internal/CatalogImpl/requireTableExists(java.lang.String,java.lang.String)#org/apache/spark/sql/catalyst/TableIdentifier/TableIdentifier(java.lang.String,scala.Option)
org/apache/spark/sql/internal/CatalogImpl/requireTableExists(java.lang.String,java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/internal/CatalogImpl/requireTableExists(java.lang.String,java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/internal/CatalogImpl/requireTableExists(java.lang.String,java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/internal/CatalogImpl/requireTableExists(java.lang.String,java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/internal/CatalogImpl/requireTableExists(java.lang.String,java.lang.String)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/functions/udf(scala.Function4,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/runtime/package$/universe()
org/apache/spark/sql/functions/udf(scala.Function4,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$/udf(scala.Function4,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function4,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try/toOption()
org/apache/spark/sql/functions/udf(scala.Function4,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$$anonfun$7/7(scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function4,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/api/TypeTags/typeTag(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function4,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function4,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)
org/apache/spark/sql/functions/udf(scala.Function4,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()
org/apache/spark/sql/functions/udf(scala.Function4,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try$/apply(scala.Function0)
org/apache/spark/sql/functions/udf(scala.Function1,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/runtime/package$/universe()
org/apache/spark/sql/functions/udf(scala.Function1,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try/toOption()
org/apache/spark/sql/functions/udf(scala.Function1,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/api/TypeTags/typeTag(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function1,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function1,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$$anonfun$4/4(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function1,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)
org/apache/spark/sql/functions/udf(scala.Function1,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$/udf(scala.Function1,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function1,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()
org/apache/spark/sql/functions/udf(scala.Function1,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try$/apply(scala.Function0)
org/apache/spark/sql/functions/udf(scala.Function6,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/runtime/package$/universe()
org/apache/spark/sql/functions/udf(scala.Function6,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$$anonfun$9/9(scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function6,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/api/TypeTags/typeTag(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function6,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function6,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)
org/apache/spark/sql/functions/udf(scala.Function6,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$/udf(scala.Function6,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function6,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try/toOption()
org/apache/spark/sql/functions/udf(scala.Function6,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()
org/apache/spark/sql/functions/udf(scala.Function6,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try$/apply(scala.Function0)
org/apache/spark/sql/functions/json_tuple(org.apache.spark.sql.Column,scala.collection.Seq)#org/apache/spark/sql/functions$/json_tuple(org.apache.spark.sql.Column,scala.collection.Seq)
org/apache/spark/sql/functions/json_tuple(org.apache.spark.sql.Column,scala.collection.Seq)#org/apache/spark/sql/functions$$anonfun$json_tuple$2/2()
org/apache/spark/sql/functions/json_tuple(org.apache.spark.sql.Column,scala.collection.Seq)#scala/Predef$/require(boolean,scala.Function0)
org/apache/spark/sql/functions/json_tuple(org.apache.spark.sql.Column,scala.collection.Seq)#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/functions/json_tuple(org.apache.spark.sql.Column,scala.collection.Seq)#org/apache/spark/sql/functions$/withExpr(org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/functions/json_tuple(org.apache.spark.sql.Column,scala.collection.Seq)#scala/collection/SeqLike/$plus$colon(java.lang.Object,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/functions/json_tuple(org.apache.spark.sql.Column,scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/functions/json_tuple(org.apache.spark.sql.Column,scala.collection.Seq)#org/apache/spark/sql/functions$$anonfun$json_tuple$1/1()
org/apache/spark/sql/functions/json_tuple(org.apache.spark.sql.Column,scala.collection.Seq)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/functions/json_tuple(org.apache.spark.sql.Column,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/JsonTuple/JsonTuple(scala.collection.Seq)
org/apache/spark/sql/functions/udf(scala.Function8,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/runtime/package$/universe()
org/apache/spark/sql/functions/udf(scala.Function8,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try/toOption()
org/apache/spark/sql/functions/udf(scala.Function8,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$/udf(scala.Function8,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function8,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/api/TypeTags/typeTag(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function8,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function8,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$$anonfun$11/11(scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function8,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)
org/apache/spark/sql/functions/udf(scala.Function8,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()
org/apache/spark/sql/functions/udf(scala.Function8,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try$/apply(scala.Function0)
org/apache/spark/sql/functions/sha2(org.apache.spark.sql.Column,int)#org/apache/spark/sql/functions$/lit(java.lang.Object)
org/apache/spark/sql/functions/sha2(org.apache.spark.sql.Column,int)#org/apache/spark/sql/functions$$anonfun$sha2$1/1(int)
org/apache/spark/sql/functions/sha2(org.apache.spark.sql.Column,int)#scala/Predef$/require(boolean,scala.Function0)
org/apache/spark/sql/functions/sha2(org.apache.spark.sql.Column,int)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/functions/sha2(org.apache.spark.sql.Column,int)#org/apache/spark/sql/catalyst/expressions/Sha2/Sha2(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/functions/sha2(org.apache.spark.sql.Column,int)#org/apache/spark/sql/functions$/withExpr(org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/functions/sha2(org.apache.spark.sql.Column,int)#org/apache/spark/sql/functions$/sha2(org.apache.spark.sql.Column,int)
org/apache/spark/sql/functions/sha2(org.apache.spark.sql.Column,int)#scala/collection/SeqLike/contains(java.lang.Object)
org/apache/spark/sql/functions/sha2(org.apache.spark.sql.Column,int)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/functions/sha2(org.apache.spark.sql.Column,int)#scala/Predef$/wrapIntArray(int[])
org/apache/spark/sql/functions/udf(scala.Function10,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/runtime/package$/universe()
org/apache/spark/sql/functions/udf(scala.Function10,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try/toOption()
org/apache/spark/sql/functions/udf(scala.Function10,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$/udf(scala.Function10,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function10,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/api/TypeTags/typeTag(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function10,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function10,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$$anonfun$13/13(scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function10,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)
org/apache/spark/sql/functions/udf(scala.Function10,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()
org/apache/spark/sql/functions/udf(scala.Function10,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try$/apply(scala.Function0)
org/apache/spark/sql/functions/least(scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/functions/least(scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/Least/Least(scala.collection.Seq)
org/apache/spark/sql/functions/least(scala.collection.Seq)#scala/Predef$/require(boolean,scala.Function0)
org/apache/spark/sql/functions/least(scala.collection.Seq)#org/apache/spark/sql/functions$/least(scala.collection.Seq)
org/apache/spark/sql/functions/least(scala.collection.Seq)#org/apache/spark/sql/functions$$anonfun$least$1/1()
org/apache/spark/sql/functions/least(scala.collection.Seq)#org/apache/spark/sql/functions$/withExpr(org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/functions/least(scala.collection.Seq)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/functions/least(scala.collection.Seq)#scala/collection/Seq/length()
org/apache/spark/sql/functions/least(scala.collection.Seq)#org/apache/spark/sql/functions$$anonfun$least$2/2()
org/apache/spark/sql/functions/udf(scala.Function9,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/runtime/package$/universe()
org/apache/spark/sql/functions/udf(scala.Function9,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try/toOption()
org/apache/spark/sql/functions/udf(scala.Function9,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/api/TypeTags/typeTag(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function9,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$$anonfun$12/12(scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function9,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)
org/apache/spark/sql/functions/udf(scala.Function9,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()
org/apache/spark/sql/functions/udf(scala.Function9,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try$/apply(scala.Function0)
org/apache/spark/sql/functions/udf(scala.Function9,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$/udf(scala.Function9,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function9,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function7,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/runtime/package$/universe()
org/apache/spark/sql/functions/udf(scala.Function7,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try/toOption()
org/apache/spark/sql/functions/udf(scala.Function7,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/api/TypeTags/typeTag(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function7,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$$anonfun$10/10(scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function7,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function7,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$/udf(scala.Function7,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function7,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)
org/apache/spark/sql/functions/udf(scala.Function7,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()
org/apache/spark/sql/functions/udf(scala.Function7,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try$/apply(scala.Function0)
org/apache/spark/sql/functions/greatest(scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/functions/greatest(scala.collection.Seq)#org/apache/spark/sql/functions$$anonfun$greatest$1/1()
org/apache/spark/sql/functions/greatest(scala.collection.Seq)#org/apache/spark/sql/functions$$anonfun$greatest$2/2()
org/apache/spark/sql/functions/greatest(scala.collection.Seq)#scala/Predef$/require(boolean,scala.Function0)
org/apache/spark/sql/functions/greatest(scala.collection.Seq)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/functions/greatest(scala.collection.Seq)#org/apache/spark/sql/functions$/greatest(scala.collection.Seq)
org/apache/spark/sql/functions/greatest(scala.collection.Seq)#org/apache/spark/sql/functions$/withExpr(org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/functions/greatest(scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/Greatest/Greatest(scala.collection.Seq)
org/apache/spark/sql/functions/greatest(scala.collection.Seq)#scala/collection/Seq/length()
org/apache/spark/sql/functions/udf(scala.Function0,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/runtime/package$/universe()
org/apache/spark/sql/functions/udf(scala.Function0,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try/toOption()
org/apache/spark/sql/functions/udf(scala.Function0,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/api/TypeTags/typeTag(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function0,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()
org/apache/spark/sql/functions/udf(scala.Function0,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try$/apply(scala.Function0)
org/apache/spark/sql/functions/udf(scala.Function0,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function0,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$$anonfun$3/3()
org/apache/spark/sql/functions/udf(scala.Function0,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)
org/apache/spark/sql/functions/udf(scala.Function0,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$/udf(scala.Function0,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function5,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/runtime/package$/universe()
org/apache/spark/sql/functions/udf(scala.Function5,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try/toOption()
org/apache/spark/sql/functions/udf(scala.Function5,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$$anonfun$8/8(scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function5,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/api/TypeTags/typeTag(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function5,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function5,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$/udf(scala.Function5,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function5,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)
org/apache/spark/sql/functions/udf(scala.Function5,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()
org/apache/spark/sql/functions/udf(scala.Function5,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try$/apply(scala.Function0)
org/apache/spark/sql/functions/udf(scala.Function2,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try/toOption()
org/apache/spark/sql/functions/udf(scala.Function2,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$$anonfun$5/5(scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function2,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/api/TypeTags/typeTag(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function2,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function2,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)
org/apache/spark/sql/functions/udf(scala.Function2,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$/udf(scala.Function2,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function2,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/runtime/package$/universe()
org/apache/spark/sql/functions/udf(scala.Function2,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()
org/apache/spark/sql/functions/udf(scala.Function2,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try$/apply(scala.Function0)
org/apache/spark/sql/functions/udf(scala.Function3,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/runtime/package$/universe()
org/apache/spark/sql/functions/udf(scala.Function3,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$$anonfun$6/6(scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function3,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try/toOption()
org/apache/spark/sql/functions/udf(scala.Function3,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/api/TypeTags/typeTag(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function3,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function3,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$/udf(scala.Function3,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function3,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)
org/apache/spark/sql/functions/udf(scala.Function3,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()
org/apache/spark/sql/functions/udf(scala.Function3,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try$/apply(scala.Function0)
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveStructString$1(scala.Tuple2,scala.collection.Seq)#scala/collection/Seq/sorted(scala.math.Ordering)
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveStructString$1(scala.Tuple2,scala.collection.Seq)#org/apache/spark/sql/execution/QueryExecution$$anonfun$org$apache$spark$sql$execution$QueryExecution$$toHiveStructString$1$1/1(org.apache.spark.sql.execution.QueryExecution,scala.collection.Seq)
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveStructString$1(scala.Tuple2,scala.collection.Seq)#org/apache/spark/sql/types/MapType/valueType()
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveStructString$1(scala.Tuple2,scala.collection.Seq)#scala/collection/Seq/zip(scala.collection.GenIterable,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveStructString$1(scala.Tuple2,scala.collection.Seq)#scala/collection/TraversableOnce/mkString(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveStructString$1(scala.Tuple2,scala.collection.Seq)#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveStructString$1(scala.Tuple2,scala.collection.Seq)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveStructString$1(scala.Tuple2,scala.collection.Seq)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveStructString$1(scala.Tuple2,scala.collection.Seq)#org/apache/spark/sql/types/MapType/keyType()
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveStructString$1(scala.Tuple2,scala.collection.Seq)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveStructString$1(scala.Tuple2,scala.collection.Seq)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveStructString$1(scala.Tuple2,scala.collection.Seq)#org/apache/spark/sql/Row/toSeq()
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveStructString$1(scala.Tuple2,scala.collection.Seq)#scala/collection/Seq/contains(java.lang.Object)
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveStructString$1(scala.Tuple2,scala.collection.Seq)#java/lang/Object/toString()
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveStructString$1(scala.Tuple2,scala.collection.Seq)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveStructString$1(scala.Tuple2,scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveStructString$1(scala.Tuple2,scala.collection.Seq)#org/apache/spark/sql/execution/QueryExecution$$anonfun$org$apache$spark$sql$execution$QueryExecution$$toHiveStructString$1$4/4(org.apache.spark.sql.execution.QueryExecution,scala.collection.Seq,org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveStructString$1(scala.Tuple2,scala.collection.Seq)#org/apache/spark/sql/execution/QueryExecution$$anonfun$org$apache$spark$sql$execution$QueryExecution$$toHiveStructString$1$3/3(org.apache.spark.sql.execution.QueryExecution,scala.collection.Seq)
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveStructString$1(scala.Tuple2,scala.collection.Seq)#scala/Tuple2/_1()
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveStructString$1(scala.Tuple2,scala.collection.Seq)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveStructString$1(scala.Tuple2,scala.collection.Seq)#scala/Tuple2/_2()
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveStructString$1(scala.Tuple2,scala.collection.Seq)#org/apache/spark/sql/types/DecimalType$/unapply(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveStructString$1(scala.Tuple2,scala.collection.Seq)#scala/collection/immutable/Map/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveStructString$1(scala.Tuple2,scala.collection.Seq)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveStructString$1(scala.Tuple2,scala.collection.Seq)#scala/collection/TraversableOnce/toSeq()
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveStructString$1(scala.Tuple2,scala.collection.Seq)#org/apache/spark/sql/types/ArrayType/elementType()
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveStructString$1(scala.Tuple2,scala.collection.Seq)#org/apache/spark/sql/execution/QueryExecution$$anonfun$org$apache$spark$sql$execution$QueryExecution$$toHiveStructString$1$2/2(org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveStructString$1(scala.Tuple2,scala.collection.Seq)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveStructString$1(scala.Tuple2,scala.collection.Seq)#scala/collection/immutable/Iterable$/canBuildFrom()
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveString(scala.Tuple2)#scala/collection/Seq/sorted(scala.math.Ordering)
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveString(scala.Tuple2)#org/apache/spark/sql/catalyst/util/DateTimeUtils$/getTimeZone(java.lang.String)
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveString(scala.Tuple2)#org/apache/spark/sql/execution/QueryExecution$$anonfun$org$apache$spark$sql$execution$QueryExecution$$toHiveString$3/3(org.apache.spark.sql.execution.QueryExecution,scala.collection.Seq)
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveString(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveString(scala.Tuple2)#org/apache/spark/sql/types/MapType/valueType()
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveString(scala.Tuple2)#scala/collection/TraversableOnce/mkString(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveString(scala.Tuple2)#java/lang/String/String(byte[],java.nio.charset.Charset)
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveString(scala.Tuple2)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveString(scala.Tuple2)#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveString(scala.Tuple2)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveString(scala.Tuple2)#org/apache/spark/sql/catalyst/util/DateTimeUtils$/timestampToString(long,java.util.TimeZone)
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveString(scala.Tuple2)#org/apache/spark/sql/Row/toSeq()
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveString(scala.Tuple2)#scala/collection/Seq/contains(java.lang.Object)
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveString(scala.Tuple2)#org/apache/spark/sql/catalyst/util/DateTimeUtils$/dateToString(int)
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveString(scala.Tuple2)#org/apache/spark/sql/execution/QueryExecution$$anonfun$org$apache$spark$sql$execution$QueryExecution$$toHiveString$4/4(org.apache.spark.sql.execution.QueryExecution,scala.collection.Seq,org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveString(scala.Tuple2)#java/lang/Object/toString()
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveString(scala.Tuple2)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveString(scala.Tuple2)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveString(scala.Tuple2)#org/apache/spark/sql/execution/QueryExecution$$anonfun$org$apache$spark$sql$execution$QueryExecution$$toHiveString$1/1(org.apache.spark.sql.execution.QueryExecution,scala.collection.Seq)
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveString(scala.Tuple2)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveString(scala.Tuple2)#scala/Tuple2/_2()
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveString(scala.Tuple2)#org/apache/spark/sql/types/DecimalType$/unapply(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveString(scala.Tuple2)#scala/collection/immutable/Map/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveString(scala.Tuple2)#org/apache/spark/sql/internal/SQLConf/sessionLocalTimeZone()
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveString(scala.Tuple2)#scala/collection/Seq/zip(scala.collection.GenIterable,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveString(scala.Tuple2)#scala/collection/TraversableOnce/toSeq()
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveString(scala.Tuple2)#org/apache/spark/sql/types/ArrayType/elementType()
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveString(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveString(scala.Tuple2)#scala/collection/immutable/Iterable$/canBuildFrom()
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveString(scala.Tuple2)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveString(scala.Tuple2)#org/apache/spark/sql/execution/QueryExecution$$anonfun$org$apache$spark$sql$execution$QueryExecution$$toHiveString$2/2(org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveString(scala.Tuple2)#org/apache/spark/sql/catalyst/util/DateTimeUtils$/fromJavaDate(java.sql.Date)
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveString(scala.Tuple2)#org/apache/spark/sql/catalyst/util/DateTimeUtils$/fromJavaTimestamp(java.sql.Timestamp)
org/apache/spark/sql/execution/QueryExecution/org$apache$spark$sql$execution$QueryExecution$$toHiveString(scala.Tuple2)#org/apache/spark/sql/types/MapType/keyType()
org/apache/spark/sql/execution/QueryExecution/assertAnalyzed()#org/apache/spark/sql/AnalysisException/startPosition()
org/apache/spark/sql/execution/QueryExecution/assertAnalyzed()#org/apache/spark/sql/AnalysisException/setStackTrace(java.lang.StackTraceElement[])
org/apache/spark/sql/execution/QueryExecution/assertAnalyzed()#org/apache/spark/sql/AnalysisException/getStackTrace()
org/apache/spark/sql/execution/QueryExecution/assertAnalyzed()#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/QueryExecution/assertAnalyzed()#org/apache/spark/sql/AnalysisException/line()
org/apache/spark/sql/execution/QueryExecution/assertAnalyzed()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/QueryExecution/assertAnalyzed()#org/apache/spark/sql/AnalysisException/message()
org/apache/spark/sql/execution/QueryExecution/assertAnalyzed()#org/apache/spark/sql/catalyst/analysis/Analyzer/checkAnalysis(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/QueryExecution/assertAnalyzed()#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/QueryExecution/hiveResultString()#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/QueryExecution/hiveResultString()#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/QueryExecution/hiveResultString()#scala/Predef$DummyImplicit$/dummyImplicit()
org/apache/spark/sql/execution/QueryExecution/hiveResultString()#org/apache/spark/sql/execution/QueryExecution$$anonfun$hiveResultString$1/1(org.apache.spark.sql.execution.QueryExecution)
org/apache/spark/sql/execution/QueryExecution/hiveResultString()#scala/collection/mutable/ArrayOps/toSeq()
org/apache/spark/sql/execution/QueryExecution/hiveResultString()#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/QueryExecution/hiveResultString()#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/QueryExecution/hiveResultString()#org/apache/spark/sql/execution/QueryExecution$$anonfun$hiveResultString$3/3(org.apache.spark.sql.execution.QueryExecution,scala.collection.Seq)
org/apache/spark/sql/execution/QueryExecution/hiveResultString()#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/QueryExecution/hiveResultString()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/QueryExecution/hiveResultString()#scala/Array$/fallbackCanBuildFrom(scala.Predef$DummyImplicit)
org/apache/spark/sql/execution/QueryExecution/hiveResultString()#org/apache/spark/sql/execution/QueryExecution$$anonfun$hiveResultString$4/4(org.apache.spark.sql.execution.QueryExecution)
org/apache/spark/sql/execution/QueryExecution/hiveResultString()#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/output()
org/apache/spark/sql/execution/QueryExecution/hiveResultString()#org/apache/spark/sql/execution/QueryExecution$$anonfun$2/2(org.apache.spark.sql.execution.QueryExecution)
org/apache/spark/sql/execution/QueryExecution/hiveResultString()#org/apache/spark/sql/execution/QueryExecution$$anonfun$hiveResultString$2/2(org.apache.spark.sql.execution.QueryExecution)
org/apache/spark/sql/execution/QueryExecution/hiveResultString()#org/apache/spark/sql/execution/QueryExecution$$anonfun$1/1(org.apache.spark.sql.execution.QueryExecution)
org/apache/spark/sql/execution/QueryExecution/hiveResultString()#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/QueryExecution/completeString(boolean)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/QueryExecution/completeString(boolean)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/treeString(boolean,boolean)
org/apache/spark/sql/execution/QueryExecution/completeString(boolean)#org/apache/spark/sql/execution/QueryExecution$$anonfun$completeString$3/3(org.apache.spark.sql.execution.QueryExecution)
org/apache/spark/sql/execution/QueryExecution/completeString(boolean)#java/lang/String/trim()
org/apache/spark/sql/execution/QueryExecution/completeString(boolean)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/treeString$default$2()
org/apache/spark/sql/execution/QueryExecution/completeString(boolean)#org/apache/spark/sql/execution/QueryExecution$$anonfun$completeString$1/1(org.apache.spark.sql.execution.QueryExecution)
org/apache/spark/sql/execution/QueryExecution/completeString(boolean)#org/apache/spark/sql/execution/QueryExecution$$anonfun$5/5(org.apache.spark.sql.execution.QueryExecution)
org/apache/spark/sql/execution/QueryExecution/completeString(boolean)#scala/collection/TraversableOnce/mkString(java.lang.String)
org/apache/spark/sql/execution/QueryExecution/completeString(boolean)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/QueryExecution/completeString(boolean)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/QueryExecution/completeString(boolean)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/stats(org.apache.spark.sql.internal.SQLConf)
org/apache/spark/sql/execution/QueryExecution/completeString(boolean)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/QueryExecution/completeString(boolean)#org/apache/spark/sql/execution/QueryExecution$$anonfun$completeString$2/2(org.apache.spark.sql.execution.QueryExecution,java.lang.String)
org/apache/spark/sql/execution/QueryExecution/completeString(boolean)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/QueryExecution/completeString(boolean)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/QueryExecution/completeString(boolean)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/QueryExecution/completeString(boolean)#org/apache/spark/sql/execution/QueryExecution$$anonfun$4/4(org.apache.spark.sql.execution.QueryExecution)
org/apache/spark/sql/execution/QueryExecution/completeString(boolean)#scala/collection/TraversableLike/filter(scala.Function1)
org/apache/spark/sql/execution/QueryExecution/completeString(boolean)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/QueryExecution/completeString(boolean)#org/apache/spark/sql/execution/QueryExecution$$anonfun$3/3(org.apache.spark.sql.execution.QueryExecution)
org/apache/spark/sql/execution/QueryExecution/simpleString()#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/QueryExecution/simpleString()#java/lang/String/trim()
org/apache/spark/sql/execution/QueryExecution/simpleString()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/QueryExecution/simpleString()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/QueryExecution/simpleString()#org/apache/spark/sql/execution/QueryExecution$$anonfun$simpleString$1/1(org.apache.spark.sql.execution.QueryExecution)
org/apache/spark/sql/execution/QueryExecution/simpleString()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/QueryExecution/simpleString()#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/QueryExecution/simpleString()#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/QueryExecution/simpleString()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/anonfun/allFiles/1/apply(org.apache.hadoop.fs.Path)#org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$allFiles$1$$anonfun$apply$3/3(org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex$$anonfun$allFiles$1)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/anonfun/allFiles/1/apply(org.apache.hadoop.fs.Path)#scala/Option/orElse(scala.Function0)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/anonfun/allFiles/1/apply(org.apache.hadoop.fs.Path)#scala/collection/immutable/Map/get(java.lang.Object)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/anonfun/allFiles/1/apply(org.apache.hadoop.fs.Path)#org/apache/hadoop/fs/Path/isAbsolute()
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/anonfun/allFiles/1/apply(org.apache.hadoop.fs.Path)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/anonfun/allFiles/1/apply(org.apache.hadoop.fs.Path)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/anonfun/allFiles/1/apply(org.apache.hadoop.fs.Path)#org/apache/hadoop/fs/FileSystem/makeQualified(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/anonfun/allFiles/1/apply(org.apache.hadoop.fs.Path)#org/apache/hadoop/fs/Path/isRoot()
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/anonfun/allFiles/1/apply(org.apache.hadoop.fs.Path)#org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/anonfun/allFiles/1/apply(org.apache.hadoop.fs.Path)#org/apache/hadoop/fs/Path/Path(org.apache.hadoop.fs.Path,java.lang.String)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/anonfun/allFiles/1/apply(org.apache.hadoop.fs.Path)#org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$allFiles$1$$anonfun$apply$1/1(org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex$$anonfun$allFiles$1,org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/checkIfFastHashMapSupported(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/types/StructType/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/checkIfFastHashMapSupported(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/checkIfFastHashMapSupported(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/IterableLike/forall(scala.Function1)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/checkIfFastHashMapSupported(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/Seq/forall(scala.Function1)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/checkIfFastHashMapSupported(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/types/StructType/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/checkIfFastHashMapSupported(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/TraversableLike/filter(scala.Function1)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/checkIfFastHashMapSupported(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$34/34(org.apache.spark.sql.execution.aggregate.HashAggregateExec,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/checkIfFastHashMapSupported(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/types/StructType/nonEmpty()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/checkIfFastHashMapSupported(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$35/35(org.apache.spark.sql.execution.aggregate.HashAggregateExec)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/checkIfFastHashMapSupported(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$36/36(org.apache.spark.sql.execution.aggregate.HashAggregateExec)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/checkIfFastHashMapSupported(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$37/37(org.apache.spark.sql.execution.aggregate.HashAggregateExec)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/checkIfFastHashMapSupported(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$38/38(org.apache.spark.sql.execution.aggregate.HashAggregateExec)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/metrics$lzycompute()#scala/collection/immutable/Map$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/metrics$lzycompute()#scala/Predef$/Map()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/metrics$lzycompute()#scala/Predef$/ArrowAssoc(java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/metrics$lzycompute()#org/apache/spark/sql/execution/metric/SQLMetrics$/createMetric(org.apache.spark.SparkContext,java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/metrics$lzycompute()#org/apache/spark/sql/execution/metric/SQLMetrics$/createSizeMetric(org.apache.spark.SparkContext,java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/metrics$lzycompute()#scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/metrics$lzycompute()#org/apache/spark/sql/execution/metric/SQLMetrics$/createTimingMetric(org.apache.spark.SparkContext,java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/metrics$lzycompute()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/HashAggregateExec/metrics$lzycompute()#org/apache/spark/sql/execution/aggregate/HashAggregateExec/sparkContext()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/Seq/contains(java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#java/lang/String/trim()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$7/7(org.apache.spark.sql.execution.aggregate.HashAggregateExec)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Tuple2/_1()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$doProduceWithoutKeys$1/1(org.apache.spark.sql.execution.aggregate.HashAggregateExec,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addNewFunction(java.lang.String,java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/currentVars_$eq(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addMutableState(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$8/8(org.apache.spark.sql.execution.aggregate.HashAggregateExec)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Tuple2/_2()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$9/9(org.apache.spark.sql.execution.aggregate.HashAggregateExec)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$12/12(org.apache.spark.sql.execution.aggregate.HashAggregateExec,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$11/11(org.apache.spark.sql.execution.aggregate.HashAggregateExec,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$10/10(org.apache.spark.sql.execution.aggregate.HashAggregateExec,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/freshName(java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/createHashMap()#org/apache/spark/sql/catalyst/expressions/UnsafeProjection$/create(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/createHashMap()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/createHashMap()#org/apache/spark/TaskContext$/get()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/createHashMap()#org/apache/spark/sql/catalyst/expressions/UnsafeProjection/apply(org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/createHashMap()#org/apache/spark/memory/TaskMemoryManager/pageSizeBytes()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/createHashMap()#org/apache/spark/TaskContext/taskMemoryManager()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/createHashMap()#scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/createHashMap()#org/apache/spark/sql/catalyst/expressions/package$/EmptyRow()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/createHashMap()#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$23/23(org.apache.spark.sql.execution.aggregate.HashAggregateExec)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/updateRowInFastHashMap$1(boolean,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,scala.collection.Seq,scala.collection.Seq)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/updateRowInFastHashMap$1(boolean,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,scala.collection.Seq,scala.collection.Seq)#java/lang/String/trim()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/updateRowInFastHashMap$1(boolean,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,scala.collection.Seq,scala.collection.Seq)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$48/48(org.apache.spark.sql.execution.aggregate.HashAggregateExec,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,scala.collection.Seq,boolean)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/updateRowInFastHashMap$1(boolean,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,scala.collection.Seq,scala.collection.Seq)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/HashAggregateExec/updateRowInFastHashMap$1(boolean,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,scala.collection.Seq,scala.collection.Seq)#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/updateRowInFastHashMap$1(boolean,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,scala.collection.Seq,scala.collection.Seq)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/updateRowInFastHashMap$1(boolean,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,scala.collection.Seq,scala.collection.Seq)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/updateRowInFastHashMap$1(boolean,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,scala.collection.Seq,scala.collection.Seq)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$46/46(org.apache.spark.sql.execution.aggregate.HashAggregateExec,scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/updateRowInFastHashMap$1(boolean,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,scala.collection.Seq,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/subexpressionEliminationForWholeStageCodegen(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/updateRowInFastHashMap$1(boolean,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,scala.collection.Seq,scala.collection.Seq)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/updateRowInFastHashMap$1(boolean,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,scala.collection.Seq,scala.collection.Seq)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/updateRowInFastHashMap$1(boolean,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,scala.collection.Seq,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/withSubExprEliminationExprs(scala.collection.immutable.Map,scala.Function0)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/updateRowInFastHashMap$1(boolean,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,scala.collection.Seq,scala.collection.Seq)#scala/collection/Seq/zipWithIndex(scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/updateRowInFastHashMap$1(boolean,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,scala.collection.Seq,scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/updateRowInFastHashMap$1(boolean,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,scala.collection.Seq,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/SubExprCodes/states()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/updateRowInFastHashMap$1(boolean,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,scala.collection.Seq,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/SubExprCodes/codes()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/updateRowInFastHashMap$1(boolean,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,scala.collection.Seq,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/INPUT_ROW_$eq(java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/updateRowInFastHashMap$1(boolean,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,scala.collection.Seq,scala.collection.Seq)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/updateRowInFastHashMap$1(boolean,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,scala.collection.Seq,scala.collection.Seq)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/updateRowInFastHashMap$1(boolean,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,scala.collection.Seq,scala.collection.Seq)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$47/47(org.apache.spark.sql.execution.aggregate.HashAggregateExec,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/updateRowInFastHashMap$1(boolean,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,scala.collection.Seq,scala.collection.Seq)#scala/collection/Seq/mkString(java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/updateRowInFastHashMap$1(boolean,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,scala.collection.Seq,scala.collection.Seq)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/outputFromVectorizedMap$1(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String)#scala/collection/Seq/zipWithIndex(scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/outputFromVectorizedMap$1(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/outputFromVectorizedMap$1(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String)#scala/collection/Seq$/empty()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/outputFromVectorizedMap$1(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/code()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/outputFromVectorizedMap$1(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String)#org/apache/spark/sql/types/StructType/toAttributes()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/outputFromVectorizedMap$1(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String)#org/apache/spark/sql/catalyst/expressions/codegen/GenerateUnsafeProjection$/createCode$default$3()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/outputFromVectorizedMap$1(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/outputFromVectorizedMap$1(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/freshName(java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/outputFromVectorizedMap$1(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$39/39(org.apache.spark.sql.execution.aggregate.HashAggregateExec)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/outputFromVectorizedMap$1(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/outputFromVectorizedMap$1(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/outputFromVectorizedMap$1(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/currentVars_$eq(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/outputFromVectorizedMap$1(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/value()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/outputFromVectorizedMap$1(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/HashAggregateExec/outputFromVectorizedMap$1(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String)#org/apache/spark/sql/types/StructType/foreach(scala.Function1)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/outputFromVectorizedMap$1(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/outputFromVectorizedMap$1(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/INPUT_ROW_$eq(java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/outputFromVectorizedMap$1(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/outputFromVectorizedMap$1(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String)#org/apache/spark/sql/catalyst/expressions/codegen/GenerateUnsafeProjection$/createCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,boolean)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/outputFromVectorizedMap$1(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$outputFromVectorizedMap$1$1/1(org.apache.spark.sql.execution.aggregate.HashAggregateExec,scala.runtime.ObjectRef)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/outputFromVectorizedMap$1(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/outputFromVectorizedMap$1(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String)#scala/runtime/ObjectRef/create(java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/outputFromVectorizedMap$1(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/withSubExprEliminationExprs(scala.collection.immutable.Map,scala.Function0)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/Seq/zipWithIndex(scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/currentVars_$eq(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$doConsumeWithKeys$1/1(org.apache.spark.sql.execution.aggregate.HashAggregateExec)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$51/51(org.apache.spark.sql.execution.aggregate.HashAggregateExec,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/BindReferences$/bindReference$default$3()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/mutable/ArrayOps/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$doConsumeWithKeys$2/2(org.apache.spark.sql.execution.aggregate.HashAggregateExec)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#java/lang/String/trim()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Option/isDefined()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Tuple4/_1()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/code()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/Expression/genCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Tuple4/_2()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Predef$DummyImplicit$/dummyImplicit()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Tuple4/_3()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/value()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addMutableState(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Tuple4/_4()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Array$/fallbackCanBuildFrom(scala.Predef$DummyImplicit)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/TraversableOnce/mkString(java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/SubExprCodes/states()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/SubExprCodes/codes()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/GenerateUnsafeProjection$/createCode$default$3()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/generateExpressions$default$2()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/INPUT_ROW_$eq(java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/generateExpressions(scala.collection.Seq,boolean)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/package$/AttributeSeq(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/freshName(java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$49/49(org.apache.spark.sql.execution.aggregate.HashAggregateExec,scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$40/40(org.apache.spark.sql.execution.aggregate.HashAggregateExec)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$41/41(org.apache.spark.sql.execution.aggregate.HashAggregateExec)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$42/42(org.apache.spark.sql.execution.aggregate.HashAggregateExec)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$43/43(org.apache.spark.sql.execution.aggregate.HashAggregateExec)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$44/44(org.apache.spark.sql.execution.aggregate.HashAggregateExec)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$45/45(org.apache.spark.sql.execution.aggregate.HashAggregateExec)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/GenerateUnsafeProjection$/createCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,boolean)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/Seq/length()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$50/50(org.apache.spark.sql.execution.aggregate.HashAggregateExec,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/subexpressionEliminationForWholeStageCodegen(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/Murmur3Hash/Murmur3Hash(scala.collection.Seq,int)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/BindReferences$/bindReference(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.package$AttributeSeq,boolean)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Tuple2/_2$mcI$sp()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/Seq/mkString(java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Tuple2/_1$mcI$sp()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Tuple4/Tuple4(java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Option/get()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/finishAggregate(org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap,org.apache.spark.sql.execution.UnsafeKVExternalSorter,org.apache.spark.sql.execution.metric.SQLMetric,org.apache.spark.sql.execution.metric.SQLMetric)#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/finishAggregate(org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap,org.apache.spark.sql.execution.UnsafeKVExternalSorter,org.apache.spark.sql.execution.metric.SQLMetric,org.apache.spark.sql.execution.metric.SQLMetric)#org/apache/spark/TaskContext/taskMetrics()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/finishAggregate(org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap,org.apache.spark.sql.execution.UnsafeKVExternalSorter,org.apache.spark.sql.execution.metric.SQLMetric,org.apache.spark.sql.execution.metric.SQLMetric)#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/finishAggregate(org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap,org.apache.spark.sql.execution.UnsafeKVExternalSorter,org.apache.spark.sql.execution.metric.SQLMetric,org.apache.spark.sql.execution.metric.SQLMetric)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/finishAggregate(org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap,org.apache.spark.sql.execution.UnsafeKVExternalSorter,org.apache.spark.sql.execution.metric.SQLMetric,org.apache.spark.sql.execution.metric.SQLMetric)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anon$1/1(org.apache.spark.sql.execution.aggregate.HashAggregateExec,org.apache.spark.sql.execution.UnsafeKVExternalSorter,org.apache.spark.sql.execution.metric.SQLMetric,org.apache.spark.sql.execution.UnsafeKVExternalSorter$KVSorterIterator)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/finishAggregate(org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap,org.apache.spark.sql.execution.UnsafeKVExternalSorter,org.apache.spark.sql.execution.metric.SQLMetric,org.apache.spark.sql.execution.metric.SQLMetric)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$25/25(org.apache.spark.sql.execution.aggregate.HashAggregateExec)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/finishAggregate(org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap,org.apache.spark.sql.execution.UnsafeKVExternalSorter,org.apache.spark.sql.execution.metric.SQLMetric,org.apache.spark.sql.execution.metric.SQLMetric)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/finishAggregate(org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap,org.apache.spark.sql.execution.UnsafeKVExternalSorter,org.apache.spark.sql.execution.metric.SQLMetric,org.apache.spark.sql.execution.metric.SQLMetric)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$1/1(org.apache.spark.sql.execution.aggregate.HashAggregateExec)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/finishAggregate(org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap,org.apache.spark.sql.execution.UnsafeKVExternalSorter,org.apache.spark.sql.execution.metric.SQLMetric,org.apache.spark.sql.execution.metric.SQLMetric)#org/apache/spark/executor/TaskMetrics/incPeakExecutionMemory(long)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/finishAggregate(org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap,org.apache.spark.sql.execution.UnsafeKVExternalSorter,org.apache.spark.sql.execution.metric.SQLMetric,org.apache.spark.sql.execution.metric.SQLMetric)#org/apache/spark/TaskContext$/get()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/finishAggregate(org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap,org.apache.spark.sql.execution.UnsafeKVExternalSorter,org.apache.spark.sql.execution.metric.SQLMetric,org.apache.spark.sql.execution.metric.SQLMetric)#java/lang/Math/max(long,long)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/execution/aggregate/HashAggregateExec/logWarning(scala.Function0)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$doProduceWithKeys$1/1(org.apache.spark.sql.execution.aggregate.HashAggregateExec)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addNewFunction(java.lang.String,java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addReferenceObj$default$3()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/copyResult_$eq(boolean)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addMutableState(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#java/lang/Class/getName()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/internal/SQLConf/enableTwoLevelAggMap()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/freshName(java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addReferenceObj(java.lang.String,java.lang.Object,java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/execution/aggregate/HashAggregateExec/sqlContext()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doProduceWithKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/generateResultCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,java.lang.String)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/currentVars_$eq(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/generateResultCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,java.lang.String)#java/lang/Class/getName()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/generateResultCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,java.lang.String)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/freshName(java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/generateResultCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,java.lang.String)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$30/30(org.apache.spark.sql.execution.aggregate.HashAggregateExec)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/generateResultCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,java.lang.String)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$32/32(org.apache.spark.sql.execution.aggregate.HashAggregateExec,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/generateResultCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,java.lang.String)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/generateResultCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/generateResultCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,java.lang.String)#scala/collection/Seq/contains(java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/generateResultCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,java.lang.String)#scala/collection/Seq/zipWithIndex(scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/generateResultCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,java.lang.String)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/generateResultCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,java.lang.String)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addMutableState(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/generateResultCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/HashAggregateExec/generateResultCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/generateResultCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,java.lang.String)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/generateResultCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,java.lang.String)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/INPUT_ROW_$eq(java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/generateResultCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/generateResultCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,java.lang.String)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$28/28(org.apache.spark.sql.execution.aggregate.HashAggregateExec,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/generateResultCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,java.lang.String)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/generateResultCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,java.lang.String)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$33/33(org.apache.spark.sql.execution.aggregate.HashAggregateExec,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/generateResultCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,java.lang.String)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$31/31(org.apache.spark.sql.execution.aggregate.HashAggregateExec,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/generateResultCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,java.lang.String)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$29/29(org.apache.spark.sql.execution.aggregate.HashAggregateExec,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#java/lang/String/trim()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$16/16(org.apache.spark.sql.execution.aggregate.HashAggregateExec,scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/subexpressionEliminationForWholeStageCodegen(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/withSubExprEliminationExprs(scala.collection.immutable.Map,scala.Function0)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/Seq/zipWithIndex(scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/currentVars_$eq(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/SubExprCodes/states()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/SubExprCodes/codes()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$18/18(org.apache.spark.sql.execution.aggregate.HashAggregateExec)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$13/13(org.apache.spark.sql.execution.aggregate.HashAggregateExec)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$14/14(org.apache.spark.sql.execution.aggregate.HashAggregateExec)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$15/15(org.apache.spark.sql.execution.aggregate.HashAggregateExec)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$17/17(org.apache.spark.sql.execution.aggregate.HashAggregateExec,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/collection/Seq/mkString(java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/doConsumeWithoutKeys(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/generateGenerateCode$1(org.apache.spark.sql.execution.aggregate.HashMapGenerator)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/generateGenerateCode$1(org.apache.spark.sql.execution.aggregate.HashMapGenerator)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/HashAggregateExec/generateGenerateCode$1(org.apache.spark.sql.execution.aggregate.HashMapGenerator)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/generateGenerateCode$1(org.apache.spark.sql.execution.aggregate.HashMapGenerator)#org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/generate()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/generateGenerateCode$1(org.apache.spark.sql.execution.aggregate.HashMapGenerator)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/generateGenerateCode$1(org.apache.spark.sql.execution.aggregate.HashMapGenerator)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/generateGenerateCode$1(org.apache.spark.sql.execution.aggregate.HashMapGenerator)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/generateGenerateCode$1(org.apache.spark.sql.execution.aggregate.HashMapGenerator)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/generateGenerateCode$1(org.apache.spark.sql.execution.aggregate.HashMapGenerator)#org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/generate()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/toString(boolean)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/toString(boolean)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/HashAggregateExec/toString(boolean)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/toString(boolean)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/toString(boolean)#org/apache/spark/util/Utils$/truncatedString(scala.collection.Seq,java.lang.String,java.lang.String,java.lang.String,int)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/toString(boolean)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/toString(boolean)#scala/Some/x()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/toString(boolean)#org/apache/spark/util/Utils$/truncatedString$default$5()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/toString(boolean)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/toString(boolean)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/toString(boolean)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/toString(boolean)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/python/BatchEvalPythonExec/doExecute()#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/doExecute()#org/apache/spark/rdd/RDD/conf()
org/apache/spark/sql/execution/python/BatchEvalPythonExec/doExecute()#org/apache/spark/rdd/RDD/mapPartitions$default$2()
org/apache/spark/sql/execution/python/BatchEvalPythonExec/doExecute()#org/apache/spark/sql/execution/python/BatchEvalPythonExec$$anonfun$doExecute$1/1(org.apache.spark.sql.execution.python.BatchEvalPythonExec,int,boolean)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/doExecute()#org/apache/spark/sql/execution/python/BatchEvalPythonExec$$anonfun$1/1(org.apache.spark.sql.execution.python.BatchEvalPythonExec)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/doExecute()#org/apache/spark/rdd/RDD/map(scala.Function1,scala.reflect.ClassTag)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/doExecute()#org/apache/spark/rdd/RDD/mapPartitions(scala.Function1,boolean,scala.reflect.ClassTag)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/doExecute()#org/apache/spark/SparkConf/getBoolean(java.lang.String,boolean)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/doExecute()#org/apache/spark/SparkConf/getInt(java.lang.String,int)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/org$apache$spark$sql$execution$python$BatchEvalPythonExec$$collectFunctions(org.apache.spark.sql.execution.python.PythonUDF)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/python/BatchEvalPythonExec/org$apache$spark$sql$execution$python$BatchEvalPythonExec$$collectFunctions(org.apache.spark.sql.execution.python.PythonUDF)#scala/Predef$/assert(boolean)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/org$apache$spark$sql$execution$python$BatchEvalPythonExec$$collectFunctions(org.apache.spark.sql.execution.python.PythonUDF)#scala/collection/Seq/forall(scala.Function1)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/org$apache$spark$sql$execution$python$BatchEvalPythonExec$$collectFunctions(org.apache.spark.sql.execution.python.PythonUDF)#scala/Tuple2/_1()
org/apache/spark/sql/execution/python/BatchEvalPythonExec/org$apache$spark$sql$execution$python$BatchEvalPythonExec$$collectFunctions(org.apache.spark.sql.execution.python.PythonUDF)#org/apache/spark/sql/execution/python/BatchEvalPythonExec$$anonfun$org$apache$spark$sql$execution$python$BatchEvalPythonExec$$collectFunctions$1/1(org.apache.spark.sql.execution.python.BatchEvalPythonExec)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/org$apache$spark$sql$execution$python$BatchEvalPythonExec$$collectFunctions(org.apache.spark.sql.execution.python.PythonUDF)#scala/collection/SeqLike/apply(int)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/org$apache$spark$sql$execution$python$BatchEvalPythonExec$$collectFunctions(org.apache.spark.sql.execution.python.PythonUDF)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/org$apache$spark$sql$execution$python$BatchEvalPythonExec$$collectFunctions(org.apache.spark.sql.execution.python.PythonUDF)#scala/Some/get()
org/apache/spark/sql/execution/python/BatchEvalPythonExec/org$apache$spark$sql$execution$python$BatchEvalPythonExec$$collectFunctions(org.apache.spark.sql.execution.python.PythonUDF)#org/apache/spark/api/python/ChainedPythonFunctions/ChainedPythonFunctions(scala.collection.Seq)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/org$apache$spark$sql$execution$python$BatchEvalPythonExec$$collectFunctions(org.apache.spark.sql.execution.python.PythonUDF)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/python/BatchEvalPythonExec/org$apache$spark$sql$execution$python$BatchEvalPythonExec$$collectFunctions(org.apache.spark.sql.execution.python.PythonUDF)#scala/Tuple2/_2()
org/apache/spark/sql/execution/python/BatchEvalPythonExec/org$apache$spark$sql$execution$python$BatchEvalPythonExec$$collectFunctions(org.apache.spark.sql.execution.python.PythonUDF)#scala/Some/isEmpty()
org/apache/spark/sql/execution/python/BatchEvalPythonExec/org$apache$spark$sql$execution$python$BatchEvalPythonExec$$collectFunctions(org.apache.spark.sql.execution.python.PythonUDF)#scala/collection/Seq$/unapplySeq(scala.collection.Seq)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/org$apache$spark$sql$execution$python$BatchEvalPythonExec$$collectFunctions(org.apache.spark.sql.execution.python.PythonUDF)#org/apache/spark/api/python/ChainedPythonFunctions/funcs()
org/apache/spark/sql/execution/python/BatchEvalPythonExec/org$apache$spark$sql$execution$python$BatchEvalPythonExec$$collectFunctions(org.apache.spark.sql.execution.python.PythonUDF)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/org$apache$spark$sql$execution$python$BatchEvalPythonExec$$collectFunctions(org.apache.spark.sql.execution.python.PythonUDF)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/org$apache$spark$sql$execution$python$BatchEvalPythonExec$$collectFunctions(org.apache.spark.sql.execution.python.PythonUDF)#scala/collection/SeqLike/lengthCompare(int)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/org$apache$spark$sql$execution$python$BatchEvalPythonExec$$collectFunctions(org.apache.spark.sql.execution.python.PythonUDF)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/anonfun/run/4/apply(java.lang.String)#org/apache/spark/sql/catalyst/catalog/CatalogTable/properties()
org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/anonfun/run/4/apply(java.lang.String)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/anonfun/run/4/apply(java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/anonfun/run/4/apply(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/anonfun/run/4/apply(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/anonfun/run/4/apply(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/anonfun/run/4/apply(java.lang.String)#org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()
org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/anonfun/run/4/apply(java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/anonfun/run/4/apply(java.lang.String)#scala/collection/immutable/Map/contains(java.lang.Object)
org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/anonfun/run/4/apply(java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/anonfun/run/4/apply(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/command/AlterTableUnsetPropertiesCommand/anonfun/run/4/apply(java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/StreamingExplainCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/streaming/IncrementalExecution/simpleString()
org/apache/spark/sql/execution/command/StreamingExplainCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/errors/package$TreeNodeException/getMessage()
org/apache/spark/sql/execution/command/StreamingExplainCommand/run(org.apache.spark.sql.SparkSession)#java/lang/String/split(java.lang.String)
org/apache/spark/sql/execution/command/StreamingExplainCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/StreamingExplainCommand$$anonfun$run$2/2(org.apache.spark.sql.execution.command.StreamingExplainCommand)
org/apache/spark/sql/execution/command/StreamingExplainCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/StreamingExplainCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/StreamingExplainCommand/run(org.apache.spark.sql.SparkSession)#scala/Array$/fallbackCanBuildFrom(scala.Predef$DummyImplicit)
org/apache/spark/sql/execution/command/StreamingExplainCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/command/StreamingExplainCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/Row$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/command/StreamingExplainCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/command/StreamingExplainCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/command/StreamingExplainCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/command/StreamingExplainCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/streaming/IncrementalExecution/toString()
org/apache/spark/sql/execution/command/StreamingExplainCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$DummyImplicit$/dummyImplicit()
org/apache/spark/sql/execution/command/StreamingExplainCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/command/StreamingExplainCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/aggregate/TypedAggregateExpression/apply(org.apache.spark.sql.expressions.Aggregator,org.apache.spark.sql.Encoder,org.apache.spark.sql.Encoder)#org/apache/spark/sql/catalyst/analysis/UnresolvedDeserializer/UnresolvedDeserializer(org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/TypedAggregateExpression/apply(org.apache.spark.sql.expressions.Aggregator,org.apache.spark.sql.Encoder,org.apache.spark.sql.Encoder)#org/apache/spark/sql/types/StructField/nullable()
org/apache/spark/sql/execution/aggregate/TypedAggregateExpression/apply(org.apache.spark.sql.expressions.Aggregator,org.apache.spark.sql.Encoder,org.apache.spark.sql.Encoder)#org/apache/spark/sql/types/StructField/dataType()
org/apache/spark/sql/execution/aggregate/TypedAggregateExpression/apply(org.apache.spark.sql.expressions.Aggregator,org.apache.spark.sql.Encoder,org.apache.spark.sql.Encoder)#org/apache/spark/sql/catalyst/encoders/ExpressionEncoder/schema()
org/apache/spark/sql/execution/aggregate/TypedAggregateExpression/apply(org.apache.spark.sql.expressions.Aggregator,org.apache.spark.sql.Encoder,org.apache.spark.sql.Encoder)#org/apache/spark/sql/catalyst/expressions/Expression/dataType()
org/apache/spark/sql/execution/aggregate/TypedAggregateExpression/apply(org.apache.spark.sql.expressions.Aggregator,org.apache.spark.sql.Encoder,org.apache.spark.sql.Encoder)#org/apache/spark/sql/types/StructType/head()
org/apache/spark/sql/execution/aggregate/TypedAggregateExpression/apply(org.apache.spark.sql.expressions.Aggregator,org.apache.spark.sql.Encoder,org.apache.spark.sql.Encoder)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/TypedAggregateExpression/apply(org.apache.spark.sql.expressions.Aggregator,org.apache.spark.sql.Encoder,org.apache.spark.sql.Encoder)#org/apache/spark/sql/catalyst/encoders/ExpressionEncoder/serializer()
org/apache/spark/sql/execution/aggregate/TypedAggregateExpression/apply(org.apache.spark.sql.expressions.Aggregator,org.apache.spark.sql.Encoder,org.apache.spark.sql.Encoder)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/aggregate/TypedAggregateExpression/apply(org.apache.spark.sql.expressions.Aggregator,org.apache.spark.sql.Encoder,org.apache.spark.sql.Encoder)#org/apache/spark/sql/catalyst/encoders/ExpressionEncoder/resolveAndBind$default$1()
org/apache/spark/sql/execution/aggregate/TypedAggregateExpression/apply(org.apache.spark.sql.expressions.Aggregator,org.apache.spark.sql.Encoder,org.apache.spark.sql.Encoder)#org/apache/spark/sql/catalyst/encoders/ExpressionEncoder/deserializer()
org/apache/spark/sql/execution/aggregate/TypedAggregateExpression/apply(org.apache.spark.sql.expressions.Aggregator,org.apache.spark.sql.Encoder,org.apache.spark.sql.Encoder)#org/apache/spark/sql/catalyst/encoders/ExpressionEncoder/resolveAndBind$default$2()
org/apache/spark/sql/execution/aggregate/TypedAggregateExpression/apply(org.apache.spark.sql.expressions.Aggregator,org.apache.spark.sql.Encoder,org.apache.spark.sql.Encoder)#org/apache/spark/sql/catalyst/encoders/ExpressionEncoder/resolveAndBind(scala.collection.Seq,org.apache.spark.sql.catalyst.analysis.Analyzer)
org/apache/spark/sql/execution/aggregate/TypedAggregateExpression/apply(org.apache.spark.sql.expressions.Aggregator,org.apache.spark.sql.Encoder,org.apache.spark.sql.Encoder)#org/apache/spark/sql/execution/aggregate/ComplexTypedAggregateExpression$/apply$default$10()
org/apache/spark/sql/execution/aggregate/TypedAggregateExpression/apply(org.apache.spark.sql.expressions.Aggregator,org.apache.spark.sql.Encoder,org.apache.spark.sql.Encoder)#org/apache/spark/sql/execution/aggregate/TypedAggregateExpression$$anonfun$3/3()
org/apache/spark/sql/execution/aggregate/TypedAggregateExpression/apply(org.apache.spark.sql.expressions.Aggregator,org.apache.spark.sql.Encoder,org.apache.spark.sql.Encoder)#org/apache/spark/sql/catalyst/encoders/ExpressionEncoder/flat()
org/apache/spark/sql/execution/aggregate/TypedAggregateExpression/apply(org.apache.spark.sql.expressions.Aggregator,org.apache.spark.sql.Encoder,org.apache.spark.sql.Encoder)#org/apache/spark/sql/execution/aggregate/ComplexTypedAggregateExpression$/apply$default$11()
org/apache/spark/sql/execution/aggregate/TypedAggregateExpression/apply(org.apache.spark.sql.expressions.Aggregator,org.apache.spark.sql.Encoder,org.apache.spark.sql.Encoder)#org/apache/spark/sql/catalyst/encoders/ExpressionEncoder/namedExpressions()
org/apache/spark/sql/execution/aggregate/TypedAggregateExpression/apply(org.apache.spark.sql.expressions.Aggregator,org.apache.spark.sql.Encoder,org.apache.spark.sql.Encoder)#scala/collection/Seq/head()
org/apache/spark/sql/execution/aggregate/TypedAggregateExpression/apply(org.apache.spark.sql.expressions.Aggregator,org.apache.spark.sql.Encoder,org.apache.spark.sql.Encoder)#org/apache/spark/sql/catalyst/expressions/Alias/child()
org/apache/spark/sql/execution/aggregate/TypedAggregateExpression/apply(org.apache.spark.sql.expressions.Aggregator,org.apache.spark.sql.Encoder,org.apache.spark.sql.Encoder)#org/apache/spark/sql/catalyst/encoders/package$/encoderFor(org.apache.spark.sql.Encoder)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/FileSystemManager/rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)#java/io/IOException/IOException(java.lang.String)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/FileSystemManager/rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)#org/apache/hadoop/fs/FileSystem/exists(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/FileSystemManager/rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/FileSystemManager/rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/FileSystemManager/rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/FileSystemManager/rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)#org/apache/hadoop/fs/FileAlreadyExistsException/FileAlreadyExistsException(java.lang.String)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/FileSystemManager/rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)#java/io/FileNotFoundException/FileNotFoundException(java.lang.String)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/FileSystemManager/rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)#org/apache/spark/sql/execution/streaming/HDFSMetadataLog$FileSystemManager/fs()
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/FileSystemManager/rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/FileSystemManager/rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)#org/apache/hadoop/fs/FileSystem/rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/abort()#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$HDFSBackedStateStore$$anonfun$abort$3/3(org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/abort()#java/io/DataOutputStream/close()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/abort()#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$HDFSBackedStateStore$$anonfun$abort$2/2(org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/abort()#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$HDFSBackedStateStore/ABORTED()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/abort()#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$HDFSBackedStateStore$$anonfun$abort$4/4(org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/abort()#scala/runtime/BoxesRunTime/boxToBoolean(boolean)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/abort()#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$HDFSBackedStateStore/tempDeltaFile()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/abort()#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$HDFSBackedStateStore/tempDeltaFileStream()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/abort()#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$HDFSBackedStateStore/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$HDFSBackedStateStore$$$outer()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/abort()#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$HDFSBackedStateStore/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$HDFSBackedStateStore$$state_$eq(org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore$STATE)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/abort()#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$HDFSBackedStateStore$$anonfun$abort$1/1(org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/abort()#org/apache/hadoop/fs/FileSystem/delete(org.apache.hadoop.fs.Path,boolean)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/commit()#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$HDFSBackedStateStore/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$HDFSBackedStateStore$$finalDeltaFile_$eq(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/commit()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/commit()#scala/Option/isEmpty()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/commit()#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$HDFSBackedStateStore/COMMITTED()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/commit()#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$HDFSBackedStateStore/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$HDFSBackedStateStore$$newVersion()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/commit()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/commit()#scala/Option/get()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/commit()#java/lang/IllegalStateException/IllegalStateException(java.lang.String,java.lang.Throwable)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/commit()#scala/util/control/NonFatal$/unapply(java.lang.Throwable)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/commit()#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/commit()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/commit()#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$HDFSBackedStateStore/tempDeltaFile()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/commit()#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$HDFSBackedStateStore$$anonfun$commit$1/1(org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/commit()#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$HDFSBackedStateStore/tempDeltaFileStream()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/commit()#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$HDFSBackedStateStore$$anonfun$commit$2/2(org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/commit()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/commit()#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$HDFSBackedStateStore/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$HDFSBackedStateStore$$$outer()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/commit()#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$HDFSBackedStateStore/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$HDFSBackedStateStore$$state_$eq(org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore$STATE)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/remove(org.apache.spark.sql.catalyst.expressions.UnsafeRow)#scala/Some/x()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/remove(org.apache.spark.sql.catalyst.expressions.UnsafeRow)#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$HDFSBackedStateStore$$anonfun$remove$2/2(org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/remove(org.apache.spark.sql.catalyst.expressions.UnsafeRow)#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/remove(org.apache.spark.sql.catalyst.expressions.UnsafeRow)#java/util/HashMap/put(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/remove(org.apache.spark.sql.catalyst.expressions.UnsafeRow)#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$HDFSBackedStateStore/tempDeltaFileStream()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/remove(org.apache.spark.sql.catalyst.expressions.UnsafeRow)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/remove(org.apache.spark.sql.catalyst.expressions.UnsafeRow)#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$HDFSBackedStateStore/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$HDFSBackedStateStore$$$outer()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/remove(org.apache.spark.sql.catalyst.expressions.UnsafeRow)#java/util/concurrent/ConcurrentHashMap/remove(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/remove(org.apache.spark.sql.catalyst.expressions.UnsafeRow)#java/util/HashMap/get(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/remove(org.apache.spark.sql.catalyst.expressions.UnsafeRow)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/remove(org.apache.spark.sql.catalyst.expressions.UnsafeRow)#java/util/HashMap/remove(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/remove(org.apache.spark.sql.catalyst.expressions.UnsafeRow)#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$HDFSBackedStateStore/allUpdates()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/remove(org.apache.spark.sql.catalyst.expressions.UnsafeRow)#java/util/concurrent/ConcurrentHashMap/containsKey(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/remove(scala.Function1)#java/util/Iterator/remove()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/remove(scala.Function1)#java/util/Iterator/hasNext()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/remove(scala.Function1)#java/util/Map$Entry/getKey()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/remove(scala.Function1)#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/remove(scala.Function1)#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$HDFSBackedStateStore/tempDeltaFileStream()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/remove(scala.Function1)#scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/remove(scala.Function1)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/remove(scala.Function1)#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/remove(scala.Function1)#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$HDFSBackedStateStore/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$HDFSBackedStateStore$$$outer()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/remove(scala.Function1)#java/util/Map$Entry/getValue()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/remove(scala.Function1)#java/util/concurrent/ConcurrentHashMap/entrySet()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/remove(scala.Function1)#java/util/HashMap/get(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/remove(scala.Function1)#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$HDFSBackedStateStore$$anonfun$remove$1/1(org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/remove(scala.Function1)#scala/Some/x()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/remove(scala.Function1)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/remove(scala.Function1)#java/util/HashMap/remove(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/remove(scala.Function1)#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$HDFSBackedStateStore/allUpdates()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/remove(scala.Function1)#java/util/Iterator/next()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/remove(scala.Function1)#java/util/HashMap/put(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/remove(scala.Function1)#java/util/Set/iterator()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/put(org.apache.spark.sql.catalyst.expressions.UnsafeRow,org.apache.spark.sql.catalyst.expressions.UnsafeRow)#scala/Some/x()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/put(org.apache.spark.sql.catalyst.expressions.UnsafeRow,org.apache.spark.sql.catalyst.expressions.UnsafeRow)#java/util/concurrent/ConcurrentHashMap/put(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/put(org.apache.spark.sql.catalyst.expressions.UnsafeRow,org.apache.spark.sql.catalyst.expressions.UnsafeRow)#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$HDFSBackedStateStore/allUpdates()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/put(org.apache.spark.sql.catalyst.expressions.UnsafeRow,org.apache.spark.sql.catalyst.expressions.UnsafeRow)#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/put(org.apache.spark.sql.catalyst.expressions.UnsafeRow,org.apache.spark.sql.catalyst.expressions.UnsafeRow)#java/util/HashMap/put(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/put(org.apache.spark.sql.catalyst.expressions.UnsafeRow,org.apache.spark.sql.catalyst.expressions.UnsafeRow)#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$HDFSBackedStateStore/tempDeltaFileStream()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/put(org.apache.spark.sql.catalyst.expressions.UnsafeRow,org.apache.spark.sql.catalyst.expressions.UnsafeRow)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/put(org.apache.spark.sql.catalyst.expressions.UnsafeRow,org.apache.spark.sql.catalyst.expressions.UnsafeRow)#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$HDFSBackedStateStore/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$HDFSBackedStateStore$$$outer()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/put(org.apache.spark.sql.catalyst.expressions.UnsafeRow,org.apache.spark.sql.catalyst.expressions.UnsafeRow)#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$HDFSBackedStateStore$$anonfun$put$1/1(org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/put(org.apache.spark.sql.catalyst.expressions.UnsafeRow,org.apache.spark.sql.catalyst.expressions.UnsafeRow)#java/util/HashMap/get(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/put(org.apache.spark.sql.catalyst.expressions.UnsafeRow,org.apache.spark.sql.catalyst.expressions.UnsafeRow)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/HDFSBackedStateStore/put(org.apache.spark.sql.catalyst.expressions.UnsafeRow,org.apache.spark.sql.catalyst.expressions.UnsafeRow)#java/util/concurrent/ConcurrentHashMap/containsKey(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/anonfun/writeSnapshotFile/1/apply$mcV$sp()#scala/collection/mutable/ArrayOps/size()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/anonfun/writeSnapshotFile/1/apply$mcV$sp()#java/io/DataOutputStream/writeInt(int)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/anonfun/writeSnapshotFile/1/apply$mcV$sp()#java/util/Iterator/hasNext()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/anonfun/writeSnapshotFile/1/apply$mcV$sp()#java/util/Map$Entry/getKey()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/anonfun/writeSnapshotFile/1/apply$mcV$sp()#java/util/Iterator/next()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/anonfun/writeSnapshotFile/1/apply$mcV$sp()#java/util/Set/iterator()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/anonfun/writeSnapshotFile/1/apply$mcV$sp()#java/util/Map$Entry/getValue()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/anonfun/writeSnapshotFile/1/apply$mcV$sp()#java/util/concurrent/ConcurrentHashMap/entrySet()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/anonfun/writeSnapshotFile/1/apply$mcV$sp()#scala/Predef$/byteArrayOps(byte[])
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/anonfun/writeSnapshotFile/1/apply$mcV$sp()#java/io/DataOutputStream/write(byte[])
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/anonfun/writeSnapshotFile/1/apply$mcV$sp()#org/apache/spark/sql/catalyst/expressions/UnsafeRow/getBytes()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/anonfun/writeSnapshotFile/1/apply$mcV$sp()#org/apache/hadoop/fs/FileSystem/create(org.apache.hadoop.fs.Path,boolean)
org/apache/spark/sql/execution/columnar/InMemoryRelation/anonfun/1/anon/1/anonfun/next/1/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/columnar/InMemoryRelation/anonfun/1/anon/1/anonfun/next/1/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/columnar/InMemoryRelation/anonfun/1/anon/1/anonfun/next/1/apply()#org/apache/spark/sql/execution/columnar/InMemoryRelation$$anonfun$1$$anon$1/org$apache$spark$sql$execution$columnar$InMemoryRelation$$anonfun$$anon$$$outer()
org/apache/spark/sql/execution/columnar/InMemoryRelation/anonfun/1/anon/1/anonfun/next/1/apply()#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/columnar/InMemoryRelation/anonfun/1/anon/1/anonfun/next/1/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/columnar/InMemoryRelation/anonfun/1/anon/1/anonfun/next/1/apply()#org/apache/spark/sql/execution/columnar/InMemoryRelation$$anonfun$1$$anon$1$$anonfun$next$1/apply()
org/apache/spark/sql/execution/columnar/InMemoryRelation/anonfun/1/anon/1/anonfun/next/1/apply()#org/apache/spark/sql/catalyst/InternalRow/numFields()
org/apache/spark/sql/execution/columnar/InMemoryRelation/anonfun/1/anon/1/anonfun/next/1/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/columnar/InMemoryRelation/anonfun/1/anon/1/anonfun/next/1/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/columnar/InMemoryRelation/anonfun/1/anon/1/anonfun/next/1/apply()#scala/collection/Seq/size()
org/apache/spark/sql/execution/columnar/InMemoryRelation/anonfun/1/anon/1/anonfun/next/1/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/columnar/InMemoryRelation/anonfun/1/anon/1/anonfun/next/1/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.SpecializedGetters,int)#org/apache/parquet/io/api/Binary/fromReusedByteArray(byte[],int,int)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.SpecializedGetters,int)#org/apache/spark/sql/catalyst/expressions/SpecializedGetters/getDecimal(int,int,int)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.SpecializedGetters,int)#scala/runtime/BoxesRunTime/unboxToByte(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.SpecializedGetters,int)#java/util/Arrays/fill(byte[],int,int,byte)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.SpecializedGetters,int)#java/math/BigInteger/toByteArray()
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.SpecializedGetters,int)#java/lang/System/arraycopy(java.lang.Object,int,java.lang.Object,int,int)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.SpecializedGetters,int)#scala/collection/mutable/ArrayOps/head()
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.SpecializedGetters,int)#org/apache/spark/sql/types/Decimal/toJavaBigDecimal()
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.SpecializedGetters,int)#scala/Predef$/byteArrayOps(byte[])
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.SpecializedGetters,int)#org/apache/parquet/io/api/RecordConsumer/addBinary(org.apache.parquet.io.api.Binary)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.SpecializedGetters,int)#java/math/BigDecimal/unscaledValue()
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/org$apache$spark$sql$execution$datasources$FileFormatWriter$DynamicPartitionWriteTask$$newOutputWriter(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,int,scala.collection.mutable.Set)#scala/Some/x()
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/org$apache$spark$sql$execution$datasources$FileFormatWriter$DynamicPartitionWriteTask$$newOutputWriter(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,int,scala.collection.mutable.Set)#org/apache/spark/sql/execution/datasources/FileFormatWriter$WriteJobDescription/dataColumns()
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/org$apache$spark$sql$execution$datasources$FileFormatWriter$DynamicPartitionWriteTask$$newOutputWriter(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,int,scala.collection.mutable.Set)#scala/Option/foreach(scala.Function1)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/org$apache$spark$sql$execution$datasources$FileFormatWriter$DynamicPartitionWriteTask$$newOutputWriter(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,int,scala.collection.mutable.Set)#org/apache/spark/sql/execution/datasources/FileFormatWriter$WriteJobDescription/outputWriterFactory()
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/org$apache$spark$sql$execution$datasources$FileFormatWriter$DynamicPartitionWriteTask$$newOutputWriter(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,int,scala.collection.mutable.Set)#org/apache/spark/sql/execution/datasources/FileFormatWriter$WriteJobDescription/customPartitionLocations()
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/org$apache$spark$sql$execution$datasources$FileFormatWriter$DynamicPartitionWriteTask$$newOutputWriter(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,int,scala.collection.mutable.Set)#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/org$apache$spark$sql$execution$datasources$FileFormatWriter$DynamicPartitionWriteTask$$newOutputWriter(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,int,scala.collection.mutable.Set)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/getString(int)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/org$apache$spark$sql$execution$datasources$FileFormatWriter$DynamicPartitionWriteTask$$newOutputWriter(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,int,scala.collection.mutable.Set)#org/apache/spark/sql/catalyst/expressions/package$/AttributeSeq(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/org$apache$spark$sql$execution$datasources$FileFormatWriter$DynamicPartitionWriteTask$$newOutputWriter(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,int,scala.collection.mutable.Set)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/org$apache$spark$sql$execution$datasources$FileFormatWriter$DynamicPartitionWriteTask$$newOutputWriter(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,int,scala.collection.mutable.Set)#org/apache/spark/sql/execution/datasources/PartitioningUtils$/parsePathFragment(java.lang.String)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/org$apache$spark$sql$execution$datasources$FileFormatWriter$DynamicPartitionWriteTask$$newOutputWriter(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,int,scala.collection.mutable.Set)#scala/collection/Seq/isEmpty()
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/org$apache$spark$sql$execution$datasources$FileFormatWriter$DynamicPartitionWriteTask$$newOutputWriter(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,int,scala.collection.mutable.Set)#org/apache/spark/sql/execution/datasources/BucketingUtils$/bucketIdToString(int)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/org$apache$spark$sql$execution$datasources$FileFormatWriter$DynamicPartitionWriteTask$$newOutputWriter(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,int,scala.collection.mutable.Set)#org/apache/spark/sql/catalyst/InternalRow/getInt(int)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/org$apache$spark$sql$execution$datasources$FileFormatWriter$DynamicPartitionWriteTask$$newOutputWriter(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,int,scala.collection.mutable.Set)#org/apache/spark/sql/execution/datasources/FileFormatWriter$WriteJobDescription/bucketIdExpression()
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/org$apache$spark$sql$execution$datasources$FileFormatWriter$DynamicPartitionWriteTask$$newOutputWriter(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,int,scala.collection.mutable.Set)#org/apache/spark/sql/catalyst/expressions/UnsafeProjection/apply(org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/org$apache$spark$sql$execution$datasources$FileFormatWriter$DynamicPartitionWriteTask$$newOutputWriter(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,int,scala.collection.mutable.Set)#scala/Option/isDefined()
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/org$apache$spark$sql$execution$datasources$FileFormatWriter$DynamicPartitionWriteTask$$newOutputWriter(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,int,scala.collection.mutable.Set)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/org$apache$spark$sql$execution$datasources$FileFormatWriter$DynamicPartitionWriteTask$$newOutputWriter(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,int,scala.collection.mutable.Set)#org/apache/spark/sql/catalyst/expressions/package$AttributeSeq/toStructType()
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/org$apache$spark$sql$execution$datasources$FileFormatWriter$DynamicPartitionWriteTask$$newOutputWriter(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,int,scala.collection.mutable.Set)#scala/collection/immutable/Map/get(java.lang.Object)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/org$apache$spark$sql$execution$datasources$FileFormatWriter$DynamicPartitionWriteTask$$newOutputWriter(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,int,scala.collection.mutable.Set)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/org$apache$spark$sql$execution$datasources$FileFormatWriter$DynamicPartitionWriteTask$$newOutputWriter(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,int,scala.collection.mutable.Set)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/org$apache$spark$sql$execution$datasources$FileFormatWriter$DynamicPartitionWriteTask$$newOutputWriter(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,int,scala.collection.mutable.Set)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/org$apache$spark$sql$execution$datasources$FileFormatWriter$DynamicPartitionWriteTask$$newOutputWriter(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,int,scala.collection.mutable.Set)#scala/collection/Seq/length()
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/org$apache$spark$sql$execution$datasources$FileFormatWriter$DynamicPartitionWriteTask$$newOutputWriter(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,int,scala.collection.mutable.Set)#org/apache/spark/sql/execution/datasources/FileFormatWriter$DynamicPartitionWriteTask/org$apache$spark$sql$execution$datasources$FileFormatWriter$DynamicPartitionWriteTask$$currentWriter_$eq(org.apache.spark.sql.execution.datasources.OutputWriter)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/org$apache$spark$sql$execution$datasources$FileFormatWriter$DynamicPartitionWriteTask$$newOutputWriter(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,int,scala.collection.mutable.Set)#org/apache/spark/sql/execution/datasources/FileFormatWriter$WriteJobDescription/partitionColumns()
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/org$apache$spark$sql$execution$datasources$FileFormatWriter$DynamicPartitionWriteTask$$newOutputWriter(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,int,scala.collection.mutable.Set)#org/apache/spark/internal/io/FileCommitProtocol/newTaskTempFileAbsPath(org.apache.hadoop.mapreduce.TaskAttemptContext,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/org$apache$spark$sql$execution$datasources$FileFormatWriter$DynamicPartitionWriteTask$$newOutputWriter(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,int,scala.collection.mutable.Set)#org/apache/spark/sql/execution/datasources/FileFormatWriter$DynamicPartitionWriteTask$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$DynamicPartitionWriteTask$$newOutputWriter$1/1(org.apache.spark.sql.execution.datasources.FileFormatWriter$DynamicPartitionWriteTask,scala.collection.mutable.Set)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/org$apache$spark$sql$execution$datasources$FileFormatWriter$DynamicPartitionWriteTask$$newOutputWriter(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,int,scala.collection.mutable.Set)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/org$apache$spark$sql$execution$datasources$FileFormatWriter$DynamicPartitionWriteTask$$newOutputWriter(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,int,scala.collection.mutable.Set)#org/apache/spark/internal/io/FileCommitProtocol/newTaskTempFile(org.apache.hadoop.mapreduce.TaskAttemptContext,scala.Option,java.lang.String)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/org$apache$spark$sql$execution$datasources$FileFormatWriter$DynamicPartitionWriteTask$$newOutputWriter(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,int,scala.collection.mutable.Set)#scala/collection/immutable/StringOps/format(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/org$apache$spark$sql$execution$datasources$FileFormatWriter$DynamicPartitionWriteTask$$newOutputWriter(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,int,scala.collection.mutable.Set)#scala/Option/get()
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/execute(scala.collection.Iterator)#org/apache/spark/sql/execution/datasources/FileFormatWriter$WriteJobDescription/dataColumns()
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/execute(scala.collection.Iterator)#scala/collection/Iterator/foreach(scala.Function1)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/execute(scala.collection.Iterator)#org/apache/spark/sql/execution/datasources/FileFormatWriter$DynamicPartitionWriteTask/partitionPathExpression()
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/execute(scala.collection.Iterator)#scala/runtime/LongRef/create(long)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/execute(scala.collection.Iterator)#scala/runtime/IntRef/create(int)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/execute(scala.collection.Iterator)#scala/collection/mutable/Set$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/execute(scala.collection.Iterator)#org/apache/spark/sql/catalyst/expressions/Concat/Concat(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/execute(scala.collection.Iterator)#org/apache/spark/sql/execution/datasources/FileFormatWriter$WriteJobDescription/partitionColumns()
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/execute(scala.collection.Iterator)#org/apache/spark/sql/execution/datasources/FileFormatWriter$DynamicPartitionWriteTask$$anonfun$execute$2/2(org.apache.spark.sql.execution.datasources.FileFormatWriter$DynamicPartitionWriteTask,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,scala.runtime.LongRef,scala.runtime.IntRef,scala.runtime.ObjectRef,scala.collection.mutable.Set)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/execute(scala.collection.Iterator)#scala/runtime/ObjectRef/create(java.lang.Object)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/execute(scala.collection.Iterator)#org/apache/spark/sql/catalyst/expressions/UnsafeProjection$/create(scala.collection.Seq,scala.collection.Seq)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/execute(scala.collection.Iterator)#scala/collection/mutable/Set/toSet()
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/execute(scala.collection.Iterator)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/execute(scala.collection.Iterator)#org/apache/spark/sql/execution/datasources/FileFormatWriter$WriteJobDescription/allColumns()
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/execute(scala.collection.Iterator)#scala/Option$/option2Iterable(scala.Option)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/execute(scala.collection.Iterator)#org/apache/spark/sql/execution/datasources/FileFormatWriter$WriteJobDescription/bucketIdExpression()
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/execute(scala.collection.Iterator)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/execute(scala.collection.Iterator)#org/apache/spark/sql/execution/datasources/FileFormatWriter$DynamicPartitionWriteTask/releaseResources()
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/execute(scala.collection.Iterator)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/execute(scala.collection.Iterator)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/listFiles(scala.collection.Seq,scala.collection.Seq)#scala/collection/Seq/filter(scala.Function1)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/listFiles(scala.collection.Seq,scala.collection.Seq)#org/apache/spark/sql/catalyst/InternalRow$/empty()
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/listFiles(scala.collection.Seq,scala.collection.Seq)#org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$2/2(org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/listFiles(scala.collection.Seq,scala.collection.Seq)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/listFiles(scala.collection.Seq,scala.collection.Seq)#org/apache/spark/sql/types/StructType/isEmpty()
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/listFiles(scala.collection.Seq,scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/listFiles(scala.collection.Seq,scala.collection.Seq)#scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/listFiles(scala.collection.Seq,scala.collection.Seq)#org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$listFiles$1/1(org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex,scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/listFiles(scala.collection.Seq,scala.collection.Seq)#org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$3/3(org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/inferPartitioning()#scala/Some/x()
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/inferPartitioning()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/inferPartitioning()#org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$5/5(org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/inferPartitioning()#org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$inferPartitioning$1/1(org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex,java.lang.String,org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/inferPartitioning()#scala/collection/MapLike/keys()
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/inferPartitioning()#org/apache/spark/sql/catalyst/util/DateTimeUtils$/TIMEZONE_OPTION()
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/inferPartitioning()#org/apache/spark/sql/internal/SQLConf/partitionColumnTypeInferenceEnabled()
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/inferPartitioning()#org/apache/spark/sql/catalyst/util/CaseInsensitiveMap$/apply(scala.collection.immutable.Map)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/inferPartitioning()#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/inferPartitioning()#org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$6/6(org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/inferPartitioning()#scala/collection/Iterable/toSeq()
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/inferPartitioning()#org/apache/spark/sql/execution/datasources/PartitioningUtils$/parsePartitions(scala.collection.Seq,boolean,scala.collection.immutable.Set,java.lang.String)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/inferPartitioning()#scala/collection/immutable/Map/filter(scala.Function1)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/inferPartitioning()#org/apache/spark/sql/catalyst/util/CaseInsensitiveMap/get(java.lang.String)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/inferPartitioning()#org/apache/spark/sql/types/StructType/nonEmpty()
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/inferPartitioning()#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/prunePartitions(scala.collection.Seq,org.apache.spark.sql.execution.datasources.PartitionSpec)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/prunePartitions(scala.collection.Seq,org.apache.spark.sql.execution.datasources.PartitionSpec)#org/apache/spark/sql/catalyst/expressions/Expression/transform(scala.PartialFunction)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/prunePartitions(scala.collection.Seq,org.apache.spark.sql.execution.datasources.PartitionSpec)#scala/Tuple2/_1()
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/prunePartitions(scala.collection.Seq,org.apache.spark.sql.execution.datasources.PartitionSpec)#scala/Tuple2/_2()
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/prunePartitions(scala.collection.Seq,org.apache.spark.sql.execution.datasources.PartitionSpec)#org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$8/8(org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex,scala.collection.immutable.Set)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/prunePartitions(scala.collection.Seq,org.apache.spark.sql.execution.datasources.PartitionSpec)#scala/collection/TraversableOnce/toSet()
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/prunePartitions(scala.collection.Seq,org.apache.spark.sql.execution.datasources.PartitionSpec)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/prunePartitions(scala.collection.Seq,org.apache.spark.sql.execution.datasources.PartitionSpec)#org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$10/10(org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex,org.apache.spark.sql.catalyst.expressions.InterpretedPredicate)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/prunePartitions(scala.collection.Seq,org.apache.spark.sql.execution.datasources.PartitionSpec)#org/apache/spark/sql/types/StructType/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/prunePartitions(scala.collection.Seq,org.apache.spark.sql.execution.datasources.PartitionSpec)#org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$prunePartitions$1/1(org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex,scala.collection.Seq,scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/prunePartitions(scala.collection.Seq,org.apache.spark.sql.execution.datasources.PartitionSpec)#scala/collection/Seq/filter(scala.Function1)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/prunePartitions(scala.collection.Seq,org.apache.spark.sql.execution.datasources.PartitionSpec)#org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$7/7(org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/prunePartitions(scala.collection.Seq,org.apache.spark.sql.execution.datasources.PartitionSpec)#org/apache/spark/sql/catalyst/expressions/InterpretedPredicate$/create(org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/prunePartitions(scala.collection.Seq,org.apache.spark.sql.execution.datasources.PartitionSpec)#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/prunePartitions(scala.collection.Seq,org.apache.spark.sql.execution.datasources.PartitionSpec)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/prunePartitions(scala.collection.Seq,org.apache.spark.sql.execution.datasources.PartitionSpec)#org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$1/1(org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex,org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/prunePartitions(scala.collection.Seq,org.apache.spark.sql.execution.datasources.PartitionSpec)#scala/collection/Seq/reduce(scala.Function2)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/basePaths()#scala/Some/x()
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/basePaths()#org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/basePaths()#scala/Predef$/Set()
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/basePaths()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/basePaths()#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/basePaths()#scala/collection/TraversableOnce/toSet()
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/basePaths()#scala/collection/immutable/Set$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/basePaths()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/basePaths()#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/basePaths()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/basePaths()#org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/rootPaths()
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/basePaths()#org/apache/hadoop/fs/FileSystem/isDirectory(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/basePaths()#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/basePaths()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/basePaths()#scala/collection/immutable/Map/get(java.lang.Object)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/basePaths()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/basePaths()#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/basePaths()#org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$11/11(org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/basePaths()#org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$basePaths$1/1(org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/basePaths()#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/basePaths()#org/apache/hadoop/fs/FileSystem/makeQualified(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/basePaths()#org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$/BASE_PATH_PARAM()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/binaryToSQLTimestamp(org.apache.parquet.io.api.Binary)#org/apache/parquet/io/api/Binary/toByteBuffer()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/binaryToSQLTimestamp(org.apache.parquet.io.api.Binary)#org/apache/parquet/io/api/Binary/length()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/binaryToSQLTimestamp(org.apache.parquet.io.api.Binary)#scala/Predef$/assert(boolean,scala.Function0)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/binaryToSQLTimestamp(org.apache.parquet.io.api.Binary)#java/nio/ByteBuffer/order(java.nio.ByteOrder)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/binaryToSQLTimestamp(org.apache.parquet.io.api.Binary)#java/nio/ByteBuffer/getLong()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/binaryToSQLTimestamp(org.apache.parquet.io.api.Binary)#java/nio/ByteBuffer/getInt()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/binaryToSQLTimestamp(org.apache.parquet.io.api.Binary)#org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter$$anonfun$binaryToSQLTimestamp$1/1(org.apache.parquet.io.api.Binary)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/binaryToSQLTimestamp(org.apache.parquet.io.api.Binary)#org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter$/binaryToSQLTimestamp(org.apache.parquet.io.api.Binary)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/binaryToSQLTimestamp(org.apache.parquet.io.api.Binary)#org/apache/spark/sql/catalyst/util/DateTimeUtils$/fromJulianDay(int,long)
org/apache/spark/sql/SQLImplicits/typecreator14/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ModuleSymbolApi/asModule()
org/apache/spark/sql/SQLImplicits/typecreator14/1/apply(scala.reflect.api.Mirror)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/SQLImplicits/typecreator14/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticPackage(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator14/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$InternalApi/reificationSupport()
org/apache/spark/sql/SQLImplicits/typecreator14/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/ThisType(scala.reflect.api.Symbols$SymbolApi)
org/apache/spark/sql/SQLImplicits/typecreator14/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ClassSymbolApi/asType()
org/apache/spark/sql/SQLImplicits/typecreator14/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ModuleSymbolApi/moduleClass()
org/apache/spark/sql/SQLImplicits/typecreator14/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/TypeRef(scala.reflect.api.Types$TypeApi,scala.reflect.api.Symbols$SymbolApi,scala.collection.immutable.List)
org/apache/spark/sql/SQLImplicits/typecreator14/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticClass(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator14/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/universe()
org/apache/spark/sql/SQLImplicits/typecreator14/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$TypeSymbolApi/toTypeConstructor()
org/apache/spark/sql/SQLImplicits/typecreator14/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Universe/internal()
org/apache/spark/sql/SQLImplicits/typecreator14/1/apply(scala.reflect.api.Mirror)#scala/collection/immutable/List$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/StreamMetadata/read(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/streaming/StreamMetadata/read(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#scala/util/control/NonFatal$/unapply(java.lang.Throwable)
org/apache/spark/sql/execution/streaming/StreamMetadata/read(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/commons/io/IOUtils/closeQuietly(java.io.InputStream)
org/apache/spark/sql/execution/streaming/StreamMetadata/read(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/fs/FileSystem/exists(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/streaming/StreamMetadata/read(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/json4s/jackson/Serialization$/read(java.io.Reader,org.json4s.Formats,scala.reflect.Manifest)
org/apache/spark/sql/execution/streaming/StreamMetadata/read(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/streaming/StreamMetadata$/logError(scala.Function0,java.lang.Throwable)
org/apache/spark/sql/execution/streaming/StreamMetadata/read(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#java/io/InputStreamReader/InputStreamReader(java.io.InputStream,java.nio.charset.Charset)
org/apache/spark/sql/execution/streaming/StreamMetadata/read(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamMetadata/read(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/fs/FileSystem/open(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/streaming/StreamMetadata/read(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/streaming/StreamMetadata$/format()
org/apache/spark/sql/execution/streaming/StreamMetadata/read(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#scala/Option/get()
org/apache/spark/sql/execution/streaming/StreamMetadata/read(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#scala/reflect/ManifestFactory$/classType(java.lang.Class)
org/apache/spark/sql/execution/streaming/StreamMetadata/read(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/streaming/StreamMetadata$/read(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/streaming/StreamMetadata/read(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#scala/Option/isEmpty()
org/apache/spark/sql/execution/streaming/StreamMetadata/read(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/streaming/StreamMetadata$$anonfun$read$1/1(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/streaming/StreamMetadata/write(org.apache.spark.sql.execution.streaming.StreamMetadata,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/streaming/StreamMetadata/write(org.apache.spark.sql.execution.streaming.StreamMetadata,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#scala/util/control/NonFatal$/unapply(java.lang.Throwable)
org/apache/spark/sql/execution/streaming/StreamMetadata/write(org.apache.spark.sql.execution.streaming.StreamMetadata,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/streaming/StreamMetadata$/write(org.apache.spark.sql.execution.streaming.StreamMetadata,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/streaming/StreamMetadata/write(org.apache.spark.sql.execution.streaming.StreamMetadata,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/fs/FileSystem/create(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/streaming/StreamMetadata/write(org.apache.spark.sql.execution.streaming.StreamMetadata,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#scala/Option/isEmpty()
org/apache/spark/sql/execution/streaming/StreamMetadata/write(org.apache.spark.sql.execution.streaming.StreamMetadata,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/streaming/StreamMetadata$/logError(scala.Function0,java.lang.Throwable)
org/apache/spark/sql/execution/streaming/StreamMetadata/write(org.apache.spark.sql.execution.streaming.StreamMetadata,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#java/io/OutputStreamWriter/OutputStreamWriter(java.io.OutputStream)
org/apache/spark/sql/execution/streaming/StreamMetadata/write(org.apache.spark.sql.execution.streaming.StreamMetadata,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/streaming/StreamMetadata$/format()
org/apache/spark/sql/execution/streaming/StreamMetadata/write(org.apache.spark.sql.execution.streaming.StreamMetadata,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#scala/Option/get()
org/apache/spark/sql/execution/streaming/StreamMetadata/write(org.apache.spark.sql.execution.streaming.StreamMetadata,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#java/io/OutputStreamWriter/close()
org/apache/spark/sql/execution/streaming/StreamMetadata/write(org.apache.spark.sql.execution.streaming.StreamMetadata,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/streaming/StreamMetadata$$anonfun$write$1/1(org.apache.spark.sql.execution.streaming.StreamMetadata,org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/streaming/StreamMetadata/write(org.apache.spark.sql.execution.streaming.StreamMetadata,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/json4s/jackson/Serialization$/write(java.lang.Object,java.io.Writer,org.json4s.Formats)
org/apache/spark/sql/execution/streaming/StreamMetadata/write(org.apache.spark.sql.execution.streaming.StreamMetadata,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/commons/io/IOUtils/closeQuietly(java.io.OutputStream)
org/apache/spark/sql/execution/command/AlterTableAddPartitionCommand/anonfun/8/apply(scala.Tuple2)#org/apache/spark/sql/internal/SQLConf/resolver()
org/apache/spark/sql/execution/command/AlterTableAddPartitionCommand/anonfun/8/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/catalog/CatalogTablePartition/CatalogTablePartition(scala.collection.immutable.Map,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,scala.collection.immutable.Map)
org/apache/spark/sql/execution/command/AlterTableAddPartitionCommand/anonfun/8/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/command/AlterTableAddPartitionCommand/anonfun/8/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$2()
org/apache/spark/sql/execution/command/AlterTableAddPartitionCommand/anonfun/8/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$4()
org/apache/spark/sql/execution/command/AlterTableAddPartitionCommand/anonfun/8/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$5()
org/apache/spark/sql/execution/command/AlterTableAddPartitionCommand/anonfun/8/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$6()
org/apache/spark/sql/execution/command/AlterTableAddPartitionCommand/anonfun/8/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/command/AlterTableAddPartitionCommand/anonfun/8/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/catalog/CatalogTablePartition$/apply$default$3()
org/apache/spark/sql/execution/command/AlterTableAddPartitionCommand/anonfun/8/apply(scala.Tuple2)#org/apache/spark/sql/execution/datasources/PartitioningUtils$/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)
org/apache/spark/sql/execution/command/AlterTableAddPartitionCommand/anonfun/8/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()
org/apache/spark/sql/execution/command/AlterTableAddPartitionCommand/anonfun/8/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()
org/apache/spark/sql/execution/command/AlterTableAddPartitionCommand/anonfun/8/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/catalog/CatalogTable/storage()
org/apache/spark/sql/execution/command/AlterTableAddPartitionCommand/anonfun/8/apply(scala.Tuple2)#scala/Tuple2/_2()
org/apache/spark/sql/execution/command/AlterTableAddPartitionCommand/anonfun/8/apply(scala.Tuple2)#org/apache/spark/sql/execution/command/AlterTableAddPartitionCommand$$anonfun$8$$anonfun$apply$1/1(org.apache.spark.sql.execution.command.AlterTableAddPartitionCommand$$anonfun$8)
org/apache/spark/sql/execution/command/AlterTableAddPartitionCommand/anonfun/8/apply(scala.Tuple2)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/command/AlterTableAddPartitionCommand/anonfun/8/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$3()
org/apache/spark/sql/execution/command/AlterTableAddPartitionCommand/anonfun/8/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/TableIdentifier/quotedString()
org/apache/spark/sql/execution/command/AlterTableAddPartitionCommand/anonfun/8/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy(scala.Option,scala.Option,scala.Option,scala.Option,boolean,scala.collection.immutable.Map)
org/apache/spark/sql/execution/command/ShowFunctionsCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq/sorted(scala.math.Ordering)
org/apache/spark/sql/execution/command/ShowFunctionsCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/command/ShowFunctionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/listFunctions(java.lang.String,java.lang.String)
org/apache/spark/sql/execution/command/ShowFunctionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/ShowFunctionsCommand$$anonfun$3/3(org.apache.spark.sql.execution.command.ShowFunctionsCommand)
org/apache/spark/sql/execution/command/ShowFunctionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/ShowFunctionsCommand$$anonfun$1/1(org.apache.spark.sql.execution.command.ShowFunctionsCommand)
org/apache/spark/sql/execution/command/ShowFunctionsCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/ShowFunctionsCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/command/ShowFunctionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/ShowFunctionsCommand$$anonfun$2/2(org.apache.spark.sql.execution.command.ShowFunctionsCommand,org.apache.spark.sql.SparkSession)
org/apache/spark/sql/execution/command/ShowFunctionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/ShowFunctionsCommand$$anonfun$run$1/1(org.apache.spark.sql.execution.command.ShowFunctionsCommand)
org/apache/spark/sql/execution/command/ShowFunctionsCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq/collect(scala.PartialFunction,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/6/apply()#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$populateStartOffsets$6/apply()
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/6/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/6/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/6/apply()#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/6/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/6/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/6/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/6/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/6/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/exchange/ReuseExchange/anonfun/apply/2/applyOrElse(org.apache.spark.sql.execution.SparkPlan,scala.Function1)#scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object)
org/apache/spark/sql/execution/exchange/ReuseExchange/anonfun/apply/2/applyOrElse(org.apache.spark.sql.execution.SparkPlan,scala.Function1)#org/apache/spark/sql/execution/exchange/ReuseExchange$$anonfun$apply$2$$anonfun$1/1(org.apache.spark.sql.execution.exchange.ReuseExchange$$anonfun$apply$2)
org/apache/spark/sql/execution/exchange/ReuseExchange/anonfun/apply/2/applyOrElse(org.apache.spark.sql.execution.SparkPlan,scala.Function1)#scala/collection/mutable/HashMap/getOrElseUpdate(java.lang.Object,scala.Function0)
org/apache/spark/sql/execution/exchange/ReuseExchange/anonfun/apply/2/applyOrElse(org.apache.spark.sql.execution.SparkPlan,scala.Function1)#org/apache/spark/sql/execution/exchange/ReuseExchange$$anonfun$apply$2$$anonfun$2/2(org.apache.spark.sql.execution.exchange.ReuseExchange$$anonfun$apply$2,org.apache.spark.sql.execution.exchange.Exchange)
org/apache/spark/sql/execution/exchange/ReuseExchange/anonfun/apply/2/applyOrElse(org.apache.spark.sql.execution.SparkPlan,scala.Function1)#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/execution/exchange/ReuseExchange/anonfun/apply/2/applyOrElse(org.apache.spark.sql.execution.SparkPlan,scala.Function1)#scala/collection/mutable/ArrayBuffer/find(scala.Function1)
org/apache/spark/sql/execution/exchange/ReuseExchange/anonfun/apply/2/applyOrElse(org.apache.spark.sql.execution.SparkPlan,scala.Function1)#scala/Option/get()
org/apache/spark/sql/execution/exchange/ReuseExchange/anonfun/apply/2/applyOrElse(org.apache.spark.sql.execution.SparkPlan,scala.Function1)#scala/Option/isDefined()
org/apache/spark/sql/execution/exchange/ReuseExchange/anonfun/apply/2/applyOrElse(org.apache.spark.sql.execution.SparkPlan,scala.Function1)#org/apache/spark/sql/execution/exchange/Exchange/schema()
org/apache/spark/sql/execution/command/CreateFunctionCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/empty()
org/apache/spark/sql/execution/command/CreateFunctionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/command/CreateFunctionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/registerFunction$default$3()
org/apache/spark/sql/execution/command/CreateFunctionCommand/run(org.apache.spark.sql.SparkSession)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/CreateFunctionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogFunction/CatalogFunction(org.apache.spark.sql.catalyst.FunctionIdentifier,java.lang.String,scala.collection.Seq)
org/apache/spark/sql/execution/command/CreateFunctionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/command/CreateFunctionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/command/CreateFunctionCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/command/CreateFunctionCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/get()
org/apache/spark/sql/execution/command/CreateFunctionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/loadFunctionResources(scala.collection.Seq)
org/apache/spark/sql/execution/command/CreateFunctionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/registerFunction(org.apache.spark.sql.catalyst.catalog.CatalogFunction,boolean,scala.Option)
org/apache/spark/sql/execution/command/CreateFunctionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/createFunction(org.apache.spark.sql.catalyst.catalog.CatalogFunction,boolean)
org/apache/spark/sql/execution/command/CreateFunctionCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/isDefined()
org/apache/spark/sql/execution/command/CreateFunctionCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/CreateFunctionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/FunctionIdentifier/FunctionIdentifier(java.lang.String,scala.Option)
org/apache/spark/sql/execution/command/CreateFunctionCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/command/CreateFunctionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/command/CreateFunctionCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/CreateFunctionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/command/CreateFunctionCommand/run(org.apache.spark.sql.SparkSession)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/CreateFunctionCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/streaming/StreamingQueryException/toString()#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/streaming/StreamingQueryException/toString()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/streaming/StreamingQueryException/toString()#java/lang/Throwable/getMessage()
org/apache/spark/sql/streaming/StreamingQueryException/toString()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/streaming/StreamingQueryException/toString()#java/lang/Class/getName()
org/apache/spark/sql/streaming/StreamingQueryException/toString()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/streaming/StreamingQueryException/toString()#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/streaming/StreamingQueryException/toString()#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/streaming/StreamingQueryException/toString()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#org/apache/spark/util/Utils$/getLocalDir(org.apache.spark.SparkConf)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#scala/Tuple2/_1()
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#org/apache/spark/SparkEnv$/get()
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#scala/collection/Iterator/flatMap(scala.Function1)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#java/io/File/File(java.lang.String)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#scala/collection/mutable/ArrayBuffer/ArrayBuffer()
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#scala/Predef$/$conforms()
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#org/apache/spark/sql/types/StructType$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#org/apache/spark/sql/catalyst/expressions/UnsafeProjection$/create(scala.collection.Seq,scala.collection.Seq)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#org/apache/spark/api/python/PythonRunner/PythonRunner(scala.collection.Seq,int,boolean,boolean,int[][])
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#net/razorvine/pickle/Unpickler/Unpickler()
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/python/BatchEvalPythonExec$$anonfun$doExecute$1$$anonfun$8/8(org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#org/apache/spark/TaskContext$/get()
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/python/BatchEvalPythonExec$$anonfun$doExecute$1$$anonfun$apply$6/6(org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1,net.razorvine.pickle.Unpickler)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#scala/collection/mutable/ArrayBuffer/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#scala/Tuple2/_2()
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/python/EvaluatePython$/registerPicklers()
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#org/apache/spark/api/python/PythonRunner/compute(scala.collection.Iterator,int,org.apache.spark.TaskContext)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#scala/collection/Iterator$GroupedIterator/map(scala.Function1)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/python/BatchEvalPythonExec/newMutableProjection(scala.collection.Seq,scala.collection.Seq,boolean)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#scala/collection/generic/GenericTraversableTemplate/unzip(scala.Function1)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#org/apache/spark/TaskContext/partitionId()
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#org/apache/spark/TaskContext/addTaskCompletionListener(scala.Function1)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#org/apache/spark/TaskContext/taskMemoryManager()
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#scala/collection/Seq/head()
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#org/apache/spark/sql/catalyst/expressions/GenericInternalRow/GenericInternalRow(int)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#scala/collection/mutable/ArrayBuffer$/canBuildFrom()
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#org/apache/spark/sql/catalyst/expressions/JoinedRow/JoinedRow()
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#org/apache/spark/SparkEnv/conf()
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#scala/collection/Iterator/map(scala.Function1)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#scala/collection/Iterator/grouped(int)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#net/razorvine/pickle/Pickler/Pickler(boolean)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#scala/collection/mutable/ArrayBuffer/exists(scala.Function1)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/python/BatchEvalPythonExec$$anonfun$doExecute$1$$anonfun$2/2(org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/python/BatchEvalPythonExec/newMutableProjection$default$3()
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#scala/collection/Seq/length()
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/python/BatchEvalPythonExec$$anonfun$doExecute$1$$anonfun$7/7(org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1,net.razorvine.pickle.Pickler)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/python/BatchEvalPythonExec$$anonfun$doExecute$1$$anonfun$6/6(org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1,org.apache.spark.sql.execution.python.HybridRowQueue,scala.collection.mutable.ArrayBuffer,org.apache.spark.sql.catalyst.expressions.package$MutableProjection,org.apache.spark.sql.types.StructType,boolean)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/python/BatchEvalPythonExec$$anonfun$doExecute$1$$anonfun$3/3(org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1,scala.collection.mutable.ArrayBuffer,scala.collection.mutable.ArrayBuffer)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/python/BatchEvalPythonExec$$anonfun$doExecute$1$$anonfun$apply$7/7(org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1,org.apache.spark.sql.execution.python.HybridRowQueue,org.apache.spark.sql.catalyst.expressions.GenericInternalRow,org.apache.spark.sql.catalyst.expressions.JoinedRow,org.apache.spark.sql.types.DataType,org.apache.spark.sql.catalyst.expressions.UnsafeProjection)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/python/BatchEvalPythonExec$$anonfun$doExecute$1$$anonfun$4/4(org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/python/BatchEvalPythonExec$$anonfun$doExecute$1$$anonfun$apply$2/2(org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1,org.apache.spark.sql.execution.python.HybridRowQueue)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/python/BatchEvalPythonExec$$anonfun$doExecute$1$$anonfun$5/5(org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1)
org/apache/spark/sql/execution/python/BatchEvalPythonExec/anonfun/doExecute/1/apply(scala.collection.Iterator)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/columnar/ARRAY/extract(java.nio.ByteBuffer)#java/nio/ByteBuffer/position()
org/apache/spark/sql/execution/columnar/ARRAY/extract(java.nio.ByteBuffer)#scala/Predef$/assert(boolean)
org/apache/spark/sql/execution/columnar/ARRAY/extract(java.nio.ByteBuffer)#java/nio/ByteBuffer/arrayOffset()
org/apache/spark/sql/execution/columnar/ARRAY/extract(java.nio.ByteBuffer)#org/apache/spark/sql/catalyst/expressions/UnsafeArrayData/UnsafeArrayData()
org/apache/spark/sql/execution/columnar/ARRAY/extract(java.nio.ByteBuffer)#java/nio/ByteBuffer/getInt()
org/apache/spark/sql/execution/columnar/ARRAY/extract(java.nio.ByteBuffer)#java/nio/ByteBuffer/position(int)
org/apache/spark/sql/execution/columnar/ARRAY/extract(java.nio.ByteBuffer)#java/nio/ByteBuffer/array()
org/apache/spark/sql/execution/columnar/ARRAY/extract(java.nio.ByteBuffer)#java/nio/ByteBuffer/hasArray()
org/apache/spark/sql/execution/columnar/ARRAY/extract(java.nio.ByteBuffer)#org/apache/spark/sql/catalyst/expressions/UnsafeArrayData/pointTo(java.lang.Object,long,int)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitBucketSpec/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$BucketSpecContext/orderedIdentifierList()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitBucketSpec/1/apply()#org/antlr/v4/runtime/tree/TerminalNode/getText()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitBucketSpec/1/apply()#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitBucketSpec/1/apply()#scala/Option$/option2Iterable(scala.Option)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitBucketSpec/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder/visitIdentifierList(org.apache.spark.sql.catalyst.parser.SqlBaseParser$IdentifierListContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitBucketSpec/1/apply()#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitBucketSpec/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$BucketSpecContext/identifierList()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitBucketSpec/1/apply()#scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitBucketSpec/1/apply()#org/apache/spark/sql/catalyst/catalog/BucketSpec/BucketSpec(int,scala.collection.Seq,scala.collection.Seq)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitBucketSpec/1/apply()#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitBucketSpec/1/apply()#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitBucketSpec/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitBucketSpec$1/apply()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitBucketSpec/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$BucketSpecContext/INTEGER_VALUE()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitBucketSpec/1/apply()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitBucketSpec/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitBucketSpec$1$$anonfun$apply$26/26(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitBucketSpec$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitBucketSpec/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitBucketSpec$1$$anonfun$apply$27/27(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitBucketSpec$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitBucketSpec/1/apply()#scala/collection/Iterable/toSeq()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitBucketSpec/1/apply()#scala/collection/immutable/StringOps/toInt()
org/apache/spark/sql/execution/streaming/FileStreamSink/hasMetadata(scala.collection.Seq,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/streaming/FileStreamSink/hasMetadata(scala.collection.Seq,org.apache.hadoop.conf.Configuration)#scala/util/control/NonFatal$/unapply(java.lang.Throwable)
org/apache/spark/sql/execution/streaming/FileStreamSink/hasMetadata(scala.collection.Seq,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/fs/Path/Path(org.apache.hadoop.fs.Path,java.lang.String)
org/apache/spark/sql/execution/streaming/FileStreamSink/hasMetadata(scala.collection.Seq,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/fs/FileSystem/exists(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/streaming/FileStreamSink/hasMetadata(scala.collection.Seq,org.apache.hadoop.conf.Configuration)#scala/Some/isEmpty()
org/apache/spark/sql/execution/streaming/FileStreamSink/hasMetadata(scala.collection.Seq,org.apache.hadoop.conf.Configuration)#scala/collection/Seq$/unapplySeq(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/FileStreamSink/hasMetadata(scala.collection.Seq,org.apache.hadoop.conf.Configuration)#scala/collection/SeqLike/apply(int)
org/apache/spark/sql/execution/streaming/FileStreamSink/hasMetadata(scala.collection.Seq,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/streaming/FileStreamSink$/metadataDir()
org/apache/spark/sql/execution/streaming/FileStreamSink/hasMetadata(scala.collection.Seq,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/streaming/FileStreamSink$/hasMetadata(scala.collection.Seq,org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/streaming/FileStreamSink/hasMetadata(scala.collection.Seq,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/streaming/FileStreamSink$/logWarning(scala.Function0)
org/apache/spark/sql/execution/streaming/FileStreamSink/hasMetadata(scala.collection.Seq,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/fs/Path/Path(java.lang.String)
org/apache/spark/sql/execution/streaming/FileStreamSink/hasMetadata(scala.collection.Seq,org.apache.hadoop.conf.Configuration)#scala/Some/get()
org/apache/spark/sql/execution/streaming/FileStreamSink/hasMetadata(scala.collection.Seq,org.apache.hadoop.conf.Configuration)#scala/collection/SeqLike/lengthCompare(int)
org/apache/spark/sql/execution/streaming/FileStreamSink/hasMetadata(scala.collection.Seq,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/streaming/FileStreamSink$$anonfun$hasMetadata$1/1()
org/apache/spark/sql/execution/streaming/FileStreamSink/hasMetadata(scala.collection.Seq,org.apache.hadoop.conf.Configuration)#scala/Option/isEmpty()
org/apache/spark/sql/execution/streaming/FileStreamSink/ancestorIsMetadataDirectory(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/fs/Path/getParent()
org/apache/spark/sql/execution/streaming/FileStreamSink/ancestorIsMetadataDirectory(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/fs/Path/getName()
org/apache/spark/sql/execution/streaming/FileStreamSink/ancestorIsMetadataDirectory(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/streaming/FileStreamSink/ancestorIsMetadataDirectory(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/streaming/FileStreamSink/ancestorIsMetadataDirectory(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/fs/FileSystem/getUri()
org/apache/spark/sql/execution/streaming/FileStreamSink/ancestorIsMetadataDirectory(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/streaming/FileStreamSink$/metadataDir()
org/apache/spark/sql/execution/streaming/FileStreamSink/ancestorIsMetadataDirectory(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/streaming/FileStreamSink$/ancestorIsMetadataDirectory(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/streaming/FileStreamSink/ancestorIsMetadataDirectory(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/fs/Path/makeQualified(java.net.URI,org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/streaming/FileStreamSink/ancestorIsMetadataDirectory(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/fs/FileSystem/getWorkingDirectory()
org/apache/spark/sql/execution/datasources/json/JsonDataSource/checkConstraints(org.apache.spark.sql.types.StructType)#scala/collection/mutable/ArrayOps/groupBy(scala.Function1)
org/apache/spark/sql/execution/datasources/json/JsonDataSource/checkConstraints(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/types/StructType/fieldNames()
org/apache/spark/sql/execution/datasources/json/JsonDataSource/checkConstraints(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/json/JsonDataSource/checkConstraints(org.apache.spark.sql.types.StructType)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/json/JsonDataSource/checkConstraints(org.apache.spark.sql.types.StructType)#scala/collection/TraversableOnce/mkString(java.lang.String)
org/apache/spark/sql/execution/datasources/json/JsonDataSource/checkConstraints(org.apache.spark.sql.types.StructType)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/json/JsonDataSource/checkConstraints(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/json/JsonDataSource/checkConstraints(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/json/JsonDataSource/checkConstraints(org.apache.spark.sql.types.StructType)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/json/JsonDataSource/checkConstraints(org.apache.spark.sql.types.StructType)#scala/collection/mutable/ArrayOps/distinct()
org/apache/spark/sql/execution/datasources/json/JsonDataSource/checkConstraints(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/json/JsonDataSource$$anonfun$1/1(org.apache.spark.sql.execution.datasources.json.JsonDataSource)
org/apache/spark/sql/execution/datasources/json/JsonDataSource/checkConstraints(org.apache.spark.sql.types.StructType)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/json/JsonDataSource/checkConstraints(org.apache.spark.sql.types.StructType)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/json/JsonDataSource/checkConstraints(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/json/JsonDataSource/checkConstraints(org.apache.spark.sql.types.StructType)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/json/JsonDataSource/checkConstraints(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/json/JsonDataSource/checkConstraints(org.apache.spark.sql.types.StructType)#scala/collection/immutable/Iterable$/canBuildFrom()
org/apache/spark/sql/execution/datasources/json/JsonDataSource/checkConstraints(org.apache.spark.sql.types.StructType)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/json/JsonDataSource/checkConstraints(org.apache.spark.sql.types.StructType)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/json/JsonDataSource/checkConstraints(org.apache.spark.sql.types.StructType)#scala/collection/immutable/Map/collect(scala.PartialFunction,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/json/JsonDataSource/checkConstraints(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/datasources/json/JsonDataSource$$anonfun$2/2(org.apache.spark.sql.execution.datasources.json.JsonDataSource)
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/run(org.apache.spark.sql.SparkSession)#scala/util/control/NonFatal$/unapply(java.lang.Throwable)
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/empty()
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/types/StructType/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionSchema()
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/run(org.apache.spark.sql.SparkSession)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/schema()
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/TableIdentifier/quotedString()
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/get()
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/run(org.apache.spark.sql.SparkSession)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/alterTableSchema(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/log()
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/types/StructType/copy(org.apache.spark.sql.types.StructField[])
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/run(org.apache.spark.sql.SparkSession)#org/slf4j/Logger/warn(java.lang.String,java.lang.Throwable)
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/refreshTable(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/isEmpty()
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/dataSchema()
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/run(org.apache.spark.sql.SparkSession)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/verifyAlterTableAddColumn(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.TableIdentifier)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/verifyAlterTableAddColumn(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.TableIdentifier)#java/lang/Class/newInstance()
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/verifyAlterTableAddColumn(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/execution/datasources/DataSource$/lookupDataSource(java.lang.String)
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/verifyAlterTableAddColumn(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/verifyAlterTableAddColumn(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/execution/command/DDLUtils$/isDatasourceTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/verifyAlterTableAddColumn(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.TableIdentifier)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/verifyAlterTableAddColumn(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/verifyAlterTableAddColumn(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/verifyAlterTableAddColumn(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/verifyAlterTableAddColumn(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.TableIdentifier)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/verifyAlterTableAddColumn(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/catalog/CatalogTableType$/VIEW()
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/verifyAlterTableAddColumn(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.TableIdentifier)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/verifyAlterTableAddColumn(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.TableIdentifier)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/verifyAlterTableAddColumn(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.TableIdentifier)#scala/Option/get()
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/verifyAlterTableAddColumn(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/catalog/CatalogTable/provider()
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/verifyAlterTableAddColumn(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.TableIdentifier)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/verifyAlterTableAddColumn(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTempViewOrPermanentTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/verifyAlterTableAddColumn(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.TableIdentifier)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/verifyAlterTableAddColumn(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/catalog/CatalogTable/tableType()
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/verifyAlterTableAddColumn(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/command/AlterTableAddColumnsCommand/verifyAlterTableAddColumn(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.TableIdentifier)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/TextSocketSource/anonfun/5/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/TextSocketSource/anonfun/5/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/TextSocketSource/anonfun/5/apply()#scala/sys/package$/error(java.lang.String)
org/apache/spark/sql/execution/streaming/TextSocketSource/anonfun/5/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/streaming/TextSocketSource/anonfun/5/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/streaming/TextSocketSource/anonfun/5/apply()#org/apache/spark/sql/execution/streaming/TextSocketSource$$anonfun$5/apply()
org/apache/spark/sql/execution/streaming/TextSocketSource/anonfun/5/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/TextSocketSource/anonfun/5/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/TextSocketSource/anonfun/5/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/streaming/state/StateStore/org$apache$spark$sql$execution$streaming$state$StateStore$$verifyIfStoreInstanceActive(org.apache.spark.sql.execution.streaming.state.StateStoreId)#org/apache/spark/storage/BlockManagerId/executorId()
org/apache/spark/sql/execution/streaming/state/StateStore/org$apache$spark$sql$execution$streaming$state$StateStore$$verifyIfStoreInstanceActive(org.apache.spark.sql.execution.streaming.state.StateStoreId)#org/apache/spark/sql/execution/streaming/state/StateStore$$anonfun$org$apache$spark$sql$execution$streaming$state$StateStore$$verifyIfStoreInstanceActive$1/1(org.apache.spark.sql.execution.streaming.state.StateStoreId,boolean)
org/apache/spark/sql/execution/streaming/state/StateStore/org$apache$spark$sql$execution$streaming$state$StateStore$$verifyIfStoreInstanceActive(org.apache.spark.sql.execution.streaming.state.StateStoreId)#org/apache/spark/sql/execution/streaming/state/StateStore$$anonfun$3/3(org.apache.spark.sql.execution.streaming.state.StateStoreId,java.lang.String)
org/apache/spark/sql/execution/streaming/state/StateStore/org$apache$spark$sql$execution$streaming$state$StateStore$$verifyIfStoreInstanceActive(org.apache.spark.sql.execution.streaming.state.StateStoreId)#org/apache/spark/SparkEnv$/get()
org/apache/spark/sql/execution/streaming/state/StateStore/org$apache$spark$sql$execution$streaming$state$StateStore$$verifyIfStoreInstanceActive(org.apache.spark.sql.execution.streaming.state.StateStoreId)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/streaming/state/StateStore/org$apache$spark$sql$execution$streaming$state$StateStore$$verifyIfStoreInstanceActive(org.apache.spark.sql.execution.streaming.state.StateStoreId)#org/apache/spark/SparkEnv/blockManager()
org/apache/spark/sql/execution/streaming/state/StateStore/org$apache$spark$sql$execution$streaming$state$StateStore$$verifyIfStoreInstanceActive(org.apache.spark.sql.execution.streaming.state.StateStoreId)#scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/StateStore/org$apache$spark$sql$execution$streaming$state$StateStore$$verifyIfStoreInstanceActive(org.apache.spark.sql.execution.streaming.state.StateStoreId)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/streaming/state/StateStore/org$apache$spark$sql$execution$streaming$state$StateStore$$verifyIfStoreInstanceActive(org.apache.spark.sql.execution.streaming.state.StateStoreId)#org/apache/spark/sql/execution/streaming/state/StateStore$/logDebug(scala.Function0)
org/apache/spark/sql/execution/streaming/state/StateStore/org$apache$spark$sql$execution$streaming$state$StateStore$$verifyIfStoreInstanceActive(org.apache.spark.sql.execution.streaming.state.StateStoreId)#org/apache/spark/sql/execution/streaming/state/StateStore$/coordinatorRef()
org/apache/spark/sql/execution/streaming/state/StateStore/org$apache$spark$sql$execution$streaming$state$StateStore$$verifyIfStoreInstanceActive(org.apache.spark.sql.execution.streaming.state.StateStoreId)#org/apache/spark/storage/BlockManager/blockManagerId()
org/apache/spark/sql/execution/streaming/state/StateStore/org$apache$spark$sql$execution$streaming$state$StateStore$$verifyIfStoreInstanceActive(org.apache.spark.sql.execution.streaming.state.StateStoreId)#org/apache/spark/sql/execution/streaming/state/StateStore$$anonfun$1/1()
org/apache/spark/sql/execution/streaming/state/StateStore/reportActiveStoreInstance(org.apache.spark.sql.execution.streaming.state.StateStoreId)#org/apache/spark/storage/BlockManagerId/executorId()
org/apache/spark/sql/execution/streaming/state/StateStore/reportActiveStoreInstance(org.apache.spark.sql.execution.streaming.state.StateStoreId)#org/apache/spark/sql/execution/streaming/state/StateStore$$anonfun$reportActiveStoreInstance$2/2(org.apache.spark.sql.execution.streaming.state.StateStoreId)
org/apache/spark/sql/execution/streaming/state/StateStore/reportActiveStoreInstance(org.apache.spark.sql.execution.streaming.state.StateStoreId)#scala/Option/foreach(scala.Function1)
org/apache/spark/sql/execution/streaming/state/StateStore/reportActiveStoreInstance(org.apache.spark.sql.execution.streaming.state.StateStoreId)#org/apache/spark/sql/execution/streaming/state/StateStore$$anonfun$reportActiveStoreInstance$1/1(org.apache.spark.sql.execution.streaming.state.StateStoreId,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/streaming/state/StateStore/reportActiveStoreInstance(org.apache.spark.sql.execution.streaming.state.StateStoreId)#org/apache/spark/storage/BlockManager/blockManagerId()
org/apache/spark/sql/execution/streaming/state/StateStore/reportActiveStoreInstance(org.apache.spark.sql.execution.streaming.state.StateStoreId)#org/apache/spark/SparkEnv$/get()
org/apache/spark/sql/execution/streaming/state/StateStore/reportActiveStoreInstance(org.apache.spark.sql.execution.streaming.state.StateStoreId)#org/apache/spark/SparkEnv/blockManager()
org/apache/spark/sql/execution/streaming/state/StateStore/reportActiveStoreInstance(org.apache.spark.sql.execution.streaming.state.StateStoreId)#org/apache/spark/storage/BlockManagerId/host()
org/apache/spark/sql/execution/streaming/state/StateStore/reportActiveStoreInstance(org.apache.spark.sql.execution.streaming.state.StateStoreId)#org/apache/spark/sql/execution/streaming/state/StateStore$/logDebug(scala.Function0)
org/apache/spark/sql/execution/streaming/state/StateStore/reportActiveStoreInstance(org.apache.spark.sql.execution.streaming.state.StateStoreId)#org/apache/spark/sql/execution/streaming/state/StateStore$/coordinatorRef()
org/apache/spark/sql/execution/streaming/state/StateStore/startMaintenanceIfNeeded()#org/apache/spark/sql/execution/streaming/state/StateStore$$anonfun$startMaintenanceIfNeeded$1/1()
org/apache/spark/sql/execution/streaming/state/StateStore/startMaintenanceIfNeeded()#org/apache/spark/sql/execution/streaming/state/StateStore$/MAINTENANCE_INTERVAL_DEFAULT_SECS()
org/apache/spark/sql/execution/streaming/state/StateStore/startMaintenanceIfNeeded()#org/apache/spark/sql/execution/streaming/state/StateStore$$anonfun$startMaintenanceIfNeeded$2/2()
org/apache/spark/sql/execution/streaming/state/StateStore/startMaintenanceIfNeeded()#org/apache/spark/sql/execution/streaming/state/StateStore$/MAINTENANCE_INTERVAL_CONFIG()
org/apache/spark/sql/execution/streaming/state/StateStore/startMaintenanceIfNeeded()#org/apache/spark/sql/execution/streaming/state/StateStore$$anonfun$startMaintenanceIfNeeded$3/3()
org/apache/spark/sql/execution/streaming/state/StateStore/startMaintenanceIfNeeded()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/state/StateStore/startMaintenanceIfNeeded()#org/apache/spark/SparkEnv$/get()
org/apache/spark/sql/execution/streaming/state/StateStore/startMaintenanceIfNeeded()#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/streaming/state/StateStore/startMaintenanceIfNeeded()#org/apache/spark/sql/execution/streaming/state/StateStore$MaintenanceTask/MaintenanceTask(long,scala.Function0,scala.Function0)
org/apache/spark/sql/execution/streaming/state/StateStore/startMaintenanceIfNeeded()#org/apache/spark/sql/execution/streaming/state/StateStore$/isMaintenanceRunning()
org/apache/spark/sql/execution/streaming/state/StateStore/startMaintenanceIfNeeded()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/state/StateStore/startMaintenanceIfNeeded()#org/apache/spark/sql/execution/streaming/state/StateStore$/maintenanceTask_$eq(org.apache.spark.sql.execution.streaming.state.StateStore$MaintenanceTask)
org/apache/spark/sql/execution/streaming/state/StateStore/startMaintenanceIfNeeded()#org/apache/spark/sql/execution/streaming/state/StateStore$/logInfo(scala.Function0)
org/apache/spark/sql/execution/streaming/state/StateStore/startMaintenanceIfNeeded()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/state/StateStore/startMaintenanceIfNeeded()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/StateStore/startMaintenanceIfNeeded()#org/apache/spark/SparkEnv/conf()
org/apache/spark/sql/execution/streaming/state/StateStore/startMaintenanceIfNeeded()#org/apache/spark/sql/execution/streaming/state/StateStore$/org$apache$spark$sql$execution$streaming$state$StateStore$$loadedProviders()
org/apache/spark/sql/execution/streaming/state/StateStore/startMaintenanceIfNeeded()#org/apache/spark/SparkConf/getTimeAsMs(java.lang.String,java.lang.String)
org/apache/spark/sql/RelationalGroupedDataset/anonfun/3/apply(java.lang.String)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/RelationalGroupedDataset/anonfun/3/apply(java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/RelationalGroupedDataset/anonfun/3/apply(java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/RelationalGroupedDataset/anonfun/3/apply(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/RelationalGroupedDataset/anonfun/3/apply(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/RelationalGroupedDataset/anonfun/3/apply(java.lang.String)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/RelationalGroupedDataset/anonfun/3/apply(java.lang.String)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/RelationalGroupedDataset/anonfun/3/apply(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/RelationalGroupedDataset/anonfun/3/apply(java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/RelationalGroupedDataset/anonfun/3/apply(java.lang.String)#org/apache/spark/sql/catalyst/expressions/Expression/dataType()
org/apache/spark/sql/RelationalGroupedDataset/anonfun/3/apply(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/RelationalGroupedDataset/anonfun/3/apply(java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/RelationalGroupedDataset/anonfun/3/apply(java.lang.String)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/SQLImplicits/typecreator15/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ModuleSymbolApi/asModule()
org/apache/spark/sql/SQLImplicits/typecreator15/1/apply(scala.reflect.api.Mirror)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/SQLImplicits/typecreator15/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticPackage(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator15/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$InternalApi/reificationSupport()
org/apache/spark/sql/SQLImplicits/typecreator15/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/ThisType(scala.reflect.api.Symbols$SymbolApi)
org/apache/spark/sql/SQLImplicits/typecreator15/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ClassSymbolApi/asType()
org/apache/spark/sql/SQLImplicits/typecreator15/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ModuleSymbolApi/moduleClass()
org/apache/spark/sql/SQLImplicits/typecreator15/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/TypeRef(scala.reflect.api.Types$TypeApi,scala.reflect.api.Symbols$SymbolApi,scala.collection.immutable.List)
org/apache/spark/sql/SQLImplicits/typecreator15/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticClass(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator15/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/universe()
org/apache/spark/sql/SQLImplicits/typecreator15/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$TypeSymbolApi/toTypeConstructor()
org/apache/spark/sql/SQLImplicits/typecreator15/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Universe/internal()
org/apache/spark/sql/SQLImplicits/typecreator15/1/apply(scala.reflect.api.Mirror)#scala/collection/immutable/List$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/MapGroupsExec/anonfun/11/anonfun/apply/4/apply(scala.Tuple2)#org/apache/spark/sql/execution/MapGroupsExec$$anonfun$11/org$apache$spark$sql$execution$MapGroupsExec$$anonfun$$$outer()
org/apache/spark/sql/execution/MapGroupsExec/anonfun/11/anonfun/apply/4/apply(scala.Tuple2)#scala/collection/TraversableOnce$/MonadOps(scala.collection.TraversableOnce)
org/apache/spark/sql/execution/MapGroupsExec/anonfun/11/anonfun/apply/4/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/MapGroupsExec/anonfun/11/anonfun/apply/4/apply(scala.Tuple2)#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/execution/MapGroupsExec/anonfun/11/anonfun/apply/4/apply(scala.Tuple2)#scala/collection/TraversableOnce$MonadOps/map(scala.Function1)
org/apache/spark/sql/execution/MapGroupsExec/anonfun/11/anonfun/apply/4/apply(scala.Tuple2)#scala/Tuple2/_2()
org/apache/spark/sql/execution/MapGroupsExec/anonfun/11/anonfun/apply/4/apply(scala.Tuple2)#scala/Function2/apply(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/MapGroupsExec/anonfun/11/anonfun/apply/4/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/MapGroupsExec/anonfun/11/anonfun/apply/4/apply(scala.Tuple2)#scala/collection/Iterator/map(scala.Function1)
org/apache/spark/sql/execution/SparkPlanInfo/fromSparkPlan(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/SparkPlan/simpleString()
org/apache/spark/sql/execution/SparkPlanInfo/fromSparkPlan(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/SparkPlanInfo$$anonfun$fromSparkPlan$1/1()
org/apache/spark/sql/execution/SparkPlanInfo/fromSparkPlan(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/SparkPlan/subqueries()
org/apache/spark/sql/execution/SparkPlanInfo/fromSparkPlan(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/SparkPlanInfo$/fromSparkPlan(org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/SparkPlanInfo/fromSparkPlan(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/SparkPlan/nodeName()
org/apache/spark/sql/execution/SparkPlanInfo/fromSparkPlan(org.apache.spark.sql.execution.SparkPlan)#scala/collection/immutable/Map/toSeq()
org/apache/spark/sql/execution/SparkPlanInfo/fromSparkPlan(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/SparkPlanInfo/fromSparkPlan(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/SparkPlanInfo/fromSparkPlan(org.apache.spark.sql.execution.SparkPlan)#scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)
org/apache/spark/sql/execution/SparkPlanInfo/fromSparkPlan(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/SparkPlan/children()
org/apache/spark/sql/execution/SparkPlanInfo/fromSparkPlan(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/SparkPlanInfo/fromSparkPlan(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/SparkPlanInfo$$anonfun$1/1()
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#org/apache/spark/sql/execution/metric/SQLMetrics$/TIMING_METRIC()
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/collection/Seq/isEmpty()
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/collection/Seq/apply(int)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/Tuple4/_1()
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/Tuple4/_3()
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/collection/Seq$/unapplySeq(scala.collection.Seq)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#org/apache/spark/sql/execution/metric/SQLMetrics$$anonfun$2/2()
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/collection/Seq$/fill(int,scala.Function0)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#org/apache/spark/sql/execution/metric/SQLMetrics$$anonfun$4/4()
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/collection/Seq/length()
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/Predef$/wrapLongArray(long[])
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#org/apache/spark/sql/execution/metric/SQLMetrics$/SUM_METRIC()
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#java/lang/IllegalStateException/IllegalStateException(java.lang.String)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/collection/Seq/sorted(scala.math.Ordering)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/collection/Seq/filter(scala.Function1)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/Tuple4/_2()
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#org/apache/spark/sql/execution/metric/SQLMetrics$/stringValue(java.lang.String,scala.collection.Seq)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/Some/isEmpty()
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/Tuple4/_4()
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#org/apache/spark/sql/execution/metric/SQLMetrics$/SIZE_METRIC()
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/collection/SeqLike/apply(int)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#java/text/NumberFormat/format(long)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#org/apache/spark/sql/execution/metric/SQLMetrics$$anonfun$1/1()
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#org/apache/spark/sql/execution/metric/SQLMetrics$$anonfun$3/3()
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#java/text/NumberFormat/getIntegerInstance(java.util.Locale)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/Some/get()
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/collection/Seq/sum(scala.math.Numeric)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/Tuple4/Tuple4(java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/collection/SeqLike/lengthCompare(int)
org/apache/spark/sql/execution/metric/SQLMetrics/postDriverMetricUpdates(org.apache.spark.SparkContext,java.lang.String,scala.collection.Seq)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/metric/SQLMetrics/postDriverMetricUpdates(org.apache.spark.SparkContext,java.lang.String,scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/metric/SQLMetrics/postDriverMetricUpdates(org.apache.spark.SparkContext,java.lang.String,scala.collection.Seq)#org/apache/spark/sql/execution/metric/SQLMetrics$/postDriverMetricUpdates(org.apache.spark.SparkContext,java.lang.String,scala.collection.Seq)
org/apache/spark/sql/execution/metric/SQLMetrics/postDriverMetricUpdates(org.apache.spark.SparkContext,java.lang.String,scala.collection.Seq)#org/apache/spark/SparkContext/listenerBus()
org/apache/spark/sql/execution/metric/SQLMetrics/postDriverMetricUpdates(org.apache.spark.SparkContext,java.lang.String,scala.collection.Seq)#scala/collection/immutable/StringOps/toLong()
org/apache/spark/sql/execution/metric/SQLMetrics/postDriverMetricUpdates(org.apache.spark.SparkContext,java.lang.String,scala.collection.Seq)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/metric/SQLMetrics/postDriverMetricUpdates(org.apache.spark.SparkContext,java.lang.String,scala.collection.Seq)#org/apache/spark/sql/execution/metric/SQLMetrics$$anonfun$postDriverMetricUpdates$1/1()
org/apache/spark/sql/execution/metric/SQLMetrics/postDriverMetricUpdates(org.apache.spark.SparkContext,java.lang.String,scala.collection.Seq)#org/apache/spark/scheduler/LiveListenerBus/post(org.apache.spark.scheduler.SparkListenerEvent)
org/apache/spark/sql/execution/metric/SQLMetrics/postDriverMetricUpdates(org.apache.spark.SparkContext,java.lang.String,scala.collection.Seq)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/2/anonfun/1/anonfun/apply/mcI/sp/1/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/2/anonfun/1/anonfun/apply/mcI/sp/1/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/2/anonfun/1/anonfun/apply/mcI/sp/1/apply()#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/2/anonfun/1/anonfun/apply/mcI/sp/1/apply()#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$populateStartOffsets$2$$anonfun$1$$anonfun$apply$mcI$sp$1/apply()
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/2/anonfun/1/anonfun/apply/mcI/sp/1/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/2/anonfun/1/anonfun/apply/mcI/sp/1/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/2/anonfun/1/anonfun/apply/mcI/sp/1/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/2/anonfun/1/anonfun/apply/mcI/sp/1/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/populateStartOffsets/2/anonfun/1/anonfun/apply/mcI/sp/1/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitChangeColumn/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$ChangeColumnContext/tableIdentifier()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitChangeColumn/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$ChangeColumnContext/colType()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitChangeColumn/1/apply()#org/apache/spark/sql/catalyst/parser/ParserUtils$/operationNotAllowed(java.lang.String,org.antlr.v4.runtime.ParserRuleContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitChangeColumn/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$IdentifierContext/getText()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitChangeColumn/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$ChangeColumnContext/partitionSpec()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitChangeColumn/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$ChangeColumnContext/colPosition()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitChangeColumn/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$ChangeColumnContext/identifier()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitChangeColumn/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder/visitTableIdentifier(org.apache.spark.sql.catalyst.parser.SqlBaseParser$TableIdentifierContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitChangeColumn/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitChangeColumn$1/apply()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitChangeColumn/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder/visitColType(org.apache.spark.sql.catalyst.parser.SqlBaseParser$ColTypeContext)
org/apache/spark/sql/execution/command/AlterTableSetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/alterTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)
org/apache/spark/sql/execution/command/AlterTableSetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/empty()
org/apache/spark/sql/execution/command/AlterTableSetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$1()
org/apache/spark/sql/execution/command/AlterTableSetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$2()
org/apache/spark/sql/execution/command/AlterTableSetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$4()
org/apache/spark/sql/execution/command/AlterTableSetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$17()
org/apache/spark/sql/execution/command/AlterTableSetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/immutable/Map/$plus$plus(scala.collection.GenTraversableOnce)
org/apache/spark/sql/execution/command/AlterTableSetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/properties()
org/apache/spark/sql/execution/command/AlterTableSetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/AlterTableSetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/DDLUtils$/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)
org/apache/spark/sql/execution/command/AlterTableSetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$3()
org/apache/spark/sql/execution/command/AlterTableSetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$10()
org/apache/spark/sql/execution/command/AlterTableSetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$5()
org/apache/spark/sql/execution/command/AlterTableSetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$12()
org/apache/spark/sql/execution/command/AlterTableSetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)
org/apache/spark/sql/execution/command/AlterTableSetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$6()
org/apache/spark/sql/execution/command/AlterTableSetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$13()
org/apache/spark/sql/execution/command/AlterTableSetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()
org/apache/spark/sql/execution/command/AlterTableSetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$7()
org/apache/spark/sql/execution/command/AlterTableSetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$15()
org/apache/spark/sql/execution/command/AlterTableSetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$8()
org/apache/spark/sql/execution/command/AlterTableSetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$16()
org/apache/spark/sql/execution/command/AlterTableSetPropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$9()
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/doExecute()#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/doExecute()#org/apache/spark/sql/execution/streaming/state/package$StateStoreOps/mapPartitionsWithStateStore(java.lang.String,long,long,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.internal.SessionState,scala.Option,scala.Function2,scala.reflect.ClassTag)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/doExecute()#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/doExecute()#scala/Option/nonEmpty()
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/doExecute()#org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec$$anonfun$doExecute$1/1(org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExec)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/doExecute()#scala/Predef$/require(boolean)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/doExecute()#org/apache/spark/sql/catalyst/expressions/package$AttributeSeq/toStructType()
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/doExecute()#org/apache/spark/sql/execution/streaming/state/package$/StateStoreOps(org.apache.spark.rdd.RDD,scala.reflect.ClassTag)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/doExecute()#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/doExecute()#org/apache/spark/sql/catalyst/expressions/package$/AttributeSeq(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/doExecute()#org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/sqlContext()
org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap/printPerfMetrics()#java/lang/IllegalStateException/IllegalStateException(java.lang.String)
org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap/printPerfMetrics()#org/apache/spark/unsafe/map/BytesToBytesMap/getNumHashCollisions()
org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap/printPerfMetrics()#org/apache/spark/unsafe/map/BytesToBytesMap/getTimeSpentResizingNs()
org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap/printPerfMetrics()#java/lang/StringBuilder/append(long)
org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap/printPerfMetrics()#java/lang/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap/printPerfMetrics()#org/apache/spark/unsafe/map/BytesToBytesMap/getAverageProbesPerLookup()
org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap/printPerfMetrics()#java/lang/StringBuilder/toString()
org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap/printPerfMetrics()#java/lang/StringBuilder/append(java.lang.String)
org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap/printPerfMetrics()#java/io/PrintStream/println(java.lang.String)
org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap/printPerfMetrics()#org/apache/spark/unsafe/map/BytesToBytesMap/getTotalMemoryConsumption()
org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap/printPerfMetrics()#java/lang/StringBuilder/append(double)
org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap/getAggregationBufferFromUnsafeRow(org.apache.spark.sql.catalyst.expressions.UnsafeRow,int)#org/apache/spark/unsafe/map/BytesToBytesMap$Location/isDefined()
org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap/getAggregationBufferFromUnsafeRow(org.apache.spark.sql.catalyst.expressions.UnsafeRow,int)#org/apache/spark/unsafe/map/BytesToBytesMap$Location/getValueBase()
org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap/getAggregationBufferFromUnsafeRow(org.apache.spark.sql.catalyst.expressions.UnsafeRow,int)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/getSizeInBytes()
org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap/getAggregationBufferFromUnsafeRow(org.apache.spark.sql.catalyst.expressions.UnsafeRow,int)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/getBaseObject()
org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap/getAggregationBufferFromUnsafeRow(org.apache.spark.sql.catalyst.expressions.UnsafeRow,int)#org/apache/spark/unsafe/map/BytesToBytesMap/lookup(java.lang.Object,long,int,int)
org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap/getAggregationBufferFromUnsafeRow(org.apache.spark.sql.catalyst.expressions.UnsafeRow,int)#org/apache/spark/unsafe/map/BytesToBytesMap$Location/append(java.lang.Object,long,int,java.lang.Object,long,int)
org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap/getAggregationBufferFromUnsafeRow(org.apache.spark.sql.catalyst.expressions.UnsafeRow,int)#org/apache/spark/unsafe/map/BytesToBytesMap$Location/getValueLength()
org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap/getAggregationBufferFromUnsafeRow(org.apache.spark.sql.catalyst.expressions.UnsafeRow,int)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/getBaseOffset()
org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap/getAggregationBufferFromUnsafeRow(org.apache.spark.sql.catalyst.expressions.UnsafeRow,int)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/pointTo(java.lang.Object,long,int)
org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap/getAggregationBufferFromUnsafeRow(org.apache.spark.sql.catalyst.expressions.UnsafeRow,int)#org/apache/spark/unsafe/map/BytesToBytesMap$Location/getValueOffset()
org/apache/spark/sql/streaming/StreamingQueryManager/anonfun/createQuery/1/apply()#org/apache/spark/sql/internal/SQLConf$/ADAPTIVE_EXECUTION_ENABLED()
org/apache/spark/sql/streaming/StreamingQueryManager/anonfun/createQuery/1/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/streaming/StreamingQueryManager/anonfun/createQuery/1/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/streaming/StreamingQueryManager/anonfun/createQuery/1/apply()#org/apache/spark/sql/streaming/StreamingQueryManager$$anonfun$createQuery$1/apply()
org/apache/spark/sql/streaming/StreamingQueryManager/anonfun/createQuery/1/apply()#org/apache/spark/internal/config/ConfigEntry/key()
org/apache/spark/sql/streaming/StreamingQueryManager/anonfun/createQuery/1/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/streaming/StreamingQueryManager/anonfun/createQuery/1/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/streaming/StreamingQueryManager/anonfun/createQuery/1/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/streaming/StreamingQueryManager/anonfun/createQuery/1/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/streaming/StreamingQueryManager/anonfun/createQuery/1/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/aggregate/ScalaUDAF/anonfun/5/apply(scala.Tuple2)#org/apache/spark/sql/types/StructField/StructField(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.types.Metadata)
org/apache/spark/sql/execution/aggregate/ScalaUDAF/anonfun/5/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/aggregate/ScalaUDAF/anonfun/5/apply(scala.Tuple2)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/ScalaUDAF/anonfun/5/apply(scala.Tuple2)#org/apache/spark/sql/types/Metadata$/empty()
org/apache/spark/sql/execution/aggregate/ScalaUDAF/anonfun/5/apply(scala.Tuple2)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/ScalaUDAF/anonfun/5/apply(scala.Tuple2)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/aggregate/ScalaUDAF/anonfun/5/apply(scala.Tuple2)#scala/Tuple2/_2$mcI$sp()
org/apache/spark/sql/execution/aggregate/ScalaUDAF/anonfun/5/apply(scala.Tuple2)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/ScalaUDAF/anonfun/5/apply(scala.Tuple2)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/aggregate/ScalaUDAF/anonfun/5/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/Expression/nullable()
org/apache/spark/sql/execution/aggregate/ScalaUDAF/anonfun/5/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/aggregate/ScalaUDAF/anonfun/5/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/Expression/dataType()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anonfun/binaryToSQLTimestamp/1/apply()#org/apache/parquet/io/api/Binary/length()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anonfun/binaryToSQLTimestamp/1/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anonfun/binaryToSQLTimestamp/1/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anonfun/binaryToSQLTimestamp/1/apply()#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anonfun/binaryToSQLTimestamp/1/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anonfun/binaryToSQLTimestamp/1/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anonfun/binaryToSQLTimestamp/1/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anonfun/binaryToSQLTimestamp/1/apply()#org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter$$anonfun$binaryToSQLTimestamp$1/apply()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anonfun/binaryToSQLTimestamp/1/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anonfun/binaryToSQLTimestamp/1/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/command/ViewHelper/anonfun/generateQueryColumnNames/1/apply(scala.Tuple2)#scala/collection/mutable/HashMap/put(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/command/ViewHelper/anonfun/generateQueryColumnNames/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/VIEW_QUERY_OUTPUT_COLUMN_NAME_PREFIX()
org/apache/spark/sql/execution/command/ViewHelper/anonfun/generateQueryColumnNames/1/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/command/ViewHelper/anonfun/generateQueryColumnNames/1/apply(scala.Tuple2)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/ViewHelper/anonfun/generateQueryColumnNames/1/apply(scala.Tuple2)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/ViewHelper/anonfun/generateQueryColumnNames/1/apply(scala.Tuple2)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/command/ViewHelper/anonfun/generateQueryColumnNames/1/apply(scala.Tuple2)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/ViewHelper/anonfun/generateQueryColumnNames/1/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/command/ViewHelper/anonfun/generateQueryColumnNames/1/apply(scala.Tuple2)#scala/Tuple2/_2$mcI$sp()
org/apache/spark/sql/execution/command/ViewHelper/anonfun/generateQueryColumnNames/1/apply(scala.Tuple2)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/BucketingUtils/getBucketId(java.lang.String)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/datasources/BucketingUtils/getBucketId(java.lang.String)#scala/collection/LinearSeqOptimized/apply(int)
org/apache/spark/sql/execution/datasources/BucketingUtils/getBucketId(java.lang.String)#scala/Option/isEmpty()
org/apache/spark/sql/execution/datasources/BucketingUtils/getBucketId(java.lang.String)#scala/util/matching/Regex/unapplySeq(java.lang.CharSequence)
org/apache/spark/sql/execution/datasources/BucketingUtils/getBucketId(java.lang.String)#org/apache/spark/sql/execution/datasources/BucketingUtils$/bucketedFileName()
org/apache/spark/sql/execution/datasources/BucketingUtils/getBucketId(java.lang.String)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/datasources/BucketingUtils/getBucketId(java.lang.String)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/datasources/BucketingUtils/getBucketId(java.lang.String)#scala/Option/get()
org/apache/spark/sql/execution/datasources/BucketingUtils/getBucketId(java.lang.String)#org/apache/spark/sql/execution/datasources/BucketingUtils$/getBucketId(java.lang.String)
org/apache/spark/sql/execution/datasources/BucketingUtils/getBucketId(java.lang.String)#scala/collection/immutable/StringOps/toInt()
org/apache/spark/sql/execution/datasources/BucketingUtils/getBucketId(java.lang.String)#scala/collection/LinearSeqOptimized/lengthCompare(int)
org/apache/spark/sql/execution/datasources/BucketingUtils/getBucketId(java.lang.String)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/SQLImplicits/typecreator16/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ModuleSymbolApi/asModule()
org/apache/spark/sql/SQLImplicits/typecreator16/1/apply(scala.reflect.api.Mirror)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/SQLImplicits/typecreator16/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticPackage(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator16/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$InternalApi/reificationSupport()
org/apache/spark/sql/SQLImplicits/typecreator16/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/ThisType(scala.reflect.api.Symbols$SymbolApi)
org/apache/spark/sql/SQLImplicits/typecreator16/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ClassSymbolApi/asType()
org/apache/spark/sql/SQLImplicits/typecreator16/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ModuleSymbolApi/moduleClass()
org/apache/spark/sql/SQLImplicits/typecreator16/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/TypeRef(scala.reflect.api.Types$TypeApi,scala.reflect.api.Symbols$SymbolApi,scala.collection.immutable.List)
org/apache/spark/sql/SQLImplicits/typecreator16/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticClass(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator16/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/universe()
org/apache/spark/sql/SQLImplicits/typecreator16/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$TypeSymbolApi/toTypeConstructor()
org/apache/spark/sql/SQLImplicits/typecreator16/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Universe/internal()
org/apache/spark/sql/SQLImplicits/typecreator16/1/apply(scala.reflect.api.Mirror)#scala/collection/immutable/List$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVInferSchema$/findTightestCommonType()
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVInferSchema$$anonfun$inferField$1/1()
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVInferSchema$/tryParseInteger(java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/Function2/apply(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#java/lang/String/isEmpty()
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVInferSchema$/tryParseLong(java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVInferSchema$/tryParseDecimal(java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVInferSchema$/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVInferSchema$/tryParseTimestamp(java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVInferSchema$/org$apache$spark$sql$execution$datasources$csv$CSVInferSchema$$tryParseDouble(java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#java/lang/UnsupportedOperationException/UnsupportedOperationException(java.lang.String)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVInferSchema$/tryParseBoolean(java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/infer(org.apache.spark.rdd.RDD,java.lang.String[],org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/infer(org.apache.spark.rdd.RDD,java.lang.String[],org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/collection/mutable/ArrayOps/zip(scala.collection.GenIterable,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/infer(org.apache.spark.rdd.RDD,java.lang.String[],org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/infer(org.apache.spark.rdd.RDD,java.lang.String[],org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVInferSchema$$anonfun$2/2(org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/infer(org.apache.spark.rdd.RDD,java.lang.String[],org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/infer(org.apache.spark.rdd.RDD,java.lang.String[],org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/infer(org.apache.spark.rdd.RDD,java.lang.String[],org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVInferSchema$$anonfun$1/1()
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/infer(org.apache.spark.rdd.RDD,java.lang.String[],org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVInferSchema$$anonfun$3/3()
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/infer(org.apache.spark.rdd.RDD,java.lang.String[],org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVInferSchema$$anonfun$4/4()
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/infer(org.apache.spark.rdd.RDD,java.lang.String[],org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVInferSchema$$anonfun$5/5()
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/infer(org.apache.spark.rdd.RDD,java.lang.String[],org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/infer(org.apache.spark.rdd.RDD,java.lang.String[],org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/Array$/fill(int,scala.Function0,scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/infer(org.apache.spark.rdd.RDD,java.lang.String[],org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/types/StructType/StructType(org.apache.spark.sql.types.StructField[])
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/infer(org.apache.spark.rdd.RDD,java.lang.String[],org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/rdd/RDD/aggregate(java.lang.Object,scala.Function2,scala.Function2,scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/infer(org.apache.spark.rdd.RDD,java.lang.String[],org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVInferSchema$/infer(org.apache.spark.rdd.RDD,java.lang.String[],org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/infer(org.apache.spark.rdd.RDD,java.lang.String[],org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/ui/ExecutionPage/anonfun/2/apply(org.apache.spark.sql.execution.ui.SQLExecutionUIData)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/ui/ExecutionPage/anonfun/2/apply(org.apache.spark.sql.execution.ui.SQLExecutionUIData)#scala/xml/NodeSeq$/canBuildFrom()
org/apache/spark/sql/execution/ui/ExecutionPage/anonfun/2/apply(org.apache.spark.sql.execution.ui.SQLExecutionUIData)#scala/xml/Text/Text(java.lang.String)
org/apache/spark/sql/execution/ui/ExecutionPage/anonfun/2/apply(org.apache.spark.sql.execution.ui.SQLExecutionUIData)#org/apache/spark/sql/execution/ui/ExecutionPage$$anonfun$2$$anonfun$1/1(org.apache.spark.sql.execution.ui.ExecutionPage$$anonfun$2,long)
org/apache/spark/sql/execution/ui/ExecutionPage/anonfun/2/apply(org.apache.spark.sql.execution.ui.SQLExecutionUIData)#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/execution/ui/ExecutionPage/anonfun/2/apply(org.apache.spark.sql.execution.ui.SQLExecutionUIData)#scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/ui/ExecutionPage/anonfun/2/apply(org.apache.spark.sql.execution.ui.SQLExecutionUIData)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/ui/ExecutionPage/anonfun/2/apply(org.apache.spark.sql.execution.ui.SQLExecutionUIData)#org/apache/spark/sql/execution/ui/ExecutionPage$$anonfun$2$$anonfun$3/3(org.apache.spark.sql.execution.ui.ExecutionPage$$anonfun$2)
org/apache/spark/sql/execution/ui/ExecutionPage/anonfun/2/apply(org.apache.spark.sql.execution.ui.SQLExecutionUIData)#org/apache/spark/sql/execution/ui/ExecutionPage$$anonfun$2$$anonfun$4/4(org.apache.spark.sql.execution.ui.ExecutionPage$$anonfun$2)
org/apache/spark/sql/execution/ui/ExecutionPage/anonfun/2/apply(org.apache.spark.sql.execution.ui.SQLExecutionUIData)#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/ui/ExecutionPage/anonfun/2/apply(org.apache.spark.sql.execution.ui.SQLExecutionUIData)#org/apache/spark/ui/UIUtils$/formatDuration(long)
org/apache/spark/sql/execution/ui/ExecutionPage/anonfun/2/apply(org.apache.spark.sql.execution.ui.SQLExecutionUIData)#org/apache/spark/sql/execution/ui/ExecutionPage$$anonfun$2$$anonfun$5/5(org.apache.spark.sql.execution.ui.ExecutionPage$$anonfun$2)
org/apache/spark/sql/execution/ui/ExecutionPage/anonfun/2/apply(org.apache.spark.sql.execution.ui.SQLExecutionUIData)#scala/collection/Seq/sorted(scala.math.Ordering)
org/apache/spark/sql/execution/ui/ExecutionPage/anonfun/2/apply(org.apache.spark.sql.execution.ui.SQLExecutionUIData)#scala/xml/Elem/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/ui/ExecutionPage/anonfun/2/apply(org.apache.spark.sql.execution.ui.SQLExecutionUIData)#scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)
org/apache/spark/sql/execution/ui/ExecutionPage/anonfun/2/apply(org.apache.spark.sql.execution.ui.SQLExecutionUIData)#scala/xml/NodeBuffer/NodeBuffer()
org/apache/spark/sql/execution/ui/ExecutionPage/anonfun/2/apply(org.apache.spark.sql.execution.ui.SQLExecutionUIData)#scala/xml/NodeBuffer/$amp$plus(java.lang.Object)
org/apache/spark/sql/execution/ui/ExecutionPage/anonfun/2/apply(org.apache.spark.sql.execution.ui.SQLExecutionUIData)#org/apache/spark/ui/UIUtils$/formatDate(long)
org/apache/spark/sql/execution/ui/ExecutionPage/anonfun/2/apply(org.apache.spark.sql.execution.ui.SQLExecutionUIData)#java/lang/System/currentTimeMillis()
org/apache/spark/sql/execution/ui/ExecutionPage/anonfun/2/apply(org.apache.spark.sql.execution.ui.SQLExecutionUIData)#scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,scala.collection.Seq,scala.xml.MetaData)
org/apache/spark/sql/execution/ui/ExecutionPage/anonfun/2/apply(org.apache.spark.sql.execution.ui.SQLExecutionUIData)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/withNewStorage$default$3()
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/empty()
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand$$anonfun$7/7(org.apache.spark.sql.execution.command.AlterTableSerDePropertiesCommand)
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/isEmpty()
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$2()
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$5()
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTablePartition/storage()
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand$$anonfun$4/4(org.apache.spark.sql.execution.command.AlterTableSerDePropertiesCommand,org.apache.spark.sql.catalyst.catalog.CatalogTable)
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/withNewStorage$default$4()
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/properties()
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/alterTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand$$anonfun$6/6(org.apache.spark.sql.execution.command.AlterTableSerDePropertiesCommand,org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/isDefined()
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand$$anonfun$5/5(org.apache.spark.sql.execution.command.AlterTableSerDePropertiesCommand)
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/DDLUtils$/isDatasourceTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/orElse(scala.Function0)
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$1()
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/getPartition(org.apache.spark.sql.catalyst.TableIdentifier,scala.collection.immutable.Map)
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/withNewStorage(scala.Option,scala.Option,scala.Option,boolean,scala.Option,scala.collection.immutable.Map)
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$3()
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/DDLUtils$/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/alterPartitions(org.apache.spark.sql.catalyst.TableIdentifier,scala.collection.Seq)
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTablePartition/copy$default$1()
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTablePartition/copy$default$3()
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTablePartition/copy(scala.collection.immutable.Map,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,scala.collection.immutable.Map)
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy(scala.Option,scala.Option,scala.Option,scala.Option,boolean,scala.collection.immutable.Map)
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/immutable/Map/$plus$plus(scala.collection.GenTraversableOnce)
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/withNewStorage$default$1()
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/storage()
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/get()
org/apache/spark/sql/execution/command/AlterTableSerDePropertiesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/withNewStorage$default$2()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/getCustomPartitionLocations/1/apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/getCustomPartitionLocations/1/apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)#scala/Option$/option2Iterable(scala.Option)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/getCustomPartitionLocations/1/apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)#org/apache/hadoop/fs/FileSystem/getUri()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/getCustomPartitionLocations/1/apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)#scala/Predef$/ArrowAssoc(java.lang.Object)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/getCustomPartitionLocations/1/apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)#org/apache/spark/sql/catalyst/catalog/CatalogTablePartition/location()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/getCustomPartitionLocations/1/apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)#org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionSchema()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/getCustomPartitionLocations/1/apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)#scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/getCustomPartitionLocations/1/apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)#org/apache/spark/sql/catalyst/catalog/CatalogTablePartition/spec()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/getCustomPartitionLocations/1/apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)#org/apache/hadoop/fs/Path/Path(java.net.URI)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/getCustomPartitionLocations/1/apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/getCustomPartitionLocations/1/apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/getCustomPartitionLocations/1/apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/getCustomPartitionLocations/1/apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)#org/apache/spark/sql/execution/datasources/PartitioningUtils$/getPathFragment(scala.collection.immutable.Map,org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/getCustomPartitionLocations/1/apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/getCustomPartitionLocations/1/apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)#org/apache/hadoop/fs/Path/toString()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/getCustomPartitionLocations/1/apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)#org/apache/hadoop/fs/Path/suffix(java.lang.String)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/getCustomPartitionLocations/1/apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)#org/apache/hadoop/fs/Path/makeQualified(java.net.URI,org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/getCustomPartitionLocations/1/apply(org.apache.spark.sql.catalyst.catalog.CatalogTablePartition)#org/apache/hadoop/fs/FileSystem/getWorkingDirectory()
org/apache/spark/sql/execution/streaming/RateStreamSource/anon/1/serialize(org.apache.spark.sql.execution.streaming.LongOffset,java.io.OutputStream)#java/io/BufferedWriter/BufferedWriter(java.io.Writer)
org/apache/spark/sql/execution/streaming/RateStreamSource/anon/1/serialize(org.apache.spark.sql.execution.streaming.LongOffset,java.io.OutputStream)#java/io/OutputStreamWriter/OutputStreamWriter(java.io.OutputStream,java.nio.charset.Charset)
org/apache/spark/sql/execution/streaming/RateStreamSource/anon/1/serialize(org.apache.spark.sql.execution.streaming.LongOffset,java.io.OutputStream)#org/apache/spark/sql/execution/streaming/RateSourceProvider$/VERSION()
org/apache/spark/sql/execution/streaming/RateStreamSource/anon/1/serialize(org.apache.spark.sql.execution.streaming.LongOffset,java.io.OutputStream)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/streaming/RateStreamSource/anon/1/serialize(org.apache.spark.sql.execution.streaming.LongOffset,java.io.OutputStream)#java/io/BufferedWriter/flush()
org/apache/spark/sql/execution/streaming/RateStreamSource/anon/1/serialize(org.apache.spark.sql.execution.streaming.LongOffset,java.io.OutputStream)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/streaming/RateStreamSource/anon/1/serialize(org.apache.spark.sql.execution.streaming.LongOffset,java.io.OutputStream)#java/io/BufferedWriter/write(java.lang.String)
org/apache/spark/sql/execution/streaming/RateStreamSource/anon/1/serialize(org.apache.spark.sql.execution.streaming.LongOffset,java.io.OutputStream)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/streaming/RateStreamSource/anon/1/serialize(org.apache.spark.sql.execution.streaming.LongOffset,java.io.OutputStream)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/streaming/RateStreamSource/anon/1/deserialize(java.io.InputStream)#scala/collection/immutable/StringOps$/apply$extension(java.lang.String,int)
org/apache/spark/sql/execution/streaming/RateStreamSource/anon/1/deserialize(java.io.InputStream)#scala/Predef$/assert(boolean)
org/apache/spark/sql/execution/streaming/RateStreamSource/anon/1/deserialize(java.io.InputStream)#java/lang/String/length()
org/apache/spark/sql/execution/streaming/RateStreamSource/anon/1/deserialize(java.io.InputStream)#org/apache/spark/sql/execution/streaming/RateSourceProvider$/VERSION()
org/apache/spark/sql/execution/streaming/RateStreamSource/anon/1/deserialize(java.io.InputStream)#java/lang/String/substring(int,int)
org/apache/spark/sql/execution/streaming/RateStreamSource/anon/1/deserialize(java.io.InputStream)#java/lang/String/indexOf(java.lang.String)
org/apache/spark/sql/execution/streaming/RateStreamSource/anon/1/deserialize(java.io.InputStream)#java/lang/String/substring(int)
org/apache/spark/sql/execution/streaming/RateStreamSource/anon/1/deserialize(java.io.InputStream)#org/apache/commons/io/IOUtils/toString(java.io.Reader)
org/apache/spark/sql/execution/streaming/RateStreamSource/anon/1/deserialize(java.io.InputStream)#java/lang/IllegalStateException/IllegalStateException(java.lang.String)
org/apache/spark/sql/execution/streaming/RateStreamSource/anon/1/deserialize(java.io.InputStream)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/streaming/RateStreamSource/anon/1/deserialize(java.io.InputStream)#org/apache/spark/sql/execution/streaming/LongOffset$/apply(org.apache.spark.sql.execution.streaming.SerializedOffset)
org/apache/spark/sql/execution/streaming/RateStreamSource/anon/1/deserialize(java.io.InputStream)#org/apache/spark/sql/execution/streaming/RateStreamSource$$anon$1/parseVersion(java.lang.String,int)
org/apache/spark/sql/execution/streaming/RateStreamSource/anon/1/deserialize(java.io.InputStream)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/RateStreamSource/anon/1/deserialize(java.io.InputStream)#org/apache/spark/sql/execution/streaming/RateStreamSource$$anon$1/deserialize(java.io.InputStream)
org/apache/spark/sql/execution/streaming/RateStreamSource/anon/1/deserialize(java.io.InputStream)#java/io/InputStreamReader/InputStreamReader(java.io.InputStream,java.nio.charset.Charset)
org/apache/spark/sql/execution/streaming/RateStreamSource/anon/1/deserialize(java.io.InputStream)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/RateStreamSource/anon/1/deserialize(java.io.InputStream)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/anonfun/doExecute/1/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#scala/Some/x()
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/anonfun/doExecute/1/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#scala/collection/Iterator/$plus$plus(scala.Function0)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/anonfun/doExecute/1/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec$StateStoreUpdater/updateStateForKeysWithData(scala.collection.Iterator)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/anonfun/doExecute/1/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#scala/Tuple2/_2()
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/anonfun/doExecute/1/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/anonfun/doExecute/1/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec$$anonfun$doExecute$1$$anonfun$apply$1/1(org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExec$$anonfun$doExecute$1,org.apache.spark.sql.execution.streaming.state.StateStore)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/anonfun/doExecute/1/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#org/apache/spark/util/CompletionIterator$/apply(scala.collection.Iterator,scala.Function0)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/anonfun/doExecute/1/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec$$anonfun$doExecute$1$$anonfun$3/3(org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExec$$anonfun$doExecute$1,org.apache.spark.sql.catalyst.expressions.codegen.Predicate)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/anonfun/doExecute/1/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/anonfun/doExecute/1/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec$StateStoreUpdater/StateStoreUpdater(org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExec,org.apache.spark.sql.execution.streaming.state.StateStore)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/anonfun/doExecute/1/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec$$anonfun$doExecute$1$$anonfun$4/4(org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExec$$anonfun$doExecute$1,org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExec$StateStoreUpdater)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/anonfun/doExecute/1/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#scala/collection/Iterator/filter(scala.Function1)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/anonfun/doExecute/1/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#scala/Tuple2/_1()
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/anonfun/doExecute/1/apply(org.apache.spark.sql.execution.streaming.state.StateStore,scala.collection.Iterator)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/datasources/FindDataSourceTable/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/copy(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.immutable.Map,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,boolean,boolean)
org/apache/spark/sql/execution/datasources/FindDataSourceTable/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/copy$default$2()
org/apache/spark/sql/execution/datasources/FindDataSourceTable/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/copy$default$4()
org/apache/spark/sql/execution/datasources/FindDataSourceTable/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/copy$default$5()
org/apache/spark/sql/execution/datasources/FindDataSourceTable/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/CatalogRelation/tableMeta()
org/apache/spark/sql/execution/datasources/FindDataSourceTable/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/command/DDLUtils$/isDatasourceTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)
org/apache/spark/sql/execution/datasources/FindDataSourceTable/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/copy$default$3()
org/apache/spark/sql/execution/datasources/FindDataSourceTable/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/table()
org/apache/spark/sql/execution/datasources/FindDataSourceTable/anonfun/apply/2/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/anonfun/8/apply()#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/DataSource/anonfun/8/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/DataSource/anonfun/8/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSource/anonfun/8/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/DataSource/anonfun/8/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/DataSource/anonfun/8/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/DataSource/anonfun/8/apply()#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$8/apply()
org/apache/spark/sql/execution/datasources/DataSource/anonfun/8/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSource/anonfun/8/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/DataSource/anonfun/8/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/genEqualsForKeys/1/1/apply(scala.Tuple2)#org/apache/spark/sql/execution/aggregate/HashMapGenerator$Buffer/dataType()
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/genEqualsForKeys/1/1/apply(scala.Tuple2)#org/apache/spark/sql/execution/aggregate/HashMapGenerator$Buffer/name()
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/genEqualsForKeys/1/1/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/genEqualsForKeys/1/1/apply(scala.Tuple2)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/genEqualsForKeys/1/1/apply(scala.Tuple2)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/genEqualsForKeys/1/1/apply(scala.Tuple2)#scala/Tuple2/_2$mcI$sp()
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/genEqualsForKeys/1/1/apply(scala.Tuple2)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/genEqualsForKeys/1/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/genEqual(org.apache.spark.sql.types.DataType,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/genEqualsForKeys/1/1/apply(scala.Tuple2)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/genEqualsForKeys/1/1/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/aggregate/VectorizedHashMapGenerator/anonfun/genEqualsForKeys/1/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/getValue(java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType,int)
org/apache/spark/sql/execution/datasources/FailureSafeParser/parse(java.lang.Object)#scala/Function0/apply()
org/apache/spark/sql/execution/datasources/FailureSafeParser/parse(java.lang.Object)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/FailureSafeParser/parse(java.lang.Object)#scala/Function2/apply(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/datasources/FailureSafeParser/parse(java.lang.Object)#org/apache/spark/sql/catalyst/util/BadRecordException/cause()
org/apache/spark/sql/execution/datasources/FailureSafeParser/parse(java.lang.Object)#scala/collection/Iterator$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/FailureSafeParser/parse(java.lang.Object)#scala/package$/Iterator()
org/apache/spark/sql/execution/datasources/FailureSafeParser/parse(java.lang.Object)#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/execution/datasources/FailureSafeParser/parse(java.lang.Object)#scala/collection/IterableLike/toIterator()
org/apache/spark/sql/execution/datasources/FailureSafeParser/parse(java.lang.Object)#org/apache/spark/sql/catalyst/util/BadRecordException/record()
org/apache/spark/sql/execution/datasources/FailureSafeParser/parse(java.lang.Object)#org/apache/spark/sql/catalyst/util/BadRecordException/partialResult()
org/apache/spark/sql/execution/datasources/FailureSafeParser/parse(java.lang.Object)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/FailureSafeParser/parse(java.lang.Object)#org/apache/spark/sql/execution/datasources/FailureSafeParser$$anonfun$parse$1/1(org.apache.spark.sql.execution.datasources.FailureSafeParser)
org/apache/spark/sql/execution/datasources/FailureSafeParser/parse(java.lang.Object)#scala/collection/Iterator$/empty()
org/apache/spark/sql/execution/datasources/FailureSafeParser/parse(java.lang.Object)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/datasources/FailureSafeParser/parse(java.lang.Object)#scala/collection/Iterator/map(scala.Function1)
org/apache/spark/sql/execution/ui/SQLHistoryListener/onTaskEnd(org.apache.spark.scheduler.SparkListenerTaskEnd)#org/apache/spark/scheduler/TaskInfo/accumulables()
org/apache/spark/sql/execution/ui/SQLHistoryListener/onTaskEnd(org.apache.spark.scheduler.SparkListenerTaskEnd)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/ui/SQLHistoryListener/onTaskEnd(org.apache.spark.scheduler.SparkListenerTaskEnd)#org/apache/spark/scheduler/SparkListenerTaskEnd/stageId()
org/apache/spark/sql/execution/ui/SQLHistoryListener/onTaskEnd(org.apache.spark.scheduler.SparkListenerTaskEnd)#org/apache/spark/sql/execution/ui/SQLHistoryListener$$anonfun$onTaskEnd$2/2(org.apache.spark.sql.execution.ui.SQLHistoryListener)
org/apache/spark/sql/execution/ui/SQLHistoryListener/onTaskEnd(org.apache.spark.scheduler.SparkListenerTaskEnd)#org/apache/spark/sql/execution/ui/SQLHistoryListener/updateTaskAccumulatorValues(long,int,int,scala.collection.Seq,boolean)
org/apache/spark/sql/execution/ui/SQLHistoryListener/onTaskEnd(org.apache.spark.scheduler.SparkListenerTaskEnd)#org/apache/spark/scheduler/SparkListenerTaskEnd/stageAttemptId()
org/apache/spark/sql/execution/ui/SQLHistoryListener/onTaskEnd(org.apache.spark.scheduler.SparkListenerTaskEnd)#scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/ui/SQLHistoryListener/onTaskEnd(org.apache.spark.scheduler.SparkListenerTaskEnd)#org/apache/spark/scheduler/TaskInfo/taskId()
org/apache/spark/sql/execution/ui/SQLHistoryListener/onTaskEnd(org.apache.spark.scheduler.SparkListenerTaskEnd)#org/apache/spark/scheduler/SparkListenerTaskEnd/taskInfo()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/anonfun/columnPartition/1/apply()#org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation$$anonfun$columnPartition$1/apply()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/anonfun/columnPartition/1/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/anonfun/columnPartition/1/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/anonfun/columnPartition/1/apply()#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/anonfun/columnPartition/1/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/anonfun/columnPartition/1/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/anonfun/columnPartition/1/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/anonfun/columnPartition/1/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JDBCRelation/anonfun/columnPartition/1/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/GenerateExec/anonfun/8/apply(scala.Tuple2)#java/lang/Object/toString()
org/apache/spark/sql/execution/GenerateExec/anonfun/8/apply(scala.Tuple2)#org/apache/spark/sql/types/StructField/nullable()
org/apache/spark/sql/execution/GenerateExec/anonfun/8/apply(scala.Tuple2)#org/apache/spark/sql/types/StructField/dataType()
org/apache/spark/sql/execution/GenerateExec/anonfun/8/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/GenerateExec/anonfun/8/apply(scala.Tuple2)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/GenerateExec/anonfun/8/apply(scala.Tuple2)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/GenerateExec/anonfun/8/apply(scala.Tuple2)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/GenerateExec/anonfun/8/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/GenerateExec/anonfun/8/apply(scala.Tuple2)#scala/Tuple2/_2$mcI$sp()
org/apache/spark/sql/execution/GenerateExec/anonfun/8/apply(scala.Tuple2)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/GenerateExec/anonfun/8/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/value()
org/apache/spark/sql/execution/GenerateExec/anonfun/8/apply(scala.Tuple2)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/TextSocketSource/anon/1/run()#java/text/SimpleDateFormat/format(java.util.Date)
org/apache/spark/sql/execution/streaming/TextSocketSource/anon/1/run()#java/util/Calendar/getTime()
org/apache/spark/sql/execution/streaming/TextSocketSource/anon/1/run()#org/apache/spark/sql/execution/streaming/TextSocketSource$$anon$1$$anonfun$run$1/1(org.apache.spark.sql.execution.streaming.TextSocketSource$$anon$1)
org/apache/spark/sql/execution/streaming/TextSocketSource/anon/1/run()#org/apache/spark/sql/execution/streaming/TextSocketSource$/DATE_FORMAT()
org/apache/spark/sql/execution/streaming/TextSocketSource/anon/1/run()#java/util/Calendar/getInstance()
org/apache/spark/sql/execution/streaming/TextSocketSource/anon/1/run()#java/sql/Timestamp/valueOf(java.lang.String)
org/apache/spark/sql/execution/streaming/TextSocketSource/anon/1/run()#java/io/BufferedReader/readLine()
org/apache/spark/sql/execution/streaming/TextSocketSource/anon/1/run()#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/streaming/TextSocketSource/anon/1/run()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/TextSocketSource/anon/1/run()#scala/collection/mutable/ListBuffer/append(scala.collection.Seq)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/Seq/forall(scala.Function1)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Option/isEmpty()
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/plans/logical/Aggregate/child()
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery$$anonfun$apply$1$$anonfun$2/2(org.apache.spark.sql.execution.OptimizeMetadataOnlyQuery$$anonfun$apply$1)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery$$anonfun$apply$1$$anonfun$1/1(org.apache.spark.sql.execution.OptimizeMetadataOnlyQuery$$anonfun$apply$1)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/plans/logical/Aggregate/aggregateExpressions()
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Option/get()
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/plans/logical/Aggregate/references()
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Tuple2/_1()
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Tuple2/_2()
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/expressions/AttributeSet/subsetOf(org.apache.spark.sql.catalyst.expressions.AttributeSet)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/plans/logical/Aggregate/withNewChildren(scala.collection.Seq)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery$PartitionedRelation$/unapply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/streaming/DataStreamWriter/assertNotPartitioned(java.lang.String)#scala/Option/isDefined()
org/apache/spark/sql/streaming/DataStreamWriter/assertNotPartitioned(java.lang.String)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/streaming/DataStreamWriter/assertNotPartitioned(java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/streaming/DataStreamWriter/assertNotPartitioned(java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/streaming/DataStreamWriter/assertNotPartitioned(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/streaming/DataStreamWriter/assertNotPartitioned(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/streaming/DataStreamWriter/assertNotPartitioned(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/streaming/DataStreamWriter/assertNotPartitioned(java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/streaming/DataStreamWriter/assertNotPartitioned(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/streaming/DataStreamWriter/assertNotPartitioned(java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/streaming/DataStreamWriter/start()#scala/collection/mutable/HashMap/get(java.lang.Object)
org/apache/spark/sql/streaming/DataStreamWriter/start()#org/apache/spark/sql/Dataset$/ofRows(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/streaming/DataStreamWriter/start()#scala/collection/mutable/HashMap/toMap(scala.Predef$$less$colon$less)
org/apache/spark/sql/streaming/DataStreamWriter/start()#org/apache/spark/sql/execution/datasources/DataSource$/apply$default$3()
org/apache/spark/sql/streaming/DataStreamWriter/start()#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/streaming/DataStreamWriter/start()#org/apache/spark/sql/execution/datasources/DataSource$/apply$default$4()
org/apache/spark/sql/streaming/DataStreamWriter/start()#org/apache/spark/sql/execution/command/DDLUtils$/HIVE_PROVIDER()
org/apache/spark/sql/streaming/DataStreamWriter/start()#org/apache/spark/sql/execution/datasources/DataSource$/apply$default$6()
org/apache/spark/sql/streaming/DataStreamWriter/start()#org/apache/spark/sql/execution/datasources/DataSource$/apply$default$8()
org/apache/spark/sql/streaming/DataStreamWriter/start()#org/apache/spark/sql/streaming/OutputMode/Complete()
org/apache/spark/sql/streaming/DataStreamWriter/start()#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/streaming/DataStreamWriter/start()#java/lang/String/toLowerCase(java.util.Locale)
org/apache/spark/sql/streaming/DataStreamWriter/start()#scala/Option/isEmpty()
org/apache/spark/sql/streaming/DataStreamWriter/start()#scala/Tuple2/_2$mcZ$sp()
org/apache/spark/sql/streaming/DataStreamWriter/start()#scala/Tuple2/_1$mcZ$sp()
org/apache/spark/sql/streaming/DataStreamWriter/start()#org/apache/spark/sql/streaming/DataStreamWriter$$anonfun$1/1(org.apache.spark.sql.streaming.DataStreamWriter)
org/apache/spark/sql/streaming/DataStreamWriter/start()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/streaming/DataStreamWriter/start()#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/streaming/DataStreamWriter/start()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/streaming/DataStreamWriter/start()#scala/Predef$/$conforms()
org/apache/spark/sql/streaming/DataStreamWriter/start()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/streaming/DataStreamWriter/start()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/streaming/DataStreamWriter/start()#scala/Tuple2$mcZZ$sp/sp(boolean,boolean)
org/apache/spark/sql/streaming/DataStreamWriter/start()#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAddTablePartition/1/apply()#scala/collection/mutable/Buffer$/canBuildFrom()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAddTablePartition/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$AddTablePartitionContext/partitionSpecLocation()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAddTablePartition/1/apply()#org/apache/spark/sql/catalyst/parser/ParserUtils$/operationNotAllowed(java.lang.String,org.antlr.v4.runtime.ParserRuleContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAddTablePartition/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$AddTablePartitionContext/EXISTS()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAddTablePartition/1/apply()#scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAddTablePartition/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$AddTablePartitionContext/VIEW()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAddTablePartition/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitAddTablePartition$1/apply()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAddTablePartition/1/apply()#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAddTablePartition/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder/visitTableIdentifier(org.apache.spark.sql.catalyst.parser.SqlBaseParser$TableIdentifierContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAddTablePartition/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$AddTablePartitionContext/tableIdentifier()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAddTablePartition/1/apply()#java/util/List/isEmpty()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAddTablePartition/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$AddTablePartitionContext/partitionSpec()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAddTablePartition/1/apply()#scala/collection/convert/Decorators$AsScala/asScala()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAddTablePartition/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitAddTablePartition$1$$anonfun$23/23(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitAddTablePartition$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAddTablePartition/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitAddTablePartition$1$$anonfun$21/21(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitAddTablePartition$1)
org/apache/spark/sql/execution/FileSourceScanExec/anonfun/28/anonfun/apply/5/apply(org.apache.hadoop.fs.FileStatus)#scala/Predef$/longWrapper(long)
org/apache/spark/sql/execution/FileSourceScanExec/anonfun/28/anonfun/apply/5/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/FileStatus/getPath()
org/apache/spark/sql/execution/FileSourceScanExec/anonfun/28/anonfun/apply/5/apply(org.apache.hadoop.fs.FileStatus)#org/apache/spark/sql/execution/datasources/PartitionDirectory/values()
org/apache/spark/sql/execution/FileSourceScanExec/anonfun/28/anonfun/apply/5/apply(org.apache.hadoop.fs.FileStatus)#java/net/URI/toString()
org/apache/spark/sql/execution/FileSourceScanExec/anonfun/28/anonfun/apply/5/apply(org.apache.hadoop.fs.FileStatus)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/FileSourceScanExec/anonfun/28/anonfun/apply/5/apply(org.apache.hadoop.fs.FileStatus)#scala/collection/immutable/IndexedSeq$/canBuildFrom()
org/apache/spark/sql/execution/FileSourceScanExec/anonfun/28/anonfun/apply/5/apply(org.apache.hadoop.fs.FileStatus)#scala/runtime/RichLong/RichLong(long)
org/apache/spark/sql/execution/FileSourceScanExec/anonfun/28/anonfun/apply/5/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/FileStatus/getLen()
org/apache/spark/sql/execution/FileSourceScanExec/anonfun/28/anonfun/apply/5/apply(org.apache.hadoop.fs.FileStatus)#scala/collection/immutable/NumericRange/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/FileSourceScanExec/anonfun/28/anonfun/apply/5/apply(org.apache.hadoop.fs.FileStatus)#scala/collection/immutable/NumericRange$Exclusive/by(java.lang.Object)
org/apache/spark/sql/execution/FileSourceScanExec/anonfun/28/anonfun/apply/5/apply(org.apache.hadoop.fs.FileStatus)#scala/runtime/RichLong/until(java.lang.Object)
org/apache/spark/sql/execution/FileSourceScanExec/anonfun/28/anonfun/apply/5/apply(org.apache.hadoop.fs.FileStatus)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/FileSourceScanExec/anonfun/28/anonfun/apply/5/apply(org.apache.hadoop.fs.FileStatus)#org/apache/spark/sql/execution/FileSourceScanExec$$anonfun$28$$anonfun$apply$5$$anonfun$apply$6/6(org.apache.spark.sql.execution.FileSourceScanExec$$anonfun$28$$anonfun$apply$5,org.apache.hadoop.fs.BlockLocation[],org.apache.hadoop.fs.FileStatus)
org/apache/spark/sql/execution/FileSourceScanExec/anonfun/28/anonfun/apply/5/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/Path/toUri()
org/apache/spark/sql/execution/FileSourceScanExec/anonfun/28/anonfun/apply/5/apply(org.apache.hadoop.fs.FileStatus)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/FileSourceScanExec/anonfun/28/anonfun/apply/5/apply(org.apache.hadoop.fs.FileStatus)#org/apache/spark/sql/execution/FileSourceScanExec$$anonfun$28/org$apache$spark$sql$execution$FileSourceScanExec$$anonfun$$$outer()
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/logicalPlan/2/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/logicalPlan/2/apply()#java/lang/Thread/currentThread()
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/logicalPlan/2/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/logicalPlan/2/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/logicalPlan/2/apply()#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$logicalPlan$2/apply()
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/logicalPlan/2/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/logicalPlan/2/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/logicalPlan/2/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/logicalPlan/2/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/command/SetCommand/anonfun/1/apply(org.apache.spark.sql.SparkSession)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/command/SetCommand/anonfun/1/apply(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/SetCommand$$anonfun$1$$anonfun$apply$1/1(org.apache.spark.sql.execution.command.SetCommand$$anonfun$1)
org/apache/spark/sql/execution/command/SetCommand/anonfun/1/apply(org.apache.spark.sql.SparkSession)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/SetCommand/anonfun/1/apply(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/command/SetCommand/anonfun/1/apply(org.apache.spark.sql.SparkSession)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/command/SetCommand/anonfun/1/apply(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/internal/SQLConf$/SHUFFLE_PARTITIONS()
org/apache/spark/sql/execution/command/SetCommand/anonfun/1/apply(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/SetCommand/logWarning(scala.Function0)
org/apache/spark/sql/execution/command/SetCommand/anonfun/1/apply(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/internal/SQLConf$Deprecated$/MAPRED_REDUCE_TASKS()
org/apache/spark/sql/execution/command/SetCommand/anonfun/1/apply(org.apache.spark.sql.SparkSession)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/SetCommand/anonfun/1/apply(org.apache.spark.sql.SparkSession)#scala/collection/immutable/StringOps/toInt()
org/apache/spark/sql/execution/command/SetCommand/anonfun/1/apply(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/command/SetCommand/anonfun/1/apply(org.apache.spark.sql.SparkSession)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/SetCommand/anonfun/1/apply(org.apache.spark.sql.SparkSession)#org/apache/spark/internal/config/ConfigEntry/key()
org/apache/spark/sql/execution/command/SetCommand/anonfun/1/apply(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/Row$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/command/SetCommand/anonfun/1/apply(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/command/SetCommand/anonfun/1/apply(org.apache.spark.sql.SparkSession)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/execution/command/SetCommand/anonfun/1/apply(org.apache.spark.sql.SparkSession)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/SetCommand/anonfun/1/apply(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/GenerateExec/anonfun/1/anonfun/2/apply(org.apache.spark.sql.catalyst.InternalRow)#scala/collection/TraversableOnce$/MonadOps(scala.collection.TraversableOnce)
org/apache/spark/sql/execution/GenerateExec/anonfun/1/anonfun/2/apply(org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/sql/execution/GenerateExec$$anonfun$1$$anonfun$2$$anonfun$apply$1/1(org.apache.spark.sql.execution.GenerateExec$$anonfun$1$$anonfun$2)
org/apache/spark/sql/execution/GenerateExec/anonfun/1/anonfun/2/apply(org.apache.spark.sql.catalyst.InternalRow)#scala/collection/TraversableOnce$MonadOps/map(scala.Function1)
org/apache/spark/sql/execution/GenerateExec/anonfun/1/anonfun/2/apply(org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/sql/catalyst/expressions/JoinedRow/withLeft(org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/GenerateExec/anonfun/1/anonfun/2/apply(org.apache.spark.sql.catalyst.InternalRow)#scala/collection/TraversableOnce/isEmpty()
org/apache/spark/sql/execution/GenerateExec/anonfun/1/anonfun/2/apply(org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/sql/execution/GenerateExec$$anonfun$1/org$apache$spark$sql$execution$GenerateExec$$anonfun$$$outer()
org/apache/spark/sql/execution/GenerateExec/anonfun/1/anonfun/2/apply(org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/sql/catalyst/expressions/JoinedRow/withRight(org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/GenerateExec/anonfun/1/anonfun/2/apply(org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/sql/catalyst/expressions/Generator/eval(org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/GenerateExec/anonfun/1/anonfun/2/apply(org.apache.spark.sql.catalyst.InternalRow)#scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/anonfun/26/apply(java.net.URI)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/anonfun/26/apply(java.net.URI)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/anonfun/26/apply(java.net.URI)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/anonfun/26/apply(java.net.URI)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/anonfun/26/apply(java.net.URI)#org/apache/spark/sql/catalyst/catalog/CatalogUtils$/URIToString(java.net.URI)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/anonfun/26/apply(java.net.URI)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/ShowCreateTableCommand/anonfun/26/apply(java.net.URI)#org/apache/spark/sql/catalyst/catalog/CatalogTable/tableType()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/anonfun/26/apply(java.net.URI)#org/apache/spark/sql/catalyst/catalog/CatalogTableType$/MANAGED()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/anonfun/26/apply(java.net.URI)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/selectFilters(org.apache.spark.sql.sources.BaseRelation,scala.collection.Seq)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/selectFilters(org.apache.spark.sql.sources.BaseRelation,scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/selectFilters(org.apache.spark.sql.sources.BaseRelation,scala.collection.Seq)#scala/collection/Seq/toSet()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/selectFilters(org.apache.spark.sql.sources.BaseRelation,scala.collection.Seq)#scala/collection/Iterable/toSeq()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/selectFilters(org.apache.spark.sql.sources.BaseRelation,scala.collection.Seq)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/selectFilters(org.apache.spark.sql.sources.BaseRelation,scala.collection.Seq)#scala/collection/MapLike/keys()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/selectFilters(org.apache.spark.sql.sources.BaseRelation,scala.collection.Seq)#scala/Tuple3/Tuple3(java.lang.Object,java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/selectFilters(org.apache.spark.sql.sources.BaseRelation,scala.collection.Seq)#scala/Predef$/$conforms()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/selectFilters(org.apache.spark.sql.sources.BaseRelation,scala.collection.Seq)#scala/collection/immutable/Map/filter(scala.Function1)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/selectFilters(org.apache.spark.sql.sources.BaseRelation,scala.collection.Seq)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/DataSourceStrategy/selectFilters(org.apache.spark.sql.sources.BaseRelation,scala.collection.Seq)#scala/collection/immutable/Map/values()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/selectFilters(org.apache.spark.sql.sources.BaseRelation,scala.collection.Seq)#org/apache/spark/sql/execution/datasources/DataSourceStrategy$$anonfun$26/26()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/selectFilters(org.apache.spark.sql.sources.BaseRelation,scala.collection.Seq)#scala/collection/Iterable/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/selectFilters(org.apache.spark.sql.sources.BaseRelation,scala.collection.Seq)#scala/collection/immutable/Set/$minus$minus(scala.collection.GenTraversableOnce)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/selectFilters(org.apache.spark.sql.sources.BaseRelation,scala.collection.Seq)#scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/selectFilters(org.apache.spark.sql.sources.BaseRelation,scala.collection.Seq)#scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/selectFilters(org.apache.spark.sql.sources.BaseRelation,scala.collection.Seq)#org/apache/spark/sql/execution/datasources/DataSourceStrategy$$anonfun$28/28(scala.collection.immutable.Set)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/selectFilters(org.apache.spark.sql.sources.BaseRelation,scala.collection.Seq)#scala/collection/Seq/filterNot(scala.Function1)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/selectFilters(org.apache.spark.sql.sources.BaseRelation,scala.collection.Seq)#org/apache/spark/sql/execution/datasources/DataSourceStrategy$$anonfun$27/27(scala.collection.immutable.Map)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/selectFilters(org.apache.spark.sql.sources.BaseRelation,scala.collection.Seq)#scala/collection/mutable/ArrayOps/toSet()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#scala/reflect/ClassTag$/Any()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/Or/left()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/EndsWith/left()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#scala/collection/TraversableOnce/reduceOption(scala.Function2)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/execution/datasources/DataSourceStrategy$$anonfun$translateFilter$2/2(org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/CatalystTypeConverters$/createToScalaConverter(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/LessThan/right()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/In/value()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/EqualTo/left()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#scala/Predef$/genericArrayOps(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/execution/datasources/DataSourceStrategy$/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/In/list()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/GreaterThan/right()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#scala/collection/Seq/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#scala/Option$/option2Iterable(scala.Option)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#scala/collection/Seq/exists(scala.Function1)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/InSet/child()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/EndsWith/right()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/execution/datasources/DataSourceStrategy$$anonfun$25/25()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/InSet/hset()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/Contains/right()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/CatalystTypeConverters$/convertToScala(java.lang.Object,org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#scala/Option/flatMap(scala.Function1)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/EqualNullSafe/left()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#scala/collection/Iterable$/canBuildFrom()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/Or/right()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/Literal/value()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/execution/datasources/DataSourceStrategy$$anonfun$translateFilter$1/1()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/EqualNullSafe/right()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/Contains/left()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/Attribute/dataType()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#scala/collection/Iterable/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/GreaterThan/left()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/EqualTo/right()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/LessThan/left()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/IsNotNull/child()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/Attribute/name()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/IsNull/child()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/LessThanOrEqual/left()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/And/left()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/StartsWith/left()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/GreaterThanOrEqual/left()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#scala/collection/immutable/Set/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/Not/child()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/unsafe/types/UTF8String/toString()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/GreaterThanOrEqual/right()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/LessThanOrEqual/right()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/StartsWith/right()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/Literal/dataType()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/translateFilter(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/And/right()
org/apache/spark/sql/SQLImplicits/typecreator17/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/universe()
org/apache/spark/sql/SQLImplicits/typecreator17/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/selectType(scala.reflect.api.Symbols$SymbolApi,java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator17/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ModuleSymbolApi/asModule()
org/apache/spark/sql/SQLImplicits/typecreator17/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticPackage(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator17/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$InternalApi/reificationSupport()
org/apache/spark/sql/SQLImplicits/typecreator17/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ModuleSymbolApi/moduleClass()
org/apache/spark/sql/SQLImplicits/typecreator17/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/TypeRef(scala.reflect.api.Types$TypeApi,scala.reflect.api.Symbols$SymbolApi,scala.collection.immutable.List)
org/apache/spark/sql/SQLImplicits/typecreator17/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticClass(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator17/1/apply(scala.reflect.api.Mirror)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/SQLImplicits/typecreator17/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/SingleType(scala.reflect.api.Types$TypeApi,scala.reflect.api.Symbols$SymbolApi)
org/apache/spark/sql/SQLImplicits/typecreator17/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticModule(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator17/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/ThisType(scala.reflect.api.Symbols$SymbolApi)
org/apache/spark/sql/SQLImplicits/typecreator17/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Universe/internal()
org/apache/spark/sql/SQLImplicits/typecreator17/1/apply(scala.reflect.api.Mirror)#scala/collection/immutable/List$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap/1/next()#org/apache/spark/unsafe/map/BytesToBytesMap$Location/getValueBase()
org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap/1/next()#org/apache/spark/unsafe/map/BytesToBytesMap$Location/getKeyLength()
org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap/1/next()#org/apache/spark/unsafe/map/BytesToBytesMap$Location/getKeyBase()
org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap/1/next()#org/apache/spark/unsafe/map/BytesToBytesMap$Location/getKeyOffset()
org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap/1/next()#org/apache/spark/unsafe/map/BytesToBytesMap$MapIterator/next()
org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap/1/next()#org/apache/spark/unsafe/map/BytesToBytesMap$Location/getValueLength()
org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap/1/next()#org/apache/spark/unsafe/map/BytesToBytesMap$MapIterator/hasNext()
org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap/1/next()#org/apache/spark/sql/catalyst/expressions/UnsafeRow/pointTo(java.lang.Object,long,int)
org/apache/spark/sql/execution/UnsafeFixedWidthAggregationMap/1/next()#org/apache/spark/unsafe/map/BytesToBytesMap$Location/getValueOffset()
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/17/apply()#org/apache/spark/sql/execution/streaming/ProgressReporter$class/toString$1(org.apache.spark.sql.execution.streaming.ProgressReporter,scala.collection.Seq)
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/17/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/17/apply()#org/apache/spark/sql/execution/streaming/ProgressReporter$$anonfun$17/apply()
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/17/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/17/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/17/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/17/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/17/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/ProgressReporter/anonfun/17/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/csv/CSVDataSource/anonfun/makeSafeHeader/1/apply(scala.Tuple2)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/CSVDataSource/anonfun/makeSafeHeader/1/apply(scala.Tuple2)#java/lang/String/toLowerCase()
org/apache/spark/sql/execution/datasources/csv/CSVDataSource/anonfun/makeSafeHeader/1/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/datasources/csv/CSVDataSource/anonfun/makeSafeHeader/1/apply(scala.Tuple2)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/csv/CSVDataSource/anonfun/makeSafeHeader/1/apply(scala.Tuple2)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/CSVDataSource/anonfun/makeSafeHeader/1/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/CSVDataSource/anonfun/makeSafeHeader/1/apply(scala.Tuple2)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/csv/CSVDataSource/anonfun/makeSafeHeader/1/apply(scala.Tuple2)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/csv/CSVDataSource/anonfun/makeSafeHeader/1/apply(scala.Tuple2)#scala/collection/mutable/ArrayOps/contains(java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/CSVDataSource/anonfun/makeSafeHeader/1/apply(scala.Tuple2)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/datasources/csv/CSVDataSource/anonfun/makeSafeHeader/1/apply(scala.Tuple2)#java/lang/String/isEmpty()
org/apache/spark/sql/execution/datasources/csv/CSVDataSource/anonfun/makeSafeHeader/1/apply(scala.Tuple2)#scala/Tuple2/_2$mcI$sp()
org/apache/spark/sql/execution/datasources/csv/CSVDataSource/anonfun/makeSafeHeader/1/apply(scala.Tuple2)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatDelimited/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat$/empty()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatDelimited/1/apply()#scala/Option$/option2Iterable(scala.Option)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatDelimited/1/apply()#scala/collection/Iterable/toSeq()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatDelimited/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$2()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatDelimited/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$3()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatDelimited/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitRowFormatDelimited$1$$anonfun$46/46(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitRowFormatDelimited$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatDelimited/1/apply()#scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatDelimited/1/apply()#scala/Predef$/$conforms()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatDelimited/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy(scala.Option,scala.Option,scala.Option,scala.Option,boolean,scala.collection.immutable.Map)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatDelimited/1/apply()#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatDelimited/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitRowFormatDelimited$1/apply()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatDelimited/1/apply()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatDelimited/1/apply()#scala/collection/Seq/toMap(scala.Predef$$less$colon$less)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatDelimited/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$1()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatDelimited/1/apply()#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatDelimited/1/apply()#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatDelimited/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$4()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatDelimited/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitRowFormatDelimited$1/entry$1(java.lang.String,org.antlr.v4.runtime.Token)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatDelimited/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$5()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitShowFunctions/1/apply()#scala/Some/x()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitShowFunctions/1/apply()#scala/Tuple2/_1()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitShowFunctions/1/apply()#scala/Tuple2/_1$mcZ$sp()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitShowFunctions/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder/visitFunctionName(org.apache.spark.sql.catalyst.parser.SqlBaseParser$QualifiedNameContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitShowFunctions/1/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitShowFunctions/1/apply()#scala/Tuple2$mcZZ$sp/sp(boolean,boolean)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitShowFunctions/1/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitShowFunctions/1/apply()#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitShowFunctions/1/apply()#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitShowFunctions/1/apply()#org/apache/spark/sql/catalyst/parser/ParseException/ParseException(java.lang.String,org.antlr.v4.runtime.ParserRuleContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitShowFunctions/1/apply()#org/apache/spark/sql/catalyst/parser/ParserUtils$/string(org.antlr.v4.runtime.Token)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitShowFunctions/1/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitShowFunctions/1/apply()#scala/Tuple2/_2()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitShowFunctions/1/apply()#org/apache/spark/sql/catalyst/FunctionIdentifier/funcName()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitShowFunctions/1/apply()#scala/Tuple2/_2$mcZ$sp()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitShowFunctions/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$ShowFunctionsContext/qualifiedName()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitShowFunctions/1/apply()#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitShowFunctions/1/apply()#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitShowFunctions/1/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitShowFunctions/1/apply()#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitShowFunctions/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitShowFunctions$1/apply()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitShowFunctions/1/apply()#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitShowFunctions/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$ShowFunctionsContext/identifier()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitShowFunctions/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitShowFunctions$1$$anonfun$19/19(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitShowFunctions$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitShowFunctions/1/apply()#org/apache/spark/sql/catalyst/FunctionIdentifier/database()
org/apache/spark/sql/execution/ScalarSubquery/updateResult()#org/apache/spark/sql/execution/ScalarSubquery$$anonfun$updateResult$1/1(org.apache.spark.sql.execution.ScalarSubquery,org.apache.spark.sql.catalyst.InternalRow[])
org/apache/spark/sql/execution/ScalarSubquery/updateResult()#scala/Predef$/assert(boolean,scala.Function0)
org/apache/spark/sql/execution/ScalarSubquery/updateResult()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/ScalarSubquery/updateResult()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/ScalarSubquery/updateResult()#scala/sys/package$/error(java.lang.String)
org/apache/spark/sql/execution/ScalarSubquery/updateResult()#org/apache/spark/sql/catalyst/InternalRow/numFields()
org/apache/spark/sql/execution/ScalarSubquery/updateResult()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/ScalarSubquery/updateResult()#org/apache/spark/sql/catalyst/InternalRow/get(int,org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/ScalarSubquery/updateResult()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/saveDataIntoTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.Option,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,org.apache.spark.sql.SaveMode,boolean)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/properties()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/saveDataIntoTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.Option,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,org.apache.spark.sql.SaveMode,boolean)#org/apache/spark/sql/execution/datasources/DataSource$/apply$default$3()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/saveDataIntoTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.Option,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,org.apache.spark.sql.SaveMode,boolean)#scala/Option$/option2Iterable(scala.Option)
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/saveDataIntoTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.Option,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,org.apache.spark.sql.SaveMode,boolean)#org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand$$anonfun$4/4(org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand)
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/saveDataIntoTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.Option,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,org.apache.spark.sql.SaveMode,boolean)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/saveDataIntoTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.Option,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,org.apache.spark.sql.SaveMode,boolean)#org/apache/spark/sql/Dataset$/ofRows(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/saveDataIntoTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.Option,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,org.apache.spark.sql.SaveMode,boolean)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/saveDataIntoTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.Option,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,org.apache.spark.sql.SaveMode,boolean)#org/apache/spark/sql/catalyst/catalog/CatalogTable/bucketSpec()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/saveDataIntoTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.Option,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,org.apache.spark.sql.SaveMode,boolean)#scala/collection/immutable/Map/$plus$plus(scala.collection.GenTraversableOnce)
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/saveDataIntoTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.Option,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,org.apache.spark.sql.SaveMode,boolean)#scala/Option/get()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/saveDataIntoTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.Option,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,org.apache.spark.sql.SaveMode,boolean)#org/apache/spark/sql/catalyst/catalog/CatalogTable/provider()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/saveDataIntoTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.Option,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,org.apache.spark.sql.SaveMode,boolean)#org/apache/spark/sql/execution/datasources/DataSource$/apply$default$4()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/saveDataIntoTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.Option,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,org.apache.spark.sql.SaveMode,boolean)#org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/saveDataIntoTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.Option,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,org.apache.spark.sql.SaveMode,boolean)#org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/logError(scala.Function0,java.lang.Throwable)
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/saveDataIntoTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.Option,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,org.apache.spark.sql.SaveMode,boolean)#org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand$$anonfun$saveDataIntoTable$1/1(org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand,org.apache.spark.sql.catalyst.catalog.CatalogTable)
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/saveDataIntoTable(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.Option,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,org.apache.spark.sql.SaveMode,boolean)#org/apache/spark/sql/catalyst/catalog/CatalogTable/storage()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/internal/SQLConf/manageFilesourcePartitions()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/defaultTablePath(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/empty()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/tableExists(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/provider()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/assert(boolean,scala.Function0)
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/assert(boolean)
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/isDefined()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/tableType()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand$/apply$default$2()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/TableIdentifier/copy(java.lang.String,scala.Option)
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$2()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/createTable(org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$3()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/TableIdentifier/unquotedString()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$4()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand$$anonfun$3/3(org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand,org.apache.spark.sql.internal.SessionState)
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$1()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$5()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$6()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$2()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$10()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/schema()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand$$anonfun$run$1/1(org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand,java.lang.String)
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$11()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$5()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$12()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$6()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$13()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy(scala.Option,scala.Option,scala.Option,scala.Option,boolean,scala.collection.immutable.Map)
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$7()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$15()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$8()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$16()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/TableIdentifier/database()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$9()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$17()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTableType$/VIEW()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/locationUri()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTableType$/MANAGED()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/types/StructType/isEmpty()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/storage()
org/apache/spark/sql/execution/command/CreateDataSourceTableAsSelectCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/TableIdentifier/copy$default$1()
org/apache/spark/sql/execution/window/SlidingWindowFunctionFrame/write(int,org.apache.spark.sql.catalyst.InternalRow)#java/util/ArrayDeque/add(java.lang.Object)
org/apache/spark/sql/execution/window/SlidingWindowFunctionFrame/write(int,org.apache.spark.sql.catalyst.InternalRow)#java/util/Iterator/hasNext()
org/apache/spark/sql/execution/window/SlidingWindowFunctionFrame/write(int,org.apache.spark.sql.catalyst.InternalRow)#java/util/ArrayDeque/remove()
org/apache/spark/sql/execution/window/SlidingWindowFunctionFrame/write(int,org.apache.spark.sql.catalyst.InternalRow)#java/util/Iterator/next()
org/apache/spark/sql/execution/window/SlidingWindowFunctionFrame/write(int,org.apache.spark.sql.catalyst.InternalRow)#java/util/ArrayDeque/iterator()
org/apache/spark/sql/execution/window/SlidingWindowFunctionFrame/write(int,org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/sql/execution/window/WindowFunctionFrame$/getNextOrNull(scala.collection.Iterator)
org/apache/spark/sql/execution/window/SlidingWindowFunctionFrame/write(int,org.apache.spark.sql.catalyst.InternalRow)#java/util/ArrayDeque/peek()
org/apache/spark/sql/execution/window/SlidingWindowFunctionFrame/write(int,org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/sql/catalyst/InternalRow/copy()
org/apache/spark/sql/execution/window/SlidingWindowFunctionFrame/write(int,org.apache.spark.sql.catalyst.InternalRow)#java/util/ArrayDeque/isEmpty()
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/1/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/types/DecimalType/scale()
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/1/apply(org.apache.spark.sql.types.StructField)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/1/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addReferenceMinorObj(java.lang.Object,java.lang.String)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/1/apply(org.apache.spark.sql.types.StructField)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/1/apply(org.apache.spark.sql.types.StructField)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/1/apply(org.apache.spark.sql.types.StructField)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/1/apply(org.apache.spark.sql.types.StructField)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/1/apply(org.apache.spark.sql.types.StructField)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/1/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/types/StructField/dataType()
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/1/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addReferenceMinorObj$default$2()
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/1/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/types/StructField/name()
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/1/apply(org.apache.spark.sql.types.StructField)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/1/apply(org.apache.spark.sql.types.StructField)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/1/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/types/DecimalType/precision()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/execution/columnar/InMemoryTableScanExec$$anonfun$1$$anonfun$applyOrElse$2/2(org.apache.spark.sql.execution.columnar.InMemoryTableScanExec$$anonfun$1)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/expressions/Or/right()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/expressions/EqualNullSafe/right()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/expressions/IsNull/child()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/expressions/GreaterThanOrEqual/left()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/expressions/GreaterThanOrEqual/right()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/expressions/LessThan/right()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/expressions/In/value()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/expressions/EqualTo/left()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/dsl/package$ExpressionConversions$DslExpression/$minus(org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/expressions/In/list()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/expressions/GreaterThan/right()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#scala/PartialFunction/lift()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#scala/Option$/option2Iterable(scala.Option)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#scala/collection/TraversableOnce/reduce(scala.Function2)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#scala/collection/Seq/forall(scala.Function1)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/execution/columnar/InMemoryTableScanExec$$anonfun$1$$anonfun$applyOrElse$3/3(org.apache.spark.sql.execution.columnar.InMemoryTableScanExec$$anonfun$1)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/expressions/EqualNullSafe/left()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#scala/collection/Iterable$/canBuildFrom()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/dsl/package$ExpressionConversions$DslExpression/$less$eq(org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/dsl/package$ExpressionConversions$DslExpression/$less(org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/execution/columnar/InMemoryTableScanExec$$anonfun$1$$anonfun$applyOrElse$5/5(org.apache.spark.sql.execution.columnar.InMemoryTableScanExec$$anonfun$1)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/execution/columnar/InMemoryTableScanExec$$anonfun$1$$anonfun$applyOrElse$4/4(org.apache.spark.sql.execution.columnar.InMemoryTableScanExec$$anonfun$1,org.apache.spark.sql.catalyst.expressions.AttributeReference)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/dsl/package$expressions$/DslExpression(org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/dsl/package$ExpressionConversions$DslExpression/$greater(org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/dsl/package$expressions$/intToLiteral(int)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/expressions/Or/left()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/dsl/package$ExpressionConversions$DslExpression/$bar$bar(org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#scala/collection/Iterable/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/expressions/GreaterThan/left()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/dsl/package$ExpressionConversions$DslExpression/$amp$amp(org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/expressions/EqualTo/right()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/expressions/LessThan/left()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/expressions/IsNotNull/child()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#scala/PartialFunction/apply(java.lang.Object)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/expressions/LessThanOrEqual/left()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/expressions/And/left()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#scala/PartialFunction/isDefinedAt(java.lang.Object)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/expressions/LessThanOrEqual/right()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/expressions/And/right()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/isDefinedAt(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/Or/right()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/isDefinedAt(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/execution/columnar/InMemoryTableScanExec$$anonfun$1$$anonfun$isDefinedAt$1/1(org.apache.spark.sql.execution.columnar.InMemoryTableScanExec$$anonfun$1)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/isDefinedAt(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/Or/left()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/isDefinedAt(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/EqualNullSafe/right()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/isDefinedAt(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/GreaterThan/left()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/isDefinedAt(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/IsNotNull/child()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/isDefinedAt(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/IsNull/child()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/isDefinedAt(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/GreaterThanOrEqual/left()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/isDefinedAt(org.apache.spark.sql.catalyst.expressions.Expression)#scala/PartialFunction/isDefinedAt(java.lang.Object)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/isDefinedAt(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/GreaterThanOrEqual/right()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/isDefinedAt(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/LessThan/right()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/isDefinedAt(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/In/value()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/isDefinedAt(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/EqualTo/left()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/isDefinedAt(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/In/list()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/isDefinedAt(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/GreaterThan/right()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/isDefinedAt(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/EqualNullSafe/left()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/isDefinedAt(org.apache.spark.sql.catalyst.expressions.Expression)#scala/collection/Seq/forall(scala.Function1)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/isDefinedAt(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/EqualTo/right()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/isDefinedAt(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/LessThan/left()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/isDefinedAt(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/LessThanOrEqual/left()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/isDefinedAt(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/And/left()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/isDefinedAt(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/LessThanOrEqual/right()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/1/isDefinedAt(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/And/right()
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/anonfun/3/apply(org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogStatistics$/apply$default$2()
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/anonfun/3/apply(org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogStatistics$/apply$default$3()
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/anonfun/3/apply(org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$1()
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/anonfun/3/apply(org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$17()
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/anonfun/3/apply(org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex/sizeInBytes()
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/anonfun/3/apply(org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogStatistics/CatalogStatistics(scala.math.BigInt,scala.Option,scala.collection.immutable.Map)
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/anonfun/3/apply(org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$2()
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/anonfun/3/apply(org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/package$/BigInt()
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/anonfun/3/apply(org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$3()
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/anonfun/3/apply(org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/math/BigInt$/apply(long)
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/anonfun/3/apply(org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$10()
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/anonfun/3/apply(org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$4()
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/anonfun/3/apply(org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$11()
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/anonfun/3/apply(org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$5()
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/anonfun/3/apply(org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/anonfun/3/apply(org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/anonfun/3/apply(org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$6()
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/anonfun/3/apply(org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$13()
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/anonfun/3/apply(org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/anonfun/3/apply(org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$7()
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/anonfun/3/apply(org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$15()
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/anonfun/3/apply(org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$8()
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/anonfun/3/apply(org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$16()
org/apache/spark/sql/execution/datasources/PruneFileSourcePartitions/anonfun/apply/1/anonfun/3/apply(org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$9()
org/apache/spark/sql/execution/PlanSubqueries/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/expressions/ScalarSubquery/exprId()
org/apache/spark/sql/execution/PlanSubqueries/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/PlanSubqueries/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/execution/PlanSubqueries/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/PlanSubqueries/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/expressions/ScalarSubquery/plan()
org/apache/spark/sql/execution/PlanSubqueries/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/PlanSubqueries/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/PlanSubqueries/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/PlanSubqueries/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/expressions/ExprId/id()
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/verifyTemporaryObjectsNotExists/1/anonfun/applyOrElse/1/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/isTemporaryFunction(org.apache.spark.sql.catalyst.FunctionIdentifier)
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/verifyTemporaryObjectsNotExists/1/anonfun/applyOrElse/1/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/analysis/UnresolvedFunction/name()
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/verifyTemporaryObjectsNotExists/1/anonfun/applyOrElse/1/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/verifyTemporaryObjectsNotExists/1/anonfun/applyOrElse/1/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/verifyTemporaryObjectsNotExists/1/anonfun/applyOrElse/1/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/execution/command/CreateViewCommand$$anonfun$verifyTemporaryObjectsNotExists$1/org$apache$spark$sql$execution$command$CreateViewCommand$$anonfun$$$outer()
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/verifyTemporaryObjectsNotExists/1/anonfun/applyOrElse/1/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/verifyTemporaryObjectsNotExists/1/anonfun/applyOrElse/1/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/verifyTemporaryObjectsNotExists/1/anonfun/applyOrElse/1/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/verifyTemporaryObjectsNotExists/1/anonfun/applyOrElse/1/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/verifyTemporaryObjectsNotExists/1/anonfun/applyOrElse/1/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/verifyTemporaryObjectsNotExists/1/anonfun/applyOrElse/1/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/verifyTemporaryObjectsNotExists/1/anonfun/applyOrElse/1/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/verifyTemporaryObjectsNotExists/1/anonfun/applyOrElse/1/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/execution/command/CreateViewCommand$$anonfun$verifyTemporaryObjectsNotExists$1$$anonfun$applyOrElse$1/org$apache$spark$sql$execution$command$CreateViewCommand$$anonfun$$anonfun$$$outer()
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/verifyTemporaryObjectsNotExists/1/anonfun/applyOrElse/1/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/verifyTemporaryObjectsNotExists/1/anonfun/applyOrElse/1/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/verifyTemporaryObjectsNotExists/1/anonfun/applyOrElse/1/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/CreateViewCommand/anonfun/verifyTemporaryObjectsNotExists/1/anonfun/applyOrElse/1/anonfun/apply/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/command/SetCommand/anonfun/2/apply(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/SetCommand/logWarning(scala.Function0)
org/apache/spark/sql/execution/command/SetCommand/anonfun/2/apply(org.apache.spark.sql.SparkSession)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/command/SetCommand/anonfun/2/apply(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/SetCommand$$anonfun$2$$anonfun$apply$2/2(org.apache.spark.sql.execution.command.SetCommand$$anonfun$2)
org/apache/spark/sql/execution/command/SetCommand/anonfun/2/apply(org.apache.spark.sql.SparkSession)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/SetCommand/anonfun/2/apply(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/command/SetCommand/anonfun/2/apply(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/internal/SQLConf$Replaced$/MAPREDUCE_JOB_REDUCES()
org/apache/spark/sql/execution/command/SetCommand/anonfun/2/apply(org.apache.spark.sql.SparkSession)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/command/SetCommand/anonfun/2/apply(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/internal/SQLConf$/SHUFFLE_PARTITIONS()
org/apache/spark/sql/execution/command/SetCommand/anonfun/2/apply(org.apache.spark.sql.SparkSession)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/SetCommand/anonfun/2/apply(org.apache.spark.sql.SparkSession)#scala/collection/immutable/StringOps/toInt()
org/apache/spark/sql/execution/command/SetCommand/anonfun/2/apply(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/command/SetCommand/anonfun/2/apply(org.apache.spark.sql.SparkSession)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/SetCommand/anonfun/2/apply(org.apache.spark.sql.SparkSession)#org/apache/spark/internal/config/ConfigEntry/key()
org/apache/spark/sql/execution/command/SetCommand/anonfun/2/apply(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/Row$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/command/SetCommand/anonfun/2/apply(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/command/SetCommand/anonfun/2/apply(org.apache.spark.sql.SparkSession)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/execution/command/SetCommand/anonfun/2/apply(org.apache.spark.sql.SparkSession)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/SetCommand/anonfun/2/apply(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/ExpandExec/anonfun/4/apply(int)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/ExprCode(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/ExpandExec/anonfun/4/apply(int)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/ExpandExec/anonfun/4/apply(int)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/defaultValue(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/ExpandExec/anonfun/4/apply(int)#org/apache/spark/sql/catalyst/expressions/BindReferences$/bindReference$default$3()
org/apache/spark/sql/execution/ExpandExec/anonfun/4/apply(int)#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/ExpandExec/anonfun/4/apply(int)#org/apache/spark/sql/catalyst/expressions/Expression/genCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)
org/apache/spark/sql/execution/ExpandExec/anonfun/4/apply(int)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/ExpandExec/anonfun/4/apply(int)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/ExpandExec/anonfun/4/apply(int)#org/apache/spark/sql/catalyst/expressions/Expression/dataType()
org/apache/spark/sql/execution/ExpandExec/anonfun/4/apply(int)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/ExpandExec/anonfun/4/apply(int)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/ExpandExec/anonfun/4/apply(int)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/ExpandExec/anonfun/4/apply(int)#scala/collection/SeqLike/apply(int)
org/apache/spark/sql/execution/ExpandExec/anonfun/4/apply(int)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/ExpandExec/anonfun/4/apply(int)#scala/collection/Seq/head()
org/apache/spark/sql/execution/ExpandExec/anonfun/4/apply(int)#org/apache/spark/sql/catalyst/expressions/package$/AttributeSeq(scala.collection.Seq)
org/apache/spark/sql/execution/ExpandExec/anonfun/4/apply(int)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/freshName(java.lang.String)
org/apache/spark/sql/execution/ExpandExec/anonfun/4/apply(int)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/javaType(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/ExpandExec/anonfun/4/apply(int)#org/apache/spark/sql/catalyst/expressions/BindReferences$/bindReference(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.package$AttributeSeq,boolean)
org/apache/spark/sql/SQLImplicits/typecreator18/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/universe()
org/apache/spark/sql/SQLImplicits/typecreator18/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/TypeTags$TypeTag/in(scala.reflect.api.Mirror)
org/apache/spark/sql/SQLImplicits/typecreator18/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ModuleSymbolApi/asModule()
org/apache/spark/sql/SQLImplicits/typecreator18/1/apply(scala.reflect.api.Mirror)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/SQLImplicits/typecreator18/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticPackage(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator18/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$InternalApi/reificationSupport()
org/apache/spark/sql/SQLImplicits/typecreator18/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/ThisType(scala.reflect.api.Symbols$SymbolApi)
org/apache/spark/sql/SQLImplicits/typecreator18/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Symbols$ModuleSymbolApi/moduleClass()
org/apache/spark/sql/SQLImplicits/typecreator18/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Internals$ReificationSupportApi/TypeRef(scala.reflect.api.Types$TypeApi,scala.reflect.api.Symbols$SymbolApi,scala.collection.immutable.List)
org/apache/spark/sql/SQLImplicits/typecreator18/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Mirror/staticClass(java.lang.String)
org/apache/spark/sql/SQLImplicits/typecreator18/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/TypeTags$TypeTag/tpe()
org/apache/spark/sql/SQLImplicits/typecreator18/1/apply(scala.reflect.api.Mirror)#scala/reflect/api/Universe/internal()
org/apache/spark/sql/SQLImplicits/typecreator18/1/apply(scala.reflect.api.Mirror)#scala/collection/immutable/List$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashMapGenerator/genComputeHash(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,org.apache.spark.sql.types.DataType,java.lang.String)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashMapGenerator/genComputeHash(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,org.apache.spark.sql.types.DataType,java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/HashMapGenerator/genComputeHash(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,org.apache.spark.sql.types.DataType,java.lang.String)#org/apache/spark/sql/types/Decimal$/MAX_LONG_DIGITS()
org/apache/spark/sql/execution/aggregate/HashMapGenerator/genComputeHash(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,org.apache.spark.sql.types.DataType,java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashMapGenerator/genComputeHash(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,org.apache.spark.sql.types.DataType,java.lang.String)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashMapGenerator/genComputeHash(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,org.apache.spark.sql.types.DataType,java.lang.String)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/freshName(java.lang.String)
org/apache/spark/sql/execution/aggregate/HashMapGenerator/genComputeHash(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,org.apache.spark.sql.types.DataType,java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashMapGenerator/genComputeHash(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,org.apache.spark.sql.types.DataType,java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashMapGenerator/genComputeHash(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,org.apache.spark.sql.types.DataType,java.lang.String)#org/apache/spark/sql/types/DecimalType/precision()
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/constructNextBatch/2/apply$mcV$sp()#org/apache/spark/sql/execution/streaming/OffsetSeqLog/purge(long)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/constructNextBatch/2/apply$mcV$sp()#scala/Option/isDefined()
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/constructNextBatch/2/apply$mcV$sp()#org/apache/spark/sql/execution/streaming/OffsetSeqLog/get(long)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/constructNextBatch/2/apply$mcV$sp()#org/apache/spark/sql/execution/streaming/OffsetSeqLog/add(long,java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/constructNextBatch/2/apply$mcV$sp()#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch$2$$anonfun$apply$mcV$sp$2/2(org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch$2)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/constructNextBatch/2/apply$mcV$sp()#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch$2$$anonfun$apply$mcV$sp$3/3(org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch$2)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/constructNextBatch/2/apply$mcV$sp()#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch$2$$anonfun$apply$mcV$sp$4/4(org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$constructNextBatch$2)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/constructNextBatch/2/apply$mcV$sp()#scala/Option/get()
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/constructNextBatch/2/apply$mcV$sp()#scala/Predef$/assert(boolean,scala.Function0)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/constructNextBatch/2/apply$mcV$sp()#org/apache/spark/sql/execution/streaming/BatchCommitLog/purge(long)
org/apache/spark/sql/execution/datasources/csv/CSVUtils/filterCommentAndEmpty(org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#java/lang/Object/toString()
org/apache/spark/sql/execution/datasources/csv/CSVUtils/filterCommentAndEmpty(org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/csv/CSVUtils/filterCommentAndEmpty(org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/functions$/length(org.apache.spark.sql.Column)
org/apache/spark/sql/execution/datasources/csv/CSVUtils/filterCommentAndEmpty(org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/ColumnName/startsWith(java.lang.String)
org/apache/spark/sql/execution/datasources/csv/CSVUtils/filterCommentAndEmpty(org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/runtime/BoxesRunTime/boxToCharacter(char)
org/apache/spark/sql/execution/datasources/csv/CSVUtils/filterCommentAndEmpty(org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/functions$/trim(org.apache.spark.sql.Column)
org/apache/spark/sql/execution/datasources/csv/CSVUtils/filterCommentAndEmpty(org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/csv/CSVUtils/filterCommentAndEmpty(org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/datasources/csv/CSVUtils/filterCommentAndEmpty(org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/SQLContext$implicits$/StringToColumn(scala.StringContext)
org/apache/spark/sql/execution/datasources/csv/CSVUtils/filterCommentAndEmpty(org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVUtils$/filterCommentAndEmpty(org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/CSVUtils/filterCommentAndEmpty(org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/SQLImplicits$StringToColumn/$(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/csv/CSVUtils/toChar(java.lang.String)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/CSVUtils/toChar(java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/csv/CSVUtils/toChar(java.lang.String)#java/lang/String/length()
org/apache/spark/sql/execution/datasources/csv/CSVUtils/toChar(java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/csv/CSVUtils/toChar(java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/CSVUtils/toChar(java.lang.String)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/execution/datasources/csv/CSVUtils/toChar(java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/csv/CSVUtils/toChar(java.lang.String)#java/lang/String/charAt(int)
org/apache/spark/sql/execution/datasources/csv/CSVUtils/toChar(java.lang.String)#org/apache/spark/sql/execution/datasources/csv/CSVUtils$/toChar(java.lang.String)
org/apache/spark/sql/catalog/Column/toString()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/catalog/Column/toString()#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/catalog/Column/toString()#org/apache/spark/sql/catalog/Column$$anonfun$toString$7/7(org.apache.spark.sql.catalog.Column)
org/apache/spark/sql/catalog/Column/toString()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/catalog/Column/toString()#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/catalog/Column/toString()#scala/runtime/BoxesRunTime/boxToBoolean(boolean)
org/apache/spark/sql/catalog/Column/toString()#org/apache/spark/sql/catalog/Column$$anonfun$toString$8/8(org.apache.spark.sql.catalog.Column)
org/apache/spark/sql/catalog/Column/toString()#scala/Option/map(scala.Function1)
org/apache/spark/sql/catalog/Column/toString()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/catalog/Column/toString()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/catalog/Column/toString()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/catalog/Column/toString()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/catalog/Column/toString()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/3/anonfun/applyOrElse/3/anonfun/apply/10/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/3/anonfun/applyOrElse/3/anonfun/apply/10/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/3/anonfun/applyOrElse/3/anonfun/apply/10/apply()#org/apache/spark/util/Utils$/truncatedString(scala.collection.Seq,java.lang.String)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/3/anonfun/applyOrElse/3/anonfun/apply/10/apply()#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$3$$anonfun$applyOrElse$3$$anonfun$apply$10/apply()
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/3/anonfun/applyOrElse/3/anonfun/apply/10/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/3/anonfun/applyOrElse/3/anonfun/apply/10/apply()#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/output()
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/3/anonfun/applyOrElse/3/anonfun/apply/10/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/3/anonfun/applyOrElse/3/anonfun/apply/10/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/3/anonfun/applyOrElse/3/anonfun/apply/10/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/3/anonfun/applyOrElse/3/anonfun/apply/10/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/getValue(org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/unsafe/map/BytesToBytesMap$Location/isDefined()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/getValue(org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/getSizeInBytes()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/getValue(org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/getBaseObject()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/getValue(org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/hashCode()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/getValue(org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/unsafe/map/BytesToBytesMap$Location/getValueLength()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/getValue(org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/getBaseOffset()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/getValue(org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/unsafe/map/BytesToBytesMap/safeLookup(java.lang.Object,long,int,org.apache.spark.unsafe.map.BytesToBytesMap$Location,int)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/getValue(org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/unsafe/map/BytesToBytesMap$Location/getValueBase()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/getValue(org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/pointTo(java.lang.Object,long,int)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/getValue(org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/unsafe/map/BytesToBytesMap$Location/getValueOffset()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/getValue(org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/unsafe/map/BytesToBytesMap$Location/Location(org.apache.spark.unsafe.map.BytesToBytesMap)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/org$apache$spark$sql$execution$joins$UnsafeHashedRelation$$read(scala.Function0,scala.Function0,scala.Function3)#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/org$apache$spark$sql$execution$joins$UnsafeHashedRelation$$read(scala.Function0,scala.Function0,scala.Function3)#org/apache/spark/SparkConf/SparkConf()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/org$apache$spark$sql$execution$joins$UnsafeHashedRelation$$read(scala.Function0,scala.Function0,scala.Function3)#org/apache/spark/sql/execution/joins/UnsafeHashedRelation$$anonfun$4/4(org.apache.spark.sql.execution.joins.UnsafeHashedRelation)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/org$apache$spark$sql$execution$joins$UnsafeHashedRelation$$read(scala.Function0,scala.Function0,scala.Function3)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/org$apache$spark$sql$execution$joins$UnsafeHashedRelation$$read(scala.Function0,scala.Function0,scala.Function3)#org/apache/spark/unsafe/map/BytesToBytesMap/free()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/org$apache$spark$sql$execution$joins$UnsafeHashedRelation$$read(scala.Function0,scala.Function0,scala.Function3)#org/apache/spark/sql/execution/joins/UnsafeHashedRelation$$anonfun$1/1(org.apache.spark.sql.execution.joins.UnsafeHashedRelation)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/org$apache$spark$sql$execution$joins$UnsafeHashedRelation$$read(scala.Function0,scala.Function0,scala.Function3)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/org$apache$spark$sql$execution$joins$UnsafeHashedRelation$$read(scala.Function0,scala.Function0,scala.Function3)#scala/Function0/apply$mcI$sp()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/org$apache$spark$sql$execution$joins$UnsafeHashedRelation$$read(scala.Function0,scala.Function0,scala.Function3)#java/io/IOException/IOException(java.lang.String)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/org$apache$spark$sql$execution$joins$UnsafeHashedRelation$$read(scala.Function0,scala.Function0,scala.Function3)#org/apache/spark/unsafe/map/BytesToBytesMap/BytesToBytesMap(org.apache.spark.memory.TaskMemoryManager,int,long)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/org$apache$spark$sql$execution$joins$UnsafeHashedRelation$$read(scala.Function0,scala.Function0,scala.Function3)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/UnsafeRow(int)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/org$apache$spark$sql$execution$joins$UnsafeHashedRelation$$read(scala.Function0,scala.Function0,scala.Function3)#org/apache/spark/SparkEnv$/get()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/org$apache$spark$sql$execution$joins$UnsafeHashedRelation$$read(scala.Function0,scala.Function0,scala.Function3)#org/apache/spark/memory/StaticMemoryManager/StaticMemoryManager(org.apache.spark.SparkConf,long,long,int)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/org$apache$spark$sql$execution$joins$UnsafeHashedRelation$$read(scala.Function0,scala.Function0,scala.Function3)#org/apache/spark/unsafe/map/BytesToBytesMap/lookup(java.lang.Object,long,int)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/org$apache$spark$sql$execution$joins$UnsafeHashedRelation$$read(scala.Function0,scala.Function0,scala.Function3)#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/org$apache$spark$sql$execution$joins$UnsafeHashedRelation$$read(scala.Function0,scala.Function0,scala.Function3)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/org$apache$spark$sql$execution$joins$UnsafeHashedRelation$$read(scala.Function0,scala.Function0,scala.Function3)#org/apache/spark/unsafe/map/BytesToBytesMap$Location/append(java.lang.Object,long,int,java.lang.Object,long,int)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/org$apache$spark$sql$execution$joins$UnsafeHashedRelation$$read(scala.Function0,scala.Function0,scala.Function3)#org/apache/spark/SparkConf/set(java.lang.String,java.lang.String)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/org$apache$spark$sql$execution$joins$UnsafeHashedRelation$$read(scala.Function0,scala.Function0,scala.Function3)#org/apache/spark/memory/TaskMemoryManager/TaskMemoryManager(org.apache.spark.memory.MemoryManager,long)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/org$apache$spark$sql$execution$joins$UnsafeHashedRelation$$read(scala.Function0,scala.Function0,scala.Function3)#scala/Function3/apply(java.lang.Object,java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/org$apache$spark$sql$execution$joins$UnsafeHashedRelation$$read(scala.Function0,scala.Function0,scala.Function3)#scala/Function0/apply$mcJ$sp()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/getSizeInBytes()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#org/apache/spark/sql/catalyst/expressions/UnsafeProjection/apply(org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#org/apache/spark/unsafe/map/BytesToBytesMap/BytesToBytesMap(org.apache.spark.memory.TaskMemoryManager,int,long)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#org/apache/spark/SparkEnv$/get()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#org/apache/spark/unsafe/map/BytesToBytesMap/free()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#org/apache/spark/sql/execution/joins/UnsafeHashedRelation$$anonfun$5/5()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/getBaseOffset()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/numFields()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#org/apache/spark/sql/catalyst/expressions/UnsafeProjection$/create(scala.collection.Seq)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#org/apache/spark/SparkException/SparkException(java.lang.String)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#org/apache/spark/sql/execution/joins/UnsafeHashedRelation$/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#scala/collection/Iterator/next()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/getBaseObject()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#org/apache/spark/unsafe/map/BytesToBytesMap/lookup(java.lang.Object,long,int)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#scala/collection/Iterator/hasNext()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#org/apache/spark/sql/execution/joins/UnsafeHashedRelation$$anonfun$2/2()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#org/apache/spark/unsafe/map/BytesToBytesMap$Location/append(java.lang.Object,long,int,java.lang.Object,long,int)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/anyNull()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/org$apache$spark$sql$execution$joins$UnsafeHashedRelation$$write(scala.Function1,scala.Function1,scala.Function3)#scala/Function1/apply$mcVJ$sp(long)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/org$apache$spark$sql$execution$joins$UnsafeHashedRelation$$write(scala.Function1,scala.Function1,scala.Function3)#org/apache/spark/unsafe/map/BytesToBytesMap$Location/getKeyLength()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/org$apache$spark$sql$execution$joins$UnsafeHashedRelation$$write(scala.Function1,scala.Function1,scala.Function3)#org/apache/spark/unsafe/map/BytesToBytesMap/numKeys()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/org$apache$spark$sql$execution$joins$UnsafeHashedRelation$$write(scala.Function1,scala.Function1,scala.Function3)#org/apache/spark/unsafe/map/BytesToBytesMap$Location/getKeyOffset()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/org$apache$spark$sql$execution$joins$UnsafeHashedRelation$$write(scala.Function1,scala.Function1,scala.Function3)#org/apache/spark/unsafe/map/BytesToBytesMap$MapIterator/next()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/org$apache$spark$sql$execution$joins$UnsafeHashedRelation$$write(scala.Function1,scala.Function1,scala.Function3)#org/apache/spark/unsafe/map/BytesToBytesMap/iterator()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/org$apache$spark$sql$execution$joins$UnsafeHashedRelation$$write(scala.Function1,scala.Function1,scala.Function3)#org/apache/spark/unsafe/map/BytesToBytesMap$Location/getValueLength()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/org$apache$spark$sql$execution$joins$UnsafeHashedRelation$$write(scala.Function1,scala.Function1,scala.Function3)#scala/runtime/ObjectRef/create(java.lang.Object)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/org$apache$spark$sql$execution$joins$UnsafeHashedRelation$$write(scala.Function1,scala.Function1,scala.Function3)#org/apache/spark/unsafe/map/BytesToBytesMap$Location/getValueBase()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/org$apache$spark$sql$execution$joins$UnsafeHashedRelation$$write(scala.Function1,scala.Function1,scala.Function3)#org/apache/spark/unsafe/map/BytesToBytesMap$MapIterator/hasNext()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/org$apache$spark$sql$execution$joins$UnsafeHashedRelation$$write(scala.Function1,scala.Function1,scala.Function3)#org/apache/spark/unsafe/map/BytesToBytesMap$Location/getValueOffset()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/org$apache$spark$sql$execution$joins$UnsafeHashedRelation$$write(scala.Function1,scala.Function1,scala.Function3)#org/apache/spark/unsafe/map/BytesToBytesMap$Location/getKeyBase()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/org$apache$spark$sql$execution$joins$UnsafeHashedRelation$$write(scala.Function1,scala.Function1,scala.Function3)#org/apache/spark/unsafe/map/BytesToBytesMap/numValues()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/org$apache$spark$sql$execution$joins$UnsafeHashedRelation$$write(scala.Function1,scala.Function1,scala.Function3)#scala/Function1/apply$mcVI$sp(int)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/2/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/types/DecimalType/scale()
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/2/apply(org.apache.spark.sql.types.StructField)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/2/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addReferenceMinorObj(java.lang.Object,java.lang.String)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/2/apply(org.apache.spark.sql.types.StructField)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/2/apply(org.apache.spark.sql.types.StructField)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/2/apply(org.apache.spark.sql.types.StructField)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/2/apply(org.apache.spark.sql.types.StructField)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/2/apply(org.apache.spark.sql.types.StructField)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/2/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/types/StructField/dataType()
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/2/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addReferenceMinorObj$default$2()
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/2/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/types/StructField/name()
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/2/apply(org.apache.spark.sql.types.StructField)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/2/apply(org.apache.spark.sql.types.StructField)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/2/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/types/DecimalType/precision()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/org/apache/spark/sql/execution/datasources/InMemoryFileIndex/bulkListLeafFiles/3/anonfun/7/apply(org.apache.spark.sql.execution.datasources.InMemoryFileIndex$SerializableFileStatus)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/org/apache/spark/sql/execution/datasources/InMemoryFileIndex/bulkListLeafFiles/3/anonfun/7/apply(org.apache.spark.sql.execution.datasources.InMemoryFileIndex$SerializableFileStatus)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/blockReplication()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/org/apache/spark/sql/execution/datasources/InMemoryFileIndex/bulkListLeafFiles/3/anonfun/7/apply(org.apache.spark.sql.execution.datasources.InMemoryFileIndex$SerializableFileStatus)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/org/apache/spark/sql/execution/datasources/InMemoryFileIndex/bulkListLeafFiles/3/anonfun/7/apply(org.apache.spark.sql.execution.datasources.InMemoryFileIndex$SerializableFileStatus)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/path()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/org/apache/spark/sql/execution/datasources/InMemoryFileIndex/bulkListLeafFiles/3/anonfun/7/apply(org.apache.spark.sql.execution.datasources.InMemoryFileIndex$SerializableFileStatus)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/length()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/org/apache/spark/sql/execution/datasources/InMemoryFileIndex/bulkListLeafFiles/3/anonfun/7/apply(org.apache.spark.sql.execution.datasources.InMemoryFileIndex$SerializableFileStatus)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/modificationTime()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/org/apache/spark/sql/execution/datasources/InMemoryFileIndex/bulkListLeafFiles/3/anonfun/7/apply(org.apache.spark.sql.execution.datasources.InMemoryFileIndex$SerializableFileStatus)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/isDir()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/org/apache/spark/sql/execution/datasources/InMemoryFileIndex/bulkListLeafFiles/3/anonfun/7/apply(org.apache.spark.sql.execution.datasources.InMemoryFileIndex$SerializableFileStatus)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/blockLocations()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/org/apache/spark/sql/execution/datasources/InMemoryFileIndex/bulkListLeafFiles/3/anonfun/7/apply(org.apache.spark.sql.execution.datasources.InMemoryFileIndex$SerializableFileStatus)#org/apache/hadoop/fs/LocatedFileStatus/LocatedFileStatus(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.BlockLocation[])
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/org/apache/spark/sql/execution/datasources/InMemoryFileIndex/bulkListLeafFiles/3/anonfun/7/apply(org.apache.spark.sql.execution.datasources.InMemoryFileIndex$SerializableFileStatus)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/org/apache/spark/sql/execution/datasources/InMemoryFileIndex/bulkListLeafFiles/3/anonfun/7/apply(org.apache.spark.sql.execution.datasources.InMemoryFileIndex$SerializableFileStatus)#org/apache/hadoop/fs/FileStatus/FileStatus(long,boolean,int,long,long,org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/org/apache/spark/sql/execution/datasources/InMemoryFileIndex/bulkListLeafFiles/3/anonfun/7/apply(org.apache.spark.sql.execution.datasources.InMemoryFileIndex$SerializableFileStatus)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$SerializableFileStatus/blockSize()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/org/apache/spark/sql/execution/datasources/InMemoryFileIndex/bulkListLeafFiles/3/anonfun/7/apply(org.apache.spark.sql.execution.datasources.InMemoryFileIndex$SerializableFileStatus)#org/apache/hadoop/fs/Path/Path(java.lang.String)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/org/apache/spark/sql/execution/datasources/InMemoryFileIndex/bulkListLeafFiles/3/anonfun/7/apply(org.apache.spark.sql.execution.datasources.InMemoryFileIndex$SerializableFileStatus)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/org/apache/spark/sql/execution/datasources/InMemoryFileIndex/bulkListLeafFiles/3/anonfun/7/apply(org.apache.spark.sql.execution.datasources.InMemoryFileIndex$SerializableFileStatus)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$$anonfun$org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$bulkListLeafFiles$3$$anonfun$7$$anonfun$8/8(org.apache.spark.sql.execution.datasources.InMemoryFileIndex$$anonfun$org$apache$spark$sql$execution$datasources$InMemoryFileIndex$$bulkListLeafFiles$3$$anonfun$7)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/declareAddedFunctions()
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#scala/Tuple2/_1()
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#scala/collection/TraversableOnce/mkString(java.lang.String)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#scala/collection/Seq/length()
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#scala/reflect/ClassTag$/Any()
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#scala/collection/Seq/zipWithIndex(scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#scala/Predef$/intWrapper(int)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#scala/collection/immutable/Range$Inclusive/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#org/apache/spark/sql/execution/columnar/GenerateColumnAccessor$/newCodeGenContext()
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator$/compile(org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#scala/collection/Iterator/foreach(scala.Function1)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#org/apache/spark/sql/execution/columnar/GenerateColumnAccessor$/logDebug(scala.Function0)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/CodeFormatter$/stripOverlappingComments(org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#scala/Tuple2/_2()
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/declareMutableStates()
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#scala/collection/generic/GenericTraversableTemplate/unzip(scala.Function1)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#scala/Array$/empty(scala.reflect.ClassTag)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#org/apache/spark/sql/execution/columnar/GenerateColumnAccessor$$anonfun$4/4()
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#org/apache/spark/sql/execution/columnar/GenerateColumnAccessor$$anonfun$2/2(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.runtime.IntRef)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#scala/collection/immutable/IndexedSeq$/canBuildFrom()
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#java/lang/Class/getName()
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#org/apache/spark/sql/execution/columnar/GenerateColumnAccessor$$anonfun$5/5()
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#org/apache/spark/sql/execution/columnar/GenerateColumnAccessor$$anonfun$1/1(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/CodeAndComment/CodeAndComment(java.lang.String,scala.collection.Map)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#scala/runtime/IntRef/create(int)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#scala/collection/Seq/size()
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/GeneratedClass/generate(java.lang.Object[])
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#org/apache/spark/sql/execution/columnar/GenerateColumnAccessor$$anonfun$create$1/1(org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#scala/Predef$/$conforms()
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#org/apache/spark/sql/execution/columnar/GenerateColumnAccessor$$anonfun$3/3(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#scala/collection/Seq/grouped(int)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/getPlaceHolderToComments()
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#scala/collection/Seq/mkString(java.lang.String)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#scala/collection/Iterator/zipWithIndex()
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/create(scala.collection.Seq)#scala/runtime/RichInt$/to$extension0(int,int)
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/refreshTable(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/util/Utils$/resolveURI(java.lang.String)
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#java/io/File/exists()
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/LoadDataCommand$$anonfun$4/4(org.apache.spark.sql.execution.command.LoadDataCommand,java.nio.file.FileSystem,java.nio.file.PathMatcher)
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#java/lang/Object/toString()
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Iterable/foreach(scala.Function1)
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#java/io/File/getName()
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#java/net/URI/URI(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/TraversableOnce/size()
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#java/net/URI/URI(java.lang.String)
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#java/nio/file/FileSystem/getPathMatcher(java.lang.String)
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/empty()
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#java/net/URI/toString()
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#java/net/URI/getAuthority()
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/tableType()
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/isEmpty()
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/nonEmpty()
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/DDLUtils$/isDatasourceTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/LoadDataCommand$$anonfun$run$1/1(org.apache.spark.sql.execution.command.LoadDataCommand,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#java/net/URI/getQuery()
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/loadPartition(org.apache.spark.sql.catalyst.TableIdentifier,java.lang.String,scala.collection.immutable.Map,boolean,boolean,boolean)
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#java/io/File/File(java.lang.String)
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#java/nio/file/Path/toAbsolutePath()
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#java/io/File/File(java.lang.String,java.lang.String)
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#java/net/URI/getPath()
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#java/net/URI/getScheme()
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#java/lang/System/getProperty(java.lang.String)
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/MapLike/keys()
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#java/io/File/toURI()
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#java/io/File/getAbsolutePath()
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq/size()
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#java/nio/file/FileSystems/getDefault()
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/TableIdentifier/quotedString()
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#java/lang/String/startsWith(java.lang.String)
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/loadTable(org.apache.spark.sql.catalyst.TableIdentifier,java.lang.String,boolean,boolean)
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/ArrayOps/exists(scala.Function1)
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#java/lang/String/contains(java.lang.CharSequence)
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#org/apache/hadoop/conf/Configuration/get(java.lang.String)
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#java/nio/file/FileSystem/getPath(java.lang.String,java.lang.String[])
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTableType$/VIEW()
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/immutable/StringOps/stripPrefix(java.lang.String)
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#java/io/File/listFiles()
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/util/Utils$/isWindows()
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#java/net/URI/getFragment()
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#java/io/File/getParentFile()
org/apache/spark/sql/execution/command/LoadDataCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/get()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/2/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/Option/foreach(scala.Function1)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/2/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/PartialFunction/lift()
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/2/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/Option$/option2Iterable(scala.Option)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/2/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/execution/columnar/InMemoryTableScanExec$$anonfun$2$$anonfun$apply$1/1(org.apache.spark.sql.execution.columnar.InMemoryTableScanExec$$anonfun$2,scala.Option,org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/2/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/execution/columnar/InMemoryTableScanExec$$anonfun$2$$anonfun$apply$4/4(org.apache.spark.sql.execution.columnar.InMemoryTableScanExec$$anonfun$2)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/2/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/2/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/execution/columnar/InMemoryTableScanExec$$anonfun$2$$anonfun$3/3(org.apache.spark.sql.execution.columnar.InMemoryTableScanExec$$anonfun$2)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/2/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/Option/filter(scala.Function1)
org/apache/spark/sql/execution/columnar/InMemoryTableScanExec/anonfun/2/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTableHeader/1/apply()#org/apache/spark/sql/catalyst/parser/ParserUtils$/operationNotAllowed(java.lang.String,org.antlr.v4.runtime.ParserRuleContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTableHeader/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableHeaderContext/EXISTS()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTableHeader/1/apply()#scala/runtime/BoxesRunTime/boxToBoolean(boolean)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTableHeader/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableHeaderContext/EXTERNAL()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTableHeader/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder/visitTableIdentifier(org.apache.spark.sql.catalyst.parser.SqlBaseParser$TableIdentifierContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTableHeader/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableHeaderContext/tableIdentifier()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTableHeader/1/apply()#scala/Tuple4/Tuple4(java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTableHeader/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateTableHeaderContext/TEMPORARY()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateTableHeader/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateTableHeader$1/apply()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAddTablePartition/1/anonfun/21/apply(org.apache.spark.sql.catalyst.parser.SqlBaseParser$PartitionSpecLocationContext)#scala/Predef$/ArrowAssoc(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAddTablePartition/1/anonfun/21/apply(org.apache.spark.sql.catalyst.parser.SqlBaseParser$PartitionSpecLocationContext)#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAddTablePartition/1/anonfun/21/apply(org.apache.spark.sql.catalyst.parser.SqlBaseParser$PartitionSpecLocationContext)#scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAddTablePartition/1/anonfun/21/apply(org.apache.spark.sql.catalyst.parser.SqlBaseParser$PartitionSpecLocationContext)#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitAddTablePartition$1$$anonfun$21$$anonfun$22/22(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitAddTablePartition$1$$anonfun$21)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAddTablePartition/1/anonfun/21/apply(org.apache.spark.sql.catalyst.parser.SqlBaseParser$PartitionSpecLocationContext)#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitAddTablePartition$1/org$apache$spark$sql$execution$SparkSqlAstBuilder$$anonfun$$$outer()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAddTablePartition/1/anonfun/21/apply(org.apache.spark.sql.catalyst.parser.SqlBaseParser$PartitionSpecLocationContext)#org/apache/spark/sql/catalyst/parser/SqlBaseParser$PartitionSpecLocationContext/partitionSpec()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAddTablePartition/1/anonfun/21/apply(org.apache.spark.sql.catalyst.parser.SqlBaseParser$PartitionSpecLocationContext)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAddTablePartition/1/anonfun/21/apply(org.apache.spark.sql.catalyst.parser.SqlBaseParser$PartitionSpecLocationContext)#org/apache/spark/sql/execution/SparkSqlAstBuilder/visitNonOptionalPartitionSpec(org.apache.spark.sql.catalyst.parser.SqlBaseParser$PartitionSpecContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitAddTablePartition/1/anonfun/21/apply(org.apache.spark.sql.catalyst.parser.SqlBaseParser$PartitionSpecLocationContext)#org/apache/spark/sql/catalyst/parser/SqlBaseParser$PartitionSpecLocationContext/locationSpec()
org/apache/spark/sql/execution/debug/package/anonfun/codegenString/3/apply(scala.Tuple2)#scala/collection/mutable/HashSet/size()
org/apache/spark/sql/execution/debug/package/anonfun/codegenString/3/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/debug/package/anonfun/codegenString/3/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/codegen/CodeFormatter$/format(org.apache.spark.sql.catalyst.expressions.codegen.CodeAndComment)
org/apache/spark/sql/execution/debug/package/anonfun/codegenString/3/apply(scala.Tuple2)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/debug/package/anonfun/codegenString/3/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/debug/package/anonfun/codegenString/3/apply(scala.Tuple2)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/debug/package/anonfun/codegenString/3/apply(scala.Tuple2)#scala/Tuple2/_2$mcI$sp()
org/apache/spark/sql/execution/debug/package/anonfun/codegenString/3/apply(scala.Tuple2)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/debug/package/anonfun/codegenString/3/apply(scala.Tuple2)#scala/Tuple2/_2()
org/apache/spark/sql/execution/debug/package/anonfun/codegenString/3/apply(scala.Tuple2)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/debug/package/anonfun/codegenString/3/apply(scala.Tuple2)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/debug/package/anonfun/codegenString/3/apply(scala.Tuple2)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/debug/package/anonfun/codegenString/3/apply(scala.Tuple2)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/debug/package/anonfun/codegenString/3/apply(scala.Tuple2)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/anonfun/apply/3/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/anonfun/apply/3/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/datasources/PreprocessTableInsertion$$anonfun$apply$3$$anonfun$applyOrElse$3/3(org.apache.spark.sql.execution.datasources.PreprocessTableInsertion$$anonfun$apply$3)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/anonfun/apply/3/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/anonfun/apply/3/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/datasources/PreprocessTableInsertion$$anonfun$apply$3$$anonfun$27/27(org.apache.spark.sql.execution.datasources.PreprocessTableInsertion$$anonfun$apply$3)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/anonfun/apply/3/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/datasources/PreprocessTableInsertion$$anonfun$apply$3$$anonfun$28/28(org.apache.spark.sql.execution.datasources.PreprocessTableInsertion$$anonfun$apply$3)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/anonfun/apply/3/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/datasources/PreprocessTableInsertion$$anonfun$apply$3$$anonfun$25/25(org.apache.spark.sql.execution.datasources.PreprocessTableInsertion$$anonfun$apply$3)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/anonfun/apply/3/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/datasources/PreprocessTableInsertion$$anonfun$apply$3$$anonfun$26/26(org.apache.spark.sql.execution.datasources.PreprocessTableInsertion$$anonfun$apply$3)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/anonfun/apply/3/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/types/StructType/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/anonfun/apply/3/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/resolved()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/anonfun/apply/3/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/CatalogRelation/tableMeta()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/anonfun/apply/3/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/query()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/anonfun/apply/3/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/plans/logical/InsertIntoTable/table()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/anonfun/apply/3/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/TableIdentifier/quotedString()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/anonfun/apply/3/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/anonfun/apply/3/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/anonfun/apply/3/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/anonfun/apply/3/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()
org/apache/spark/sql/Dataset/anonfun/dropDuplicates/1/apply()#org/apache/spark/sql/Dataset$$anonfun$dropDuplicates$1$$anonfun$36/36(org.apache.spark.sql.Dataset$$anonfun$dropDuplicates$1,scala.Function2,scala.collection.Seq)
org/apache/spark/sql/Dataset/anonfun/dropDuplicates/1/apply()#org/apache/spark/sql/catalyst/analysis/Analyzer/resolver()
org/apache/spark/sql/Dataset/anonfun/dropDuplicates/1/apply()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/Dataset/anonfun/dropDuplicates/1/apply()#scala/collection/Seq/toSet()
org/apache/spark/sql/Dataset/anonfun/dropDuplicates/1/apply()#org/apache/spark/sql/catalyst/plans/logical/Deduplicate/Deduplicate(scala.collection.Seq,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,boolean)
org/apache/spark/sql/Dataset/anonfun/dropDuplicates/1/apply()#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/output()
org/apache/spark/sql/Dataset/anonfun/dropDuplicates/1/apply()#scala/collection/immutable/Set/toSeq()
org/apache/spark/sql/Dataset/anonfun/dropDuplicates/1/apply()#org/apache/spark/sql/Dataset$$anonfun$dropDuplicates$1/apply()
org/apache/spark/sql/Dataset/anonfun/dropDuplicates/1/apply()#scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/createBaseDataset(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/createBaseDataset(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/DataSource$/apply$default$5()
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/createBaseDataset(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/DataSource$/apply$default$6()
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/createBaseDataset(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/DataSource$/apply$default$7()
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/createBaseDataset(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/createBaseDataset(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/SparkContext/hadoopFile(java.lang.String,scala.reflect.ClassTag,scala.reflect.ClassTag,scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/createBaseDataset(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource$$anonfun$6/6()
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/createBaseDataset(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/Encoders$/STRING()
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/createBaseDataset(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/createBaseDataset(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/collection/Seq/mkString(java.lang.String)
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/createBaseDataset(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/rdd/RDD/mapPartitions(scala.Function1,boolean,scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/createBaseDataset(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/createBaseDataset(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/createBaseDataset(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/DataSource$/apply$default$4()
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/createBaseDataset(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/rdd/RDD/mapPartitions$default$2()
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/createBaseDataset(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/DataSource$/apply$default$8()
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/createBaseDataset(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource$$anonfun$7/7(java.lang.String)
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/createBaseDataset(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#java/lang/Class/getName()
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/createBaseDataset(org.apache.spark.sql.SparkSession,scala.collection.Seq,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#java/nio/charset/Charset/forName(java.lang.String)
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/inferFromDataset(org.apache.spark.sql.SparkSession,org.apache.spark.sql.Dataset,scala.Option,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/rdd/RDD/mapPartitions(scala.Function1,boolean,scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/inferFromDataset(org.apache.spark.sql.SparkSession,org.apache.spark.sql.Dataset,scala.Option,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/rdd/RDD/mapPartitions$default$2()
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/inferFromDataset(org.apache.spark.sql.SparkSession,org.apache.spark.sql.Dataset,scala.Option,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource$/inferFromDataset(org.apache.spark.sql.SparkSession,org.apache.spark.sql.Dataset,scala.Option,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/inferFromDataset(org.apache.spark.sql.SparkSession,org.apache.spark.sql.Dataset,scala.Option,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource$/makeSafeHeader(java.lang.String[],boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/inferFromDataset(org.apache.spark.sql.SparkSession,org.apache.spark.sql.Dataset,scala.Option,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/inferFromDataset(org.apache.spark.sql.SparkSession,org.apache.spark.sql.Dataset,scala.Option,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource$$anonfun$5/5(org.apache.spark.sql.execution.datasources.csv.CSVOptions,java.lang.String)
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/inferFromDataset(org.apache.spark.sql.SparkSession,org.apache.spark.sql.Dataset,scala.Option,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/inferFromDataset(org.apache.spark.sql.SparkSession,org.apache.spark.sql.Dataset,scala.Option,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/Some/x()
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/inferFromDataset(org.apache.spark.sql.SparkSession,org.apache.spark.sql.Dataset,scala.Option,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#com/univocity/parsers/csv/CsvParser/CsvParser(com.univocity.parsers.csv.CsvParserSettings)
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/inferFromDataset(org.apache.spark.sql.SparkSession,org.apache.spark.sql.Dataset,scala.Option,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/inferFromDataset(org.apache.spark.sql.SparkSession,org.apache.spark.sql.Dataset,scala.Option,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#com/univocity/parsers/csv/CsvParser/parseLine(java.lang.String)
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/inferFromDataset(org.apache.spark.sql.SparkSession,org.apache.spark.sql.Dataset,scala.Option,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVInferSchema$/infer(org.apache.spark.rdd.RDD,java.lang.String[],org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/inferFromDataset(org.apache.spark.sql.SparkSession,org.apache.spark.sql.Dataset,scala.Option,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/inferFromDataset(org.apache.spark.sql.SparkSession,org.apache.spark.sql.Dataset,scala.Option,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/internal/SQLConf/caseSensitiveAnalysis()
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/inferFromDataset(org.apache.spark.sql.SparkSession,org.apache.spark.sql.Dataset,scala.Option,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/types/StructType$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/SparkPlanner/pruneFilterProject(scala.collection.Seq,scala.collection.Seq,scala.Function1,scala.Function1)#scala/collection/TraversableOnce/reduceLeftOption(scala.Function2)
org/apache/spark/sql/execution/SparkPlanner/pruneFilterProject(scala.collection.Seq,scala.collection.Seq,scala.Function1,scala.Function1)#org/apache/spark/sql/execution/SparkPlanner$$anonfun$pruneFilterProject$2/2(org.apache.spark.sql.execution.SparkPlanner,org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/SparkPlanner/pruneFilterProject(scala.collection.Seq,scala.collection.Seq,scala.Function1,scala.Function1)#org/apache/spark/sql/execution/SparkPlanner$$anonfun$pruneFilterProject$1/1(org.apache.spark.sql.execution.SparkPlanner)
org/apache/spark/sql/execution/SparkPlanner/pruneFilterProject(scala.collection.Seq,scala.collection.Seq,scala.Function1,scala.Function1)#scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/SparkPlanner/pruneFilterProject(scala.collection.Seq,scala.collection.Seq,scala.Function1,scala.Function1)#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/execution/SparkPlanner/pruneFilterProject(scala.collection.Seq,scala.collection.Seq,scala.Function1,scala.Function1)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/SparkPlanner/pruneFilterProject(scala.collection.Seq,scala.collection.Seq,scala.Function1,scala.Function1)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/SparkPlanner/pruneFilterProject(scala.collection.Seq,scala.collection.Seq,scala.Function1,scala.Function1)#org/apache/spark/sql/catalyst/expressions/AttributeSet/$plus$plus(org.apache.spark.sql.catalyst.expressions.AttributeSet)
org/apache/spark/sql/execution/SparkPlanner/pruneFilterProject(scala.collection.Seq,scala.collection.Seq,scala.Function1,scala.Function1)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/SparkPlanner/pruneFilterProject(scala.collection.Seq,scala.collection.Seq,scala.Function1,scala.Function1)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/SparkPlanner/pruneFilterProject(scala.collection.Seq,scala.collection.Seq,scala.Function1,scala.Function1)#org/apache/spark/sql/execution/SparkPlanner$$anonfun$pruneFilterProject$3/3(org.apache.spark.sql.execution.SparkPlanner,org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/SparkPlanner/pruneFilterProject(scala.collection.Seq,scala.collection.Seq,scala.Function1,scala.Function1)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/SparkPlanner/pruneFilterProject(scala.collection.Seq,scala.collection.Seq,scala.Function1,scala.Function1)#org/apache/spark/sql/execution/SparkPlanner$$anonfun$2/2(org.apache.spark.sql.execution.SparkPlanner)
org/apache/spark/sql/execution/SparkPlanner/pruneFilterProject(scala.collection.Seq,scala.collection.Seq,scala.Function1,scala.Function1)#org/apache/spark/sql/execution/SparkPlanner$$anonfun$pruneFilterProject$4/4(org.apache.spark.sql.execution.SparkPlanner,org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/SparkPlanner/pruneFilterProject(scala.collection.Seq,scala.collection.Seq,scala.Function1,scala.Function1)#org/apache/spark/sql/execution/SparkPlanner$$anonfun$pruneFilterProject$5/5(org.apache.spark.sql.execution.SparkPlanner,org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/SparkPlanner/pruneFilterProject(scala.collection.Seq,scala.collection.Seq,scala.Function1,scala.Function1)#org/apache/spark/sql/execution/SparkPlanner$$anonfun$1/1(org.apache.spark.sql.execution.SparkPlanner)
org/apache/spark/sql/execution/SparkPlanner/pruneFilterProject(scala.collection.Seq,scala.collection.Seq,scala.Function1,scala.Function1)#org/apache/spark/sql/catalyst/expressions/AttributeSet$/apply(scala.collection.Iterable)
org/apache/spark/sql/execution/SparkPlanner/pruneFilterProject(scala.collection.Seq,scala.collection.Seq,scala.Function1,scala.Function1)#org/apache/spark/sql/catalyst/expressions/AttributeSet/subsetOf(org.apache.spark.sql.catalyst.expressions.AttributeSet)
org/apache/spark/sql/execution/SparkPlanner/pruneFilterProject(scala.collection.Seq,scala.collection.Seq,scala.Function1,scala.Function1)#org/apache/spark/sql/catalyst/expressions/AttributeSet/toSeq()
org/apache/spark/sql/execution/SparkPlanner/strategies()#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/SparkPlanner/strategies()#org/apache/spark/sql/execution/SparkPlanner/InMemoryScans()
org/apache/spark/sql/execution/SparkPlanner/strategies()#org/apache/spark/sql/execution/SparkPlanner/SpecialLimits()
org/apache/spark/sql/execution/SparkPlanner/strategies()#org/apache/spark/sql/execution/SparkPlanner/BasicOperators()
org/apache/spark/sql/execution/SparkPlanner/strategies()#scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/SparkPlanner/strategies()#scala/collection/immutable/List/$colon$colon(java.lang.Object)
org/apache/spark/sql/execution/SparkPlanner/strategies()#org/apache/spark/sql/execution/SparkPlanner/Aggregation()
org/apache/spark/sql/execution/SparkPlanner/strategies()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/SparkPlanner/strategies()#scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)
org/apache/spark/sql/execution/SparkPlanner/strategies()#org/apache/spark/sql/execution/SparkPlanner/JoinSelection()
org/apache/spark/sql/execution/datasources/csv/CSVFileFormat/anonfun/buildReader/1/apply$mcVI$sp(int)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/CSVFileFormat/anonfun/buildReader/1/apply$mcVI$sp(int)#org/apache/spark/sql/types/StructField/nullable()
org/apache/spark/sql/execution/datasources/csv/CSVFileFormat/anonfun/buildReader/1/apply$mcVI$sp(int)#org/apache/spark/sql/types/StructField/dataType()
org/apache/spark/sql/execution/datasources/csv/CSVFileFormat/anonfun/buildReader/1/apply$mcVI$sp(int)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/csv/CSVFileFormat/anonfun/buildReader/1/apply$mcVI$sp(int)#org/apache/spark/sql/types/StructType/apply(int)
org/apache/spark/sql/execution/datasources/csv/CSVFileFormat/anonfun/buildReader/1/apply$mcVI$sp(int)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/csv/CSVFileFormat/anonfun/buildReader/1/apply$mcVI$sp(int)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/csv/CSVFileFormat/anonfun/buildReader/1/apply$mcVI$sp(int)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/csv/CSVFileFormat/anonfun/buildReader/1/apply$mcVI$sp(int)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/KeyValueGroupedDataset/aggUntyped(scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/KeyValueGroupedDataset/aggUntyped(scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/Alias/Alias(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String,org.apache.spark.sql.catalyst.expressions.ExprId,scala.Option,scala.Option,java.lang.Boolean)
org/apache/spark/sql/KeyValueGroupedDataset/aggUntyped(scala.collection.Seq)#scala/Predef$/assert(boolean)
org/apache/spark/sql/KeyValueGroupedDataset/aggUntyped(scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$6(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)
org/apache/spark/sql/KeyValueGroupedDataset/aggUntyped(scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$5(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)
org/apache/spark/sql/KeyValueGroupedDataset/aggUntyped(scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$4(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)
org/apache/spark/sql/KeyValueGroupedDataset/aggUntyped(scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/CreateStruct$/apply(scala.collection.Seq)
org/apache/spark/sql/KeyValueGroupedDataset/aggUntyped(scala.collection.Seq)#scala/collection/Seq/length()
org/apache/spark/sql/KeyValueGroupedDataset/aggUntyped(scala.collection.Seq)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/KeyValueGroupedDataset/aggUntyped(scala.collection.Seq)#org/apache/spark/sql/catalyst/plans/logical/Aggregate/Aggregate(scala.collection.Seq,scala.collection.Seq,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/KeyValueGroupedDataset/aggUntyped(scala.collection.Seq)#org/apache/spark/sql/catalyst/encoders/ExpressionEncoder$/tuple(scala.collection.Seq)
org/apache/spark/sql/KeyValueGroupedDataset/aggUntyped(scala.collection.Seq)#org/apache/spark/sql/KeyValueGroupedDataset$$anonfun$6/6(org.apache.spark.sql.KeyValueGroupedDataset)
org/apache/spark/sql/KeyValueGroupedDataset/aggUntyped(scala.collection.Seq)#org/apache/spark/sql/KeyValueGroupedDataset$$anonfun$5/5(org.apache.spark.sql.KeyValueGroupedDataset)
org/apache/spark/sql/KeyValueGroupedDataset/aggUntyped(scala.collection.Seq)#org/apache/spark/sql/catalyst/encoders/ExpressionEncoder/flat()
org/apache/spark/sql/KeyValueGroupedDataset/aggUntyped(scala.collection.Seq)#scala/collection/Seq/$plus$colon(java.lang.Object,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/KeyValueGroupedDataset/aggUntyped(scala.collection.Seq)#scala/collection/Seq/head()
org/apache/spark/sql/KeyValueGroupedDataset/aggUntyped(scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$3(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)
org/apache/spark/sql/execution/ExpandExec/anonfun/5/apply(scala.Tuple2)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/ExpandExec/anonfun/5/apply(scala.Tuple2)#java/lang/String/trim()
org/apache/spark/sql/execution/ExpandExec/anonfun/5/apply(scala.Tuple2)#scala/collection/Seq/indices()
org/apache/spark/sql/execution/ExpandExec/anonfun/5/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/ExpandExec/anonfun/5/apply(scala.Tuple2)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/ExpandExec/anonfun/5/apply(scala.Tuple2)#org/apache/spark/sql/execution/ExpandExec$$anonfun$5$$anonfun$apply$1/1(org.apache.spark.sql.execution.ExpandExec$$anonfun$5,scala.collection.Seq,scala.runtime.ObjectRef)
org/apache/spark/sql/execution/ExpandExec/anonfun/5/apply(scala.Tuple2)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/ExpandExec/anonfun/5/apply(scala.Tuple2)#scala/collection/immutable/Range/foreach$mVc$sp(scala.Function1)
org/apache/spark/sql/execution/ExpandExec/anonfun/5/apply(scala.Tuple2)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/ExpandExec/anonfun/5/apply(scala.Tuple2)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/ExpandExec/anonfun/5/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/ExpandExec/anonfun/5/apply(scala.Tuple2)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/ExpandExec/anonfun/5/apply(scala.Tuple2)#scala/Tuple2/_2$mcI$sp()
org/apache/spark/sql/execution/ExpandExec/anonfun/5/apply(scala.Tuple2)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/ExpandExec/anonfun/5/apply(scala.Tuple2)#scala/runtime/ObjectRef/create(java.lang.Object)
org/apache/spark/sql/execution/ExpandExec/anonfun/5/apply(scala.Tuple2)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/CodegenSupport/class/consume(org.apache.spark.sql.execution.CodegenSupport,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,java.lang.String)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/CodegenSupport/class/consume(org.apache.spark.sql.execution.CodegenSupport,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,java.lang.String)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/code()
org/apache/spark/sql/execution/CodegenSupport/class/consume(org.apache.spark.sql.execution.CodegenSupport,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,java.lang.String)#org/apache/spark/sql/execution/CodegenSupport$$anonfun$4/4(org.apache.spark.sql.execution.CodegenSupport)
org/apache/spark/sql/execution/CodegenSupport/class/consume(org.apache.spark.sql.execution.CodegenSupport,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,java.lang.String)#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/execution/CodegenSupport/class/consume(org.apache.spark.sql.execution.CodegenSupport,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,java.lang.String)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/registerComment(scala.Function0)
org/apache/spark/sql/execution/CodegenSupport/class/consume(org.apache.spark.sql.execution.CodegenSupport,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,java.lang.String)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/CodegenSupport/class/consume(org.apache.spark.sql.execution.CodegenSupport,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,java.lang.String)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/ExprCode(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/CodegenSupport/class/consume(org.apache.spark.sql.execution.CodegenSupport,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,java.lang.String)#scala/collection/Seq/zipWithIndex(scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/CodegenSupport/class/consume(org.apache.spark.sql.execution.CodegenSupport,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,java.lang.String)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/CodegenSupport/class/consume(org.apache.spark.sql.execution.CodegenSupport,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,java.lang.String)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/currentVars_$eq(scala.collection.Seq)
org/apache/spark/sql/execution/CodegenSupport/class/consume(org.apache.spark.sql.execution.CodegenSupport,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,java.lang.String)#java/lang/String/trim()
org/apache/spark/sql/execution/CodegenSupport/class/consume(org.apache.spark.sql.execution.CodegenSupport,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,java.lang.String)#scala/Predef$/assert(boolean)
org/apache/spark/sql/execution/CodegenSupport/class/consume(org.apache.spark.sql.execution.CodegenSupport,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,java.lang.String)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/value()
org/apache/spark/sql/execution/CodegenSupport/class/consume(org.apache.spark.sql.execution.CodegenSupport,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,java.lang.String)#org/apache/spark/sql/catalyst/plans/QueryPlan/output()
org/apache/spark/sql/execution/CodegenSupport/class/consume(org.apache.spark.sql.execution.CodegenSupport,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/CodegenSupport/class/consume(org.apache.spark.sql.execution.CodegenSupport,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,java.lang.String)#org/apache/spark/sql/execution/CodegenSupport$$anonfun$3/3(org.apache.spark.sql.execution.CodegenSupport)
org/apache/spark/sql/execution/CodegenSupport/class/consume(org.apache.spark.sql.execution.CodegenSupport,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/CodegenSupport/class/consume(org.apache.spark.sql.execution.CodegenSupport,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,java.lang.String)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/INPUT_ROW_$eq(java.lang.String)
org/apache/spark/sql/execution/CodegenSupport/class/consume(org.apache.spark.sql.execution.CodegenSupport,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/CodegenSupport/class/consume(org.apache.spark.sql.execution.CodegenSupport,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,java.lang.String)#org/apache/spark/sql/execution/CodegenSupport$class/org$apache$spark$sql$execution$CodegenSupport$$variablePrefix(org.apache.spark.sql.execution.CodegenSupport)
org/apache/spark/sql/execution/CodegenSupport/class/consume(org.apache.spark.sql.execution.CodegenSupport,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,java.lang.String)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/CodegenSupport/class/consume(org.apache.spark.sql.execution.CodegenSupport,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,java.lang.String)#org/apache/spark/sql/catalyst/expressions/codegen/GenerateUnsafeProjection$/createCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,boolean)
org/apache/spark/sql/execution/CodegenSupport/class/consume(org.apache.spark.sql.execution.CodegenSupport,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,java.lang.String)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/CodegenSupport/class/consume(org.apache.spark.sql.execution.CodegenSupport,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,java.lang.String)#scala/collection/Seq/length()
org/apache/spark/sql/execution/CodegenSupport/class/consume(org.apache.spark.sql.execution.CodegenSupport,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,java.lang.String)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/freshNamePrefix_$eq(java.lang.String)
org/apache/spark/sql/execution/CodegenSupport/class/consume(org.apache.spark.sql.execution.CodegenSupport,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,java.lang.String)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/CodegenSupport/class/consume(org.apache.spark.sql.execution.CodegenSupport,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,java.lang.String)#org/apache/spark/sql/execution/CodegenSupport$$anonfun$consume$1/1(org.apache.spark.sql.execution.CodegenSupport)
org/apache/spark/sql/execution/CodegenSupport/class/consume(org.apache.spark.sql.execution.CodegenSupport,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/CodegenSupport/class/consume(org.apache.spark.sql.execution.CodegenSupport,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,java.lang.String)#org/apache/spark/sql/execution/CodegenSupport$$anonfun$2/2(org.apache.spark.sql.execution.CodegenSupport,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)
org/apache/spark/sql/execution/QueryExecution/anonfun/org/apache/spark/sql/execution/QueryExecution/toHiveString/1/apply(scala.Tuple2)#org/apache/spark/sql/types/StructField/dataType()
org/apache/spark/sql/execution/QueryExecution/anonfun/org/apache/spark/sql/execution/QueryExecution/toHiveString/1/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/QueryExecution/anonfun/org/apache/spark/sql/execution/QueryExecution/toHiveString/1/apply(scala.Tuple2)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/QueryExecution/anonfun/org/apache/spark/sql/execution/QueryExecution/toHiveString/1/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/QueryExecution/anonfun/org/apache/spark/sql/execution/QueryExecution/toHiveString/1/apply(scala.Tuple2)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/QueryExecution/anonfun/org/apache/spark/sql/execution/QueryExecution/toHiveString/1/apply(scala.Tuple2)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/QueryExecution/anonfun/org/apache/spark/sql/execution/QueryExecution/toHiveString/1/apply(scala.Tuple2)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/QueryExecution/anonfun/org/apache/spark/sql/execution/QueryExecution/toHiveString/1/apply(scala.Tuple2)#scala/Tuple2/_2()
org/apache/spark/sql/execution/QueryExecution/anonfun/org/apache/spark/sql/execution/QueryExecution/toHiveString/1/apply(scala.Tuple2)#org/apache/spark/sql/types/StructField/name()
org/apache/spark/sql/execution/QueryExecution/anonfun/org/apache/spark/sql/execution/QueryExecution/toHiveString/1/apply(scala.Tuple2)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/expressions()
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/execution/command/ViewHelper$$anonfun$checkCyclicViewReference$1/1(org.apache.spark.sql.catalyst.TableIdentifier,scala.collection.Seq)
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#scala/collection/Seq/foreach(scala.Function1)
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/children()
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/execution/command/ViewHelper$$anonfun$checkCyclicViewReference$3/3(scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/plans/logical/View/desc()
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/execution/command/ViewHelper$$anonfun$checkCyclicViewReference$2/2(scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/plans/logical/View/children()
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#scala/collection/Seq/$colon$plus(java.lang.Object,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#scala/collection/Seq/mkString(java.lang.String)
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/execution/command/ViewHelper$/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/ViewHelper/generateViewProperties(scala.collection.immutable.Map,org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/getCurrentDatabase()
org/apache/spark/sql/execution/command/ViewHelper/generateViewProperties(scala.collection.immutable.Map,org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/command/ViewHelper$/generateQueryColumnNames(scala.collection.Seq)
org/apache/spark/sql/execution/command/ViewHelper/generateViewProperties(scala.collection.immutable.Map,org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/types/StructType/fieldNames()
org/apache/spark/sql/execution/command/ViewHelper/generateViewProperties(scala.collection.immutable.Map,org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/ViewHelper/generateViewProperties(scala.collection.immutable.Map,org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/schema()
org/apache/spark/sql/execution/command/ViewHelper/generateViewProperties(scala.collection.immutable.Map,org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/command/ViewHelper$/generateViewDefaultDatabase(java.lang.String)
org/apache/spark/sql/execution/command/ViewHelper/generateViewProperties(scala.collection.immutable.Map,org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/mutable/ArrayOps/distinct()
org/apache/spark/sql/execution/command/ViewHelper/generateViewProperties(scala.collection.immutable.Map,org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/immutable/Map/$plus$plus(scala.collection.GenTraversableOnce)
org/apache/spark/sql/execution/command/ViewHelper/generateViewProperties(scala.collection.immutable.Map,org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/command/ViewHelper/generateViewProperties(scala.collection.immutable.Map,org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/command/ViewHelper$$anonfun$generateViewProperties$1/1(java.lang.String[])
org/apache/spark/sql/execution/command/ViewHelper/generateViewProperties(scala.collection.immutable.Map,org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/command/ViewHelper$/generateViewProperties(scala.collection.immutable.Map,org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/command/ViewHelper/generateViewProperties(scala.collection.immutable.Map,org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/mutable/ArrayOps/size()
org/apache/spark/sql/execution/command/ViewHelper/generateViewProperties(scala.collection.immutable.Map,org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/command/ViewHelper$/removeQueryColumnNames(scala.collection.immutable.Map)
org/apache/spark/sql/execution/command/ViewHelper/generateViewProperties(scala.collection.immutable.Map,org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Predef$/assert(boolean,scala.Function0)
org/apache/spark/sql/execution/command/ViewHelper/generateQueryColumnNames(scala.collection.Seq)#scala/collection/Seq/zipWithIndex(scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/ViewHelper/generateQueryColumnNames(scala.collection.Seq)#scala/collection/mutable/HashMap/put(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/command/ViewHelper/generateQueryColumnNames(scala.collection.Seq)#scala/collection/IterableLike/foreach(scala.Function1)
org/apache/spark/sql/execution/command/ViewHelper/generateQueryColumnNames(scala.collection.Seq)#org/apache/spark/sql/catalyst/catalog/CatalogTable$/VIEW_QUERY_OUTPUT_NUM_COLUMNS()
org/apache/spark/sql/execution/command/ViewHelper/generateQueryColumnNames(scala.collection.Seq)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/command/ViewHelper/generateQueryColumnNames(scala.collection.Seq)#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/execution/command/ViewHelper/generateQueryColumnNames(scala.collection.Seq)#scala/Predef$/$conforms()
org/apache/spark/sql/execution/command/ViewHelper/generateQueryColumnNames(scala.collection.Seq)#scala/collection/Seq/length()
org/apache/spark/sql/execution/command/ViewHelper/generateQueryColumnNames(scala.collection.Seq)#org/apache/spark/sql/execution/command/ViewHelper$$anonfun$generateQueryColumnNames$1/1(scala.collection.mutable.HashMap)
org/apache/spark/sql/execution/command/ViewHelper/generateQueryColumnNames(scala.collection.Seq)#scala/collection/mutable/HashMap/HashMap()
org/apache/spark/sql/execution/command/ViewHelper/generateQueryColumnNames(scala.collection.Seq)#scala/collection/mutable/HashMap/toMap(scala.Predef$$less$colon$less)
org/apache/spark/sql/execution/command/ViewHelper/generateQueryColumnNames(scala.collection.Seq)#java/lang/Object/toString()
org/apache/spark/sql/execution/command/ViewHelper/generateQueryColumnNames(scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/command/ListFilesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/SparkContext/listFiles()
org/apache/spark/sql/execution/command/ListFilesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/ListFilesCommand$$anonfun$run$1/1(org.apache.spark.sql.execution.command.ListFilesCommand,scala.collection.Seq)
org/apache/spark/sql/execution/command/ListFilesCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/command/ListFilesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/ListFilesCommand$$anonfun$run$4/4(org.apache.spark.sql.execution.command.ListFilesCommand)
org/apache/spark/sql/execution/command/ListFilesCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/TraversableLike/collect(scala.PartialFunction,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/ListFilesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/ListFilesCommand$$anonfun$run$3/3(org.apache.spark.sql.execution.command.ListFilesCommand)
org/apache/spark/sql/execution/command/ListFilesCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/ListFilesCommand$$anonfun$run$2/2(org.apache.spark.sql.execution.command.ListFilesCommand)
org/apache/spark/sql/execution/command/ListFilesCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq/size()
org/apache/spark/sql/execution/command/ListFilesCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/ListFilesCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/streaming/ProcessingTime/apply(java.lang.String)#org/apache/spark/sql/streaming/ProcessingTime$/apply(java.lang.String)
org/apache/spark/sql/streaming/ProcessingTime/apply(java.lang.String)#org/apache/commons/lang3/StringUtils/isBlank(java.lang.CharSequence)
org/apache/spark/sql/streaming/ProcessingTime/apply(java.lang.String)#org/apache/spark/unsafe/types/CalendarInterval/fromString(java.lang.String)
org/apache/spark/sql/streaming/ProcessingTime/apply(java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/streaming/ProcessingTime/apply(java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/streaming/ProcessingTime/apply(java.lang.String)#java/lang/String/startsWith(java.lang.String)
org/apache/spark/sql/streaming/ProcessingTime/apply(java.lang.String)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/streaming/ProcessingTime/apply(java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/streaming/ProcessingTime/apply(java.lang.String)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/streaming/ProcessingTime/apply(java.lang.String)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/streaming/ProcessingTime/apply(java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/streaming/ProcessingTime/apply(java.lang.String)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/anonfun/deleteExpiredLog/2/apply()#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog$$anonfun$deleteExpiredLog$2/apply()
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/anonfun/deleteExpiredLog/2/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/anonfun/deleteExpiredLog/2/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/anonfun/deleteExpiredLog/2/apply()#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/anonfun/deleteExpiredLog/2/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/anonfun/deleteExpiredLog/2/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/anonfun/deleteExpiredLog/2/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/anonfun/deleteExpiredLog/2/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog/anonfun/deleteExpiredLog/2/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/joins/UnsafeCartesianRDD/compute(org.apache.spark.Partition,org.apache.spark.TaskContext)#org/apache/spark/rdd/CartesianPartition/s1()
org/apache/spark/sql/execution/joins/UnsafeCartesianRDD/compute(org.apache.spark.Partition,org.apache.spark.TaskContext)#org/apache/spark/rdd/CartesianPartition/s2()
org/apache/spark/sql/execution/joins/UnsafeCartesianRDD/compute(org.apache.spark.Partition,org.apache.spark.TaskContext)#scala/collection/Iterator/foreach(scala.Function1)
org/apache/spark/sql/execution/joins/UnsafeCartesianRDD/compute(org.apache.spark.Partition,org.apache.spark.TaskContext)#scala/collection/Iterator/flatMap(scala.Function1)
org/apache/spark/sql/execution/joins/UnsafeCartesianRDD/compute(org.apache.spark.Partition,org.apache.spark.TaskContext)#org/apache/spark/sql/execution/joins/UnsafeCartesianRDD/rdd1()
org/apache/spark/sql/execution/joins/UnsafeCartesianRDD/compute(org.apache.spark.Partition,org.apache.spark.TaskContext)#org/apache/spark/sql/execution/joins/UnsafeCartesianRDD/rdd2()
org/apache/spark/sql/execution/joins/UnsafeCartesianRDD/compute(org.apache.spark.Partition,org.apache.spark.TaskContext)#org/apache/spark/sql/execution/joins/UnsafeCartesianRDD$$anonfun$compute$2/2(org.apache.spark.sql.execution.joins.UnsafeCartesianRDD,org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray)
org/apache/spark/sql/execution/joins/UnsafeCartesianRDD/compute(org.apache.spark.Partition,org.apache.spark.TaskContext)#org/apache/spark/sql/execution/joins/UnsafeCartesianRDD$$anonfun$compute$1/1(org.apache.spark.sql.execution.joins.UnsafeCartesianRDD,org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray)
org/apache/spark/sql/execution/joins/UnsafeCartesianRDD/compute(org.apache.spark.Partition,org.apache.spark.TaskContext)#org/apache/spark/util/CompletionIterator$/apply(scala.collection.Iterator,scala.Function0)
org/apache/spark/sql/execution/joins/UnsafeCartesianRDD/compute(org.apache.spark.Partition,org.apache.spark.TaskContext)#org/apache/spark/rdd/RDD/iterator(org.apache.spark.Partition,org.apache.spark.TaskContext)
org/apache/spark/sql/execution/joins/UnsafeCartesianRDD/compute(org.apache.spark.Partition,org.apache.spark.TaskContext)#org/apache/spark/sql/execution/joins/UnsafeCartesianRDD$$anonfun$1/1(org.apache.spark.sql.execution.joins.UnsafeCartesianRDD,org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/4/apply(scala.Tuple2)#scala/collection/Seq/apply(int)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/4/apply(scala.Tuple2)#java/lang/String/trim()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/4/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/4/apply(scala.Tuple2)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/4/apply(scala.Tuple2)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/4/apply(scala.Tuple2)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/4/apply(scala.Tuple2)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/4/apply(scala.Tuple2)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/4/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/genComp(org.apache.spark.sql.types.DataType,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/4/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/value()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/4/apply(scala.Tuple2)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/4/apply(scala.Tuple2)#scala/Tuple2/_2()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/4/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/4/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/Expression/dataType()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/4/apply(scala.Tuple2)#scala/Tuple2/_2$mcI$sp()
org/apache/spark/sql/execution/joins/SortMergeJoinExec/anonfun/4/apply(scala.Tuple2)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/datasources/FileScanRDD/getPreferredLocations(org.apache.spark.Partition)#scala/collection/mutable/HashMap$/empty()
org/apache/spark/sql/execution/datasources/FileScanRDD/getPreferredLocations(org.apache.spark.Partition)#org/apache/spark/sql/execution/datasources/FileScanRDD$$anonfun$getPreferredLocations$2/2(org.apache.spark.sql.execution.datasources.FileScanRDD)
org/apache/spark/sql/execution/datasources/FileScanRDD/getPreferredLocations(org.apache.spark.Partition)#scala/collection/Seq/sortBy(scala.Function1,scala.math.Ordering)
org/apache/spark/sql/execution/datasources/FileScanRDD/getPreferredLocations(org.apache.spark.Partition)#scala/collection/Seq/foreach(scala.Function1)
org/apache/spark/sql/execution/datasources/FileScanRDD/getPreferredLocations(org.apache.spark.Partition)#org/apache/spark/sql/execution/datasources/FileScanRDD$$anonfun$getPreferredLocations$1/1(org.apache.spark.sql.execution.datasources.FileScanRDD,scala.collection.mutable.HashMap)
org/apache/spark/sql/execution/datasources/FileScanRDD/getPreferredLocations(org.apache.spark.Partition)#scala/collection/mutable/HashMap/toSeq()
org/apache/spark/sql/execution/datasources/FileScanRDD/getPreferredLocations(org.apache.spark.Partition)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/FileScanRDD/getPreferredLocations(org.apache.spark.Partition)#scala/collection/IterableLike/take(int)
org/apache/spark/sql/execution/datasources/FileScanRDD/getPreferredLocations(org.apache.spark.Partition)#scala/collection/SeqLike/reverse()
org/apache/spark/sql/execution/datasources/FileScanRDD/getPreferredLocations(org.apache.spark.Partition)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/FileScanRDD/getPreferredLocations(org.apache.spark.Partition)#org/apache/spark/sql/execution/datasources/FileScanRDD$$anonfun$getPreferredLocations$3/3(org.apache.spark.sql.execution.datasources.FileScanRDD)
org/apache/spark/sql/execution/ColumnarBatchScan/anonfun/3/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/ColumnarBatchScan/anonfun/3/apply(scala.Tuple2)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/ColumnarBatchScan/anonfun/3/apply(scala.Tuple2)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/ColumnarBatchScan/anonfun/3/apply(scala.Tuple2)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/ColumnarBatchScan/anonfun/3/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/ColumnarBatchScan/anonfun/3/apply(scala.Tuple2)#scala/Tuple2/_2$mcI$sp()
org/apache/spark/sql/execution/ColumnarBatchScan/anonfun/3/apply(scala.Tuple2)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/ColumnarBatchScan/anonfun/3/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addMutableState(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/ColumnarBatchScan/anonfun/3/apply(scala.Tuple2)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/columnar/ColumnType/apply(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/columnar/COMPACT_DECIMAL$/apply(org.apache.spark.sql.types.DecimalType)
org/apache/spark/sql/execution/columnar/ColumnType/apply(org.apache.spark.sql.types.DataType)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/columnar/ColumnType/apply(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/columnar/LARGE_DECIMAL$/apply(org.apache.spark.sql.types.DecimalType)
org/apache/spark/sql/execution/columnar/ColumnType/apply(org.apache.spark.sql.types.DataType)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/columnar/ColumnType/apply(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/UserDefinedType/sqlType()
org/apache/spark/sql/execution/columnar/ColumnType/apply(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/Decimal$/MAX_LONG_DIGITS()
org/apache/spark/sql/execution/columnar/ColumnType/apply(org.apache.spark.sql.types.DataType)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/columnar/ColumnType/apply(org.apache.spark.sql.types.DataType)#java/lang/Exception/Exception(java.lang.String)
org/apache/spark/sql/execution/columnar/ColumnType/apply(org.apache.spark.sql.types.DataType)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/columnar/ColumnType/apply(org.apache.spark.sql.types.DataType)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/columnar/ColumnType/apply(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/DecimalType/precision()
org/apache/spark/sql/execution/columnar/ColumnType/apply(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/columnar/ColumnType$/apply(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/anonfun/findColumnByName/2/apply()#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/anonfun/findColumnByName/2/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/anonfun/findColumnByName/2/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/anonfun/findColumnByName/2/apply()#org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand$$anonfun$findColumnByName$2/apply()
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/anonfun/findColumnByName/2/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/anonfun/findColumnByName/2/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/anonfun/findColumnByName/2/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/anonfun/findColumnByName/2/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/anonfun/findColumnByName/2/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/command/AlterTableChangeColumnCommand/anonfun/findColumnByName/2/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry/register(java.lang.String)#scala/Option/isDefined()
org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry/register(java.lang.String)#scala/Option/isEmpty()
org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry/register(java.lang.String)#java/sql/DriverManager/registerDriver(java.sql.Driver)
org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry/register(java.lang.String)#org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry$$anonfun$register$1/1(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry/register(java.lang.String)#org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry$/logTrace(scala.Function0)
org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry/register(java.lang.String)#java/lang/Class/getClassLoader()
org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry/register(java.lang.String)#org/apache/spark/util/Utils$/getContextOrSparkClassLoader()
org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry/register(java.lang.String)#org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry$/wrapperMap()
org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry/register(java.lang.String)#java/lang/Class/newInstance()
org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry/register(java.lang.String)#scala/collection/mutable/Map/update(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry/register(java.lang.String)#java/lang/ClassLoader/loadClass(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry/register(java.lang.String)#org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry$$anonfun$register$2/2(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry/register(java.lang.String)#scala/collection/mutable/Map/get(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry/register(java.lang.String)#org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry$/register(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry/register(java.lang.String)#org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry$$anonfun$register$3/3(java.lang.String)
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#scala/collection/Seq/zipWithIndex(scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#scala/collection/Seq/isEmpty()
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#scala/collection/immutable/Range/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#scala/collection/Seq/head()
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#scala/collection/Seq/size()
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#scala/collection/SeqLike/distinct()
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#org/apache/spark/sql/execution/datasources/PartitioningUtils$PartitionValues/columnNames()
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#scala/runtime/RichInt$/until$extension0(int,int)
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#scala/Predef$/intWrapper(int)
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#scala/Predef$/assert(boolean,scala.Function0)
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$resolvePartitions$1/1(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#scala/collection/Seq$/empty()
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$13/13()
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$14/14()
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$15/15(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#scala/collection/immutable/IndexedSeq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$resolvePartitions$2/2(scala.collection.immutable.IndexedSeq)
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#org/apache/spark/sql/execution/datasources/PartitioningUtils$/resolvePartitions(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#scala/collection/SeqLike/length()
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#scala/collection/immutable/Iterable/mkString(java.lang.String)
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$1/1()
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$normalizePartitionSpec$1/1()
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#scala/collection/Seq/length()
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#scala/collection/Seq/toMap(scala.Predef$$less$colon$less)
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$11/11()
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$12/12()
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#scala/collection/immutable/Map/toSeq()
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#scala/collection/immutable/Iterable$/canBuildFrom()
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#scala/Predef$/$conforms()
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#scala/collection/TraversableLike/groupBy(scala.Function1)
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$8/8(scala.collection.Seq,java.lang.String,scala.Function2)
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#scala/collection/SeqLike/distinct()
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#org/apache/spark/sql/execution/datasources/PartitioningUtils$/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#scala/collection/immutable/Map/collect(scala.PartialFunction,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PartitioningUtils/validatePartitionColumn(org.apache.spark.sql.types.StructType,scala.collection.Seq,boolean)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/PartitioningUtils/validatePartitionColumn(org.apache.spark.sql.types.StructType,scala.collection.Seq,boolean)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/PartitioningUtils/validatePartitionColumn(org.apache.spark.sql.types.StructType,scala.collection.Seq,boolean)#org/apache/spark/sql/execution/datasources/PartitioningUtils$/validatePartitionColumn(org.apache.spark.sql.types.StructType,scala.collection.Seq,boolean)
org/apache/spark/sql/execution/datasources/PartitioningUtils/validatePartitionColumn(org.apache.spark.sql.types.StructType,scala.collection.Seq,boolean)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$validatePartitionColumn$1/1()
org/apache/spark/sql/execution/datasources/PartitioningUtils/validatePartitionColumn(org.apache.spark.sql.types.StructType,scala.collection.Seq,boolean)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/PartitioningUtils/validatePartitionColumn(org.apache.spark.sql.types.StructType,scala.collection.Seq,boolean)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/PartitioningUtils/validatePartitionColumn(org.apache.spark.sql.types.StructType,scala.collection.Seq,boolean)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/PartitioningUtils/validatePartitionColumn(org.apache.spark.sql.types.StructType,scala.collection.Seq,boolean)#org/apache/spark/sql/execution/datasources/PartitioningUtils$/partitionColumnsSchema(org.apache.spark.sql.types.StructType,scala.collection.Seq,boolean)
org/apache/spark/sql/execution/datasources/PartitioningUtils/validatePartitionColumn(org.apache.spark.sql.types.StructType,scala.collection.Seq,boolean)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PartitioningUtils/validatePartitionColumn(org.apache.spark.sql.types.StructType,scala.collection.Seq,boolean)#org/apache/spark/sql/types/StructType/foreach(scala.Function1)
org/apache/spark/sql/execution/datasources/PartitioningUtils/validatePartitionColumn(org.apache.spark.sql.types.StructType,scala.collection.Seq,boolean)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PartitioningUtils/validatePartitionColumn(org.apache.spark.sql.types.StructType,scala.collection.Seq,boolean)#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/execution/datasources/PartitioningUtils/validatePartitionColumn(org.apache.spark.sql.types.StructType,scala.collection.Seq,boolean)#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/execution/datasources/PartitioningUtils/validatePartitionColumn(org.apache.spark.sql.types.StructType,scala.collection.Seq,boolean)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/PartitioningUtils/validatePartitionColumn(org.apache.spark.sql.types.StructType,scala.collection.Seq,boolean)#scala/collection/Seq/size()
org/apache/spark/sql/execution/streaming/IncrementalExecution/anon/2/anonfun/apply/1/applyOrElse(org.apache.spark.sql.execution.SparkPlan,scala.Function1)#scala/Tuple2/_1()
org/apache/spark/sql/execution/streaming/IncrementalExecution/anon/2/anonfun/apply/1/applyOrElse(org.apache.spark.sql.execution.SparkPlan,scala.Function1)#scala/Tuple2/_2()
org/apache/spark/sql/execution/streaming/IncrementalExecution/anon/2/anonfun/apply/1/applyOrElse(org.apache.spark.sql.execution.SparkPlan,scala.Function1)#scala/Option/isEmpty()
org/apache/spark/sql/execution/streaming/IncrementalExecution/anon/2/anonfun/apply/1/applyOrElse(org.apache.spark.sql.execution.SparkPlan,scala.Function1)#org/apache/spark/sql/execution/UnaryExecNode$/unapply(java.lang.Object)
org/apache/spark/sql/execution/streaming/IncrementalExecution/anon/2/anonfun/apply/1/applyOrElse(org.apache.spark.sql.execution.SparkPlan,scala.Function1)#java/util/concurrent/atomic/AtomicInteger/getAndIncrement()
org/apache/spark/sql/execution/streaming/IncrementalExecution/anon/2/anonfun/apply/1/applyOrElse(org.apache.spark.sql.execution.SparkPlan,scala.Function1)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/streaming/IncrementalExecution/anon/2/anonfun/apply/1/applyOrElse(org.apache.spark.sql.execution.SparkPlan,scala.Function1)#scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)
org/apache/spark/sql/execution/streaming/IncrementalExecution/anon/2/anonfun/apply/1/applyOrElse(org.apache.spark.sql.execution.SparkPlan,scala.Function1)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/streaming/IncrementalExecution/anon/2/anonfun/apply/1/applyOrElse(org.apache.spark.sql.execution.SparkPlan,scala.Function1)#org/apache/spark/sql/execution/SparkPlan/withNewChildren(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/IncrementalExecution/anon/2/anonfun/apply/1/applyOrElse(org.apache.spark.sql.execution.SparkPlan,scala.Function1)#org/apache/spark/sql/execution/streaming/IncrementalExecution$$anon$2/org$apache$spark$sql$execution$streaming$IncrementalExecution$$anon$$$outer()
org/apache/spark/sql/execution/streaming/IncrementalExecution/anon/2/anonfun/apply/1/applyOrElse(org.apache.spark.sql.execution.SparkPlan,scala.Function1)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/streaming/IncrementalExecution/anon/2/anonfun/apply/1/applyOrElse(org.apache.spark.sql.execution.SparkPlan,scala.Function1)#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/execution/streaming/IncrementalExecution/anon/2/anonfun/apply/1/applyOrElse(org.apache.spark.sql.execution.SparkPlan,scala.Function1)#scala/Option/get()
org/apache/spark/sql/execution/streaming/FileStreamSourceOffset/apply(org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/streaming/FileStreamSourceOffset$/apply(org.apache.spark.sql.execution.streaming.Offset)
org/apache/spark/sql/execution/streaming/FileStreamSourceOffset/apply(org.apache.spark.sql.execution.streaming.Offset)#scala/util/control/Exception$Catch/opt(scala.Function0)
org/apache/spark/sql/execution/streaming/FileStreamSourceOffset/apply(org.apache.spark.sql.execution.streaming.Offset)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/FileStreamSourceOffset/apply(org.apache.spark.sql.execution.streaming.Offset)#scala/util/control/Exception$/catching(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/FileStreamSourceOffset/apply(org.apache.spark.sql.execution.streaming.Offset)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/FileStreamSourceOffset/apply(org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/streaming/FileStreamSourceOffset$$anonfun$apply$1/1(java.lang.String)
org/apache/spark/sql/execution/streaming/FileStreamSourceOffset/apply(org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/streaming/FileStreamSourceOffset$$anonfun$apply$2/2(java.lang.String)
org/apache/spark/sql/execution/streaming/FileStreamSourceOffset/apply(org.apache.spark.sql.execution.streaming.Offset)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/FileStreamSourceOffset/apply(org.apache.spark.sql.execution.streaming.Offset)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/execution/streaming/FileStreamSourceOffset/apply(org.apache.spark.sql.execution.streaming.Offset)#java/lang/Object/getClass()
org/apache/spark/sql/execution/streaming/FileStreamSourceOffset/apply(org.apache.spark.sql.execution.streaming.Offset)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/streaming/FileStreamSourceOffset/apply(org.apache.spark.sql.execution.streaming.Offset)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/Dataset/anonfun/4/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/outputSet()
org/apache/spark/sql/Dataset/anonfun/4/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/encoders/ExpressionEncoder/flat()
org/apache/spark/sql/Dataset/anonfun/4/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#scala/collection/Seq/indexWhere(scala.Function1)
org/apache/spark/sql/Dataset/anonfun/4/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/expressions/AttributeSet/contains(org.apache.spark.sql.catalyst.expressions.NamedExpression)
org/apache/spark/sql/Dataset/anonfun/4/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#scala/collection/Seq/head()
org/apache/spark/sql/Dataset/anonfun/4/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/expressions/GetStructField/GetStructField(org.apache.spark.sql.catalyst.expressions.Expression,int,scala.Option)
org/apache/spark/sql/Dataset/anonfun/4/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/Dataset/anonfun/4/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/expressions/GetStructField$/apply$default$3()
org/apache/spark/sql/Dataset/anonfun/4/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#scala/runtime/ObjectRef/create(java.lang.Object)
org/apache/spark/sql/Dataset/anonfun/4/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/plans/logical/Join/right()
org/apache/spark/sql/Dataset/anonfun/4/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/plans/logical/Project/output()
org/apache/spark/sql/Dataset/anonfun/4/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/Dataset$$anonfun$4$$anonfun$19/19(org.apache.spark.sql.Dataset$$anonfun$4,scala.runtime.ObjectRef)
org/apache/spark/sql/Dataset/anonfun/4/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/Dataset$$anonfun$4$$anonfun$18/18(org.apache.spark.sql.Dataset$$anonfun$4,scala.runtime.ObjectRef)
org/apache/spark/sql/Dataset/anonfun/4/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/plans/logical/Join/left()
org/apache/spark/sql/Dataset/anonfun/4/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/output()
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#org/apache/spark/sql/types/StructType/toAttributes()
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#scala/Predef$/require(boolean,scala.Function0)
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#org/apache/spark/sql/Row$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#org/apache/spark/sql/execution/stat/FrequentItems$$anonfun$7/7()
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#org/apache/spark/sql/execution/stat/FrequentItems$$anonfun$1/1(int)
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#scala/collection/Seq/length()
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#org/apache/spark/sql/execution/stat/FrequentItems$$anonfun$2/2(org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#org/apache/spark/sql/execution/stat/FrequentItems$$anonfun$singlePassFreqItems$1/1(double)
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#org/apache/spark/sql/types/StructType/StructType(org.apache.spark.sql.types.StructField[])
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#org/apache/spark/rdd/RDD/aggregate(java.lang.Object,scala.Function2,scala.Function2,scala.reflect.ClassTag)
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#org/apache/spark/sql/execution/stat/FrequentItems$$anonfun$3/3()
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#scala/collection/Seq$/tabulate(int,scala.Function1)
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#org/apache/spark/sql/execution/stat/FrequentItems$$anonfun$5/5(int)
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#org/apache/spark/sql/execution/stat/FrequentItems$$anonfun$4/4(int)
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#org/apache/spark/sql/Dataset$/ofRows(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#org/apache/spark/sql/catalyst/plans/logical/LocalRelation$/fromExternalRows(scala.collection.Seq,scala.collection.Seq)
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#org/apache/spark/sql/execution/stat/FrequentItems$$anonfun$6/6()
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#org/apache/spark/sql/execution/stat/FrequentItems$/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/generateIterator(int)#org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter/getIterator()
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/generateIterator(int)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/generateIterator(int)#org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray$SpillableArrayIterator/SpillableArrayIterator(org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray,org.apache.spark.util.collection.unsafe.sort.UnsafeSorterIterator,int,int)
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/generateIterator(int)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/generateIterator(int)#org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray$InMemoryBufferIterator/InMemoryBufferIterator(org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray,int)
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/generateIterator(int)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/generateIterator(int)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/generateIterator(int)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/generateIterator(int)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/generateIterator(int)#java/lang/ArrayIndexOutOfBoundsException/ArrayIndexOutOfBoundsException(java.lang.String)
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/generateIterator(int)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/generateIterator(int)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/add(org.apache.spark.sql.catalyst.expressions.UnsafeRow)#scala/collection/mutable/ArrayBuffer/clear()
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/add(org.apache.spark.sql.catalyst.expressions.UnsafeRow)#scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object)
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/add(org.apache.spark.sql.catalyst.expressions.UnsafeRow)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/getBaseObject()
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/add(org.apache.spark.sql.catalyst.expressions.UnsafeRow)#org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter/create(org.apache.spark.memory.TaskMemoryManager,org.apache.spark.storage.BlockManager,org.apache.spark.serializer.SerializerManager,org.apache.spark.TaskContext,org.apache.spark.util.collection.unsafe.sort.RecordComparator,org.apache.spark.util.collection.unsafe.sort.PrefixComparator,int,long,long,boolean)
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/add(org.apache.spark.sql.catalyst.expressions.UnsafeRow)#org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray$$anonfun$add$2/2(org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray)
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/add(org.apache.spark.sql.catalyst.expressions.UnsafeRow)#scala/collection/mutable/ArrayBuffer/foreach(scala.Function1)
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/add(org.apache.spark.sql.catalyst.expressions.UnsafeRow)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/getBaseOffset()
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/add(org.apache.spark.sql.catalyst.expressions.UnsafeRow)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/numFields()
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/add(org.apache.spark.sql.catalyst.expressions.UnsafeRow)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/getSizeInBytes()
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/add(org.apache.spark.sql.catalyst.expressions.UnsafeRow)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/copy()
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/add(org.apache.spark.sql.catalyst.expressions.UnsafeRow)#org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray$$anonfun$add$1/1(org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray)
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/add(org.apache.spark.sql.catalyst.expressions.UnsafeRow)#org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter/insertRecord(java.lang.Object,long,int,long,boolean)
org/apache/spark/sql/execution/debug/package/DebugExec/anonfun/3/anon/1/next()#org/apache/spark/sql/execution/debug/package$DebugExec/output()
org/apache/spark/sql/execution/debug/package/DebugExec/anonfun/3/anon/1/next()#scala/collection/Iterator/next()
org/apache/spark/sql/execution/debug/package/DebugExec/anonfun/3/anon/1/next()#scala/collection/Seq/apply(int)
org/apache/spark/sql/execution/debug/package/DebugExec/anonfun/3/anon/1/next()#org/apache/spark/sql/execution/debug/package$DebugExec/tupleCount()
org/apache/spark/sql/execution/debug/package/DebugExec/anonfun/3/anon/1/next()#org/apache/spark/sql/execution/debug/package$DebugExec$$anonfun$3$$anon$1/next()
org/apache/spark/sql/execution/debug/package/DebugExec/anonfun/3/anon/1/next()#org/apache/spark/sql/catalyst/InternalRow/get(int,org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/debug/package/DebugExec/anonfun/3/anon/1/next()#java/lang/Class/getName()
org/apache/spark/sql/execution/debug/package/DebugExec/anonfun/3/anon/1/next()#org/apache/spark/sql/execution/debug/package$DebugExec$ColumnMetrics/elementTypes()
org/apache/spark/sql/execution/debug/package/DebugExec/anonfun/3/anon/1/next()#org/apache/spark/sql/catalyst/expressions/Expression/dataType()
org/apache/spark/sql/execution/debug/package/DebugExec/anonfun/3/anon/1/next()#org/apache/spark/util/LongAccumulator/add(long)
org/apache/spark/sql/execution/debug/package/DebugExec/anonfun/3/anon/1/next()#org/apache/spark/sql/execution/debug/package$DebugExec/numColumns()
org/apache/spark/sql/execution/debug/package/DebugExec/anonfun/3/anon/1/next()#org/apache/spark/sql/execution/debug/package$DebugExec$SetAccumulator/add(java.lang.Object)
org/apache/spark/sql/execution/debug/package/DebugExec/anonfun/3/anon/1/next()#java/lang/Object/getClass()
org/apache/spark/sql/execution/debug/package/DebugExec/anonfun/3/anon/1/next()#org/apache/spark/sql/execution/debug/package$DebugExec$$anonfun$3/org$apache$spark$sql$execution$debug$DebugExec$$anonfun$$$outer()
org/apache/spark/sql/execution/debug/package/DebugExec/anonfun/3/anon/1/next()#org/apache/spark/sql/execution/debug/package$DebugExec/columnStats()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitTablePropertyList/1/apply()#scala/collection/mutable/Buffer$/canBuildFrom()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitTablePropertyList/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$TablePropertyListContext/tableProperty()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitTablePropertyList/1/apply()#scala/collection/mutable/Buffer/toMap(scala.Predef$$less$colon$less)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitTablePropertyList/1/apply()#scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitTablePropertyList/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitTablePropertyList$1$$anonfun$17/17(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitTablePropertyList$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitTablePropertyList/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitTablePropertyList$1/apply()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitTablePropertyList/1/apply()#org/apache/spark/sql/catalyst/parser/ParserUtils$/checkDuplicateKeys(scala.collection.Seq,org.antlr.v4.runtime.ParserRuleContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitTablePropertyList/1/apply()#scala/collection/convert/Decorators$AsScala/asScala()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitTablePropertyList/1/apply()#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitTablePropertyList/1/apply()#scala/Predef$/$conforms()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/9/apply(org.apache.parquet.hadoop.Footer)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$9$$anonfun$apply$7/7(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$9)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/9/apply(org.apache.parquet.hadoop.Footer)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$9$$anonfun$apply$2/2(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$9,scala.Option)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/9/apply(org.apache.parquet.hadoop.Footer)#scala/util/Try/getOrElse(scala.Function0)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/9/apply(org.apache.parquet.hadoop.Footer)#org/apache/parquet/hadoop/Footer/getParquetMetadata()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/9/apply(org.apache.parquet.hadoop.Footer)#scala/Option$/option2Iterable(scala.Option)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/9/apply(org.apache.parquet.hadoop.Footer)#scala/collection/mutable/HashSet/$plus$eq(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/9/apply(org.apache.parquet.hadoop.Footer)#org/apache/parquet/hadoop/metadata/ParquetMetadata/getFileMetaData()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/9/apply(org.apache.parquet.hadoop.Footer)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$9$$anonfun$apply$6/6(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$9,scala.Option)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/9/apply(org.apache.parquet.hadoop.Footer)#scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/9/apply(org.apache.parquet.hadoop.Footer)#scala/Option/isEmpty()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/9/apply(org.apache.parquet.hadoop.Footer)#scala/util/Try$/apply(scala.Function0)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/9/apply(org.apache.parquet.hadoop.Footer)#scala/util/Try/recover(scala.PartialFunction)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/9/apply(org.apache.parquet.hadoop.Footer)#scala/collection/mutable/HashSet/contains(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/9/apply(org.apache.parquet.hadoop.Footer)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$/org$apache$spark$sql$execution$datasources$parquet$ParquetFileFormat$$parseParquetSchema$1(org.apache.parquet.schema.MessageType,org.apache.spark.sql.SparkSession)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/9/apply(org.apache.parquet.hadoop.Footer)#scala/collection/convert/Decorators$AsScala/asScala()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/9/apply(org.apache.parquet.hadoop.Footer)#scala/collection/immutable/Map/get(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/9/apply(org.apache.parquet.hadoop.Footer)#scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/9/apply(org.apache.parquet.hadoop.Footer)#org/apache/parquet/hadoop/metadata/FileMetaData/getSchema()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/9/apply(org.apache.parquet.hadoop.Footer)#org/apache/parquet/hadoop/metadata/FileMetaData/getKeyValueMetaData()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/9/apply(org.apache.parquet.hadoop.Footer)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/9/apply(org.apache.parquet.hadoop.Footer)#scala/util/Try/map(scala.Function1)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/9/apply(org.apache.parquet.hadoop.Footer)#scala/Predef$/$conforms()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/9/apply(org.apache.parquet.hadoop.Footer)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$9$$anonfun$apply$8/8(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$9,org.apache.parquet.hadoop.metadata.FileMetaData)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/9/apply(org.apache.parquet.hadoop.Footer)#org/apache/spark/sql/execution/datasources/parquet/ParquetReadSupport$/SPARK_METADATA_KEY()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/9/apply(org.apache.parquet.hadoop.Footer)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$9$$anonfun$apply$1/1(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$9,scala.Option)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/9/apply(org.apache.parquet.hadoop.Footer)#scala/Option/get()
org/apache/spark/sql/execution/datasources/PartitioningUtils/inferPartitionColumnValue(java.lang.String,boolean,java.util.TimeZone)#scala/util/Try/getOrElse(scala.Function0)
org/apache/spark/sql/execution/datasources/PartitioningUtils/inferPartitionColumnValue(java.lang.String,boolean,java.util.TimeZone)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$inferPartitionColumnValue$1/1(java.lang.String)
org/apache/spark/sql/execution/datasources/PartitioningUtils/inferPartitionColumnValue(java.lang.String,boolean,java.util.TimeZone)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$inferPartitionColumnValue$2/2(java.lang.String)
org/apache/spark/sql/execution/datasources/PartitioningUtils/inferPartitionColumnValue(java.lang.String,boolean,java.util.TimeZone)#scala/util/Try/orElse(scala.Function0)
org/apache/spark/sql/execution/datasources/PartitioningUtils/inferPartitionColumnValue(java.lang.String,boolean,java.util.TimeZone)#org/apache/spark/sql/catalyst/expressions/Literal$/create(java.lang.Object,org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/datasources/PartitioningUtils/inferPartitionColumnValue(java.lang.String,boolean,java.util.TimeZone)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$inferPartitionColumnValue$6/6(java.lang.String)
org/apache/spark/sql/execution/datasources/PartitioningUtils/inferPartitionColumnValue(java.lang.String,boolean,java.util.TimeZone)#scala/util/Try$/apply(scala.Function0)
org/apache/spark/sql/execution/datasources/PartitioningUtils/inferPartitionColumnValue(java.lang.String,boolean,java.util.TimeZone)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$inferPartitionColumnValue$7/7(java.lang.String)
org/apache/spark/sql/execution/datasources/PartitioningUtils/inferPartitionColumnValue(java.lang.String,boolean,java.util.TimeZone)#org/apache/spark/sql/catalyst/catalog/ExternalCatalogUtils$/unescapePathName(java.lang.String)
org/apache/spark/sql/execution/datasources/PartitioningUtils/inferPartitionColumnValue(java.lang.String,boolean,java.util.TimeZone)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$inferPartitionColumnValue$3/3(scala.util.Try)
org/apache/spark/sql/execution/datasources/PartitioningUtils/inferPartitionColumnValue(java.lang.String,boolean,java.util.TimeZone)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$22/22(java.lang.String)
org/apache/spark/sql/execution/datasources/PartitioningUtils/inferPartitionColumnValue(java.lang.String,boolean,java.util.TimeZone)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/PartitioningUtils/inferPartitionColumnValue(java.lang.String,boolean,java.util.TimeZone)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$inferPartitionColumnValue$5/5(java.lang.String,java.util.TimeZone)
org/apache/spark/sql/execution/datasources/PartitioningUtils/inferPartitionColumnValue(java.lang.String,boolean,java.util.TimeZone)#org/apache/spark/sql/catalyst/catalog/ExternalCatalogUtils$/DEFAULT_PARTITION_NAME()
org/apache/spark/sql/execution/datasources/PartitioningUtils/inferPartitionColumnValue(java.lang.String,boolean,java.util.TimeZone)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$inferPartitionColumnValue$4/4(java.lang.String)
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#scala/collection/Seq/zipWithIndex(scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#scala/collection/Seq/isEmpty()
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#scala/collection/immutable/Range/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#scala/collection/Seq/head()
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#scala/collection/Seq/size()
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#scala/collection/SeqLike/distinct()
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#org/apache/spark/sql/execution/datasources/PartitioningUtils$PartitionValues/columnNames()
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#scala/runtime/RichInt$/until$extension0(int,int)
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#scala/Predef$/intWrapper(int)
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#scala/Predef$/assert(boolean,scala.Function0)
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$resolvePartitions$1/1(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#scala/collection/Seq$/empty()
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$13/13()
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$14/14()
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$15/15(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#scala/collection/immutable/IndexedSeq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$resolvePartitions$2/2(scala.collection.immutable.IndexedSeq)
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PartitioningUtils/resolvePartitions(scala.collection.Seq)#org/apache/spark/sql/execution/datasources/PartitioningUtils$/resolvePartitions(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitions(scala.collection.Seq,boolean,scala.collection.immutable.Set,java.util.TimeZone)#scala/collection/Seq/isEmpty()
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitions(scala.collection.Seq,boolean,scala.collection.immutable.Set,java.util.TimeZone)#scala/Tuple2/_1()
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitions(scala.collection.Seq,boolean,scala.collection.immutable.Set,java.util.TimeZone)#scala/Tuple2/_2()
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitions(scala.collection.Seq,boolean,scala.collection.immutable.Set,java.util.TimeZone)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$3/3()
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitions(scala.collection.Seq,boolean,scala.collection.immutable.Set,java.util.TimeZone)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$2/2(boolean,scala.collection.immutable.Set,java.util.TimeZone)
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitions(scala.collection.Seq,boolean,scala.collection.immutable.Set,java.util.TimeZone)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$4/4()
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitions(scala.collection.Seq,boolean,scala.collection.immutable.Set,java.util.TimeZone)#scala/collection/Seq/head()
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitions(scala.collection.Seq,boolean,scala.collection.immutable.Set,java.util.TimeZone)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitions(scala.collection.Seq,boolean,scala.collection.immutable.Set,java.util.TimeZone)#org/apache/spark/sql/types/StructType$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitions(scala.collection.Seq,boolean,scala.collection.immutable.Set,java.util.TimeZone)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$7/7()
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitions(scala.collection.Seq,boolean,scala.collection.immutable.Set,java.util.TimeZone)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitions(scala.collection.Seq,boolean,scala.collection.immutable.Set,java.util.TimeZone)#org/apache/spark/sql/execution/datasources/PartitioningUtils$PartitionValues/columnNames()
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitions(scala.collection.Seq,boolean,scala.collection.immutable.Set,java.util.TimeZone)#scala/collection/SeqLike/size()
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitions(scala.collection.Seq,boolean,scala.collection.immutable.Set,java.util.TimeZone)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitions(scala.collection.Seq,boolean,scala.collection.immutable.Set,java.util.TimeZone)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitions(scala.collection.Seq,boolean,scala.collection.immutable.Set,java.util.TimeZone)#scala/Predef$/assert(boolean,scala.Function0)
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitions(scala.collection.Seq,boolean,scala.collection.immutable.Set,java.util.TimeZone)#scala/collection/Seq/zip(scala.collection.GenIterable,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitions(scala.collection.Seq,boolean,scala.collection.immutable.Set,java.util.TimeZone)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$parsePartitions$1/1(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitions(scala.collection.Seq,boolean,scala.collection.immutable.Set,java.util.TimeZone)#org/apache/spark/sql/execution/datasources/PartitioningUtils$PartitionValues/literals()
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitions(scala.collection.Seq,boolean,scala.collection.immutable.Set,java.util.TimeZone)#scala/collection/Seq/flatten(scala.Function1)
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitions(scala.collection.Seq,boolean,scala.collection.immutable.Set,java.util.TimeZone)#scala/collection/TraversableLike/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitions(scala.collection.Seq,boolean,scala.collection.immutable.Set,java.util.TimeZone)#scala/collection/generic/GenericTraversableTemplate/unzip(scala.Function1)
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitions(scala.collection.Seq,boolean,scala.collection.immutable.Set,java.util.TimeZone)#scala/collection/Seq/distinct()
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitions(scala.collection.Seq,boolean,scala.collection.immutable.Set,java.util.TimeZone)#org/apache/spark/sql/execution/datasources/PartitionSpec$/emptySpec()
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitions(scala.collection.Seq,boolean,scala.collection.immutable.Set,java.util.TimeZone)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$5/5()
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitions(scala.collection.Seq,boolean,scala.collection.immutable.Set,java.util.TimeZone)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitions(scala.collection.Seq,boolean,scala.collection.immutable.Set,java.util.TimeZone)#scala/Predef$/$conforms()
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitions(scala.collection.Seq,boolean,scala.collection.immutable.Set,java.util.TimeZone)#org/apache/spark/sql/execution/datasources/PartitioningUtils$/resolvePartitions(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitions(scala.collection.Seq,boolean,scala.collection.immutable.Set,java.util.TimeZone)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$6/6()
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartition(org.apache.hadoop.fs.Path,boolean,scala.collection.immutable.Set,java.util.TimeZone)#org/apache/hadoop/fs/Path/getParent()
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartition(org.apache.hadoop.fs.Path,boolean,scala.collection.immutable.Set,java.util.TimeZone)#org/apache/hadoop/fs/Path/getName()
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartition(org.apache.hadoop.fs.Path,boolean,scala.collection.immutable.Set,java.util.TimeZone)#scala/Option/foreach(scala.Function1)
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartition(org.apache.hadoop.fs.Path,boolean,scala.collection.immutable.Set,java.util.TimeZone)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartition(org.apache.hadoop.fs.Path,boolean,scala.collection.immutable.Set,java.util.TimeZone)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$parsePartition$1/1(scala.collection.mutable.ArrayBuffer)
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartition(org.apache.hadoop.fs.Path,boolean,scala.collection.immutable.Set,java.util.TimeZone)#scala/Tuple2/_2()
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartition(org.apache.hadoop.fs.Path,boolean,scala.collection.immutable.Set,java.util.TimeZone)#scala/collection/generic/GenericTraversableTemplate/unzip(scala.Function1)
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartition(org.apache.hadoop.fs.Path,boolean,scala.collection.immutable.Set,java.util.TimeZone)#scala/collection/mutable/ArrayBuffer/reverse()
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartition(org.apache.hadoop.fs.Path,boolean,scala.collection.immutable.Set,java.util.TimeZone)#scala/Predef$/$conforms()
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartition(org.apache.hadoop.fs.Path,boolean,scala.collection.immutable.Set,java.util.TimeZone)#org/apache/spark/sql/execution/datasources/PartitioningUtils$/parsePartitionColumn(java.lang.String,boolean,java.util.TimeZone)
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartition(org.apache.hadoop.fs.Path,boolean,scala.collection.immutable.Set,java.util.TimeZone)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartition(org.apache.hadoop.fs.Path,boolean,scala.collection.immutable.Set,java.util.TimeZone)#org/apache/spark/sql/execution/datasources/PartitioningUtils$PartitionValues/PartitionValues(scala.collection.Seq,scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartition(org.apache.hadoop.fs.Path,boolean,scala.collection.immutable.Set,java.util.TimeZone)#scala/Tuple2/_1()
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartition(org.apache.hadoop.fs.Path,boolean,scala.collection.immutable.Set,java.util.TimeZone)#scala/collection/immutable/Set/contains(java.lang.Object)
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartition(org.apache.hadoop.fs.Path,boolean,scala.collection.immutable.Set,java.util.TimeZone)#java/lang/String/toLowerCase(java.util.Locale)
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartition(org.apache.hadoop.fs.Path,boolean,scala.collection.immutable.Set,java.util.TimeZone)#scala/Option/isEmpty()
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartition(org.apache.hadoop.fs.Path,boolean,scala.collection.immutable.Set,java.util.TimeZone)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartition(org.apache.hadoop.fs.Path,boolean,scala.collection.immutable.Set,java.util.TimeZone)#scala/collection/mutable/ArrayBuffer$/empty()
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartition(org.apache.hadoop.fs.Path,boolean,scala.collection.immutable.Set,java.util.TimeZone)#scala/collection/mutable/ArrayBuffer/isEmpty()
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartition(org.apache.hadoop.fs.Path,boolean,scala.collection.immutable.Set,java.util.TimeZone)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitionColumn(java.lang.String,boolean,java.util.TimeZone)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$parsePartitionColumn$1/1(java.lang.String)
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitionColumn(java.lang.String,boolean,java.util.TimeZone)#scala/Predef$/assert(boolean,scala.Function0)
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitionColumn(java.lang.String,boolean,java.util.TimeZone)#org/apache/spark/sql/execution/datasources/PartitioningUtils$/inferPartitionColumnValue(java.lang.String,boolean,java.util.TimeZone)
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitionColumn(java.lang.String,boolean,java.util.TimeZone)#org/apache/spark/sql/catalyst/catalog/ExternalCatalogUtils$/unescapePathName(java.lang.String)
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitionColumn(java.lang.String,boolean,java.util.TimeZone)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitionColumn(java.lang.String,boolean,java.util.TimeZone)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitionColumn(java.lang.String,boolean,java.util.TimeZone)#java/lang/String/indexOf(int)
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitionColumn(java.lang.String,boolean,java.util.TimeZone)#scala/collection/immutable/StringOps/nonEmpty()
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitionColumn(java.lang.String,boolean,java.util.TimeZone)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$parsePartitionColumn$2/2(java.lang.String)
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitionColumn(java.lang.String,boolean,java.util.TimeZone)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitionColumn(java.lang.String,boolean,java.util.TimeZone)#scala/collection/immutable/StringOps/drop(int)
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitionColumn(java.lang.String,boolean,java.util.TimeZone)#scala/Predef$/ArrowAssoc(java.lang.Object)
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitionColumn(java.lang.String,boolean,java.util.TimeZone)#scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/datasources/PartitioningUtils/parsePartitionColumn(java.lang.String,boolean,java.util.TimeZone)#scala/collection/immutable/StringOps/take(int)
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#scala/collection/SeqLike/length()
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#scala/collection/immutable/Iterable/mkString(java.lang.String)
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$1/1()
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$normalizePartitionSpec$1/1()
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#scala/collection/Seq/length()
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#scala/collection/Seq/toMap(scala.Predef$$less$colon$less)
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$11/11()
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$12/12()
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#scala/collection/immutable/Map/toSeq()
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#scala/collection/immutable/Iterable$/canBuildFrom()
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#scala/Predef$/$conforms()
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#scala/collection/TraversableLike/groupBy(scala.Function1)
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$8/8(scala.collection.Seq,java.lang.String,scala.Function2)
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#scala/collection/SeqLike/distinct()
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#org/apache/spark/sql/execution/datasources/PartitioningUtils$/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/PartitioningUtils/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)#scala/collection/immutable/Map/collect(scala.PartialFunction,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PartitioningUtils/listConflictingPartitionColumns(scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/PartitioningUtils/listConflictingPartitionColumns(scala.collection.Seq)#scala/collection/Seq/sortBy(scala.Function1,scala.math.Ordering)
org/apache/spark/sql/execution/datasources/PartitioningUtils/listConflictingPartitionColumns(scala.collection.Seq)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/PartitioningUtils/listConflictingPartitionColumns(scala.collection.Seq)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$listConflictingPartitionColumns$1/1()
org/apache/spark/sql/execution/datasources/PartitioningUtils/listConflictingPartitionColumns(scala.collection.Seq)#scala/collection/TraversableOnce/mkString(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/datasources/PartitioningUtils/listConflictingPartitionColumns(scala.collection.Seq)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/PartitioningUtils/listConflictingPartitionColumns(scala.collection.Seq)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PartitioningUtils/listConflictingPartitionColumns(scala.collection.Seq)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$17/17()
org/apache/spark/sql/execution/datasources/PartitioningUtils/listConflictingPartitionColumns(scala.collection.Seq)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$18/18()
org/apache/spark/sql/execution/datasources/PartitioningUtils/listConflictingPartitionColumns(scala.collection.Seq)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$19/19()
org/apache/spark/sql/execution/datasources/PartitioningUtils/listConflictingPartitionColumns(scala.collection.Seq)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$20/20()
org/apache/spark/sql/execution/datasources/PartitioningUtils/listConflictingPartitionColumns(scala.collection.Seq)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$21/21()
org/apache/spark/sql/execution/datasources/PartitioningUtils/listConflictingPartitionColumns(scala.collection.Seq)#scala/collection/TraversableLike/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PartitioningUtils/listConflictingPartitionColumns(scala.collection.Seq)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PartitioningUtils/listConflictingPartitionColumns(scala.collection.Seq)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/PartitioningUtils/listConflictingPartitionColumns(scala.collection.Seq)#scala/collection/SeqLike/distinct()
org/apache/spark/sql/execution/datasources/PartitioningUtils/listConflictingPartitionColumns(scala.collection.Seq)#scala/collection/IterableLike/zipWithIndex(scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PartitioningUtils/listConflictingPartitionColumns(scala.collection.Seq)#org/apache/spark/sql/execution/datasources/PartitioningUtils$/groupByKey$1(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PartitioningUtils/listConflictingPartitionColumns(scala.collection.Seq)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PartitioningUtils/listConflictingPartitionColumns(scala.collection.Seq)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PartitioningUtils/listConflictingPartitionColumns(scala.collection.Seq)#scala/collection/Seq/mkString(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/datasources/PartitioningUtils/listConflictingPartitionColumns(scala.collection.Seq)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/PartitioningUtils/validatePartitionColumn(org.apache.spark.sql.types.StructType,scala.collection.Seq,boolean)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/PartitioningUtils/validatePartitionColumn(org.apache.spark.sql.types.StructType,scala.collection.Seq,boolean)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/PartitioningUtils/validatePartitionColumn(org.apache.spark.sql.types.StructType,scala.collection.Seq,boolean)#org/apache/spark/sql/execution/datasources/PartitioningUtils$/validatePartitionColumn(org.apache.spark.sql.types.StructType,scala.collection.Seq,boolean)
org/apache/spark/sql/execution/datasources/PartitioningUtils/validatePartitionColumn(org.apache.spark.sql.types.StructType,scala.collection.Seq,boolean)#org/apache/spark/sql/execution/datasources/PartitioningUtils$$anonfun$validatePartitionColumn$1/1()
org/apache/spark/sql/execution/datasources/PartitioningUtils/validatePartitionColumn(org.apache.spark.sql.types.StructType,scala.collection.Seq,boolean)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/PartitioningUtils/validatePartitionColumn(org.apache.spark.sql.types.StructType,scala.collection.Seq,boolean)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/PartitioningUtils/validatePartitionColumn(org.apache.spark.sql.types.StructType,scala.collection.Seq,boolean)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/PartitioningUtils/validatePartitionColumn(org.apache.spark.sql.types.StructType,scala.collection.Seq,boolean)#org/apache/spark/sql/execution/datasources/PartitioningUtils$/partitionColumnsSchema(org.apache.spark.sql.types.StructType,scala.collection.Seq,boolean)
org/apache/spark/sql/execution/datasources/PartitioningUtils/validatePartitionColumn(org.apache.spark.sql.types.StructType,scala.collection.Seq,boolean)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PartitioningUtils/validatePartitionColumn(org.apache.spark.sql.types.StructType,scala.collection.Seq,boolean)#org/apache/spark/sql/types/StructType/foreach(scala.Function1)
org/apache/spark/sql/execution/datasources/PartitioningUtils/validatePartitionColumn(org.apache.spark.sql.types.StructType,scala.collection.Seq,boolean)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PartitioningUtils/validatePartitionColumn(org.apache.spark.sql.types.StructType,scala.collection.Seq,boolean)#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/execution/datasources/PartitioningUtils/validatePartitionColumn(org.apache.spark.sql.types.StructType,scala.collection.Seq,boolean)#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/execution/datasources/PartitioningUtils/validatePartitionColumn(org.apache.spark.sql.types.StructType,scala.collection.Seq,boolean)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/PartitioningUtils/validatePartitionColumn(org.apache.spark.sql.types.StructType,scala.collection.Seq,boolean)#scala/collection/Seq/size()
org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry/register(java.lang.String)#scala/Option/isDefined()
org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry/register(java.lang.String)#scala/Option/isEmpty()
org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry/register(java.lang.String)#java/sql/DriverManager/registerDriver(java.sql.Driver)
org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry/register(java.lang.String)#org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry$$anonfun$register$1/1(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry/register(java.lang.String)#org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry$/logTrace(scala.Function0)
org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry/register(java.lang.String)#java/lang/Class/getClassLoader()
org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry/register(java.lang.String)#org/apache/spark/util/Utils$/getContextOrSparkClassLoader()
org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry/register(java.lang.String)#org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry$/wrapperMap()
org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry/register(java.lang.String)#java/lang/Class/newInstance()
org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry/register(java.lang.String)#scala/collection/mutable/Map/update(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry/register(java.lang.String)#java/lang/ClassLoader/loadClass(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry/register(java.lang.String)#org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry$$anonfun$register$2/2(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry/register(java.lang.String)#scala/collection/mutable/Map/get(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry/register(java.lang.String)#org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry$/register(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry/register(java.lang.String)#org/apache/spark/sql/execution/datasources/jdbc/DriverRegistry$$anonfun$register$3/3(java.lang.String)
org/apache/spark/sql/execution/columnar/ColumnType/apply(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/columnar/COMPACT_DECIMAL$/apply(org.apache.spark.sql.types.DecimalType)
org/apache/spark/sql/execution/columnar/ColumnType/apply(org.apache.spark.sql.types.DataType)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/columnar/ColumnType/apply(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/columnar/LARGE_DECIMAL$/apply(org.apache.spark.sql.types.DecimalType)
org/apache/spark/sql/execution/columnar/ColumnType/apply(org.apache.spark.sql.types.DataType)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/columnar/ColumnType/apply(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/UserDefinedType/sqlType()
org/apache/spark/sql/execution/columnar/ColumnType/apply(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/Decimal$/MAX_LONG_DIGITS()
org/apache/spark/sql/execution/columnar/ColumnType/apply(org.apache.spark.sql.types.DataType)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/columnar/ColumnType/apply(org.apache.spark.sql.types.DataType)#java/lang/Exception/Exception(java.lang.String)
org/apache/spark/sql/execution/columnar/ColumnType/apply(org.apache.spark.sql.types.DataType)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/columnar/ColumnType/apply(org.apache.spark.sql.types.DataType)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/columnar/ColumnType/apply(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/DecimalType/precision()
org/apache/spark/sql/execution/columnar/ColumnType/apply(org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/columnar/ColumnType$/apply(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/aggregate/HashMapGenerator/anonfun/genHashForKeys/1/1/apply(org.apache.spark.sql.execution.aggregate.HashMapGenerator$Buffer)#org/apache/spark/sql/execution/aggregate/HashMapGenerator$Buffer/dataType()
org/apache/spark/sql/execution/aggregate/HashMapGenerator/anonfun/genHashForKeys/1/1/apply(org.apache.spark.sql.execution.aggregate.HashMapGenerator$Buffer)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/aggregate/HashMapGenerator/anonfun/genHashForKeys/1/1/apply(org.apache.spark.sql.execution.aggregate.HashMapGenerator$Buffer)#org/apache/spark/sql/execution/aggregate/HashMapGenerator$Buffer/name()
org/apache/spark/sql/execution/aggregate/HashMapGenerator/anonfun/genHashForKeys/1/1/apply(org.apache.spark.sql.execution.aggregate.HashMapGenerator$Buffer)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/HashMapGenerator/anonfun/genHashForKeys/1/1/apply(org.apache.spark.sql.execution.aggregate.HashMapGenerator$Buffer)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/HashMapGenerator/anonfun/genHashForKeys/1/1/apply(org.apache.spark.sql.execution.aggregate.HashMapGenerator$Buffer)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/aggregate/HashMapGenerator/anonfun/genHashForKeys/1/1/apply(org.apache.spark.sql.execution.aggregate.HashMapGenerator$Buffer)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/freshName(java.lang.String)
org/apache/spark/sql/execution/aggregate/HashMapGenerator/anonfun/genHashForKeys/1/1/apply(org.apache.spark.sql.execution.aggregate.HashMapGenerator$Buffer)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/aggregate/HashMapGenerator/anonfun/genHashForKeys/1/1/apply(org.apache.spark.sql.execution.aggregate.HashMapGenerator$Buffer)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/aggregate/HashMapGenerator/anonfun/genHashForKeys/1/1/apply(org.apache.spark.sql.execution.aggregate.HashMapGenerator$Buffer)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/validatePartitionColumn/1/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/types/StructField/dataType()
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/validatePartitionColumn/1/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/validatePartitionColumn/1/apply(org.apache.spark.sql.types.StructField)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/validatePartitionColumn/1/apply(org.apache.spark.sql.types.StructField)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/validatePartitionColumn/1/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/validatePartitionColumn/1/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/validatePartitionColumn/1/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/validatePartitionColumn/1/apply(org.apache.spark.sql.types.StructField)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/validatePartitionColumn/1/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/PartitioningUtils/anonfun/validatePartitionColumn/1/apply(org.apache.spark.sql.types.StructField)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/window/AggregateProcessor/anonfun/apply/3/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/aggregate/ImperativeAggregate/withNewMutableAggBufferOffset(int)
org/apache/spark/sql/execution/window/AggregateProcessor/anonfun/apply/3/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/aggregate/DeclarativeAggregate/updateExpressions()
org/apache/spark/sql/execution/window/AggregateProcessor/anonfun/apply/3/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/aggregate/ImperativeAggregate/withNewInputAggBufferOffset(int)
org/apache/spark/sql/execution/window/AggregateProcessor/anonfun/apply/3/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/BindReferences$/bindReference$default$3()
org/apache/spark/sql/execution/window/AggregateProcessor/anonfun/apply/3/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/aggregate/ImperativeAggregate/aggBufferAttributes()
org/apache/spark/sql/execution/window/AggregateProcessor/anonfun/apply/3/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/collection/mutable/Buffer/$plus$plus$eq(scala.collection.TraversableOnce)
org/apache/spark/sql/execution/window/AggregateProcessor/anonfun/apply/3/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/aggregate/DeclarativeAggregate/aggBufferAttributes()
org/apache/spark/sql/execution/window/AggregateProcessor/anonfun/apply/3/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/window/AggregateProcessor/anonfun/apply/3/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/execution/window/AggregateProcessor$$anonfun$apply$3$$anonfun$4/4(org.apache.spark.sql.execution.window.AggregateProcessor$$anonfun$apply$3)
org/apache/spark/sql/execution/window/AggregateProcessor/anonfun/apply/3/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/window/AggregateProcessor/anonfun/apply/3/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/collection/Seq$/fill(int,scala.Function0)
org/apache/spark/sql/execution/window/AggregateProcessor/anonfun/apply/3/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/aggregate/DeclarativeAggregate/evaluateExpression()
org/apache/spark/sql/execution/window/AggregateProcessor/anonfun/apply/3/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/package$/AttributeSeq(scala.collection.Seq)
org/apache/spark/sql/execution/window/AggregateProcessor/anonfun/apply/3/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/collection/mutable/Buffer/size()
org/apache/spark/sql/execution/window/AggregateProcessor/anonfun/apply/3/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/BindReferences$/bindReference(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.package$AttributeSeq,boolean)
org/apache/spark/sql/execution/window/AggregateProcessor/anonfun/apply/3/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/window/AggregateProcessor/anonfun/apply/3/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/window/AggregateProcessor/anonfun/apply/3/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/collection/Seq/size()
org/apache/spark/sql/execution/window/AggregateProcessor/anonfun/apply/3/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/sys/package$/error(java.lang.String)
org/apache/spark/sql/execution/window/AggregateProcessor/anonfun/apply/3/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/collection/mutable/Buffer/$plus$eq(java.lang.Object)
org/apache/spark/sql/execution/window/AggregateProcessor/anonfun/apply/3/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/aggregate/DeclarativeAggregate/initialValues()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitTableFileFormat/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat$/empty()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitTableFileFormat/1/apply()#org/apache/spark/sql/catalyst/parser/ParserUtils$/string(org.antlr.v4.runtime.Token)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitTableFileFormat/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$4()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitTableFileFormat/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$5()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitTableFileFormat/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$6()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitTableFileFormat/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy(scala.Option,scala.Option,scala.Option,scala.Option,boolean,scala.collection.immutable.Map)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitTableFileFormat/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitTableFileFormat$1/apply()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitTableFileFormat/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$1()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitTableFileFormat/1/apply()#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/parseUserSpecifiedCreateTableColumnTypes/1/apply(java.lang.String)#scala/collection/mutable/ArrayOps/size()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/parseUserSpecifiedCreateTableColumnTypes/1/apply(java.lang.String)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$parseUserSpecifiedCreateTableColumnTypes$1$$anonfun$24/24(org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$org$apache$spark$sql$execution$datasources$jdbc$JdbcUtils$$parseUserSpecifiedCreateTableColumnTypes$1,java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/parseUserSpecifiedCreateTableColumnTypes/1/apply(java.lang.String)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/parseUserSpecifiedCreateTableColumnTypes/1/apply(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/parseUserSpecifiedCreateTableColumnTypes/1/apply(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/parseUserSpecifiedCreateTableColumnTypes/1/apply(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/parseUserSpecifiedCreateTableColumnTypes/1/apply(java.lang.String)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/parseUserSpecifiedCreateTableColumnTypes/1/apply(java.lang.String)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/parseUserSpecifiedCreateTableColumnTypes/1/apply(java.lang.String)#org/apache/spark/sql/types/StructType/fieldNames()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/parseUserSpecifiedCreateTableColumnTypes/1/apply(java.lang.String)#scala/collection/mutable/ArrayOps/mkString(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/parseUserSpecifiedCreateTableColumnTypes/1/apply(java.lang.String)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/parseUserSpecifiedCreateTableColumnTypes/1/apply(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/parseUserSpecifiedCreateTableColumnTypes/1/apply(java.lang.String)#scala/collection/mutable/ArrayOps/filter(scala.Function1)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/parseUserSpecifiedCreateTableColumnTypes/1/apply(java.lang.String)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/streaming/ProcessingTime/apply(java.lang.String)#org/apache/spark/sql/streaming/ProcessingTime$/apply(java.lang.String)
org/apache/spark/sql/streaming/ProcessingTime/apply(java.lang.String)#org/apache/commons/lang3/StringUtils/isBlank(java.lang.CharSequence)
org/apache/spark/sql/streaming/ProcessingTime/apply(java.lang.String)#org/apache/spark/unsafe/types/CalendarInterval/fromString(java.lang.String)
org/apache/spark/sql/streaming/ProcessingTime/apply(java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/streaming/ProcessingTime/apply(java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/streaming/ProcessingTime/apply(java.lang.String)#java/lang/String/startsWith(java.lang.String)
org/apache/spark/sql/streaming/ProcessingTime/apply(java.lang.String)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/streaming/ProcessingTime/apply(java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/streaming/ProcessingTime/apply(java.lang.String)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/streaming/ProcessingTime/apply(java.lang.String)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/streaming/ProcessingTime/apply(java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/streaming/ProcessingTime/apply(java.lang.String)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/4/anonfun/6/apply()#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$4$$anonfun$6/apply()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/4/anonfun/6/apply()#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/4/anonfun/6/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/4/anonfun/6/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/4/anonfun/6/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/4/anonfun/6/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/4/anonfun/6/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/4/anonfun/6/apply()#org/apache/spark/sql/types/StructField/name()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/4/anonfun/6/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/4/anonfun/6/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/4/anonfun/6/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/checkEndOfRowGroup()#java/io/IOException/IOException(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/checkEndOfRowGroup()#java/lang/StringBuilder/append(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/checkEndOfRowGroup()#org/apache/parquet/schema/MessageType/getColumns()
org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/checkEndOfRowGroup()#java/util/List/size()
org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/checkEndOfRowGroup()#java/lang/StringBuilder/append(long)
org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/checkEndOfRowGroup()#java/lang/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/checkEndOfRowGroup()#java/util/List/get(int)
org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/checkEndOfRowGroup()#org/apache/parquet/column/page/PageReadStore/getRowCount()
org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/checkEndOfRowGroup()#org/apache/parquet/hadoop/ParquetFileReader/readNextRowGroup()
org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/checkEndOfRowGroup()#java/lang/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/checkEndOfRowGroup()#org/apache/parquet/column/page/PageReadStore/getPageReader(org.apache.parquet.column.ColumnDescriptor)
org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/initializeInternal()#org/apache/parquet/schema/MessageType/getFields()
org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/initializeInternal()#org/apache/parquet/schema/MessageType/containsPath(java.lang.String[])
org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/initializeInternal()#java/io/IOException/IOException(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/initializeInternal()#org/apache/parquet/column/ColumnDescriptor/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/initializeInternal()#org/apache/parquet/schema/MessageType/getColumns()
org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/initializeInternal()#org/apache/parquet/schema/MessageType/getColumnDescription(java.lang.String[])
org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/initializeInternal()#java/lang/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/initializeInternal()#java/lang/UnsupportedOperationException/UnsupportedOperationException(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/initializeInternal()#java/util/List/get(int)
org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/initializeInternal()#org/apache/parquet/schema/Type/isRepetition(org.apache.parquet.schema.Type$Repetition)
org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/initializeInternal()#org/apache/parquet/column/ColumnDescriptor/getMaxDefinitionLevel()
org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/initializeInternal()#org/apache/parquet/schema/MessageType/getFieldCount()
org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/initializeInternal()#org/apache/parquet/schema/MessageType/getPaths()
org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/initializeInternal()#java/util/Arrays/toString(java.lang.Object[])
org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/initializeInternal()#java/lang/StringBuilder/append(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/initializeInternal()#java/lang/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader/initializeInternal()#org/apache/parquet/schema/Type/isPrimitive()
org/apache/spark/sql/catalog/Function/toString()#org/apache/spark/sql/catalog/Function$$anonfun$toString$9/9(org.apache.spark.sql.catalog.Function)
org/apache/spark/sql/catalog/Function/toString()#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/catalog/Function/toString()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/catalog/Function/toString()#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/catalog/Function/toString()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/catalog/Function/toString()#scala/runtime/BoxesRunTime/boxToBoolean(boolean)
org/apache/spark/sql/catalog/Function/toString()#scala/Option/map(scala.Function1)
org/apache/spark/sql/catalog/Function/toString()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/catalog/Function/toString()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/catalog/Function/toString()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/catalog/Function/toString()#org/apache/spark/sql/catalog/Function$$anonfun$toString$11/11(org.apache.spark.sql.catalog.Function)
org/apache/spark/sql/catalog/Function/toString()#org/apache/spark/sql/catalog/Function$$anonfun$toString$10/10(org.apache.spark.sql.catalog.Function)
org/apache/spark/sql/catalog/Function/toString()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/catalog/Function/toString()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/catalog/Function/toString()#org/apache/spark/sql/catalog/Function$$anonfun$toString$12/12(org.apache.spark.sql.catalog.Function)
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/expressions()
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/execution/command/ViewHelper$$anonfun$checkCyclicViewReference$1/1(org.apache.spark.sql.catalyst.TableIdentifier,scala.collection.Seq)
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#scala/collection/Seq/foreach(scala.Function1)
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/children()
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/execution/command/ViewHelper$$anonfun$checkCyclicViewReference$3/3(scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/plans/logical/View/desc()
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/execution/command/ViewHelper$$anonfun$checkCyclicViewReference$2/2(scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/catalyst/plans/logical/View/children()
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#scala/collection/Seq/$colon$plus(java.lang.Object,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#scala/collection/Seq/mkString(java.lang.String)
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/command/ViewHelper/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)#org/apache/spark/sql/execution/command/ViewHelper$/checkCyclicViewReference(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.collection.Seq,org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/ViewHelper/generateViewProperties(scala.collection.immutable.Map,org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/getCurrentDatabase()
org/apache/spark/sql/execution/command/ViewHelper/generateViewProperties(scala.collection.immutable.Map,org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/command/ViewHelper$/generateQueryColumnNames(scala.collection.Seq)
org/apache/spark/sql/execution/command/ViewHelper/generateViewProperties(scala.collection.immutable.Map,org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/types/StructType/fieldNames()
org/apache/spark/sql/execution/command/ViewHelper/generateViewProperties(scala.collection.immutable.Map,org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/ViewHelper/generateViewProperties(scala.collection.immutable.Map,org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/schema()
org/apache/spark/sql/execution/command/ViewHelper/generateViewProperties(scala.collection.immutable.Map,org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/command/ViewHelper$/generateViewDefaultDatabase(java.lang.String)
org/apache/spark/sql/execution/command/ViewHelper/generateViewProperties(scala.collection.immutable.Map,org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/mutable/ArrayOps/distinct()
org/apache/spark/sql/execution/command/ViewHelper/generateViewProperties(scala.collection.immutable.Map,org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/immutable/Map/$plus$plus(scala.collection.GenTraversableOnce)
org/apache/spark/sql/execution/command/ViewHelper/generateViewProperties(scala.collection.immutable.Map,org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/command/ViewHelper/generateViewProperties(scala.collection.immutable.Map,org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/command/ViewHelper$$anonfun$generateViewProperties$1/1(java.lang.String[])
org/apache/spark/sql/execution/command/ViewHelper/generateViewProperties(scala.collection.immutable.Map,org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/command/ViewHelper$/generateViewProperties(scala.collection.immutable.Map,org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/command/ViewHelper/generateViewProperties(scala.collection.immutable.Map,org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/mutable/ArrayOps/size()
org/apache/spark/sql/execution/command/ViewHelper/generateViewProperties(scala.collection.immutable.Map,org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/command/ViewHelper$/removeQueryColumnNames(scala.collection.immutable.Map)
org/apache/spark/sql/execution/command/ViewHelper/generateViewProperties(scala.collection.immutable.Map,org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Predef$/assert(boolean,scala.Function0)
org/apache/spark/sql/execution/TakeOrderedAndProjectExec/executeCollect()#org/apache/spark/sql/catalyst/expressions/UnsafeProjection$/create(scala.collection.Seq,scala.collection.Seq)
org/apache/spark/sql/execution/TakeOrderedAndProjectExec/executeCollect()#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/TakeOrderedAndProjectExec/executeCollect()#org/apache/spark/rdd/RDD/takeOrdered(int,scala.math.Ordering)
org/apache/spark/sql/execution/TakeOrderedAndProjectExec/executeCollect()#org/apache/spark/rdd/RDD/map(scala.Function1,scala.reflect.ClassTag)
org/apache/spark/sql/execution/TakeOrderedAndProjectExec/executeCollect()#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/TakeOrderedAndProjectExec/executeCollect()#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/TakeOrderedAndProjectExec/executeCollect()#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/TakeOrderedAndProjectExec/executeCollect()#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/TakeOrderedAndProjectExec/executeCollect()#org/apache/spark/sql/execution/TakeOrderedAndProjectExec$$anonfun$3/3(org.apache.spark.sql.execution.TakeOrderedAndProjectExec)
org/apache/spark/sql/execution/TakeOrderedAndProjectExec/executeCollect()#org/apache/spark/sql/catalyst/expressions/codegen/LazilyGeneratedOrdering/LazilyGeneratedOrdering(scala.collection.Seq,scala.collection.Seq)
org/apache/spark/sql/execution/TakeOrderedAndProjectExec/executeCollect()#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/TakeOrderedAndProjectExec/executeCollect()#org/apache/spark/sql/execution/TakeOrderedAndProjectExec$$anonfun$executeCollect$1/1(org.apache.spark.sql.execution.TakeOrderedAndProjectExec,org.apache.spark.sql.catalyst.expressions.UnsafeProjection)
org/apache/spark/sql/execution/TakeOrderedAndProjectExec/doExecute()#org/apache/spark/rdd/RDD/mapPartitions(scala.Function1,boolean,scala.reflect.ClassTag)
org/apache/spark/sql/execution/TakeOrderedAndProjectExec/doExecute()#org/apache/spark/rdd/RDD/mapPartitions$default$2()
org/apache/spark/sql/execution/TakeOrderedAndProjectExec/doExecute()#org/apache/spark/sql/execution/TakeOrderedAndProjectExec$$anonfun$4/4(org.apache.spark.sql.execution.TakeOrderedAndProjectExec)
org/apache/spark/sql/execution/TakeOrderedAndProjectExec/doExecute()#org/apache/spark/sql/execution/ShuffledRowRDD$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/TakeOrderedAndProjectExec/doExecute()#org/apache/spark/sql/execution/ShuffledRowRDD/mapPartitions(scala.Function1,boolean,scala.reflect.ClassTag)
org/apache/spark/sql/execution/TakeOrderedAndProjectExec/doExecute()#org/apache/spark/sql/catalyst/expressions/codegen/LazilyGeneratedOrdering/LazilyGeneratedOrdering(scala.collection.Seq,scala.collection.Seq)
org/apache/spark/sql/execution/TakeOrderedAndProjectExec/doExecute()#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/TakeOrderedAndProjectExec/doExecute()#org/apache/spark/sql/execution/ShuffledRowRDD/mapPartitions$default$2()
org/apache/spark/sql/execution/TakeOrderedAndProjectExec/doExecute()#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/TakeOrderedAndProjectExec/doExecute()#org/apache/spark/sql/execution/exchange/ShuffleExchange$/prepareShuffleDependency(org.apache.spark.rdd.RDD,scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning,org.apache.spark.serializer.Serializer)
org/apache/spark/sql/execution/TakeOrderedAndProjectExec/doExecute()#org/apache/spark/sql/execution/TakeOrderedAndProjectExec$$anonfun$5/5(org.apache.spark.sql.execution.TakeOrderedAndProjectExec,org.apache.spark.sql.catalyst.expressions.codegen.LazilyGeneratedOrdering)
org/apache/spark/sql/execution/TakeOrderedAndProjectExec/doExecute()#org/apache/spark/sql/execution/TakeOrderedAndProjectExec$$anonfun$doExecute$2/2(org.apache.spark.sql.execution.TakeOrderedAndProjectExec,org.apache.spark.sql.catalyst.expressions.codegen.LazilyGeneratedOrdering)
org/apache/spark/sql/execution/TakeOrderedAndProjectExec/doExecute()#org/apache/spark/rdd/RDD/map(scala.Function1,scala.reflect.ClassTag)
org/apache/spark/sql/Dataset/anonfun/dropDuplicates/1/anonfun/36/apply(java.lang.String)#scala/collection/Seq/isEmpty()
org/apache/spark/sql/Dataset/anonfun/dropDuplicates/1/anonfun/36/apply(java.lang.String)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/Dataset/anonfun/dropDuplicates/1/anonfun/36/apply(java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/Dataset/anonfun/dropDuplicates/1/anonfun/36/apply(java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/Dataset/anonfun/dropDuplicates/1/anonfun/36/apply(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/Dataset/anonfun/dropDuplicates/1/anonfun/36/apply(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/Dataset/anonfun/dropDuplicates/1/anonfun/36/apply(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/Dataset/anonfun/dropDuplicates/1/anonfun/36/apply(java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/Dataset/anonfun/dropDuplicates/1/anonfun/36/apply(java.lang.String)#org/apache/spark/sql/types/StructType/fieldNames()
org/apache/spark/sql/Dataset/anonfun/dropDuplicates/1/anonfun/36/apply(java.lang.String)#scala/collection/mutable/ArrayOps/mkString(java.lang.String)
org/apache/spark/sql/Dataset/anonfun/dropDuplicates/1/anonfun/36/apply(java.lang.String)#scala/collection/Seq/filter(scala.Function1)
org/apache/spark/sql/Dataset/anonfun/dropDuplicates/1/anonfun/36/apply(java.lang.String)#org/apache/spark/sql/Dataset$$anonfun$dropDuplicates$1/org$apache$spark$sql$Dataset$$anonfun$$$outer()
org/apache/spark/sql/Dataset/anonfun/dropDuplicates/1/anonfun/36/apply(java.lang.String)#org/apache/spark/sql/Dataset$$anonfun$dropDuplicates$1$$anonfun$36$$anonfun$37/37(org.apache.spark.sql.Dataset$$anonfun$dropDuplicates$1$$anonfun$36,java.lang.String)
org/apache/spark/sql/Dataset/anonfun/dropDuplicates/1/anonfun/36/apply(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/Dataset/anonfun/dropDuplicates/1/anonfun/36/apply(java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/Dataset/anonfun/dropDuplicates/1/anonfun/36/apply(java.lang.String)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#java/net/URI/URI(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/parquet/hadoop/ParquetRecordReader/ParquetRecordReader(org.apache.parquet.hadoop.api.ReadSupport,org.apache.parquet.filter2.compat.FilterCompat$Filter)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1$$anonfun$8/8(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/spark/sql/catalyst/expressions/JoinedRow/JoinedRow()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/hadoop/mapreduce/lib/input/FileSplit/getPath()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#scala/Some/x()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1$$anonfun$apply$5/5(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1,org.apache.spark.sql.catalyst.expressions.JoinedRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,org.apache.spark.sql.execution.datasources.PartitionedFile)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/hadoop/mapreduce/lib/input/FileSplit/FileSplit(org.apache.hadoop.fs.Path,long,long,java.lang.String[])
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/spark/sql/types/StructType/length()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#scala/Option/foreach(scala.Function1)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/parquet/filter2/compat/FilterCompat/get(org.apache.parquet.filter2.predicate.FilterPredicate,org.apache.parquet.filter.UnboundRecordFilter)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/hadoop/mapreduce/JobID/JobID()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/hadoop/mapreduce/lib/input/FileSplit/getLocations()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/spark/sql/catalyst/expressions/codegen/GenerateUnsafeProjection$/generate(java.lang.Object,scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/spark/TaskContext$/get()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/hadoop/mapreduce/lib/input/FileSplit/getStart()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/spark/broadcast/Broadcast/value()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#scala/Predef$/assert(boolean)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#scala/Option/isDefined()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/spark/sql/types/StructType/toAttributes()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1$$anonfun$7/7(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1,org.apache.spark.sql.execution.datasources.PartitionedFile)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/spark/util/SerializableConfiguration/value()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/hadoop/mapreduce/task/TaskAttemptContextImpl/TaskAttemptContextImpl(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.TaskAttemptID)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#scala/Array$/empty(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/hadoop/fs/Path/Path(java.net.URI)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/parquet/hadoop/ParquetInputFormat/setFilterPredicate(org.apache.hadoop.conf.Configuration,org.apache.parquet.filter2.predicate.FilterPredicate)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/hadoop/mapreduce/TaskAttemptID/TaskAttemptID(org.apache.hadoop.mapreduce.TaskID,int)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/parquet/hadoop/ParquetInputSplit/ParquetInputSplit(org.apache.hadoop.fs.Path,long,long,long,java.lang.String[],long[])
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/hadoop/mapreduce/lib/input/FileSplit/getLength()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/hadoop/mapreduce/task/TaskAttemptContextImpl/getConfiguration()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/parquet/hadoop/ParquetRecordReader/initialize(org.apache.hadoop.mapreduce.InputSplit,org.apache.hadoop.mapreduce.TaskAttemptContext)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/spark/sql/types/StructType/size()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/spark/sql/catalyst/InternalRow/numFields()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/hadoop/mapreduce/TaskID/TaskID(org.apache.hadoop.mapreduce.JobID,org.apache.hadoop.mapreduce.TaskType,int)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/parquet/hadoop/ParquetRecordReader/ParquetRecordReader(org.apache.parquet.hadoop.api.ReadSupport)
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#scala/Option/get()
org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat/anonfun/buildReaderWithPartitionValues/1/apply(org.apache.spark.sql.execution.datasources.PartitionedFile)#org/apache/spark/sql/execution/datasources/parquet/ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1$$anonfun$apply$3/3(org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$buildReaderWithPartitionValues$1,org.apache.spark.sql.execution.datasources.RecordReaderIterator)
org/apache/spark/sql/execution/ColumnarBatchScan/class/doProduce(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/ColumnarBatchScan/class/doProduce(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#java/lang/String/trim()
org/apache/spark/sql/execution/ColumnarBatchScan/class/doProduce(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/Seq/indices()
org/apache/spark/sql/execution/ColumnarBatchScan/class/doProduce(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/execution/ColumnarBatchScan$$anonfun$3/3(org.apache.spark.sql.execution.ColumnarBatchScan,java.lang.String,java.lang.String,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)
org/apache/spark/sql/execution/ColumnarBatchScan/class/doProduce(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/execution/ColumnarBatchScan$$anonfun$2/2(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)
org/apache/spark/sql/execution/ColumnarBatchScan/class/doProduce(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/immutable/IndexedSeq$/canBuildFrom()
org/apache/spark/sql/execution/ColumnarBatchScan/class/doProduce(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/execution/ColumnarBatchScan/metricTerm(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String)
org/apache/spark/sql/execution/ColumnarBatchScan/class/doProduce(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/ColumnarBatchScan/class/doProduce(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/freshName(java.lang.String)
org/apache/spark/sql/execution/ColumnarBatchScan/class/doProduce(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/execution/ColumnarBatchScan/isShouldStopRequired()
org/apache/spark/sql/execution/ColumnarBatchScan/class/doProduce(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/ColumnarBatchScan/class/doProduce(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/ColumnarBatchScan/class/doProduce(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/ColumnarBatchScan/class/doProduce(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/execution/ColumnarBatchScan/consume$default$3()
org/apache/spark/sql/execution/ColumnarBatchScan/class/doProduce(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/ColumnarBatchScan/class/doProduce(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/currentVars_$eq(scala.collection.Seq)
org/apache/spark/sql/execution/ColumnarBatchScan/class/doProduce(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/plans/QueryPlan/output()
org/apache/spark/sql/execution/ColumnarBatchScan/class/doProduce(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addMutableState(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/ColumnarBatchScan/class/doProduce(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/ColumnarBatchScan/class/doProduce(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/immutable/Range/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/ColumnarBatchScan/class/doProduce(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/immutable/IndexedSeq/zipWithIndex(scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/ColumnarBatchScan/class/doProduce(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/Seq/zip(scala.collection.GenIterable,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/ColumnarBatchScan/class/doProduce(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/collection/immutable/IndexedSeq/mkString(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/ColumnarBatchScan/class/doProduce(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/ColumnarBatchScan/class/doProduce(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/execution/ColumnarBatchScan/consume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,java.lang.String)
org/apache/spark/sql/execution/ColumnarBatchScan/class/doProduce(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addNewFunction(java.lang.String,java.lang.String)
org/apache/spark/sql/execution/ColumnarBatchScan/class/doProduce(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/ColumnarBatchScan/class/doProduce(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)#org/apache/spark/sql/execution/ColumnarBatchScan$$anonfun$4/4(org.apache.spark.sql.execution.ColumnarBatchScan,java.lang.String,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)
org/apache/spark/sql/execution/ColumnarBatchScan/class/org$apache$spark$sql$execution$ColumnarBatchScan$$genCodeColumnVector(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType,boolean)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/ExprCode(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/ColumnarBatchScan/class/org$apache$spark$sql$execution$ColumnarBatchScan$$genCodeColumnVector(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType,boolean)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/defaultValue(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/ColumnarBatchScan/class/org$apache$spark$sql$execution$ColumnarBatchScan$$genCodeColumnVector(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType,boolean)#java/lang/String/trim()
org/apache/spark/sql/execution/ColumnarBatchScan/class/org$apache$spark$sql$execution$ColumnarBatchScan$$genCodeColumnVector(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType,boolean)#org/apache/spark/sql/types/DataType/simpleString()
org/apache/spark/sql/execution/ColumnarBatchScan/class/org$apache$spark$sql$execution$ColumnarBatchScan$$genCodeColumnVector(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType,boolean)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/ColumnarBatchScan/class/org$apache$spark$sql$execution$ColumnarBatchScan$$genCodeColumnVector(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType,boolean)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/ColumnarBatchScan/class/org$apache$spark$sql$execution$ColumnarBatchScan$$genCodeColumnVector(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType,boolean)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/registerComment(scala.Function0)
org/apache/spark/sql/execution/ColumnarBatchScan/class/org$apache$spark$sql$execution$ColumnarBatchScan$$genCodeColumnVector(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType,boolean)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/ColumnarBatchScan/class/org$apache$spark$sql$execution$ColumnarBatchScan$$genCodeColumnVector(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType,boolean)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/getValue(java.lang.String,org.apache.spark.sql.types.DataType,java.lang.String)
org/apache/spark/sql/execution/ColumnarBatchScan/class/org$apache$spark$sql$execution$ColumnarBatchScan$$genCodeColumnVector(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType,boolean)#org/apache/spark/sql/execution/ColumnarBatchScan$$anonfun$1/1(org.apache.spark.sql.execution.ColumnarBatchScan,java.lang.String)
org/apache/spark/sql/execution/ColumnarBatchScan/class/org$apache$spark$sql$execution$ColumnarBatchScan$$genCodeColumnVector(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType,boolean)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/ColumnarBatchScan/class/org$apache$spark$sql$execution$ColumnarBatchScan$$genCodeColumnVector(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType,boolean)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/ColumnarBatchScan/class/org$apache$spark$sql$execution$ColumnarBatchScan$$genCodeColumnVector(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType,boolean)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/freshName(java.lang.String)
org/apache/spark/sql/execution/ColumnarBatchScan/class/org$apache$spark$sql$execution$ColumnarBatchScan$$genCodeColumnVector(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType,boolean)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/javaType(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/ColumnarBatchScan/class/org$apache$spark$sql$execution$ColumnarBatchScan$$genCodeColumnVector(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType,boolean)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/ColumnarBatchScan/class/org$apache$spark$sql$execution$ColumnarBatchScan$$genCodeColumnVector(org.apache.spark.sql.execution.ColumnarBatchScan,org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,java.lang.String,java.lang.String,org.apache.spark.sql.types.DataType,boolean)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/1/anonfun/apply/1/apply()#org/apache/spark/sql/types/StructField/dataType()
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/1/anonfun/apply/1/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/1/anonfun/apply/1/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/1/anonfun/apply/1/apply()#org/apache/spark/sql/execution/stat/StatFunctions$$anonfun$1$$anonfun$apply$1/apply()
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/1/anonfun/apply/1/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/1/anonfun/apply/1/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/1/anonfun/apply/1/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/1/anonfun/apply/1/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/1/anonfun/apply/1/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/Dataset/anonfun/5/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/Dataset/anonfun/5/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/command/DDLUtils$/isHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)
org/apache/spark/sql/Dataset/anonfun/5/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Option$/option2Iterable(scala.Option)
org/apache/spark/sql/Dataset/anonfun/5/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/Dataset$$anonfun$5$$anonfun$applyOrElse$3/3(org.apache.spark.sql.Dataset$$anonfun$5)
org/apache/spark/sql/Dataset/anonfun/5/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/Iterable/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/Dataset/anonfun/5/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Option/map(scala.Function1)
org/apache/spark/sql/Dataset/anonfun/5/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/CatalogRelation/tableMeta()
org/apache/spark/sql/Dataset/anonfun/5/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/Dataset/anonfun/5/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/locationUri()
org/apache/spark/sql/Dataset/anonfun/5/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/CatalogTable/storage()
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#org/apache/spark/sql/types/StructType/toAttributes()
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#scala/Predef$/require(boolean,scala.Function0)
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#org/apache/spark/sql/Row$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#org/apache/spark/sql/execution/stat/FrequentItems$$anonfun$7/7()
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#org/apache/spark/sql/execution/stat/FrequentItems$$anonfun$1/1(int)
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#scala/collection/Seq/length()
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#org/apache/spark/sql/execution/stat/FrequentItems$$anonfun$2/2(org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#org/apache/spark/sql/execution/stat/FrequentItems$$anonfun$singlePassFreqItems$1/1(double)
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#org/apache/spark/sql/types/StructType/StructType(org.apache.spark.sql.types.StructField[])
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#org/apache/spark/rdd/RDD/aggregate(java.lang.Object,scala.Function2,scala.Function2,scala.reflect.ClassTag)
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#org/apache/spark/sql/execution/stat/FrequentItems$$anonfun$3/3()
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#scala/collection/Seq$/tabulate(int,scala.Function1)
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#org/apache/spark/sql/execution/stat/FrequentItems$$anonfun$5/5(int)
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#org/apache/spark/sql/execution/stat/FrequentItems$$anonfun$4/4(int)
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#org/apache/spark/sql/Dataset$/ofRows(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#org/apache/spark/sql/catalyst/plans/logical/LocalRelation$/fromExternalRows(scala.collection.Seq,scala.collection.Seq)
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#org/apache/spark/sql/execution/stat/FrequentItems$$anonfun$6/6()
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#org/apache/spark/sql/execution/stat/FrequentItems$/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/stat/FrequentItems/singlePassFreqItems(org.apache.spark.sql.Dataset,scala.collection.Seq,double)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/anonfun/5/anonfun/apply/5/apply()#org/apache/spark/sql/types/StructType/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/anonfun/5/anonfun/apply/5/apply()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/anonfun/5/anonfun/apply/5/apply()#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/anonfun/5/anonfun/apply/5/apply()#scala/collection/TraversableOnce/mkString(java.lang.String)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/anonfun/5/anonfun/apply/5/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/anonfun/5/anonfun/apply/5/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/anonfun/5/anonfun/apply/5/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/anonfun/5/anonfun/apply/5/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/anonfun/5/anonfun/apply/5/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/anonfun/5/anonfun/apply/5/apply()#org/apache/spark/sql/execution/datasources/PreprocessTableCreation$$anonfun$apply$2$$anonfun$5$$anonfun$apply$5$$anonfun$6/6(org.apache.spark.sql.execution.datasources.PreprocessTableCreation$$anonfun$apply$2$$anonfun$5$$anonfun$apply$5)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/anonfun/5/anonfun/apply/5/apply()#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/schema()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/anonfun/5/anonfun/apply/5/apply()#org/apache/spark/sql/execution/datasources/PreprocessTableCreation$$anonfun$apply$2$$anonfun$5$$anonfun$apply$5/apply()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/anonfun/5/anonfun/apply/5/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/anonfun/5/anonfun/apply/5/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/anonfun/apply/2/anonfun/5/anonfun/apply/5/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/streaming/StreamingQueryManager/anonfun/3/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/streaming/StreamingQueryManager/anonfun/3/apply()#org/apache/spark/sql/streaming/StreamingQueryManager$$anonfun$3/apply()
org/apache/spark/sql/streaming/StreamingQueryManager/anonfun/3/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/streaming/StreamingQueryManager/anonfun/3/apply()#org/apache/spark/sql/internal/SQLConf$/CHECKPOINT_LOCATION()
org/apache/spark/sql/streaming/StreamingQueryManager/anonfun/3/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/streaming/StreamingQueryManager/anonfun/3/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/streaming/StreamingQueryManager/anonfun/3/apply()#org/apache/spark/util/Utils$/createTempDir(java.lang.String,java.lang.String)
org/apache/spark/sql/streaming/StreamingQueryManager/anonfun/3/apply()#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/streaming/StreamingQueryManager/anonfun/3/apply()#org/apache/spark/util/Utils$/createTempDir$default$1()
org/apache/spark/sql/streaming/StreamingQueryManager/anonfun/3/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/streaming/StreamingQueryManager/anonfun/3/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/streaming/StreamingQueryManager/anonfun/3/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/streaming/StreamingQueryManager/anonfun/3/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/streaming/StreamingQueryManager/anonfun/3/apply()#java/io/File/getCanonicalPath()
org/apache/spark/sql/streaming/StreamingQueryManager/anonfun/3/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/streaming/StreamingQueryManager/anonfun/3/apply()#org/apache/spark/internal/config/OptionalConfigEntry/key()
org/apache/spark/sql/streaming/StreamingQueryManager/anonfun/3/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/streaming/StreamingQueryManager/anonfun/3/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatDelimited/1/anonfun/46/apply(org.antlr.v4.runtime.Token)#scala/Predef$/ArrowAssoc(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatDelimited/1/anonfun/46/apply(org.antlr.v4.runtime.Token)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatDelimited/1/anonfun/46/apply(org.antlr.v4.runtime.Token)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatDelimited/1/anonfun/46/apply(org.antlr.v4.runtime.Token)#scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatDelimited/1/anonfun/46/apply(org.antlr.v4.runtime.Token)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatDelimited/1/anonfun/46/apply(org.antlr.v4.runtime.Token)#org/apache/spark/sql/catalyst/parser/ParserUtils$/validate(scala.Function0,java.lang.String,org.antlr.v4.runtime.ParserRuleContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatDelimited/1/anonfun/46/apply(org.antlr.v4.runtime.Token)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatDelimited/1/anonfun/46/apply(org.antlr.v4.runtime.Token)#org/apache/spark/sql/catalyst/parser/ParserUtils$/string(org.antlr.v4.runtime.Token)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatDelimited/1/anonfun/46/apply(org.antlr.v4.runtime.Token)#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitRowFormatDelimited$1$$anonfun$46$$anonfun$apply$1/1(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitRowFormatDelimited$1$$anonfun$46,java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/anonfun/init/3/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/anonfun/init/3/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/anonfun/init/3/apply()#org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport$$anonfun$init$3/apply()
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/anonfun/init/3/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/anonfun/init/3/apply()#org/apache/spark/sql/types/StructType/prettyJson()
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/anonfun/init/3/apply()#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/anonfun/init/3/apply()#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/anonfun/init/3/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetWriteSupport/anonfun/init/3/apply()#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/joins/HashJoin/anonfun/rewriteKeyExpr/3/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/ShiftLeft/ShiftLeft(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/execution/joins/HashJoin/anonfun/rewriteKeyExpr/3/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/Cast/Cast(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.types.DataType,scala.Option)
org/apache/spark/sql/execution/joins/HashJoin/anonfun/rewriteKeyExpr/3/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/Cast$/apply$default$3()
org/apache/spark/sql/execution/joins/HashJoin/anonfun/rewriteKeyExpr/3/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/BitwiseOr/BitwiseOr(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/execution/joins/HashJoin/anonfun/rewriteKeyExpr/3/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/joins/HashJoin/anonfun/rewriteKeyExpr/3/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/types/DataType/defaultSize()
org/apache/spark/sql/execution/joins/HashJoin/anonfun/rewriteKeyExpr/3/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/joins/HashJoin/anonfun/rewriteKeyExpr/3/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/Expression/dataType()
org/apache/spark/sql/execution/joins/HashJoin/anonfun/rewriteKeyExpr/3/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/BitwiseAnd/BitwiseAnd(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/execution/joins/HashJoin/anonfun/rewriteKeyExpr/3/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/Literal$/apply(java.lang.Object)
org/apache/spark/sql/execution/ui/SparkPlanGraphCluster/makeDotNode(scala.collection.immutable.Map)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/ui/SparkPlanGraphCluster/makeDotNode(scala.collection.immutable.Map)#org/apache/spark/sql/execution/ui/SparkPlanGraphCluster$$anonfun$makeDotNode$1/1(org.apache.spark.sql.execution.ui.SparkPlanGraphCluster,scala.collection.immutable.Map)
org/apache/spark/sql/execution/ui/SparkPlanGraphCluster/makeDotNode(scala.collection.immutable.Map)#scala/collection/Seq/apply(int)
org/apache/spark/sql/execution/ui/SparkPlanGraphCluster/makeDotNode(scala.collection.immutable.Map)#scala/collection/Seq/filter(scala.Function1)
org/apache/spark/sql/execution/ui/SparkPlanGraphCluster/makeDotNode(scala.collection.immutable.Map)#scala/collection/TraversableOnce/mkString(java.lang.String)
org/apache/spark/sql/execution/ui/SparkPlanGraphCluster/makeDotNode(scala.collection.immutable.Map)#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/execution/ui/SparkPlanGraphCluster/makeDotNode(scala.collection.immutable.Map)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/ui/SparkPlanGraphCluster/makeDotNode(scala.collection.immutable.Map)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/ui/SparkPlanGraphCluster/makeDotNode(scala.collection.immutable.Map)#scala/Option/get()
org/apache/spark/sql/execution/ui/SparkPlanGraphCluster/makeDotNode(scala.collection.immutable.Map)#scala/collection/mutable/ArrayBuffer/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/ui/SparkPlanGraphCluster/makeDotNode(scala.collection.immutable.Map)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/ui/SparkPlanGraphCluster/makeDotNode(scala.collection.immutable.Map)#org/apache/spark/sql/execution/ui/SparkPlanGraphCluster$$anonfun$5/5(org.apache.spark.sql.execution.ui.SparkPlanGraphCluster)
org/apache/spark/sql/execution/ui/SparkPlanGraphCluster/makeDotNode(scala.collection.immutable.Map)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/ui/SparkPlanGraphCluster/makeDotNode(scala.collection.immutable.Map)#scala/collection/immutable/Map/contains(java.lang.Object)
org/apache/spark/sql/execution/ui/SparkPlanGraphCluster/makeDotNode(scala.collection.immutable.Map)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/ui/SparkPlanGraphCluster/makeDotNode(scala.collection.immutable.Map)#org/apache/commons/lang3/StringEscapeUtils/escapeJava(java.lang.String)
org/apache/spark/sql/execution/ui/SparkPlanGraphCluster/makeDotNode(scala.collection.immutable.Map)#scala/Predef$/require(boolean)
org/apache/spark/sql/execution/ui/SparkPlanGraphCluster/makeDotNode(scala.collection.immutable.Map)#scala/collection/mutable/ArrayBuffer$/canBuildFrom()
org/apache/spark/sql/execution/ui/SparkPlanGraphCluster/makeDotNode(scala.collection.immutable.Map)#scala/collection/immutable/Map/get(java.lang.Object)
org/apache/spark/sql/execution/ui/SparkPlanGraphCluster/makeDotNode(scala.collection.immutable.Map)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/ui/SparkPlanGraphCluster/makeDotNode(scala.collection.immutable.Map)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/ui/SparkPlanGraphCluster/makeDotNode(scala.collection.immutable.Map)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/ui/SparkPlanGraphCluster/makeDotNode(scala.collection.immutable.Map)#scala/collection/Seq/length()
org/apache/spark/sql/execution/ui/SparkPlanGraphCluster/makeDotNode(scala.collection.immutable.Map)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/ui/SparkPlanGraphCluster/makeDotNode(scala.collection.immutable.Map)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/streaming/FileStreamSourceOffset/apply(org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/streaming/FileStreamSourceOffset$/apply(org.apache.spark.sql.execution.streaming.Offset)
org/apache/spark/sql/execution/streaming/FileStreamSourceOffset/apply(org.apache.spark.sql.execution.streaming.Offset)#scala/util/control/Exception$Catch/opt(scala.Function0)
org/apache/spark/sql/execution/streaming/FileStreamSourceOffset/apply(org.apache.spark.sql.execution.streaming.Offset)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/FileStreamSourceOffset/apply(org.apache.spark.sql.execution.streaming.Offset)#scala/util/control/Exception$/catching(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/FileStreamSourceOffset/apply(org.apache.spark.sql.execution.streaming.Offset)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/FileStreamSourceOffset/apply(org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/streaming/FileStreamSourceOffset$$anonfun$apply$1/1(java.lang.String)
org/apache/spark/sql/execution/streaming/FileStreamSourceOffset/apply(org.apache.spark.sql.execution.streaming.Offset)#org/apache/spark/sql/execution/streaming/FileStreamSourceOffset$$anonfun$apply$2/2(java.lang.String)
org/apache/spark/sql/execution/streaming/FileStreamSourceOffset/apply(org.apache.spark.sql.execution.streaming.Offset)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/FileStreamSourceOffset/apply(org.apache.spark.sql.execution.streaming.Offset)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/execution/streaming/FileStreamSourceOffset/apply(org.apache.spark.sql.execution.streaming.Offset)#java/lang/Object/getClass()
org/apache/spark/sql/execution/streaming/FileStreamSourceOffset/apply(org.apache.spark.sql.execution.streaming.Offset)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/streaming/FileStreamSourceOffset/apply(org.apache.spark.sql.execution.streaming.Offset)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/LoadDataCommand/anonfun/run/1/apply(java.lang.String)#scala/collection/Seq/contains(java.lang.Object)
org/apache/spark/sql/execution/command/LoadDataCommand/anonfun/run/1/apply(java.lang.String)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/command/LoadDataCommand/anonfun/run/1/apply(java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/LoadDataCommand/anonfun/run/1/apply(java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/LoadDataCommand/anonfun/run/1/apply(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/command/LoadDataCommand/anonfun/run/1/apply(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/command/LoadDataCommand/anonfun/run/1/apply(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/command/LoadDataCommand/anonfun/run/1/apply(java.lang.String)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/command/LoadDataCommand/anonfun/run/1/apply(java.lang.String)#org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()
org/apache/spark/sql/execution/command/LoadDataCommand/anonfun/run/1/apply(java.lang.String)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/command/LoadDataCommand/anonfun/run/1/apply(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/command/LoadDataCommand/anonfun/run/1/apply(java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/LoadDataCommand/anonfun/run/1/apply(java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/LoadDataCommand/anonfun/run/1/apply(java.lang.String)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/anonfun/cleanup/3/apply()#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$$anonfun$cleanup$3/apply()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/anonfun/cleanup/3/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/anonfun/cleanup/3/apply()#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/anonfun/cleanup/3/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/anonfun/cleanup/3/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/anonfun/cleanup/3/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/anonfun/cleanup/3/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/anonfun/cleanup/3/apply()#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$StoreFile/version()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/anonfun/cleanup/3/apply()#scala/collection/Seq/mkString(java.lang.String)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/anonfun/cleanup/3/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/anonfun/cleanup/3/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/4/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec$$anonfun$4$$anonfun$apply$2/2(org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec$$anonfun$4,org.apache.spark.sql.catalyst.InternalRow[],org.apache.spark.sql.catalyst.expressions.JoinedRow)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/4/apply(scala.collection.Iterator)#scala/collection/Iterator$/empty()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/4/apply(scala.collection.Iterator)#org/apache/spark/broadcast/Broadcast/value()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/4/apply(scala.collection.Iterator)#scala/Option/isDefined()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/4/apply(scala.collection.Iterator)#scala/collection/Iterator/filter(scala.Function1)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/4/apply(scala.collection.Iterator)#scala/package$/Iterator()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/4/apply(scala.collection.Iterator)#org/apache/spark/sql/catalyst/expressions/JoinedRow/JoinedRow()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/4/apply(scala.collection.Iterator)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/4/apply(scala.collection.Iterator)#scala/collection/mutable/ArrayOps/nonEmpty()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/getSizeInBytes()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#org/apache/spark/sql/catalyst/expressions/UnsafeProjection/apply(org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#org/apache/spark/unsafe/map/BytesToBytesMap/BytesToBytesMap(org.apache.spark.memory.TaskMemoryManager,int,long)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#org/apache/spark/SparkEnv$/get()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#org/apache/spark/unsafe/map/BytesToBytesMap/free()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#org/apache/spark/sql/execution/joins/UnsafeHashedRelation$$anonfun$5/5()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/getBaseOffset()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/numFields()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#org/apache/spark/sql/catalyst/expressions/UnsafeProjection$/create(scala.collection.Seq)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#org/apache/spark/SparkException/SparkException(java.lang.String)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#org/apache/spark/sql/execution/joins/UnsafeHashedRelation$/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#scala/collection/Iterator/next()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/getBaseObject()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#org/apache/spark/unsafe/map/BytesToBytesMap/lookup(java.lang.Object,long,int)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#scala/collection/Iterator/hasNext()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#org/apache/spark/sql/execution/joins/UnsafeHashedRelation$$anonfun$2/2()
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#org/apache/spark/unsafe/map/BytesToBytesMap$Location/append(java.lang.Object,long,int,java.lang.Object,long,int)
org/apache/spark/sql/execution/joins/UnsafeHashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/anyNull()
org/apache/spark/sql/execution/ui/ExecutionPage/anonfun/7/apply(org.apache.spark.sql.execution.ui.SparkPlanGraphNode)#scala/xml/UnprefixedAttribute/UnprefixedAttribute(java.lang.String,java.lang.String,scala.xml.MetaData)
org/apache/spark/sql/execution/ui/ExecutionPage/anonfun/7/apply(org.apache.spark.sql.execution.ui.SparkPlanGraphNode)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/ui/ExecutionPage/anonfun/7/apply(org.apache.spark.sql.execution.ui.SparkPlanGraphNode)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/ui/ExecutionPage/anonfun/7/apply(org.apache.spark.sql.execution.ui.SparkPlanGraphNode)#scala/xml/NodeBuffer/$amp$plus(java.lang.Object)
org/apache/spark/sql/execution/ui/ExecutionPage/anonfun/7/apply(org.apache.spark.sql.execution.ui.SparkPlanGraphNode)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/ui/ExecutionPage/anonfun/7/apply(org.apache.spark.sql.execution.ui.SparkPlanGraphNode)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/ui/ExecutionPage/anonfun/7/apply(org.apache.spark.sql.execution.ui.SparkPlanGraphNode)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/ui/ExecutionPage/anonfun/7/apply(org.apache.spark.sql.execution.ui.SparkPlanGraphNode)#scala/xml/Elem/Elem(java.lang.String,java.lang.String,scala.xml.MetaData,scala.xml.NamespaceBinding,boolean,scala.collection.Seq)
org/apache/spark/sql/execution/ui/ExecutionPage/anonfun/7/apply(org.apache.spark.sql.execution.ui.SparkPlanGraphNode)#scala/xml/NodeBuffer/NodeBuffer()
org/apache/spark/sql/execution/datasources/csv/CSVUtils/toChar(java.lang.String)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/CSVUtils/toChar(java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/csv/CSVUtils/toChar(java.lang.String)#java/lang/String/length()
org/apache/spark/sql/execution/datasources/csv/CSVUtils/toChar(java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/csv/CSVUtils/toChar(java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/CSVUtils/toChar(java.lang.String)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/execution/datasources/csv/CSVUtils/toChar(java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/csv/CSVUtils/toChar(java.lang.String)#java/lang/String/charAt(int)
org/apache/spark/sql/execution/datasources/csv/CSVUtils/toChar(java.lang.String)#org/apache/spark/sql/execution/datasources/csv/CSVUtils$/toChar(java.lang.String)
org/apache/spark/sql/execution/datasources/csv/CSVUtils/filterCommentAndEmpty(org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#java/lang/Object/toString()
org/apache/spark/sql/execution/datasources/csv/CSVUtils/filterCommentAndEmpty(org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/csv/CSVUtils/filterCommentAndEmpty(org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/functions$/length(org.apache.spark.sql.Column)
org/apache/spark/sql/execution/datasources/csv/CSVUtils/filterCommentAndEmpty(org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/ColumnName/startsWith(java.lang.String)
org/apache/spark/sql/execution/datasources/csv/CSVUtils/filterCommentAndEmpty(org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/runtime/BoxesRunTime/boxToCharacter(char)
org/apache/spark/sql/execution/datasources/csv/CSVUtils/filterCommentAndEmpty(org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/functions$/trim(org.apache.spark.sql.Column)
org/apache/spark/sql/execution/datasources/csv/CSVUtils/filterCommentAndEmpty(org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/csv/CSVUtils/filterCommentAndEmpty(org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/datasources/csv/CSVUtils/filterCommentAndEmpty(org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/SQLContext$implicits$/StringToColumn(scala.StringContext)
org/apache/spark/sql/execution/datasources/csv/CSVUtils/filterCommentAndEmpty(org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVUtils$/filterCommentAndEmpty(org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/CSVUtils/filterCommentAndEmpty(org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/SQLImplicits$StringToColumn/$(scala.collection.Seq)
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/SpillableArrayIterator/init()#org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray$SpillableArrayIterator/org$apache$spark$sql$execution$ExternalAppendOnlyUnsafeRowArray$SpillableArrayIterator$$$outer()
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/SpillableArrayIterator/init()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/SpillableArrayIterator/init()#org/apache/spark/util/collection/unsafe/sort/UnsafeSorterIterator/hasNext()
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/SpillableArrayIterator/init()#org/apache/spark/util/collection/unsafe/sort/UnsafeSorterIterator/loadNext()
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/SpillableArrayIterator/init()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/SpillableArrayIterator/init()#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/SpillableArrayIterator/init()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/SpillableArrayIterator/init()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/SpillableArrayIterator/init()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/SpillableArrayIterator/init()#java/lang/ArrayIndexOutOfBoundsException/ArrayIndexOutOfBoundsException(java.lang.String)
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/SpillableArrayIterator/init()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/SpillableArrayIterator/init()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/compatibleType/1/apply()#org/apache/spark/sql/types/DecimalType/scale()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/compatibleType/1/apply()#java/util/ArrayList/ArrayList()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/compatibleType/1/apply()#scala/math/package$/max(int,int)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/compatibleType/1/apply()#org/apache/spark/sql/execution/datasources/json/JsonInferSchema$$anonfun$compatibleType$1/apply()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/compatibleType/1/apply()#java/util/ArrayList/toArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/compatibleType/1/apply()#org/apache/spark/sql/types/StructField/name()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/compatibleType/1/apply()#java/util/ArrayList/add(java.lang.Object)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/compatibleType/1/apply()#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/compatibleType/1/apply()#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/compatibleType/1/apply()#org/apache/spark/sql/types/DecimalType/DecimalType(int,int)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/compatibleType/1/apply()#org/apache/spark/sql/execution/datasources/json/JsonInferSchema$$anonfun$compatibleType$1$$anonfun$apply$5/5(org.apache.spark.sql.execution.datasources.json.JsonInferSchema$$anonfun$compatibleType$1,org.apache.spark.sql.types.StructField[])
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/compatibleType/1/apply()#org/apache/spark/sql/execution/datasources/json/JsonInferSchema$/compatibleType(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/compatibleType/1/apply()#org/apache/spark/sql/types/DecimalType$/forType(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/compatibleType/1/apply()#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/compatibleType/1/apply()#scala/Predef$/assert(boolean,scala.Function0)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/compatibleType/1/apply()#org/apache/spark/sql/types/StructType/StructType(org.apache.spark.sql.types.StructField[])
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/compatibleType/1/apply()#org/apache/spark/sql/types/StructField$/apply$default$4()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/compatibleType/1/apply()#org/apache/spark/sql/types/StructField/dataType()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/compatibleType/1/apply()#org/apache/spark/sql/types/StructField/StructField(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.types.Metadata)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/compatibleType/1/apply()#org/apache/spark/sql/execution/datasources/json/JsonInferSchema$/org$apache$spark$sql$execution$datasources$json$JsonInferSchema$$isSorted(org.apache.spark.sql.types.StructField[])
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/compatibleType/1/apply()#scala/Tuple2/_1()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/compatibleType/1/apply()#scala/Tuple2/_2()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/compatibleType/1/apply()#java/lang/String/compareTo(java.lang.String)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/compatibleType/1/apply()#org/apache/spark/sql/types/ArrayType/elementType()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/compatibleType/1/apply()#org/apache/spark/sql/types/DecimalType/precision()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/compatibleType/1/apply()#org/apache/spark/sql/types/ArrayType/containsNull()
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/compatibleType/1/apply()#org/apache/spark/sql/execution/datasources/json/JsonInferSchema$$anonfun$compatibleType$1$$anonfun$apply$6/6(org.apache.spark.sql.execution.datasources.json.JsonInferSchema$$anonfun$compatibleType$1,org.apache.spark.sql.types.StructField[])
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/compatibleType/1/apply()#org/apache/spark/sql/types/ArrayType/ArrayType(org.apache.spark.sql.types.DataType,boolean)
org/apache/spark/sql/execution/datasources/json/JsonInferSchema/anonfun/compatibleType/1/apply()#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/exchange/EnsureRequirements/withExchangeCoordinator(scala.collection.Seq,scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/exchange/EnsureRequirements/withExchangeCoordinator(scala.collection.Seq,scala.collection.Seq)#scala/collection/Seq/exists(scala.Function1)
org/apache/spark/sql/execution/exchange/EnsureRequirements/withExchangeCoordinator(scala.collection.Seq,scala.collection.Seq)#scala/collection/Seq/forall(scala.Function1)
org/apache/spark/sql/execution/exchange/EnsureRequirements/withExchangeCoordinator(scala.collection.Seq,scala.collection.Seq)#scala/collection/Seq/zip(scala.collection.GenIterable,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/exchange/EnsureRequirements/withExchangeCoordinator(scala.collection.Seq,scala.collection.Seq)#org/apache/spark/sql/execution/exchange/EnsureRequirements$$anonfun$1/1(org.apache.spark.sql.execution.exchange.EnsureRequirements)
org/apache/spark/sql/execution/exchange/EnsureRequirements/withExchangeCoordinator(scala.collection.Seq,scala.collection.Seq)#org/apache/spark/sql/execution/exchange/EnsureRequirements$$anonfun$2/2(org.apache.spark.sql.execution.exchange.EnsureRequirements)
org/apache/spark/sql/execution/exchange/EnsureRequirements/withExchangeCoordinator(scala.collection.Seq,scala.collection.Seq)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/exchange/EnsureRequirements/withExchangeCoordinator(scala.collection.Seq,scala.collection.Seq)#scala/collection/Seq/length()
org/apache/spark/sql/execution/exchange/EnsureRequirements/withExchangeCoordinator(scala.collection.Seq,scala.collection.Seq)#org/apache/spark/sql/execution/exchange/EnsureRequirements$$anonfun$4/4(org.apache.spark.sql.execution.exchange.EnsureRequirements,org.apache.spark.sql.execution.exchange.ExchangeCoordinator)
org/apache/spark/sql/execution/exchange/EnsureRequirements/withExchangeCoordinator(scala.collection.Seq,scala.collection.Seq)#org/apache/spark/sql/execution/exchange/EnsureRequirements$$anonfun$3/3(org.apache.spark.sql.execution.exchange.EnsureRequirements)
org/apache/spark/sql/execution/exchange/EnsureRequirements/org$apache$spark$sql$execution$exchange$EnsureRequirements$$createPartitioning(org.apache.spark.sql.catalyst.plans.physical.Distribution,int)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/exchange/EnsureRequirements/org$apache$spark$sql$execution$exchange$EnsureRequirements$$createPartitioning(org.apache.spark.sql.catalyst.plans.physical.Distribution,int)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/exchange/EnsureRequirements/org$apache$spark$sql$execution$exchange$EnsureRequirements$$createPartitioning(org.apache.spark.sql.catalyst.plans.physical.Distribution,int)#org/apache/spark/sql/catalyst/plans/physical/OrderedDistribution/ordering()
org/apache/spark/sql/execution/exchange/EnsureRequirements/org$apache$spark$sql$execution$exchange$EnsureRequirements$$createPartitioning(org.apache.spark.sql.catalyst.plans.physical.Distribution,int)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/exchange/EnsureRequirements/org$apache$spark$sql$execution$exchange$EnsureRequirements$$createPartitioning(org.apache.spark.sql.catalyst.plans.physical.Distribution,int)#scala/sys/package$/error(java.lang.String)
org/apache/spark/sql/execution/exchange/EnsureRequirements/org$apache$spark$sql$execution$exchange$EnsureRequirements$$createPartitioning(org.apache.spark.sql.catalyst.plans.physical.Distribution,int)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/exchange/EnsureRequirements/org$apache$spark$sql$execution$exchange$EnsureRequirements$$createPartitioning(org.apache.spark.sql.catalyst.plans.physical.Distribution,int)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/exchange/EnsureRequirements/org$apache$spark$sql$execution$exchange$EnsureRequirements$$createPartitioning(org.apache.spark.sql.catalyst.plans.physical.Distribution,int)#org/apache/spark/sql/catalyst/plans/physical/HashPartitioning/HashPartitioning(scala.collection.Seq,int)
org/apache/spark/sql/execution/exchange/EnsureRequirements/org$apache$spark$sql$execution$exchange$EnsureRequirements$$createPartitioning(org.apache.spark.sql.catalyst.plans.physical.Distribution,int)#org/apache/spark/sql/catalyst/plans/physical/ClusteredDistribution/clustering()
org/apache/spark/sql/execution/exchange/EnsureRequirements/org$apache$spark$sql$execution$exchange$EnsureRequirements$$createPartitioning(org.apache.spark.sql.catalyst.plans.physical.Distribution,int)#org/apache/spark/sql/catalyst/plans/physical/RangePartitioning/RangePartitioning(scala.collection.Seq,int)
org/apache/spark/sql/execution/exchange/EnsureRequirements/org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/exchange/EnsureRequirements$$anonfun$org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering$2/2(org.apache.spark.sql.execution.exchange.EnsureRequirements)
org/apache/spark/sql/execution/exchange/EnsureRequirements/org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/exchange/EnsureRequirements$$anonfun$org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering$4/4(org.apache.spark.sql.execution.exchange.EnsureRequirements,int)
org/apache/spark/sql/execution/exchange/EnsureRequirements/org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/exchange/EnsureRequirements$$anonfun$org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering$5/5(org.apache.spark.sql.execution.exchange.EnsureRequirements)
org/apache/spark/sql/execution/exchange/EnsureRequirements/org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/catalyst/plans/physical/Partitioning$/allCompatible(scala.collection.Seq)
org/apache/spark/sql/execution/exchange/EnsureRequirements/org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/SparkPlan/withNewChildren(scala.collection.Seq)
org/apache/spark/sql/execution/exchange/EnsureRequirements/org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/exchange/EnsureRequirements$$anonfun$6/6(org.apache.spark.sql.execution.exchange.EnsureRequirements,int)
org/apache/spark/sql/execution/exchange/EnsureRequirements/org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/exchange/EnsureRequirements$$anonfun$7/7(org.apache.spark.sql.execution.exchange.EnsureRequirements,int)
org/apache/spark/sql/execution/exchange/EnsureRequirements/org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/length()
org/apache/spark/sql/execution/exchange/EnsureRequirements/org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering(org.apache.spark.sql.execution.SparkPlan)#scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)
org/apache/spark/sql/execution/exchange/EnsureRequirements/org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/exchange/EnsureRequirements/org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering(org.apache.spark.sql.execution.SparkPlan)#scala/collection/TraversableOnce/max(scala.math.Ordering)
org/apache/spark/sql/execution/exchange/EnsureRequirements/org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/exchange/EnsureRequirements/org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/exchange/EnsureRequirements$$anonfun$org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering$1/1(org.apache.spark.sql.execution.exchange.EnsureRequirements)
org/apache/spark/sql/execution/exchange/EnsureRequirements/org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/SparkPlan/children()
org/apache/spark/sql/execution/exchange/EnsureRequirements/org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering(org.apache.spark.sql.execution.SparkPlan)#scala/collection/IterableLike/forall(scala.Function1)
org/apache/spark/sql/execution/exchange/EnsureRequirements/org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/exists(scala.Function1)
org/apache/spark/sql/execution/exchange/EnsureRequirements/org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/exchange/EnsureRequirements$$anonfun$org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering$3/3(org.apache.spark.sql.execution.exchange.EnsureRequirements)
org/apache/spark/sql/execution/exchange/EnsureRequirements/org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering(org.apache.spark.sql.execution.SparkPlan)#scala/Predef$/assert(boolean)
org/apache/spark/sql/execution/exchange/EnsureRequirements/org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/zip(scala.collection.GenIterable,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/exchange/EnsureRequirements/org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering(org.apache.spark.sql.execution.SparkPlan)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/exchange/EnsureRequirements/org$apache$spark$sql$execution$exchange$EnsureRequirements$$ensureDistributionAndOrdering(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/exchange/EnsureRequirements$$anonfun$5/5(org.apache.spark.sql.execution.exchange.EnsureRequirements)
org/apache/spark/sql/execution/SparkOptimizer/batches()#org/apache/spark/sql/catalyst/optimizer/Optimizer/batches()
org/apache/spark/sql/execution/SparkOptimizer/batches()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/SparkOptimizer/batches()#org/apache/spark/sql/catalyst/rules/RuleExecutor$Batch/Batch(org.apache.spark.sql.catalyst.rules.RuleExecutor,java.lang.String,org.apache.spark.sql.catalyst.rules.RuleExecutor$Strategy,scala.collection.Seq)
org/apache/spark/sql/execution/SparkOptimizer/batches()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/SparkOptimizer/batches()#org/apache/spark/sql/execution/SparkOptimizer/fixedPoint()
org/apache/spark/sql/execution/SparkOptimizer/batches()#scala/collection/SeqLike/$colon$plus(java.lang.Object,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/SparkOptimizer/batches()#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/SparkOptimizer/batches()#org/apache/spark/sql/execution/SparkOptimizer/Once()
org/apache/spark/sql/execution/SparkOptimizer/batches()#scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/vectorized/OffHeapColumnVector/reserveInternal(int)#org/apache/spark/sql/types/DecimalType/is32BitDecimalType(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/vectorized/OffHeapColumnVector/reserveInternal(int)#org/apache/spark/sql/types/DecimalType/is64BitDecimalType(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/vectorized/OffHeapColumnVector/reserveInternal(int)#java/lang/StringBuilder/append(java.lang.String)
org/apache/spark/sql/execution/vectorized/OffHeapColumnVector/reserveInternal(int)#org/apache/spark/unsafe/Platform/reallocateMemory(long,long,long)
org/apache/spark/sql/execution/vectorized/OffHeapColumnVector/reserveInternal(int)#org/apache/spark/unsafe/Platform/setMemory(long,byte,long)
org/apache/spark/sql/execution/vectorized/OffHeapColumnVector/reserveInternal(int)#java/lang/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/vectorized/OffHeapColumnVector/reserveInternal(int)#java/lang/RuntimeException/RuntimeException(java.lang.String)
org/apache/spark/sql/execution/vectorized/OffHeapColumnVector/reserveInternal(int)#java/lang/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/vectorized/OffHeapColumnVector/reserveInternal(int)#java/lang/StringBuilder/toString()
org/apache/spark/sql/execution/ui/SQLListener/anonfun/updateTaskAccumulatorValues/1/apply()#org/apache/spark/sql/execution/ui/SQLListener$$anonfun$updateTaskAccumulatorValues$1/apply()
org/apache/spark/sql/execution/ui/SQLListener/anonfun/updateTaskAccumulatorValues/1/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/ui/SQLListener/anonfun/updateTaskAccumulatorValues/1/apply()#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/ui/SQLListener/anonfun/updateTaskAccumulatorValues/1/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/ui/SQLListener/anonfun/updateTaskAccumulatorValues/1/apply()#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/ui/SQLListener/anonfun/updateTaskAccumulatorValues/1/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/ui/SQLListener/anonfun/updateTaskAccumulatorValues/1/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/ui/SQLListener/anonfun/updateTaskAccumulatorValues/1/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/ui/SQLListener/anonfun/updateTaskAccumulatorValues/1/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/ui/SQLListener/anonfun/updateTaskAccumulatorValues/1/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/11/anonfun/apply/5/apply(org.apache.hadoop.fs.Path)#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/11/anonfun/apply/5/apply(org.apache.hadoop.fs.Path)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/11/anonfun/apply/5/apply(org.apache.hadoop.fs.Path)#org/apache/spark/util/SerializableConfiguration/value()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/11/anonfun/apply/5/apply(org.apache.hadoop.fs.Path)#scala/collection/mutable/ArrayOps/sum(scala.math.Numeric)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/11/anonfun/apply/5/apply(org.apache.hadoop.fs.Path)#org/apache/hadoop/fs/FileSystem/listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/11/anonfun/apply/5/apply(org.apache.hadoop.fs.Path)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/11/anonfun/apply/5/apply(org.apache.hadoop.fs.Path)#scala/reflect/ClassTag$/Long()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/11/anonfun/apply/5/apply(org.apache.hadoop.fs.Path)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/11/anonfun/apply/5/apply(org.apache.hadoop.fs.Path)#org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/11/anonfun/apply/5/apply(org.apache.hadoop.fs.Path)#org/apache/hadoop/fs/Path/toString()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/11/anonfun/apply/5/apply(org.apache.hadoop.fs.Path)#scala/Predef$/longArrayOps(long[])
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/11/anonfun/apply/5/apply(org.apache.hadoop.fs.Path)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/11/anonfun/apply/5/apply(org.apache.hadoop.fs.Path)#org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand$$anonfun$11$$anonfun$apply$5$$anonfun$apply$6/6(org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsCommand$$anonfun$11$$anonfun$apply$5)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCacheTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCacheTable$1/apply()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCacheTable/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CacheTableContext/LAZY()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCacheTable/1/apply()#org/apache/spark/sql/catalyst/parser/ParseException/ParseException(java.lang.String,org.antlr.v4.runtime.ParserRuleContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCacheTable/1/apply()#scala/Option/isDefined()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCacheTable/1/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCacheTable/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CacheTableContext/query()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCacheTable/1/apply()#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCacheTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCacheTable$1$$anonfun$5/5(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCacheTable$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCacheTable/1/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCacheTable/1/apply()#org/apache/spark/sql/catalyst/TableIdentifier/database()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCacheTable/1/apply()#scala/Option/get()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCacheTable/1/apply()#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCacheTable/1/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCacheTable/1/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCacheTable/1/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCacheTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder/visitTableIdentifier(org.apache.spark.sql.catalyst.parser.SqlBaseParser$TableIdentifierContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCacheTable/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CacheTableContext/tableIdentifier()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCacheTable/1/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCacheTable/1/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/5/apply(scala.Tuple2)#org/apache/spark/sql/execution/aggregate/HashMapGenerator$Buffer/dataType()
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/5/apply(scala.Tuple2)#org/apache/spark/sql/types/DecimalType/scale()
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/5/apply(scala.Tuple2)#org/apache/spark/sql/execution/aggregate/HashMapGenerator$Buffer/name()
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/5/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/5/apply(scala.Tuple2)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/5/apply(scala.Tuple2)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/5/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/5/apply(scala.Tuple2)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/5/apply(scala.Tuple2)#scala/Tuple2/_2$mcI$sp()
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/5/apply(scala.Tuple2)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/5/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/isPrimitiveType(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/5/apply(scala.Tuple2)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/5/apply(scala.Tuple2)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/aggregate/RowBasedHashMapGenerator/anonfun/5/apply(scala.Tuple2)#org/apache/spark/sql/types/DecimalType/precision()
org/apache/spark/sql/execution/command/SetCommand/anonfun/1/anonfun/apply/1/apply()#org/apache/spark/sql/internal/SQLConf$Deprecated$/MAPRED_REDUCE_TASKS()
org/apache/spark/sql/execution/command/SetCommand/anonfun/1/anonfun/apply/1/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/SetCommand/anonfun/1/anonfun/apply/1/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/SetCommand/anonfun/1/anonfun/apply/1/apply()#org/apache/spark/sql/execution/command/SetCommand$$anonfun$1$$anonfun$apply$1/apply()
org/apache/spark/sql/execution/command/SetCommand/anonfun/1/anonfun/apply/1/apply()#org/apache/spark/internal/config/ConfigEntry/key()
org/apache/spark/sql/execution/command/SetCommand/anonfun/1/anonfun/apply/1/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/command/SetCommand/anonfun/1/anonfun/apply/1/apply()#org/apache/spark/sql/internal/SQLConf$/SHUFFLE_PARTITIONS()
org/apache/spark/sql/execution/command/SetCommand/anonfun/1/anonfun/apply/1/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/command/SetCommand/anonfun/1/anonfun/apply/1/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/SetCommand/anonfun/1/anonfun/apply/1/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/SetCommand/anonfun/1/anonfun/apply/1/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#scala/Some/x()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/execution/command/DDLUtils$/HIVE_PROVIDER()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#scala/Tuple4/_3()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#scala/Option/orElse(scala.Function0)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$2()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTableType$/MANAGED()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#scala/Tuple4/Tuple4(java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTableType$/EXTERNAL()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateHiveTableContext/createTableHeader()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat$/empty()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/properties()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateHiveTableContext/createFileFormat()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateHiveTableContext/locationSpec()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/CatalogStorageFormat(scala.Option,scala.Option,scala.Option,scala.Option,boolean,scala.collection.immutable.Map)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateHiveTableContext/tablePropertyList()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/parser/ParseException/ParseException(java.lang.String,org.antlr.v4.runtime.ParserRuleContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/internal/SQLConf/defaultDataSourceName()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/outputFormat()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/internal/HiveSerDe$/getDefaultStorage(org.apache.spark.sql.internal.SQLConf)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/inputFormat()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/parser/ParserUtils$/operationNotAllowed(java.lang.String,org.antlr.v4.runtime.ParserRuleContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/serde()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#scala/Option/isDefined()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#scala/Tuple4/_1()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#scala/Tuple4/_2()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateHiveTableContext/query()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#scala/Tuple4/_4()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/internal/SQLConf/convertCTAS()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#scala/Option/isEmpty()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#scala/runtime/BoxesRunTime/boxToBoolean(boolean)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$3()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$4()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$1()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$5()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$2()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$6()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1$$anonfun$38/38(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1$$anonfun$39/39(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1$$anonfun$40/40(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$10()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1$$anonfun$37/37(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateHiveTableContext/rowFormat()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$4()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$10()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$11()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$12()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$6()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$7()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$12()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$13()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1$$anonfun$29/29(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1$$anonfun$28/28(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$8()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$13()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1$$anonfun$27/27(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1$$anonfun$26/26(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateHiveTableContext/bucketSpec()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1$$anonfun$34/34(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1$$anonfun$33/33(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$CreateHiveTableContext/skewSpec()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1$$anonfun$32/32(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/types/StructType$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy(scala.Option,scala.Option,scala.Option,scala.Option,boolean,scala.collection.immutable.Map)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$7()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1$$anonfun$36/36(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1$$anonfun$25/25(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1$$anonfun$35/35(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1$$anonfun$24/24(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1$$anonfun$42/42(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1$$anonfun$31/31(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1$$anonfun$41/41(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1$$anonfun$30/30(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$9()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$15()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$8()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$15()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$16()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitCreateHiveTable$1/apply()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$9()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$17()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/types/StructType/nonEmpty()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$16()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#scala/collection/immutable/Map/$plus$plus(scala.collection.GenTraversableOnce)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTable$/apply$default$17()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitCreateHiveTable/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogTable/CatalogTable(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/inferFromDataset(org.apache.spark.sql.SparkSession,org.apache.spark.sql.Dataset,scala.Option,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/rdd/RDD/mapPartitions(scala.Function1,boolean,scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/inferFromDataset(org.apache.spark.sql.SparkSession,org.apache.spark.sql.Dataset,scala.Option,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/rdd/RDD/mapPartitions$default$2()
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/inferFromDataset(org.apache.spark.sql.SparkSession,org.apache.spark.sql.Dataset,scala.Option,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource$/inferFromDataset(org.apache.spark.sql.SparkSession,org.apache.spark.sql.Dataset,scala.Option,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/inferFromDataset(org.apache.spark.sql.SparkSession,org.apache.spark.sql.Dataset,scala.Option,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource$/makeSafeHeader(java.lang.String[],boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/inferFromDataset(org.apache.spark.sql.SparkSession,org.apache.spark.sql.Dataset,scala.Option,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/inferFromDataset(org.apache.spark.sql.SparkSession,org.apache.spark.sql.Dataset,scala.Option,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource$$anonfun$5/5(org.apache.spark.sql.execution.datasources.csv.CSVOptions,java.lang.String)
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/inferFromDataset(org.apache.spark.sql.SparkSession,org.apache.spark.sql.Dataset,scala.Option,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/inferFromDataset(org.apache.spark.sql.SparkSession,org.apache.spark.sql.Dataset,scala.Option,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/Some/x()
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/inferFromDataset(org.apache.spark.sql.SparkSession,org.apache.spark.sql.Dataset,scala.Option,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#com/univocity/parsers/csv/CsvParser/CsvParser(com.univocity.parsers.csv.CsvParserSettings)
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/inferFromDataset(org.apache.spark.sql.SparkSession,org.apache.spark.sql.Dataset,scala.Option,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/inferFromDataset(org.apache.spark.sql.SparkSession,org.apache.spark.sql.Dataset,scala.Option,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#com/univocity/parsers/csv/CsvParser/parseLine(java.lang.String)
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/inferFromDataset(org.apache.spark.sql.SparkSession,org.apache.spark.sql.Dataset,scala.Option,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVInferSchema$/infer(org.apache.spark.rdd.RDD,java.lang.String[],org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/inferFromDataset(org.apache.spark.sql.SparkSession,org.apache.spark.sql.Dataset,scala.Option,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/inferFromDataset(org.apache.spark.sql.SparkSession,org.apache.spark.sql.Dataset,scala.Option,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/internal/SQLConf/caseSensitiveAnalysis()
org/apache/spark/sql/execution/datasources/csv/TextInputCSVDataSource/inferFromDataset(org.apache.spark.sql.SparkSession,org.apache.spark.sql.Dataset,scala.Option,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/types/StructType$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/command/ExplainCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/ExplainCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/streaming/OutputMode/Append()
org/apache/spark/sql/execution/command/ExplainCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/ExplainCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/debug/package$/codegenString(org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/command/ExplainCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/command/ExplainCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/isStreaming()
org/apache/spark/sql/execution/command/ExplainCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/errors/package$TreeNodeException/getMessage()
org/apache/spark/sql/execution/command/ExplainCommand/run(org.apache.spark.sql.SparkSession)#java/lang/String/split(java.lang.String)
org/apache/spark/sql/execution/command/ExplainCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$DummyImplicit$/dummyImplicit()
org/apache/spark/sql/execution/command/ExplainCommand/run(org.apache.spark.sql.SparkSession)#scala/Array$/fallbackCanBuildFrom(scala.Predef$DummyImplicit)
org/apache/spark/sql/execution/command/ExplainCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/ExplainCommand$$anonfun$run$1/1(org.apache.spark.sql.execution.command.ExplainCommand)
org/apache/spark/sql/execution/command/ExplainCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/command/ExplainCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/command/ExplainCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/ExplainCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/Row$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/command/ExplainCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/streaming/OffsetSeqMetadata$/apply$default$3()
org/apache/spark/sql/execution/command/ExplainCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/command/ExplainCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/stat/FrequentItems/FreqItemCounter/add(java.lang.Object,long)#scala/collection/mutable/Map/size()
org/apache/spark/sql/execution/stat/FrequentItems/FreqItemCounter/add(java.lang.Object,long)#scala/collection/mutable/Map/values()
org/apache/spark/sql/execution/stat/FrequentItems/FreqItemCounter/add(java.lang.Object,long)#scala/Predef$/ArrowAssoc(java.lang.Object)
org/apache/spark/sql/execution/stat/FrequentItems/FreqItemCounter/add(java.lang.Object,long)#scala/collection/Iterable/isEmpty()
org/apache/spark/sql/execution/stat/FrequentItems/FreqItemCounter/add(java.lang.Object,long)#scala/collection/mutable/Map/contains(java.lang.Object)
org/apache/spark/sql/execution/stat/FrequentItems/FreqItemCounter/add(java.lang.Object,long)#scala/collection/mutable/Map/retain(scala.Function2)
org/apache/spark/sql/execution/stat/FrequentItems/FreqItemCounter/add(java.lang.Object,long)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/stat/FrequentItems/FreqItemCounter/add(java.lang.Object,long)#scala/collection/mutable/Map/transform(scala.Function2)
org/apache/spark/sql/execution/stat/FrequentItems/FreqItemCounter/add(java.lang.Object,long)#org/apache/spark/sql/execution/stat/FrequentItems$FreqItemCounter$$anonfun$add$1/1(org.apache.spark.sql.execution.stat.FrequentItems$FreqItemCounter,long)
org/apache/spark/sql/execution/stat/FrequentItems/FreqItemCounter/add(java.lang.Object,long)#org/apache/spark/sql/execution/stat/FrequentItems$FreqItemCounter$$anonfun$add$2/2(org.apache.spark.sql.execution.stat.FrequentItems$FreqItemCounter,long)
org/apache/spark/sql/execution/stat/FrequentItems/FreqItemCounter/add(java.lang.Object,long)#org/apache/spark/sql/execution/stat/FrequentItems$FreqItemCounter$$anonfun$add$3/3(org.apache.spark.sql.execution.stat.FrequentItems$FreqItemCounter,long)
org/apache/spark/sql/execution/stat/FrequentItems/FreqItemCounter/add(java.lang.Object,long)#scala/collection/Iterable/min(scala.math.Ordering)
org/apache/spark/sql/execution/stat/FrequentItems/FreqItemCounter/add(java.lang.Object,long)#org/apache/spark/sql/execution/stat/FrequentItems$FreqItemCounter/baseMap()
org/apache/spark/sql/execution/stat/FrequentItems/FreqItemCounter/add(java.lang.Object,long)#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/stat/FrequentItems/FreqItemCounter/add(java.lang.Object,long)#scala/collection/mutable/Map/$plus$eq(scala.Tuple2)
org/apache/spark/sql/execution/stat/FrequentItems/FreqItemCounter/add(java.lang.Object,long)#scala/collection/mutable/Map/update(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/stat/FrequentItems/FreqItemCounter/add(java.lang.Object,long)#scala/collection/mutable/Map/apply(java.lang.Object)
org/apache/spark/sql/execution/stat/FrequentItems/FreqItemCounter/add(java.lang.Object,long)#scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/streaming/RateSourceProvider/createSource(org.apache.spark.sql.SQLContext,java.lang.String,scala.Option,java.lang.String,scala.collection.immutable.Map)#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/streaming/RateSourceProvider/createSource(org.apache.spark.sql.SQLContext,java.lang.String,scala.Option,java.lang.String,scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/util/CaseInsensitiveMap/apply(java.lang.Object)
org/apache/spark/sql/execution/streaming/RateSourceProvider/createSource(org.apache.spark.sql.SQLContext,java.lang.String,scala.Option,java.lang.String,scala.collection.immutable.Map)#org/apache/spark/sql/execution/streaming/RateSourceProvider$$anonfun$8/8(org.apache.spark.sql.execution.streaming.RateSourceProvider)
org/apache/spark/sql/execution/streaming/RateSourceProvider/createSource(org.apache.spark.sql.SQLContext,java.lang.String,scala.Option,java.lang.String,scala.collection.immutable.Map)#org/apache/spark/sql/execution/streaming/RateSourceProvider$$anonfun$7/7(org.apache.spark.sql.execution.streaming.RateSourceProvider)
org/apache/spark/sql/execution/streaming/RateSourceProvider/createSource(org.apache.spark.sql.SQLContext,java.lang.String,scala.Option,java.lang.String,scala.collection.immutable.Map)#org/apache/spark/sql/execution/streaming/RateSourceProvider$$anonfun$createSource$2/2(org.apache.spark.sql.execution.streaming.RateSourceProvider)
org/apache/spark/sql/execution/streaming/RateSourceProvider/createSource(org.apache.spark.sql.SQLContext,java.lang.String,scala.Option,java.lang.String,scala.collection.immutable.Map)#org/apache/spark/sql/execution/streaming/RateSourceProvider$$anonfun$createSource$1/1(org.apache.spark.sql.execution.streaming.RateSourceProvider)
org/apache/spark/sql/execution/streaming/RateSourceProvider/createSource(org.apache.spark.sql.SQLContext,java.lang.String,scala.Option,java.lang.String,scala.collection.immutable.Map)#org/apache/spark/sql/execution/streaming/RateSourceProvider$$anonfun$2/2(org.apache.spark.sql.execution.streaming.RateSourceProvider)
org/apache/spark/sql/execution/streaming/RateSourceProvider/createSource(org.apache.spark.sql.SQLContext,java.lang.String,scala.Option,java.lang.String,scala.collection.immutable.Map)#scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)
org/apache/spark/sql/execution/streaming/RateSourceProvider/createSource(org.apache.spark.sql.SQLContext,java.lang.String,scala.Option,java.lang.String,scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/util/CaseInsensitiveMap$/apply(scala.collection.immutable.Map)
org/apache/spark/sql/execution/streaming/RateSourceProvider/createSource(org.apache.spark.sql.SQLContext,java.lang.String,scala.Option,java.lang.String,scala.collection.immutable.Map)#org/apache/spark/sql/execution/streaming/RateSourceProvider$$anonfun$6/6(org.apache.spark.sql.execution.streaming.RateSourceProvider)
org/apache/spark/sql/execution/streaming/RateSourceProvider/createSource(org.apache.spark.sql.SQLContext,java.lang.String,scala.Option,java.lang.String,scala.collection.immutable.Map)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/RateSourceProvider/createSource(org.apache.spark.sql.SQLContext,java.lang.String,scala.Option,java.lang.String,scala.collection.immutable.Map)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/streaming/RateSourceProvider/createSource(org.apache.spark.sql.SQLContext,java.lang.String,scala.Option,java.lang.String,scala.collection.immutable.Map)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/RateSourceProvider/createSource(org.apache.spark.sql.SQLContext,java.lang.String,scala.Option,java.lang.String,scala.collection.immutable.Map)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/streaming/RateSourceProvider/createSource(org.apache.spark.sql.SQLContext,java.lang.String,scala.Option,java.lang.String,scala.collection.immutable.Map)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/RateSourceProvider/createSource(org.apache.spark.sql.SQLContext,java.lang.String,scala.Option,java.lang.String,scala.collection.immutable.Map)#org/apache/spark/sql/execution/streaming/RateSourceProvider$$anonfun$1/1(org.apache.spark.sql.execution.streaming.RateSourceProvider)
org/apache/spark/sql/execution/streaming/RateSourceProvider/createSource(org.apache.spark.sql.SQLContext,java.lang.String,scala.Option,java.lang.String,scala.collection.immutable.Map)#org/apache/spark/sql/execution/streaming/RateSourceProvider$$anonfun$3/3(org.apache.spark.sql.execution.streaming.RateSourceProvider,org.apache.spark.sql.SQLContext)
org/apache/spark/sql/execution/streaming/RateSourceProvider/createSource(org.apache.spark.sql.SQLContext,java.lang.String,scala.Option,java.lang.String,scala.collection.immutable.Map)#org/apache/spark/sql/catalyst/util/CaseInsensitiveMap/get(java.lang.String)
org/apache/spark/sql/execution/streaming/RateSourceProvider/createSource(org.apache.spark.sql.SQLContext,java.lang.String,scala.Option,java.lang.String,scala.collection.immutable.Map)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/streaming/RateSourceProvider/createSource(org.apache.spark.sql.SQLContext,java.lang.String,scala.Option,java.lang.String,scala.collection.immutable.Map)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/execution/streaming/RateSourceProvider/createSource(org.apache.spark.sql.SQLContext,java.lang.String,scala.Option,java.lang.String,scala.collection.immutable.Map)#scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)
org/apache/spark/sql/execution/streaming/RateSourceProvider/createSource(org.apache.spark.sql.SQLContext,java.lang.String,scala.Option,java.lang.String,scala.collection.immutable.Map)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/streaming/RateSourceProvider/createSource(org.apache.spark.sql.SQLContext,java.lang.String,scala.Option,java.lang.String,scala.collection.immutable.Map)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/RateSourceProvider/createSource(org.apache.spark.sql.SQLContext,java.lang.String,scala.Option,java.lang.String,scala.collection.immutable.Map)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/command/ListFilesCommand/anonfun/run/2/apply(java.lang.String)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/command/ListFilesCommand/anonfun/run/2/apply(java.lang.String)#java/net/URI/URI(java.lang.String)
org/apache/spark/sql/execution/command/ListFilesCommand/anonfun/run/2/apply(java.lang.String)#java/net/URI/toString()
org/apache/spark/sql/execution/command/ListFilesCommand/anonfun/run/2/apply(java.lang.String)#java/io/File/getCanonicalFile()
org/apache/spark/sql/execution/command/ListFilesCommand/anonfun/run/2/apply(java.lang.String)#java/net/URI/getScheme()
org/apache/spark/sql/execution/command/ListFilesCommand/anonfun/run/2/apply(java.lang.String)#java/io/File/toURI()
org/apache/spark/sql/execution/command/ListFilesCommand/anonfun/run/2/apply(java.lang.String)#org/apache/hadoop/fs/Path/Path(java.lang.String)
org/apache/spark/sql/execution/command/ListFilesCommand/anonfun/run/2/apply(java.lang.String)#org/apache/hadoop/fs/Path/toUri()
org/apache/spark/sql/execution/command/ListFilesCommand/anonfun/run/2/apply(java.lang.String)#java/io/File/File(java.lang.String)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/5/apply(scala.collection.Iterator)#scala/reflect/ClassTag$/Any()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/5/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec$$anonfun$5$$anonfun$apply$5/5(org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec$$anonfun$5,org.apache.spark.sql.catalyst.expressions.JoinedRow,org.apache.spark.sql.catalyst.expressions.GenericInternalRow)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/5/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec$$anonfun$5$$anonfun$apply$4/4(org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec$$anonfun$5,org.apache.spark.sql.catalyst.InternalRow[],org.apache.spark.sql.catalyst.expressions.JoinedRow,org.apache.spark.sql.catalyst.expressions.GenericInternalRow)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/5/apply(scala.collection.Iterator)#org/apache/spark/broadcast/Broadcast/value()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/5/apply(scala.collection.Iterator)#scala/Option/isDefined()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/5/apply(scala.collection.Iterator)#scala/Array$/apply(scala.collection.Seq,scala.reflect.ClassTag)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/5/apply(scala.collection.Iterator)#scala/runtime/BoxesRunTime/boxToBoolean(boolean)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/5/apply(scala.collection.Iterator)#scala/collection/Iterator/map(scala.Function1)
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/5/apply(scala.collection.Iterator)#org/apache/spark/sql/catalyst/expressions/GenericInternalRow/GenericInternalRow(java.lang.Object[])
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/5/apply(scala.collection.Iterator)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/5/apply(scala.collection.Iterator)#scala/collection/mutable/ArrayOps/nonEmpty()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/5/apply(scala.collection.Iterator)#org/apache/spark/sql/catalyst/expressions/JoinedRow/JoinedRow()
org/apache/spark/sql/execution/joins/BroadcastNestedLoopJoinExec/anonfun/5/apply(scala.collection.Iterator)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec/anonfun/doExecute/1/anonfun/2/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec$$anonfun$doExecute$1/org$apache$spark$sql$execution$aggregate$ObjectHashAggregateExec$$anonfun$$$outer()
org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec/anonfun/doExecute/1/anonfun/2/apply(scala.collection.Iterator)#scala/collection/Iterator/hasNext()
org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec/anonfun/doExecute/1/anonfun/2/apply(scala.collection.Iterator)#scala/collection/Seq/isEmpty()
org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec/anonfun/doExecute/1/anonfun/2/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec/anonfun/doExecute/1/anonfun/2/apply(scala.collection.Iterator)#scala/collection/Iterator$/empty()
org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec/anonfun/doExecute/1/anonfun/2/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec$$anonfun$doExecute$1$$anonfun$2$$anonfun$3/3(org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec$$anonfun$doExecute$1$$anonfun$2)
org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec/anonfun/doExecute/1/anonfun/2/apply(scala.collection.Iterator)#scala/package$/Iterator()
org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec/anonfun/doExecute/1/anonfun/2/apply(scala.collection.Iterator)#scala/collection/Iterator$/single(java.lang.Object)
org/apache/spark/sql/execution/aggregate/ObjectHashAggregateExec/anonfun/doExecute/1/anonfun/2/apply(scala.collection.Iterator)#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/anonfun/11/apply()#org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions$/JDBC_NUM_PARTITIONS()
org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/anonfun/11/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/anonfun/11/apply()#org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions$/JDBC_PARTITION_COLUMN()
org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/anonfun/11/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/anonfun/11/apply()#org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions$/JDBC_LOWER_BOUND()
org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/anonfun/11/apply()#org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions$/JDBC_UPPER_BOUND()
org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/anonfun/11/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/anonfun/11/apply()#org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions$$anonfun$11/apply()
org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/anonfun/11/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/anonfun/11/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/anonfun/11/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/anonfun/11/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/SubqueryExec/anonfun/relationFuture/1/anonfun/apply/4/apply()#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/SubqueryExec/anonfun/relationFuture/1/anonfun/apply/4/apply()#org/apache/spark/sql/execution/SubqueryExec$$anonfun$relationFuture$1/org$apache$spark$sql$execution$SubqueryExec$$anonfun$$$outer()
org/apache/spark/sql/execution/SubqueryExec/anonfun/relationFuture/1/anonfun/apply/4/apply()#scala/collection/immutable/Map/values()
org/apache/spark/sql/execution/SubqueryExec/anonfun/relationFuture/1/anonfun/apply/4/apply()#scala/collection/Iterable/toSeq()
org/apache/spark/sql/execution/SubqueryExec/anonfun/relationFuture/1/anonfun/apply/4/apply()#org/apache/spark/sql/execution/metric/SQLMetrics$/postDriverMetricUpdates(org.apache.spark.SparkContext,java.lang.String,scala.collection.Seq)
org/apache/spark/sql/execution/SubqueryExec/anonfun/relationFuture/1/anonfun/apply/4/apply()#scala/collection/mutable/ArrayOps/sum(scala.math.Numeric)
org/apache/spark/sql/execution/SubqueryExec/anonfun/relationFuture/1/anonfun/apply/4/apply()#org/apache/spark/sql/execution/SubqueryExec/longMetric(java.lang.String)
org/apache/spark/sql/execution/SubqueryExec/anonfun/relationFuture/1/anonfun/apply/4/apply()#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/SubqueryExec/anonfun/relationFuture/1/anonfun/apply/4/apply()#scala/reflect/ClassTag$/Long()
org/apache/spark/sql/execution/SubqueryExec/anonfun/relationFuture/1/anonfun/apply/4/apply()#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/SubqueryExec/anonfun/relationFuture/1/anonfun/apply/4/apply()#org/apache/spark/sql/execution/SubqueryExec/sparkContext()
org/apache/spark/sql/execution/SubqueryExec/anonfun/relationFuture/1/anonfun/apply/4/apply()#scala/Predef$/longArrayOps(long[])
org/apache/spark/sql/execution/SubqueryExec/anonfun/relationFuture/1/anonfun/apply/4/apply()#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/SubqueryExec/anonfun/relationFuture/1/anonfun/apply/4/apply()#java/lang/System/nanoTime()
org/apache/spark/sql/execution/SubqueryExec/anonfun/relationFuture/1/anonfun/apply/4/apply()#org/apache/spark/sql/execution/SubqueryExec$$anonfun$relationFuture$1$$anonfun$apply$4/apply()
org/apache/spark/sql/execution/SubqueryExec/anonfun/relationFuture/1/anonfun/apply/4/apply()#org/apache/spark/sql/execution/SubqueryExec$$anonfun$relationFuture$1$$anonfun$apply$4$$anonfun$21/21(org.apache.spark.sql.execution.SubqueryExec$$anonfun$relationFuture$1$$anonfun$apply$4)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/anonfun/showHiveTableStorageInfo/1/apply(java.lang.String)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/properties()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/anonfun/showHiveTableStorageInfo/1/apply(java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/ShowCreateTableCommand/anonfun/showHiveTableStorageInfo/1/apply(java.lang.String)#scala/collection/mutable/StringBuilder/$plus$plus$eq(java.lang.String)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/anonfun/showHiveTableStorageInfo/1/apply(java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/anonfun/showHiveTableStorageInfo/1/apply(java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/anonfun/showHiveTableStorageInfo/1/apply(java.lang.String)#scala/collection/immutable/Iterable$/canBuildFrom()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/anonfun/showHiveTableStorageInfo/1/apply(java.lang.String)#org/apache/spark/sql/execution/command/ShowCreateTableCommand$$anonfun$showHiveTableStorageInfo$1$$anonfun$22/22(org.apache.spark.sql.execution.command.ShowCreateTableCommand$$anonfun$showHiveTableStorageInfo$1)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/anonfun/showHiveTableStorageInfo/1/apply(java.lang.String)#scala/collection/immutable/Map/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/anonfun/showHiveTableStorageInfo/1/apply(java.lang.String)#scala/collection/immutable/Iterable/mkString(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/anonfun/showHiveTableStorageInfo/1/apply(java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/anonfun/showHiveTableStorageInfo/1/apply(java.lang.String)#org/apache/spark/sql/catalyst/catalog/CatalogTable/storage()
org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider/createRelation(org.apache.spark.sql.SQLContext,org.apache.spark.sql.SaveMode,scala.collection.immutable.Map,org.apache.spark.sql.Dataset)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/tableExists(java.sql.Connection,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)
org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider/createRelation(org.apache.spark.sql.SQLContext,org.apache.spark.sql.SaveMode,scala.collection.immutable.Map,org.apache.spark.sql.Dataset)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/isCascadingTruncateTable(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider/createRelation(org.apache.spark.sql.SQLContext,org.apache.spark.sql.SaveMode,scala.collection.immutable.Map,org.apache.spark.sql.Dataset)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/truncateTable(java.sql.Connection,java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider/createRelation(org.apache.spark.sql.SQLContext,org.apache.spark.sql.SaveMode,scala.collection.immutable.Map,org.apache.spark.sql.Dataset)#java/sql/Connection/close()
org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider/createRelation(org.apache.spark.sql.SQLContext,org.apache.spark.sql.SaveMode,scala.collection.immutable.Map,org.apache.spark.sql.Dataset)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/saveTable(org.apache.spark.sql.Dataset,scala.Option,boolean,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)
org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider/createRelation(org.apache.spark.sql.SQLContext,org.apache.spark.sql.SaveMode,scala.collection.immutable.Map,org.apache.spark.sql.Dataset)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider/createRelation(org.apache.spark.sql.SQLContext,org.apache.spark.sql.SaveMode,scala.collection.immutable.Map,org.apache.spark.sql.Dataset)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider/createRelation(org.apache.spark.sql.SQLContext,org.apache.spark.sql.SaveMode,scala.collection.immutable.Map,org.apache.spark.sql.Dataset)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/getSchemaOption(java.sql.Connection,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)
org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider/createRelation(org.apache.spark.sql.SQLContext,org.apache.spark.sql.SaveMode,scala.collection.immutable.Map,org.apache.spark.sql.Dataset)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/dropTable(java.sql.Connection,java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider/createRelation(org.apache.spark.sql.SQLContext,org.apache.spark.sql.SaveMode,scala.collection.immutable.Map,org.apache.spark.sql.Dataset)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider/createRelation(org.apache.spark.sql.SQLContext,org.apache.spark.sql.SaveMode,scala.collection.immutable.Map,org.apache.spark.sql.Dataset)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider/createRelation(org.apache.spark.sql.SQLContext,org.apache.spark.sql.SaveMode,scala.collection.immutable.Map,org.apache.spark.sql.Dataset)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/createTable(java.sql.Connection,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)
org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider/createRelation(org.apache.spark.sql.SQLContext,org.apache.spark.sql.SaveMode,scala.collection.immutable.Map,org.apache.spark.sql.Dataset)#scala/Function0/apply()
org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider/createRelation(org.apache.spark.sql.SQLContext,org.apache.spark.sql.SaveMode,scala.collection.immutable.Map,org.apache.spark.sql.Dataset)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider/createRelation(org.apache.spark.sql.SQLContext,org.apache.spark.sql.SaveMode,scala.collection.immutable.Map,org.apache.spark.sql.Dataset)#scala/runtime/BoxesRunTime/boxToBoolean(boolean)
org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider/createRelation(org.apache.spark.sql.SQLContext,org.apache.spark.sql.SaveMode,scala.collection.immutable.Map,org.apache.spark.sql.Dataset)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider/createRelation(org.apache.spark.sql.SQLContext,org.apache.spark.sql.SaveMode,scala.collection.immutable.Map,org.apache.spark.sql.Dataset)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider/createRelation(org.apache.spark.sql.SQLContext,org.apache.spark.sql.SaveMode,scala.collection.immutable.Map,org.apache.spark.sql.Dataset)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider/createRelation(org.apache.spark.sql.SQLContext,org.apache.spark.sql.SaveMode,scala.collection.immutable.Map,org.apache.spark.sql.Dataset)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider/createRelation(org.apache.spark.sql.SQLContext,org.apache.spark.sql.SaveMode,scala.collection.immutable.Map,org.apache.spark.sql.Dataset)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider/createRelation(org.apache.spark.sql.SQLContext,org.apache.spark.sql.SaveMode,scala.collection.immutable.Map,org.apache.spark.sql.Dataset)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider/createRelation(org.apache.spark.sql.SQLContext,org.apache.spark.sql.SaveMode,scala.collection.immutable.Map,org.apache.spark.sql.Dataset)#org/apache/spark/sql/internal/SQLConf/caseSensitiveAnalysis()
org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider/createRelation(org.apache.spark.sql.SQLContext,org.apache.spark.sql.SaveMode,scala.collection.immutable.Map,org.apache.spark.sql.Dataset)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/jdbc/JdbcRelationProvider/createRelation(org.apache.spark.sql.SQLContext,org.apache.spark.sql.SaveMode,scala.collection.immutable.Map,org.apache.spark.sql.Dataset)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$/createConnectionFactory(org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions)
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/8/apply(scala.Tuple2)#org/apache/spark/sql/execution/stat/StatFunctions$$anonfun$8$$anonfun$apply$6/6(org.apache.spark.sql.execution.stat.StatFunctions$$anonfun$8,org.apache.spark.sql.catalyst.expressions.GenericInternalRow)
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/8/apply(scala.Tuple2)#org/apache/spark/sql/execution/stat/StatFunctions$/org$apache$spark$sql$execution$stat$StatFunctions$$cleanElement$1(java.lang.Object)
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/8/apply(scala.Tuple2)#org/apache/spark/unsafe/types/UTF8String/fromString(java.lang.String)
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/8/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/8/apply(scala.Tuple2)#scala/Tuple2/_2()
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/8/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/8/apply(scala.Tuple2)#scala/collection/mutable/ArrayOps/foreach(scala.Function1)
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/8/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/GenericInternalRow/GenericInternalRow(int)
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/8/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/GenericInternalRow/update(int,java.lang.Object)
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/8/apply(scala.Tuple2)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/DataSourceStrategy/getBucketId(org.apache.spark.sql.catalyst.expressions.Attribute,int,java.lang.Object)#org/apache/spark/sql/catalyst/expressions/UnsafeProjection$/create(scala.collection.Seq,scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/getBucketId(org.apache.spark.sql.catalyst.expressions.Attribute,int,java.lang.Object)#scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/getBucketId(org.apache.spark.sql.catalyst.expressions.Attribute,int,java.lang.Object)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/DataSourceStrategy/getBucketId(org.apache.spark.sql.catalyst.expressions.Attribute,int,java.lang.Object)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/getBucketId(org.apache.spark.sql.catalyst.expressions.Attribute,int,java.lang.Object)#org/apache/spark/sql/catalyst/plans/physical/HashPartitioning/partitionIdExpression()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/getBucketId(org.apache.spark.sql.catalyst.expressions.Attribute,int,java.lang.Object)#org/apache/spark/sql/catalyst/expressions/SpecificInternalRow/update(int,java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/getBucketId(org.apache.spark.sql.catalyst.expressions.Attribute,int,java.lang.Object)#org/apache/spark/sql/catalyst/expressions/Literal$/apply(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/getBucketId(org.apache.spark.sql.catalyst.expressions.Attribute,int,java.lang.Object)#org/apache/spark/sql/catalyst/expressions/UnsafeProjection/apply(org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/getBucketId(org.apache.spark.sql.catalyst.expressions.Attribute,int,java.lang.Object)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/getInt(int)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/getBucketId(org.apache.spark.sql.catalyst.expressions.Attribute,int,java.lang.Object)#org/apache/spark/sql/catalyst/expressions/Cast/eval(org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/getBucketId(org.apache.spark.sql.catalyst.expressions.Attribute,int,java.lang.Object)#org/apache/spark/sql/catalyst/plans/physical/HashPartitioning/HashPartitioning(scala.collection.Seq,int)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/getBucketId(org.apache.spark.sql.catalyst.expressions.Attribute,int,java.lang.Object)#org/apache/spark/sql/catalyst/expressions/SpecificInternalRow/SpecificInternalRow(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/getBucketId(org.apache.spark.sql.catalyst.expressions.Attribute,int,java.lang.Object)#org/apache/spark/sql/catalyst/expressions/Attribute/dataType()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#org/apache/spark/sql/execution/datasources/DataSourceStrategy$$anonfun$19/19(org.apache.spark.sql.execution.datasources.DataSourceStrategy,scala.collection.immutable.Set)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#scala/Tuple3/_2()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#scala/Tuple3/_3()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#org/apache/spark/sql/catalyst/expressions/AttributeSet/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#scala/Function3/apply(java.lang.Object,java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#org/apache/spark/sql/catalyst/expressions/AttributeSet/$plus$plus(org.apache.spark.sql.catalyst.expressions.AttributeSet)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#org/apache/spark/sql/types/StructType$/fromAttributes(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#org/apache/spark/sql/catalyst/expressions/AttributeSet/size()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#org/apache/spark/sql/types/StructType/catalogString()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#org/apache/spark/sql/execution/datasources/DataSourceStrategy$$anonfun$15/15(org.apache.spark.sql.execution.datasources.DataSourceStrategy,org.apache.spark.sql.execution.datasources.LogicalRelation)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#org/apache/spark/sql/execution/datasources/DataSourceStrategy$$anonfun$pruneFilterProjectRaw$1/1(org.apache.spark.sql.execution.datasources.DataSourceStrategy)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#org/apache/spark/sql/execution/datasources/DataSourceStrategy$$anonfun$21/21(org.apache.spark.sql.execution.datasources.DataSourceStrategy,org.apache.spark.sql.catalyst.expressions.AttributeSet)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#scala/Predef$/ArrowAssoc(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#scala/Tuple3/_1()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#org/apache/spark/sql/execution/datasources/DataSourceStrategy$$anonfun$pruneFilterProjectRaw$2/2(org.apache.spark.sql.execution.datasources.DataSourceStrategy,org.apache.spark.sql.execution.RowDataSourceScanExec)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#scala/collection/Traversable$/canBuildFrom()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#scala/collection/mutable/ArrayBuffer/toMap(scala.Predef$$less$colon$less)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#scala/collection/TraversableLike/filterNot(scala.Function1)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#org/apache/spark/sql/catalyst/expressions/AttributeSet$/apply(scala.collection.Iterable)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#org/apache/spark/sql/execution/datasources/DataSourceStrategy$$anonfun$pruneFilterProjectRaw$3/3(org.apache.spark.sql.execution.datasources.DataSourceStrategy,org.apache.spark.sql.execution.RowDataSourceScanExec)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#scala/collection/TraversableOnce/toSeq()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#scala/collection/Seq/size()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#org/apache/spark/sql/catalyst/plans/physical/UnknownPartitioning/UnknownPartitioning(int)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#scala/Tuple3/Tuple3(java.lang.Object,java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#org/apache/spark/sql/catalyst/expressions/AttributeSet/subsetOf(org.apache.spark.sql.catalyst.expressions.AttributeSet)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#org/apache/spark/sql/execution/datasources/DataSourceStrategy$$anonfun$pruneFilterProjectRaw$4/4(org.apache.spark.sql.execution.datasources.DataSourceStrategy,org.apache.spark.sql.execution.RowDataSourceScanExec)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#scala/collection/mutable/ArrayBuffer$/empty()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#scala/Predef$/$conforms()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#org/apache/spark/sql/catalyst/expressions/AttributeSet/$minus$minus(scala.collection.Traversable)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#org/apache/spark/sql/execution/datasources/DataSourceStrategy$$anonfun$pruneFilterProjectRaw$5/5(org.apache.spark.sql.execution.datasources.DataSourceStrategy,org.apache.spark.sql.execution.RowDataSourceScanExec)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#org/apache/spark/sql/execution/datasources/DataSourceStrategy$$anonfun$16/16(org.apache.spark.sql.execution.datasources.DataSourceStrategy,scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#scala/collection/Seq/filterNot(scala.Function1)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#org/apache/spark/sql/execution/datasources/DataSourceStrategy$$anonfun$23/23(org.apache.spark.sql.execution.datasources.DataSourceStrategy)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#org/apache/spark/sql/execution/datasources/DataSourceStrategy$$anonfun$22/22(org.apache.spark.sql.execution.datasources.DataSourceStrategy)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#org/apache/spark/sql/execution/datasources/DataSourceStrategy$$anonfun$20/20(org.apache.spark.sql.execution.datasources.DataSourceStrategy)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#org/apache/spark/sql/execution/datasources/DataSourceStrategy$$anonfun$18/18(org.apache.spark.sql.execution.datasources.DataSourceStrategy)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#org/apache/spark/sql/execution/datasources/DataSourceStrategy$$anonfun$17/17(org.apache.spark.sql.execution.datasources.DataSourceStrategy)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#org/apache/spark/sql/execution/datasources/DataSourceStrategy$$anonfun$14/14(org.apache.spark.sql.execution.datasources.DataSourceStrategy)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#org/apache/spark/sql/execution/datasources/DataSourceStrategy$$anonfun$24/24(org.apache.spark.sql.execution.datasources.DataSourceStrategy)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#org/apache/spark/sql/execution/datasources/DataSourceStrategy$$anonfun$13/13(org.apache.spark.sql.execution.datasources.DataSourceStrategy)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#org/apache/spark/sql/execution/datasources/DataSourceStrategy$/selectFilters(org.apache.spark.sql.sources.BaseRelation,scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#scala/collection/Seq/reduceLeftOption(scala.Function2)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/pruneFilterProjectRaw(org.apache.spark.sql.execution.datasources.LogicalRelation,scala.collection.Seq,scala.collection.Seq,scala.Function3)#scala/collection/Seq/mkString(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Predef$/Map()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Tuple3/_2()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Tuple3/_3()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Option/isEmpty()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/planning/PhysicalOperation$/unapply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/physical/UnknownPartitioning/UnknownPartitioning(int)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/immutable/Map$/empty()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Option/get()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/datasources/DataSourceStrategy$$anonfun$11/11(org.apache.spark.sql.execution.datasources.DataSourceStrategy,org.apache.spark.sql.execution.datasources.LogicalRelation,org.apache.spark.sql.sources.BaseRelation)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/datasources/DataSourceStrategy$$anonfun$12/12(org.apache.spark.sql.execution.datasources.DataSourceStrategy,org.apache.spark.sql.execution.datasources.LogicalRelation,org.apache.spark.sql.sources.BaseRelation)
org/apache/spark/sql/execution/datasources/DataSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Tuple3/_1()
org/apache/spark/sql/execution/datasources/DataSourceStrategy/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/datasources/DataSourceStrategy$$anonfun$10/10(org.apache.spark.sql.execution.datasources.DataSourceStrategy,org.apache.spark.sql.execution.datasources.LogicalRelation,org.apache.spark.sql.sources.BaseRelation)
org/apache/spark/sql/SparkSession/Builder/anonfun/6/apply()#scala/collection/mutable/HashMap/foreach(scala.Function1)
org/apache/spark/sql/SparkSession/Builder/anonfun/6/apply()#org/apache/spark/sql/SparkSession$Builder$$anonfun$6/apply()
org/apache/spark/sql/SparkSession/Builder/anonfun/6/apply()#org/apache/spark/sql/SparkSession$Builder$$anonfun$6$$anonfun$apply$2/2(org.apache.spark.sql.SparkSession$Builder$$anonfun$6,org.apache.spark.SparkContext)
org/apache/spark/sql/SparkSession/Builder/anonfun/6/apply()#java/util/UUID/randomUUID()
org/apache/spark/sql/SparkSession/Builder/anonfun/6/apply()#org/apache/spark/SparkContext$/getOrCreate(org.apache.spark.SparkConf)
org/apache/spark/sql/SparkSession/Builder/anonfun/6/apply()#org/apache/spark/SparkContext/conf()
org/apache/spark/sql/SparkSession/Builder/anonfun/6/apply()#org/apache/spark/SparkConf/setAppName(java.lang.String)
org/apache/spark/sql/SparkSession/Builder/anonfun/6/apply()#org/apache/spark/sql/SparkSession$Builder$$anonfun$6$$anonfun$apply$1/1(org.apache.spark.sql.SparkSession$Builder$$anonfun$6,org.apache.spark.SparkConf)
org/apache/spark/sql/SparkSession/Builder/anonfun/6/apply()#java/util/UUID/toString()
org/apache/spark/sql/SparkSession/Builder/anonfun/6/apply()#org/apache/spark/SparkConf/SparkConf()
org/apache/spark/sql/SparkSession/Builder/anonfun/6/apply()#org/apache/spark/SparkConf/contains(java.lang.String)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDescribeTable/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$DescribeTableContext/partitionSpec()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDescribeTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitDescribeTable$1$$anonfun$6/6(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitDescribeTable$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDescribeTable/1/apply()#scala/collection/immutable/Map/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDescribeTable/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$DescribeTableContext/EXTENDED()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDescribeTable/1/apply()#scala/collection/immutable/Map$/canBuildFrom()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDescribeTable/1/apply()#scala/collection/immutable/Map$/empty()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDescribeTable/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$DescribeTableContext/tableIdentifier()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDescribeTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitDescribeTable$1/apply()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDescribeTable/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$DescribeTableContext/FORMATTED()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDescribeTable/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$DescribeTableContext/describeColName()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDescribeTable/1/apply()#scala/Predef$/Map()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDescribeTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder/visitPartitionSpec(org.apache.spark.sql.catalyst.parser.SqlBaseParser$PartitionSpecContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDescribeTable/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder/visitTableIdentifier(org.apache.spark.sql.catalyst.parser.SqlBaseParser$TableIdentifierContext)
org/apache/spark/sql/execution/QueryExecution/anonfun/hiveResultString/1/apply(org.apache.spark.sql.Row)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/QueryExecution/anonfun/hiveResultString/1/apply(org.apache.spark.sql.Row)#org/apache/spark/sql/execution/QueryExecution$$anonfun$hiveResultString$1$$anonfun$apply$2/2(org.apache.spark.sql.execution.QueryExecution$$anonfun$hiveResultString$1)
org/apache/spark/sql/execution/QueryExecution/anonfun/hiveResultString/1/apply(org.apache.spark.sql.Row)#org/apache/spark/sql/execution/QueryExecution$$anonfun$hiveResultString$1$$anonfun$apply$1/1(org.apache.spark.sql.execution.QueryExecution$$anonfun$hiveResultString$1)
org/apache/spark/sql/execution/QueryExecution/anonfun/hiveResultString/1/apply(org.apache.spark.sql.Row)#org/apache/spark/sql/Row$/unapplySeq(org.apache.spark.sql.Row)
org/apache/spark/sql/execution/QueryExecution/anonfun/hiveResultString/1/apply(org.apache.spark.sql.Row)#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/QueryExecution/anonfun/hiveResultString/1/apply(org.apache.spark.sql.Row)#scala/collection/TraversableOnce/mkString(java.lang.String)
org/apache/spark/sql/execution/QueryExecution/anonfun/hiveResultString/1/apply(org.apache.spark.sql.Row)#scala/collection/SeqLike/apply(int)
org/apache/spark/sql/execution/QueryExecution/anonfun/hiveResultString/1/apply(org.apache.spark.sql.Row)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/QueryExecution/anonfun/hiveResultString/1/apply(org.apache.spark.sql.Row)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/QueryExecution/anonfun/hiveResultString/1/apply(org.apache.spark.sql.Row)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/QueryExecution/anonfun/hiveResultString/1/apply(org.apache.spark.sql.Row)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/QueryExecution/anonfun/hiveResultString/1/apply(org.apache.spark.sql.Row)#scala/Some/get()
org/apache/spark/sql/execution/QueryExecution/anonfun/hiveResultString/1/apply(org.apache.spark.sql.Row)#scala/collection/SeqLike/lengthCompare(int)
org/apache/spark/sql/execution/QueryExecution/anonfun/hiveResultString/1/apply(org.apache.spark.sql.Row)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/QueryExecution/anonfun/hiveResultString/1/apply(org.apache.spark.sql.Row)#scala/Some/isEmpty()
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/anonfun/3/apply(org.apache.spark.sql.execution.datasources.PartitionPath)#scala/Some/x()
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/anonfun/3/apply(org.apache.spark.sql.execution.datasources.PartitionPath)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/anonfun/3/apply(org.apache.spark.sql.execution.datasources.PartitionPath)#org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$3$$anonfun$4/4(org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex$$anonfun$3)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/anonfun/3/apply(org.apache.spark.sql.execution.datasources.PartitionPath)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/anonfun/3/apply(org.apache.spark.sql.execution.datasources.PartitionPath)#scala/collection/immutable/Map/get(java.lang.Object)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/anonfun/3/apply(org.apache.spark.sql.execution.datasources.PartitionPath)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/anonfun/3/apply(org.apache.spark.sql.execution.datasources.PartitionPath)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/anonfun/3/apply(org.apache.spark.sql.execution.datasources.PartitionPath)#scala/collection/mutable/ArrayOps/filter(scala.Function1)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/anonfun/3/apply(org.apache.spark.sql.execution.datasources.PartitionPath)#org/apache/spark/sql/execution/datasources/PartitionPath/values()
org/apache/spark/sql/execution/streaming/TextSocketSourceProvider/sourceSchema(org.apache.spark.sql.SQLContext,scala.Option,java.lang.String,scala.collection.immutable.Map)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/streaming/TextSocketSourceProvider/sourceSchema(org.apache.spark.sql.SQLContext,scala.Option,java.lang.String,scala.collection.immutable.Map)#org/apache/spark/sql/execution/streaming/TextSocketSource$/SCHEMA_REGULAR()
org/apache/spark/sql/execution/streaming/TextSocketSourceProvider/sourceSchema(org.apache.spark.sql.SQLContext,scala.Option,java.lang.String,scala.collection.immutable.Map)#scala/collection/immutable/Map/contains(java.lang.Object)
org/apache/spark/sql/execution/streaming/TextSocketSourceProvider/sourceSchema(org.apache.spark.sql.SQLContext,scala.Option,java.lang.String,scala.collection.immutable.Map)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/streaming/TextSocketSourceProvider/sourceSchema(org.apache.spark.sql.SQLContext,scala.Option,java.lang.String,scala.collection.immutable.Map)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/streaming/TextSocketSourceProvider/sourceSchema(org.apache.spark.sql.SQLContext,scala.Option,java.lang.String,scala.collection.immutable.Map)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/streaming/TextSocketSourceProvider/sourceSchema(org.apache.spark.sql.SQLContext,scala.Option,java.lang.String,scala.collection.immutable.Map)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/streaming/TextSocketSourceProvider/sourceSchema(org.apache.spark.sql.SQLContext,scala.Option,java.lang.String,scala.collection.immutable.Map)#org/apache/spark/sql/execution/streaming/TextSocketSource$/SCHEMA_TIMESTAMP()
org/apache/spark/sql/execution/streaming/TextSocketSourceProvider/sourceSchema(org.apache.spark.sql.SQLContext,scala.Option,java.lang.String,scala.collection.immutable.Map)#org/apache/spark/sql/execution/streaming/TextSocketSourceProvider$$anonfun$sourceSchema$1/1(org.apache.spark.sql.execution.streaming.TextSocketSourceProvider)
org/apache/spark/sql/execution/streaming/TextSocketSourceProvider/sourceSchema(org.apache.spark.sql.SQLContext,scala.Option,java.lang.String,scala.collection.immutable.Map)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/streaming/TextSocketSourceProvider/parseIncludeTimestamp(scala.collection.immutable.Map)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/streaming/TextSocketSourceProvider/parseIncludeTimestamp(scala.collection.immutable.Map)#org/apache/spark/sql/execution/streaming/TextSocketSourceProvider$$anonfun$1/1(org.apache.spark.sql.execution.streaming.TextSocketSourceProvider,scala.collection.immutable.Map)
org/apache/spark/sql/execution/streaming/TextSocketSourceProvider/parseIncludeTimestamp(scala.collection.immutable.Map)#scala/util/Try$/apply(scala.Function0)
org/apache/spark/sql/execution/streaming/TextSocketSourceProvider/parseIncludeTimestamp(scala.collection.immutable.Map)#scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)
org/apache/spark/sql/execution/streaming/TextSocketSourceProvider/parseIncludeTimestamp(scala.collection.immutable.Map)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/streaming/TextSocketSourceProvider/parseIncludeTimestamp(scala.collection.immutable.Map)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/streaming/TextSocketSourceProvider/parseIncludeTimestamp(scala.collection.immutable.Map)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/streaming/TextSocketSourceProvider/parseIncludeTimestamp(scala.collection.immutable.Map)#scala/util/Success/value()
org/apache/spark/sql/execution/streaming/TextSocketSourceProvider/parseIncludeTimestamp(scala.collection.immutable.Map)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/streaming/TextSocketSourceProvider/parseIncludeTimestamp(scala.collection.immutable.Map)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/json/MultiLineJsonDataSource/createBaseRdd(org.apache.spark.sql.SparkSession,scala.collection.Seq)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/json/MultiLineJsonDataSource/createBaseRdd(org.apache.spark.sql.SparkSession,scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/json/MultiLineJsonDataSource/createBaseRdd(org.apache.spark.sql.SparkSession,scala.collection.Seq)#scala/collection/Seq/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/json/MultiLineJsonDataSource/createBaseRdd(org.apache.spark.sql.SparkSession,scala.collection.Seq)#org/apache/spark/SparkContext/defaultMinPartitions()
org/apache/spark/sql/execution/datasources/json/MultiLineJsonDataSource/createBaseRdd(org.apache.spark.sql.SparkSession,scala.collection.Seq)#org/apache/spark/rdd/BinaryFileRDD/BinaryFileRDD(org.apache.spark.SparkContext,java.lang.Class,java.lang.Class,java.lang.Class,org.apache.hadoop.conf.Configuration,int)
org/apache/spark/sql/execution/datasources/json/MultiLineJsonDataSource/createBaseRdd(org.apache.spark.sql.SparkSession,scala.collection.Seq)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/json/MultiLineJsonDataSource/createBaseRdd(org.apache.spark.sql.SparkSession,scala.collection.Seq)#org/apache/hadoop/mapreduce/Job/getConfiguration()
org/apache/spark/sql/execution/datasources/json/MultiLineJsonDataSource/createBaseRdd(org.apache.spark.sql.SparkSession,scala.collection.Seq)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/json/MultiLineJsonDataSource/createBaseRdd(org.apache.spark.sql.SparkSession,scala.collection.Seq)#org/apache/hadoop/mapreduce/lib/input/FileInputFormat/setInputPaths(org.apache.hadoop.mapreduce.Job,org.apache.hadoop.fs.Path[])
org/apache/spark/sql/execution/datasources/json/MultiLineJsonDataSource/createBaseRdd(org.apache.spark.sql.SparkSession,scala.collection.Seq)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/json/MultiLineJsonDataSource/createBaseRdd(org.apache.spark.sql.SparkSession,scala.collection.Seq)#org/apache/spark/rdd/RDD$/rddToPairRDDFunctions(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag,scala.math.Ordering)
org/apache/spark/sql/execution/datasources/json/MultiLineJsonDataSource/createBaseRdd(org.apache.spark.sql.SparkSession,scala.collection.Seq)#org/apache/spark/rdd/PairRDDFunctions/values()
org/apache/spark/sql/execution/datasources/json/MultiLineJsonDataSource/createBaseRdd(org.apache.spark.sql.SparkSession,scala.collection.Seq)#org/apache/spark/rdd/BinaryFileRDD/setName(java.lang.String)
org/apache/spark/sql/execution/datasources/json/MultiLineJsonDataSource/createBaseRdd(org.apache.spark.sql.SparkSession,scala.collection.Seq)#org/apache/hadoop/mapreduce/Job/getInstance(org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/datasources/json/MultiLineJsonDataSource/createBaseRdd(org.apache.spark.sql.SparkSession,scala.collection.Seq)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/json/MultiLineJsonDataSource/createBaseRdd(org.apache.spark.sql.SparkSession,scala.collection.Seq)#org/apache/spark/sql/execution/datasources/json/MultiLineJsonDataSource$$anonfun$6/6()
org/apache/spark/sql/execution/datasources/json/MultiLineJsonDataSource/createBaseRdd(org.apache.spark.sql.SparkSession,scala.collection.Seq)#scala/collection/Seq/mkString(java.lang.String)
org/apache/spark/sql/execution/datasources/json/MultiLineJsonDataSource/createBaseRdd(org.apache.spark.sql.SparkSession,scala.collection.Seq)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/SetCommand/anonfun/7/apply(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/SetCommand$$anonfun$7$$anonfun$apply$6/6(org.apache.spark.sql.execution.command.SetCommand$$anonfun$7)
org/apache/spark/sql/execution/command/SetCommand/anonfun/7/apply(org.apache.spark.sql.SparkSession)#java/lang/Object/toString()
org/apache/spark/sql/execution/command/SetCommand/anonfun/7/apply(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/command/SetCommand/anonfun/7/apply(org.apache.spark.sql.SparkSession)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/command/SetCommand/anonfun/7/apply(org.apache.spark.sql.SparkSession)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/SetCommand/anonfun/7/apply(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/internal/SQLConf/numShufflePartitions()
org/apache/spark/sql/execution/command/SetCommand/anonfun/7/apply(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/internal/SQLConf$/SHUFFLE_PARTITIONS()
org/apache/spark/sql/execution/command/SetCommand/anonfun/7/apply(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/SetCommand/logWarning(scala.Function0)
org/apache/spark/sql/execution/command/SetCommand/anonfun/7/apply(org.apache.spark.sql.SparkSession)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/SetCommand/anonfun/7/apply(org.apache.spark.sql.SparkSession)#org/apache/spark/internal/config/ConfigEntry/key()
org/apache/spark/sql/execution/command/SetCommand/anonfun/7/apply(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/Row$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/next()#org/apache/spark/unsafe/KVIterator/next()
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/next()#org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator$$anonfun$1/1(org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/next()#org/apache/spark/unsafe/KVIterator/getValue()
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/next()#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/next()#java/util/NoSuchElementException/NoSuchElementException()
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/next()#org/apache/spark/unsafe/KVIterator/getKey()
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/next()#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/next()#org/apache/spark/sql/catalyst/expressions/UnsafeRow/copyFrom(org.apache.spark.sql.catalyst.expressions.UnsafeRow)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/next()#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/next()#org/apache/spark/executor/TaskMetrics/memoryBytesSpilled()
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/next()#org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/generateOutput()
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/next()#org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator$$anonfun$12/12(org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/next()#org/apache/spark/TaskContext$/get()
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/next()#org/apache/spark/TaskContext/taskMetrics()
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/next()#org/apache/spark/sql/catalyst/expressions/UnsafeRow/copy()
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/next()#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/next()#scala/Function2/apply(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/next()#java/lang/Math/max(long,long)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/next()#org/apache/spark/executor/TaskMetrics/incPeakExecutionMemory(long)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/switchToSortBasedAggregation()#org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/initializeAggregateFunctions(scala.collection.Seq,int)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/switchToSortBasedAggregation()#org/apache/spark/sql/execution/UnsafeKVExternalSorter$KVSorterIterator/getKey()
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/switchToSortBasedAggregation()#org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/generateProcessRow(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/switchToSortBasedAggregation()#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/switchToSortBasedAggregation()#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/switchToSortBasedAggregation()#scala/collection/mutable/ArrayOps/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/switchToSortBasedAggregation()#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/switchToSortBasedAggregation()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/switchToSortBasedAggregation()#org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator$$anonfun$10/10(org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/switchToSortBasedAggregation()#org/apache/spark/sql/execution/UnsafeKVExternalSorter$KVSorterIterator/getValue()
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/switchToSortBasedAggregation()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/switchToSortBasedAggregation()#org/apache/spark/sql/catalyst/expressions/UnsafeRow/copy()
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/switchToSortBasedAggregation()#org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator$$anonfun$9/9(org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/switchToSortBasedAggregation()#org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator$$anonfun$switchToSortBasedAggregation$1/1(org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/switchToSortBasedAggregation()#org/apache/spark/sql/execution/UnsafeKVExternalSorter$KVSorterIterator/next()
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/switchToSortBasedAggregation()#org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/logInfo(scala.Function0)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/switchToSortBasedAggregation()#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/createNewAggregationBuffer()#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/createNewAggregationBuffer()#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/createNewAggregationBuffer()#org/apache/spark/sql/catalyst/expressions/UnsafeProjection/apply(org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/createNewAggregationBuffer()#org/apache/spark/sql/catalyst/expressions/package$MutableProjection/target(org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/createNewAggregationBuffer()#org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator$$anonfun$createNewAggregationBuffer$2/2(org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator,org.apache.spark.sql.catalyst.expressions.UnsafeRow)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/createNewAggregationBuffer()#org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator$$anonfun$createNewAggregationBuffer$1/1(org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/createNewAggregationBuffer()#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/createNewAggregationBuffer()#org/apache/spark/sql/catalyst/expressions/package$MutableProjection/apply(java.lang.Object)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/createNewAggregationBuffer()#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/createNewAggregationBuffer()#scala/collection/mutable/ArrayOps/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/createNewAggregationBuffer()#org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/aggregateFunctions()
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/createNewAggregationBuffer()#org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator$$anonfun$2/2(org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/createNewAggregationBuffer()#org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator$$anonfun$3/3(org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/createNewAggregationBuffer()#org/apache/spark/sql/catalyst/expressions/UnsafeProjection$/create(org.apache.spark.sql.types.DataType[])
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/createNewAggregationBuffer()#scala/collection/mutable/ArrayOps/foreach(scala.Function1)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/createNewAggregationBuffer()#org/apache/spark/sql/catalyst/expressions/GenericInternalRow/GenericInternalRow(int)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/createNewAggregationBuffer()#org/apache/spark/sql/catalyst/expressions/package$/EmptyRow()
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/createNewAggregationBuffer()#org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/expressionAggInitialProjection()
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/createNewAggregationBuffer()#scala/collection/mutable/ArrayOps/collect(scala.PartialFunction,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/processInputs(scala.Tuple2)#org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/groupingProjection()
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/processInputs(scala.Tuple2)#scala/collection/Seq/isEmpty()
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/processInputs(scala.Tuple2)#scala/collection/Iterator/next()
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/processInputs(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/UnsafeProjection/apply(org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/processInputs(scala.Tuple2)#scala/Function2/apply(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/processInputs(scala.Tuple2)#scala/collection/Iterator/hasNext()
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/processInputs(scala.Tuple2)#java/lang/OutOfMemoryError/OutOfMemoryError(java.lang.String)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/processInputs(scala.Tuple2)#org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/processRow()
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/processInputs(scala.Tuple2)#scala/Tuple2/_2$mcI$sp()
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/generateResultProjection()#org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator$$anonfun$4/4(org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/generateResultProjection()#org/apache/spark/sql/catalyst/expressions/codegen/GenerateUnsafeRowJoiner$/create(org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/generateResultProjection()#org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator$$anonfun$5/5(org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/generateResultProjection()#org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator$$anonfun$6/6(org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/generateResultProjection()#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/generateResultProjection()#scala/collection/SeqLike/distinct()
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/generateResultProjection()#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/generateResultProjection()#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/generateResultProjection()#org/apache/spark/sql/types/StructType$/fromAttributes(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/generateResultProjection()#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/generateResultProjection()#scala/collection/mutable/ArrayOps/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/generateResultProjection()#scala/collection/Seq/contains(java.lang.Object)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/generateResultProjection()#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/generateResultProjection()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/generateResultProjection()#org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator$$anonfun$generateResultProjection$1/1(org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator,org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowJoiner)
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/generateResultProjection()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/generateResultProjection()#org/apache/spark/sql/execution/aggregate/TungstenAggregationIterator/aggregateFunctions()
org/apache/spark/sql/execution/command/SetCommand/anonfun/2/anonfun/apply/2/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/SetCommand/anonfun/2/anonfun/apply/2/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/SetCommand/anonfun/2/anonfun/apply/2/apply()#org/apache/spark/sql/execution/command/SetCommand$$anonfun$2$$anonfun$apply$2/apply()
org/apache/spark/sql/execution/command/SetCommand/anonfun/2/anonfun/apply/2/apply()#org/apache/spark/sql/internal/SQLConf$Replaced$/MAPREDUCE_JOB_REDUCES()
org/apache/spark/sql/execution/command/SetCommand/anonfun/2/anonfun/apply/2/apply()#org/apache/spark/internal/config/ConfigEntry/key()
org/apache/spark/sql/execution/command/SetCommand/anonfun/2/anonfun/apply/2/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/command/SetCommand/anonfun/2/anonfun/apply/2/apply()#org/apache/spark/sql/internal/SQLConf$/SHUFFLE_PARTITIONS()
org/apache/spark/sql/execution/command/SetCommand/anonfun/2/anonfun/apply/2/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/command/SetCommand/anonfun/2/anonfun/apply/2/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/SetCommand/anonfun/2/anonfun/apply/2/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/SetCommand/anonfun/2/anonfun/apply/2/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/doExecute/1/anonfun/4/apply(scala.collection.Iterator)#scala/collection/Seq/isEmpty()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/doExecute/1/anonfun/4/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$doExecute$1$$anonfun$4$$anonfun$5/5(org.apache.spark.sql.execution.aggregate.HashAggregateExec$$anonfun$doExecute$1$$anonfun$4)
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/doExecute/1/anonfun/4/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/aggregate/HashAggregateExec$$anonfun$doExecute$1/org$apache$spark$sql$execution$aggregate$HashAggregateExec$$anonfun$$$outer()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/doExecute/1/anonfun/4/apply(scala.collection.Iterator)#scala/collection/Iterator/hasNext()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/doExecute/1/anonfun/4/apply(scala.collection.Iterator)#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/doExecute/1/anonfun/4/apply(scala.collection.Iterator)#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/doExecute/1/anonfun/4/apply(scala.collection.Iterator)#scala/collection/Iterator$/empty()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/doExecute/1/anonfun/4/apply(scala.collection.Iterator)#scala/package$/Iterator()
org/apache/spark/sql/execution/aggregate/HashAggregateExec/anonfun/doExecute/1/anonfun/4/apply(scala.collection.Iterator)#scala/collection/Iterator$/single(java.lang.Object)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableStorageInfo(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableStorageInfo(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/outputFormat()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableStorageInfo(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#org/apache/spark/sql/catalyst/catalog/CatalogTable/tableType()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableStorageInfo(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/collection/mutable/StringBuilder/$plus$plus$eq(java.lang.String)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableStorageInfo(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#org/apache/spark/sql/execution/command/ShowCreateTableCommand$$anonfun$showHiveTableStorageInfo$3/3(org.apache.spark.sql.execution.command.ShowCreateTableCommand,scala.collection.mutable.StringBuilder)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableStorageInfo(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#org/apache/spark/sql/execution/command/ShowCreateTableCommand$$anonfun$showHiveTableStorageInfo$4/4(org.apache.spark.sql.execution.command.ShowCreateTableCommand,scala.collection.mutable.StringBuilder)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableStorageInfo(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#org/apache/spark/sql/catalyst/catalog/CatalogTableType$/EXTERNAL()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableStorageInfo(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/Option/foreach(scala.Function1)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableStorageInfo(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/inputFormat()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableStorageInfo(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/serde()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableStorageInfo(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/Option/isDefined()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableStorageInfo(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#org/apache/spark/sql/execution/command/ShowCreateTableCommand$$anonfun$showHiveTableStorageInfo$1/1(org.apache.spark.sql.execution.command.ShowCreateTableCommand,org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableStorageInfo(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#org/apache/spark/sql/execution/command/ShowCreateTableCommand$$anonfun$showHiveTableStorageInfo$2/2(org.apache.spark.sql.execution.command.ShowCreateTableCommand,scala.collection.mutable.StringBuilder)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableStorageInfo(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/locationUri()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableStorageInfo(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#org/apache/spark/sql/catalyst/catalog/CatalogTable/storage()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableDataColumns(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableDataColumns(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableDataColumns(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableDataColumns(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/collection/mutable/ArrayOps/mkString(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableDataColumns(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/collection/mutable/StringBuilder/$plus$plus$eq(java.lang.String)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableDataColumns(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#org/apache/spark/sql/execution/command/ShowCreateTableCommand$$anonfun$24/24(org.apache.spark.sql.execution.command.ShowCreateTableCommand)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableDataColumns(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableDataColumns(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableDataColumns(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#org/apache/spark/sql/catalyst/catalog/CatalogTable/schema()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableNonDataColumns(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#org/apache/spark/sql/types/StructType/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableNonDataColumns(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableNonDataColumns(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/Option/isDefined()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableNonDataColumns(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionSchema()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableNonDataColumns(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/collection/mutable/StringBuilder/$plus$plus$eq(java.lang.String)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableNonDataColumns(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableNonDataColumns(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#java/lang/UnsupportedOperationException/UnsupportedOperationException(java.lang.String)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableNonDataColumns(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#org/apache/spark/sql/catalyst/catalog/CatalogTable/bucketSpec()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableNonDataColumns(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#org/apache/spark/sql/execution/command/ShowCreateTableCommand$$anonfun$21/21(org.apache.spark.sql.execution.command.ShowCreateTableCommand)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableNonDataColumns(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableNonDataColumns(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/collection/Seq/mkString(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/org$apache$spark$sql$execution$command$ShowCreateTableCommand$$columnToDDLFragment(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/execution/command/ShowCreateTableCommand$$anonfun$org$apache$spark$sql$execution$command$ShowCreateTableCommand$$columnToDDLFragment$1/1(org.apache.spark.sql.execution.command.ShowCreateTableCommand)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/org$apache$spark$sql$execution$command$ShowCreateTableCommand$$columnToDDLFragment(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/types/StructField/dataType()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/org$apache$spark$sql$execution$command$ShowCreateTableCommand$$columnToDDLFragment(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/types/StructField/getComment()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/org$apache$spark$sql$execution$command$ShowCreateTableCommand$$columnToDDLFragment(org.apache.spark.sql.types.StructField)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/org$apache$spark$sql$execution$command$ShowCreateTableCommand$$columnToDDLFragment(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/types/DataType/catalogString()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/org$apache$spark$sql$execution$command$ShowCreateTableCommand$$columnToDDLFragment(org.apache.spark.sql.types.StructField)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/org$apache$spark$sql$execution$command$ShowCreateTableCommand$$columnToDDLFragment(org.apache.spark.sql.types.StructField)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/org$apache$spark$sql$execution$command$ShowCreateTableCommand$$columnToDDLFragment(org.apache.spark.sql.types.StructField)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/ShowCreateTableCommand/org$apache$spark$sql$execution$command$ShowCreateTableCommand$$columnToDDLFragment(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/catalyst/util/package$/quoteIdentifier(java.lang.String)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/org$apache$spark$sql$execution$command$ShowCreateTableCommand$$columnToDDLFragment(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/types/StructField/name()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/org$apache$spark$sql$execution$command$ShowCreateTableCommand$$columnToDDLFragment(org.apache.spark.sql.types.StructField)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/org$apache$spark$sql$execution$command$ShowCreateTableCommand$$columnToDDLFragment(org.apache.spark.sql.types.StructField)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/org$apache$spark$sql$execution$command$ShowCreateTableCommand$$columnToDDLFragment(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/execution/command/ShowCreateTableCommand$$anonfun$20/20(org.apache.spark.sql.execution.command.ShowCreateTableCommand)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/org$apache$spark$sql$execution$command$ShowCreateTableCommand$$columnToDDLFragment(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/execution/command/ShowCreateTableCommand$$anonfun$19/19(org.apache.spark.sql.execution.command.ShowCreateTableCommand)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableHeader(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableHeader(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#org/apache/spark/sql/execution/command/ShowCreateTableCommand$$anonfun$showHiveTableHeader$2/2(org.apache.spark.sql.execution.command.ShowCreateTableCommand,scala.collection.mutable.StringBuilder)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableHeader(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#org/apache/spark/sql/execution/command/ShowCreateTableCommand$$anonfun$showHiveTableHeader$1/1(org.apache.spark.sql.execution.command.ShowCreateTableCommand)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableHeader(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/collection/mutable/StringBuilder/$plus$plus$eq(java.lang.String)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableHeader(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableHeader(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#org/apache/spark/sql/execution/command/ShowCreateTableCommand$$anonfun$18/18(org.apache.spark.sql.execution.command.ShowCreateTableCommand)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableHeader(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/collection/Seq/mkString(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableHeader(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#org/apache/spark/sql/types/StructType/filterNot(scala.Function1)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableHeader(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#org/apache/spark/sql/catalyst/catalog/CatalogTable/comment()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableHeader(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/Option/foreach(scala.Function1)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableHeader(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableHeader(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#org/apache/spark/sql/catalyst/catalog/CatalogTable/schema()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableHeader(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#org/apache/spark/sql/execution/command/ShowCreateTableCommand$$anonfun$17/17(org.apache.spark.sql.execution.command.ShowCreateTableCommand,org.apache.spark.sql.catalyst.catalog.CatalogTable)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showHiveTableHeader(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showCreateHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTableType$/EXTERNAL()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showCreateHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showCreateHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/TableIdentifier/quotedString()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showCreateHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/types/StructType/nonEmpty()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showCreateHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/types/StructType/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showCreateHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showCreateHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showCreateHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/viewText()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showCreateHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/Option$/option2Iterable(scala.Option)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showCreateHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showCreateHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/tableType()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showCreateHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/collection/mutable/StringBuilder/$plus$plus$eq(java.lang.String)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showCreateHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/collection/TraversableOnce/mkString(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showCreateHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showCreateHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/schema()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showCreateHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/execution/command/ShowCreateTableCommand$$anonfun$showCreateHiveTable$1/1(org.apache.spark.sql.execution.command.ShowCreateTableCommand)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showCreateHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showCreateHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/unsupportedFeatures()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showCreateHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showCreateHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/collection/mutable/StringBuilder$/newBuilder()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showCreateHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/collection/Iterable/mkString(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showCreateHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/package$/StringBuilder()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showCreateHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTableType$/VIEW()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showCreateHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTableType$/MANAGED()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showCreateHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showCreateHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/reportUnsupportedError$1(scala.collection.Seq,org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/reportUnsupportedError$1(scala.collection.Seq,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/reportUnsupportedError$1(scala.collection.Seq,org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/ShowCreateTableCommand/reportUnsupportedError$1(scala.collection.Seq,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/execution/command/ShowCreateTableCommand$$anonfun$reportUnsupportedError$1$1/1(org.apache.spark.sql.execution.command.ShowCreateTableCommand)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/reportUnsupportedError$1(scala.collection.Seq,org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/collection/TraversableOnce/mkString(java.lang.String)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/reportUnsupportedError$1(scala.collection.Seq,org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/reportUnsupportedError$1(scala.collection.Seq,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/reportUnsupportedError$1(scala.collection.Seq,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/reportUnsupportedError$1(scala.collection.Seq,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/reportUnsupportedError$1(scala.collection.Seq,org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/reportUnsupportedError$1(scala.collection.Seq,org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/reportUnsupportedError$1(scala.collection.Seq,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/reportUnsupportedError$1(scala.collection.Seq,org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/reportUnsupportedError$1(scala.collection.Seq,org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/reportUnsupportedError$1(scala.collection.Seq,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/reportUnsupportedError$1(scala.collection.Seq,org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/reportUnsupportedError$1(scala.collection.Seq,org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableOptions(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#org/apache/spark/sql/catalyst/catalog/CatalogTable/provider()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableOptions(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/Option$/option2Iterable(scala.Option)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableOptions(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/Option/flatMap(scala.Function1)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableOptions(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableOptions(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/collection/mutable/StringBuilder/$plus$plus$eq(java.lang.String)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableOptions(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableOptions(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableOptions(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#org/apache/spark/sql/execution/command/ShowCreateTableCommand$$anonfun$26/26(org.apache.spark.sql.execution.command.ShowCreateTableCommand,org.apache.spark.sql.catalyst.catalog.CatalogTable)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableOptions(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#org/apache/spark/sql/execution/command/ShowCreateTableCommand$$anonfun$25/25(org.apache.spark.sql.execution.command.ShowCreateTableCommand)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableOptions(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/Option/get()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableOptions(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/collection/immutable/Iterable/nonEmpty()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableOptions(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/properties()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableOptions(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/collection/immutable/Map/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableOptions(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/collection/immutable/Iterable/mkString(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableOptions(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/collection/immutable/Iterable$/canBuildFrom()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableOptions(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/collection/TraversableLike/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableOptions(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/locationUri()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableOptions(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableOptions(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#org/apache/spark/sql/catalyst/catalog/CatalogTable/storage()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showCreateDataSourceTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showCreateDataSourceTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/collection/mutable/StringBuilder/$plus$plus$eq(java.lang.String)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showCreateDataSourceTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showCreateDataSourceTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/TableIdentifier/quotedString()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showCreateDataSourceTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/collection/mutable/StringBuilder$/newBuilder()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showCreateDataSourceTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/package$/StringBuilder()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showCreateDataSourceTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showCreateDataSourceTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showCreateDataSourceTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableNonDataColumns(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/Option/foreach(scala.Function1)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableNonDataColumns(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#org/apache/spark/sql/execution/command/ShowCreateTableCommand$$anonfun$showDataSourceTableNonDataColumns$1/1(org.apache.spark.sql.execution.command.ShowCreateTableCommand,scala.collection.mutable.StringBuilder)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableNonDataColumns(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableNonDataColumns(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/collection/mutable/StringBuilder/$plus$plus$eq(java.lang.String)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableNonDataColumns(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableNonDataColumns(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableNonDataColumns(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableNonDataColumns(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#org/apache/spark/sql/catalyst/catalog/CatalogTable/bucketSpec()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableNonDataColumns(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableNonDataColumns(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/ShowCreateTableCommand/showDataSourceTableNonDataColumns(org.apache.spark.sql.catalyst.catalog.CatalogTable,scala.collection.mutable.StringBuilder)#scala/collection/Seq/mkString(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/org$apache$spark$sql$execution$command$AlterTableRecoverPartitionsCommand$$scanPartitions(org.apache.spark.sql.SparkSession,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,org.apache.hadoop.fs.Path,scala.collection.immutable.Map,scala.collection.Seq,int,scala.Function2,scala.collection.parallel.ForkJoinTaskSupport)#scala/collection/Seq/isEmpty()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/org$apache$spark$sql$execution$command$AlterTableRecoverPartitionsCommand$$scanPartitions(org.apache.spark.sql.SparkSession,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,org.apache.hadoop.fs.Path,scala.collection.immutable.Map,scala.collection.Seq,int,scala.Function2,scala.collection.parallel.ForkJoinTaskSupport)#scala/collection/GenSeq$/canBuildFrom()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/org$apache$spark$sql$execution$command$AlterTableRecoverPartitionsCommand$$scanPartitions(org.apache.spark.sql.SparkSession,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,org.apache.hadoop.fs.Path,scala.collection.immutable.Map,scala.collection.Seq,int,scala.Function2,scala.collection.parallel.ForkJoinTaskSupport)#scala/Predef$/ArrowAssoc(java.lang.Object)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/org$apache$spark$sql$execution$command$AlterTableRecoverPartitionsCommand$$scanPartitions(org.apache.spark.sql.SparkSession,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,org.apache.hadoop.fs.Path,scala.collection.immutable.Map,scala.collection.Seq,int,scala.Function2,scala.collection.parallel.ForkJoinTaskSupport)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/org$apache$spark$sql$execution$command$AlterTableRecoverPartitionsCommand$$scanPartitions(org.apache.spark.sql.SparkSession,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,org.apache.hadoop.fs.Path,scala.collection.immutable.Map,scala.collection.Seq,int,scala.Function2,scala.collection.parallel.ForkJoinTaskSupport)#scala/collection/mutable/ArrayOps/par()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/org$apache$spark$sql$execution$command$AlterTableRecoverPartitionsCommand$$scanPartitions(org.apache.spark.sql.SparkSession,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,org.apache.hadoop.fs.Path,scala.collection.immutable.Map,scala.collection.Seq,int,scala.Function2,scala.collection.parallel.ForkJoinTaskSupport)#scala/collection/Seq/length()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/org$apache$spark$sql$execution$command$AlterTableRecoverPartitionsCommand$$scanPartitions(org.apache.spark.sql.SparkSession,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,org.apache.hadoop.fs.Path,scala.collection.immutable.Map,scala.collection.Seq,int,scala.Function2,scala.collection.parallel.ForkJoinTaskSupport)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/org$apache$spark$sql$execution$command$AlterTableRecoverPartitionsCommand$$scanPartitions(org.apache.spark.sql.SparkSession,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,org.apache.hadoop.fs.Path,scala.collection.immutable.Map,scala.collection.Seq,int,scala.Function2,scala.collection.parallel.ForkJoinTaskSupport)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/org$apache$spark$sql$execution$command$AlterTableRecoverPartitionsCommand$$scanPartitions(org.apache.spark.sql.SparkSession,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,org.apache.hadoop.fs.Path,scala.collection.immutable.Map,scala.collection.Seq,int,scala.Function2,scala.collection.parallel.ForkJoinTaskSupport)#scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/org$apache$spark$sql$execution$command$AlterTableRecoverPartitionsCommand$$scanPartitions(org.apache.spark.sql.SparkSession,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,org.apache.hadoop.fs.Path,scala.collection.immutable.Map,scala.collection.Seq,int,scala.Function2,scala.collection.parallel.ForkJoinTaskSupport)#scala/collection/GenSeq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/org$apache$spark$sql$execution$command$AlterTableRecoverPartitionsCommand$$scanPartitions(org.apache.spark.sql.SparkSession,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,org.apache.hadoop.fs.Path,scala.collection.immutable.Map,scala.collection.Seq,int,scala.Function2,scala.collection.parallel.ForkJoinTaskSupport)#org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand$$anonfun$org$apache$spark$sql$execution$command$AlterTableRecoverPartitionsCommand$$scanPartitions$1/1(org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsCommand,org.apache.spark.sql.SparkSession,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,org.apache.hadoop.fs.Path,scala.collection.immutable.Map,scala.collection.Seq,int,scala.Function2,scala.collection.parallel.ForkJoinTaskSupport)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/org$apache$spark$sql$execution$command$AlterTableRecoverPartitionsCommand$$scanPartitions(org.apache.spark.sql.SparkSession,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,org.apache.hadoop.fs.Path,scala.collection.immutable.Map,scala.collection.Seq,int,scala.Function2,scala.collection.parallel.ForkJoinTaskSupport)#org/apache/hadoop/fs/FileSystem/listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/org$apache$spark$sql$execution$command$AlterTableRecoverPartitionsCommand$$scanPartitions(org.apache.spark.sql.SparkSession,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,org.apache.hadoop.fs.Path,scala.collection.immutable.Map,scala.collection.Seq,int,scala.Function2,scala.collection.parallel.ForkJoinTaskSupport)#scala/collection/parallel/mutable/ParArray/tasksupport_$eq(scala.collection.parallel.TaskSupport)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/gatherPartitionStats(org.apache.spark.sql.SparkSession,scala.collection.GenSeq,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,int)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/gatherPartitionStats(org.apache.spark.sql.SparkSession,scala.collection.GenSeq,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,int)#org/apache/spark/rdd/PairRDDFunctions/collectAsMap()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/gatherPartitionStats(org.apache.spark.sql.SparkSession,scala.collection.GenSeq,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,int)#org/apache/spark/SparkContext/parallelize(scala.collection.Seq,int,scala.reflect.ClassTag)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/gatherPartitionStats(org.apache.spark.sql.SparkSession,scala.collection.GenSeq,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,int)#scala/collection/GenSeq$/canBuildFrom()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/gatherPartitionStats(org.apache.spark.sql.SparkSession,scala.collection.GenSeq,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,int)#org/apache/spark/rdd/RDD/mapPartitions$default$2()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/gatherPartitionStats(org.apache.spark.sql.SparkSession,scala.collection.GenSeq,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,int)#org/apache/spark/SparkContext/defaultParallelism()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/gatherPartitionStats(org.apache.spark.sql.SparkSession,scala.collection.GenSeq,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,int)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/gatherPartitionStats(org.apache.spark.sql.SparkSession,scala.collection.GenSeq,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,int)#org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand$$anonfun$gatherPartitionStats$1/1(org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsCommand,int)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/gatherPartitionStats(org.apache.spark.sql.SparkSession,scala.collection.GenSeq,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,int)#scala/collection/GenTraversableOnce/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/gatherPartitionStats(org.apache.spark.sql.SparkSession,scala.collection.GenSeq,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,int)#scala/Predef$/$conforms()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/gatherPartitionStats(org.apache.spark.sql.SparkSession,scala.collection.GenSeq,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,int)#scala/collection/GenSeq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/gatherPartitionStats(org.apache.spark.sql.SparkSession,scala.collection.GenSeq,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,int)#java/lang/Math/min(int,int)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/gatherPartitionStats(org.apache.spark.sql.SparkSession,scala.collection.GenSeq,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,int)#org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/logInfo(scala.Function0)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/gatherPartitionStats(org.apache.spark.sql.SparkSession,scala.collection.GenSeq,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,int)#org/apache/spark/util/SerializableConfiguration/SerializableConfiguration(org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/gatherPartitionStats(org.apache.spark.sql.SparkSession,scala.collection.GenSeq,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,int)#org/apache/spark/rdd/RDD$/rddToPairRDDFunctions(org.apache.spark.rdd.RDD,scala.reflect.ClassTag,scala.reflect.ClassTag,scala.math.Ordering)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/gatherPartitionStats(org.apache.spark.sql.SparkSession,scala.collection.GenSeq,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,int)#org/apache/spark/rdd/RDD/mapPartitions(scala.Function1,boolean,scala.reflect.ClassTag)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/gatherPartitionStats(org.apache.spark.sql.SparkSession,scala.collection.GenSeq,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,int)#org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand$$anonfun$gatherPartitionStats$2/2(org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsCommand,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/gatherPartitionStats(org.apache.spark.sql.SparkSession,scala.collection.GenSeq,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,int)#scala/collection/GenTraversableOnce/toMap(scala.Predef$$less$colon$less)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/gatherPartitionStats(org.apache.spark.sql.SparkSession,scala.collection.GenSeq,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,int)#org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand$$anonfun$11/11(org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsCommand,org.apache.spark.util.SerializableConfiguration)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/gatherPartitionStats(org.apache.spark.sql.SparkSession,scala.collection.GenSeq,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,int)#org/apache/spark/SparkContext/hadoopConfiguration()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/gatherPartitionStats(org.apache.spark.sql.SparkSession,scala.collection.GenSeq,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,int)#scala/collection/GenSeq/length()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/gatherPartitionStats(org.apache.spark.sql.SparkSession,scala.collection.GenSeq,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathFilter,int)#org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand$$anonfun$10/10(org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsCommand)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/internal/SQLConf/resolver()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/empty()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/isEmpty()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$1()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/immutable/Map$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/Map()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/GenSeq/seq()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq/isEmpty()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#scala/concurrent/forkjoin/ForkJoinPool/shutdown()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/alterTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/refreshTable(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/location()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/GenMap$/empty()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/immutable/StringOps/toInt()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/util/ThreadUtils$/newForkJoinPool(java.lang.String,int)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/DDLUtils$/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$2()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/hadoop/fs/Path/Path(java.net.URI)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$3()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$10()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$4()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$11()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/parallel/ForkJoinTaskSupport/ForkJoinTaskSupport(scala.concurrent.forkjoin.ForkJoinPool)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$5()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$12()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$6()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$13()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/TableIdentifier/quotedString()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/SparkContext/hadoopConfiguration()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$7()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$15()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$8()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq/length()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$9()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand$$anonfun$run$6/6(org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsCommand,org.apache.hadoop.fs.Path,int)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$17()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand$$anonfun$run$8/8(org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsCommand,int)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/logInfo(scala.Function0)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/locationUri()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/storage()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/internal/SQLConf/gatherFastStats()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand$$anonfun$run$7/7(org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsCommand,int)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand$$anonfun$run$5/5(org.apache.spark.sql.execution.command.AlterTableRecoverPartitionsCommand,org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVInferSchema$/findTightestCommonType()
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVInferSchema$$anonfun$inferField$1/1()
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVInferSchema$/tryParseInteger(java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/Function2/apply(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#java/lang/String/isEmpty()
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVInferSchema$/tryParseLong(java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVInferSchema$/tryParseDecimal(java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVInferSchema$/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVInferSchema$/tryParseTimestamp(java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVInferSchema$/org$apache$spark$sql$execution$datasources$csv$CSVInferSchema$$tryParseDouble(java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#java/lang/UnsupportedOperationException/UnsupportedOperationException(java.lang.String)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/inferField(org.apache.spark.sql.types.DataType,java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVInferSchema$/tryParseBoolean(java.lang.String,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/infer(org.apache.spark.rdd.RDD,java.lang.String[],org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/infer(org.apache.spark.rdd.RDD,java.lang.String[],org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/collection/mutable/ArrayOps/zip(scala.collection.GenIterable,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/infer(org.apache.spark.rdd.RDD,java.lang.String[],org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/infer(org.apache.spark.rdd.RDD,java.lang.String[],org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVInferSchema$$anonfun$2/2(org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/infer(org.apache.spark.rdd.RDD,java.lang.String[],org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/infer(org.apache.spark.rdd.RDD,java.lang.String[],org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/runtime/ScalaRunTime$/arrayClass(java.lang.Class)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/infer(org.apache.spark.rdd.RDD,java.lang.String[],org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVInferSchema$$anonfun$1/1()
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/infer(org.apache.spark.rdd.RDD,java.lang.String[],org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVInferSchema$$anonfun$3/3()
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/infer(org.apache.spark.rdd.RDD,java.lang.String[],org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVInferSchema$$anonfun$4/4()
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/infer(org.apache.spark.rdd.RDD,java.lang.String[],org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVInferSchema$$anonfun$5/5()
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/infer(org.apache.spark.rdd.RDD,java.lang.String[],org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/infer(org.apache.spark.rdd.RDD,java.lang.String[],org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/Array$/fill(int,scala.Function0,scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/infer(org.apache.spark.rdd.RDD,java.lang.String[],org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/types/StructType/StructType(org.apache.spark.sql.types.StructField[])
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/infer(org.apache.spark.rdd.RDD,java.lang.String[],org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/rdd/RDD/aggregate(java.lang.Object,scala.Function2,scala.Function2,scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/infer(org.apache.spark.rdd.RDD,java.lang.String[],org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVInferSchema$/infer(org.apache.spark.rdd.RDD,java.lang.String[],org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/infer(org.apache.spark.rdd.RDD,java.lang.String[],org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/DataFrameNaFunctions/anonfun/7/apply(org.apache.spark.sql.types.StructField)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/DataFrameNaFunctions/anonfun/7/apply(org.apache.spark.sql.types.StructField)#scala/collection/Seq/exists(scala.Function1)
org/apache/spark/sql/DataFrameNaFunctions/anonfun/7/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/types/StructField/dataType()
org/apache/spark/sql/DataFrameNaFunctions/anonfun/7/apply(org.apache.spark.sql.types.StructField)#scala/Tuple2/_1()
org/apache/spark/sql/DataFrameNaFunctions/anonfun/7/apply(org.apache.spark.sql.types.StructField)#scala/Tuple2/_2()
org/apache/spark/sql/DataFrameNaFunctions/anonfun/7/apply(org.apache.spark.sql.types.StructField)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/DataFrameNaFunctions/anonfun/7/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/DataFrameNaFunctions$$anonfun$7$$anonfun$apply$4/4(org.apache.spark.sql.DataFrameNaFunctions$$anonfun$7,org.apache.spark.sql.types.StructField)
org/apache/spark/sql/DataFrameNaFunctions/anonfun/7/apply(org.apache.spark.sql.types.StructField)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/DataFrameNaFunctions/anonfun/7/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/types/StructField/name()
org/apache/spark/sql/execution/datasources/SharedInMemoryCache/anon/1/anonfun/weigh/1/apply()#org/apache/spark/sql/execution/datasources/SharedInMemoryCache$$anon$1$$anonfun$weigh$1/apply()
org/apache/spark/sql/execution/datasources/SharedInMemoryCache/anon/1/anonfun/weigh/1/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/SharedInMemoryCache/anon/1/anonfun/weigh/1/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/SharedInMemoryCache/anon/1/anonfun/weigh/1/apply()#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/datasources/SharedInMemoryCache/anon/1/anonfun/weigh/1/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/SharedInMemoryCache/anon/1/anonfun/weigh/1/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/SharedInMemoryCache/anon/1/anonfun/weigh/1/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/SharedInMemoryCache/anon/1/anonfun/weigh/1/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/SharedInMemoryCache/anon/1/anonfun/weigh/1/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/schemaString/1/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/types/StructField/nullable()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/schemaString/1/apply(org.apache.spark.sql.types.StructField)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/schemaString/1/apply(org.apache.spark.sql.types.StructField)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/schemaString/1/apply(org.apache.spark.sql.types.StructField)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/schemaString/1/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils$$anonfun$schemaString$1$$anonfun$23/23(org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$schemaString$1,org.apache.spark.sql.types.StructField)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/schemaString/1/apply(org.apache.spark.sql.types.StructField)#scala/collection/mutable/StringBuilder/append(java.lang.String)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/schemaString/1/apply(org.apache.spark.sql.types.StructField)#scala/collection/immutable/Map/getOrElse(java.lang.Object,scala.Function0)
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/schemaString/1/apply(org.apache.spark.sql.types.StructField)#org/apache/spark/sql/types/StructField/name()
org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils/anonfun/schemaString/1/apply(org.apache.spark.sql.types.StructField)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/properties()
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/util/DateTimeUtils$/TIMEZONE_OPTION()
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/util/CaseInsensitiveMap$/apply(scala.collection.immutable.Map)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#java/lang/IllegalStateException/IllegalStateException(java.lang.String)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/types/StructType/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/listPartitions(org.apache.spark.sql.catalyst.TableIdentifier,scala.Option)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery$$anonfun$org$apache$spark$sql$execution$OptimizeMetadataOnlyQuery$$replaceTableScanWithPartitionMetadata$1$$anonfun$5/5(org.apache.spark.sql.execution.OptimizeMetadataOnlyQuery$$anonfun$org$apache$spark$sql$execution$OptimizeMetadataOnlyQuery$$replaceTableScanWithPartitionMetadata$1)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery$$anonfun$org$apache$spark$sql$execution$OptimizeMetadataOnlyQuery$$replaceTableScanWithPartitionMetadata$1$$anonfun$applyOrElse$1/1(org.apache.spark.sql.execution.OptimizeMetadataOnlyQuery$$anonfun$org$apache$spark$sql$execution$OptimizeMetadataOnlyQuery$$replaceTableScanWithPartitionMetadata$1)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery$$anonfun$org$apache$spark$sql$execution$OptimizeMetadataOnlyQuery$$replaceTableScanWithPartitionMetadata$1$$anonfun$4/4(org.apache.spark.sql.execution.OptimizeMetadataOnlyQuery$$anonfun$org$apache$spark$sql$execution$OptimizeMetadataOnlyQuery$$replaceTableScanWithPartitionMetadata$1)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/CatalogRelation/tableMeta()
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/listPartitions$default$2()
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/internal/config/ConfigEntry/key()
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/internal/SQLConf$/OPTIMIZER_METADATA_ONLY()
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/util/CaseInsensitiveMap/get(java.lang.String)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery$$anonfun$org$apache$spark$sql$execution$OptimizeMetadataOnlyQuery$$replaceTableScanWithPartitionMetadata$1$$anonfun$6/6(org.apache.spark.sql.execution.OptimizeMetadataOnlyQuery$$anonfun$org$apache$spark$sql$execution$OptimizeMetadataOnlyQuery$$replaceTableScanWithPartitionMetadata$1,scala.collection.Seq,java.lang.String)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/plans/logical/LocalRelation/LocalRelation(scala.collection.Seq,scala.collection.Seq)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/anonfun/org/apache/spark/sql/execution/OptimizeMetadataOnlyQuery/replaceTableScanWithPartitionMetadata/1/applyOrElse(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan,scala.Function1)#org/apache/spark/sql/catalyst/catalog/CatalogTable/storage()
org/apache/spark/sql/execution/debug/package/DebugQuery/debug()#org/apache/spark/rdd/RDD/count()
org/apache/spark/sql/execution/debug/package/DebugQuery/debug()#scala/collection/mutable/HashSet/HashSet()
org/apache/spark/sql/execution/debug/package/DebugQuery/debug()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/debug/package/DebugQuery/debug()#org/apache/spark/sql/execution/debug/package$DebugQuery$$anonfun$1/1(org.apache.spark.sql.execution.debug.package$DebugQuery,scala.collection.mutable.HashSet)
org/apache/spark/sql/execution/debug/package/DebugQuery/debug()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/debug/package/DebugQuery/debug()#org/apache/spark/sql/execution/SparkPlan/transform(scala.PartialFunction)
org/apache/spark/sql/execution/debug/package/DebugQuery/debug()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/debug/package/DebugQuery/debug()#org/apache/spark/sql/execution/debug/package$DebugQuery$$anonfun$debug$1/1(org.apache.spark.sql.execution.debug.package$DebugQuery)
org/apache/spark/sql/execution/debug/package/DebugQuery/debug()#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/debug/package/DebugQuery/debug()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/debug/package/DebugQuery/debug()#org/apache/spark/sql/execution/debug/package$/org$apache$spark$sql$execution$debug$package$$debugPrint(java.lang.String)
org/apache/spark/sql/execution/debug/package/DebugQuery/debug()#org/apache/spark/sql/execution/SparkPlan/foreach(scala.Function1)
org/apache/spark/sql/execution/datasources/BucketingUtils/getBucketId(java.lang.String)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/datasources/BucketingUtils/getBucketId(java.lang.String)#scala/collection/LinearSeqOptimized/apply(int)
org/apache/spark/sql/execution/datasources/BucketingUtils/getBucketId(java.lang.String)#scala/Option/isEmpty()
org/apache/spark/sql/execution/datasources/BucketingUtils/getBucketId(java.lang.String)#scala/util/matching/Regex/unapplySeq(java.lang.CharSequence)
org/apache/spark/sql/execution/datasources/BucketingUtils/getBucketId(java.lang.String)#org/apache/spark/sql/execution/datasources/BucketingUtils$/bucketedFileName()
org/apache/spark/sql/execution/datasources/BucketingUtils/getBucketId(java.lang.String)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/datasources/BucketingUtils/getBucketId(java.lang.String)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/datasources/BucketingUtils/getBucketId(java.lang.String)#scala/Option/get()
org/apache/spark/sql/execution/datasources/BucketingUtils/getBucketId(java.lang.String)#org/apache/spark/sql/execution/datasources/BucketingUtils$/getBucketId(java.lang.String)
org/apache/spark/sql/execution/datasources/BucketingUtils/getBucketId(java.lang.String)#scala/collection/immutable/StringOps/toInt()
org/apache/spark/sql/execution/datasources/BucketingUtils/getBucketId(java.lang.String)#scala/collection/LinearSeqOptimized/lengthCompare(int)
org/apache/spark/sql/execution/datasources/BucketingUtils/getBucketId(java.lang.String)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/ExpandExec/anonfun/5/anonfun/apply/1/apply$mcVI$sp(int)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/ExpandExec/anonfun/5/anonfun/apply/1/apply$mcVI$sp(int)#org/apache/spark/sql/catalyst/expressions/BindReferences$/bindReference$default$3()
org/apache/spark/sql/execution/ExpandExec/anonfun/5/anonfun/apply/1/apply$mcVI$sp(int)#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/ExpandExec/anonfun/5/anonfun/apply/1/apply$mcVI$sp(int)#scala/collection/Seq/apply(int)
org/apache/spark/sql/execution/ExpandExec/anonfun/5/anonfun/apply/1/apply$mcVI$sp(int)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/isNull()
org/apache/spark/sql/execution/ExpandExec/anonfun/5/anonfun/apply/1/apply$mcVI$sp(int)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/ExpandExec/anonfun/5/anonfun/apply/1/apply$mcVI$sp(int)#org/apache/spark/sql/execution/ExpandExec$$anonfun$5/org$apache$spark$sql$execution$ExpandExec$$anonfun$$$outer()
org/apache/spark/sql/execution/ExpandExec/anonfun/5/anonfun/apply/1/apply$mcVI$sp(int)#org/apache/spark/sql/catalyst/expressions/package$/AttributeSeq(scala.collection.Seq)
org/apache/spark/sql/execution/ExpandExec/anonfun/5/anonfun/apply/1/apply$mcVI$sp(int)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/ExpandExec/anonfun/5/anonfun/apply/1/apply$mcVI$sp(int)#org/apache/spark/sql/catalyst/expressions/BindReferences$/bindReference(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.package$AttributeSeq,boolean)
org/apache/spark/sql/execution/ExpandExec/anonfun/5/anonfun/apply/1/apply$mcVI$sp(int)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/ExpandExec/anonfun/5/anonfun/apply/1/apply$mcVI$sp(int)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/code()
org/apache/spark/sql/execution/ExpandExec/anonfun/5/anonfun/apply/1/apply$mcVI$sp(int)#org/apache/spark/sql/catalyst/expressions/Expression/genCode(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext)
org/apache/spark/sql/execution/ExpandExec/anonfun/5/anonfun/apply/1/apply$mcVI$sp(int)#org/apache/spark/sql/catalyst/expressions/codegen/ExprCode/value()
org/apache/spark/sql/execution/ExpandExec/anonfun/5/anonfun/apply/1/apply$mcVI$sp(int)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/ExpandExec/anonfun/5/anonfun/apply/1/apply$mcVI$sp(int)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/ExpandExec/anonfun/5/anonfun/apply/1/apply$mcVI$sp(int)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/ExpandExec/anonfun/5/anonfun/apply/1/apply$mcVI$sp(int)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/ExpandExec/anonfun/5/anonfun/apply/1/apply$mcVI$sp(int)#scala/collection/immutable/IndexedSeq/apply(int)
org/apache/spark/sql/execution/ExpandExec/anonfun/5/anonfun/apply/1/apply$mcVI$sp(int)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/ExpandExec/anonfun/5/anonfun/apply/1/apply$mcVI$sp(int)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/FailureSafeParser/anonfun/2/apply(scala.Option,scala.Function0)#org/apache/spark/sql/types/StructType/length()
org/apache/spark/sql/execution/datasources/FailureSafeParser/anonfun/2/apply(scala.Option,scala.Function0)#scala/Function0/apply()
org/apache/spark/sql/execution/datasources/FailureSafeParser/anonfun/2/apply(scala.Option,scala.Function0)#scala/runtime/IntRef/create(int)
org/apache/spark/sql/execution/datasources/FailureSafeParser/anonfun/2/apply(scala.Option,scala.Function0)#scala/Predef$/$conforms()
org/apache/spark/sql/execution/datasources/FailureSafeParser/anonfun/2/apply(scala.Option,scala.Function0)#scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)
org/apache/spark/sql/execution/datasources/FailureSafeParser/anonfun/2/apply(scala.Option,scala.Function0)#org/apache/spark/sql/catalyst/expressions/GenericInternalRow/update(int,java.lang.Object)
org/apache/spark/sql/execution/datasources/FailureSafeParser/anonfun/2/apply(scala.Option,scala.Function0)#scala/Option/get()
org/apache/spark/sql/execution/datasources/FailureSafeParser/anonfun/2/apply(scala.Option,scala.Function0)#org/apache/spark/sql/execution/datasources/FailureSafeParser$$anonfun$2$$anonfun$apply$1/1(org.apache.spark.sql.execution.datasources.FailureSafeParser$$anonfun$2,scala.runtime.IntRef,org.apache.spark.sql.types.StructField)
org/apache/spark/sql/execution/datasources/FailureSafeParser/anonfun/2/apply(scala.Option,scala.Function0)#org/apache/spark/sql/types/StructType/fieldIndex(java.lang.String)
org/apache/spark/sql/execution/datasources/FailureSafeParser/anonfun/2/apply(scala.Option,scala.Function0)#org/apache/spark/sql/types/StructType/apply(int)
org/apache/spark/sql/execution/datasources/FailureSafeParser/anonfun/2/apply(scala.Option,scala.Function0)#org/apache/spark/sql/types/StructField/name()
org/apache/spark/sql/execution/datasources/FailureSafeParser/anonfun/2/apply(scala.Option,scala.Function0)#scala/Option/orNull(scala.Predef$$less$colon$less)
org/apache/spark/sql/execution/datasources/FailureSafeParser/anonfun/2/apply(scala.Option,scala.Function0)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/joins/ShuffledHashJoinExec/org$apache$spark$sql$execution$joins$ShuffledHashJoinExec$$buildHashedRelation(scala.collection.Iterator)#org/apache/spark/TaskContext$/get()
org/apache/spark/sql/execution/joins/ShuffledHashJoinExec/org$apache$spark$sql$execution$joins$ShuffledHashJoinExec$$buildHashedRelation(scala.collection.Iterator)#org/apache/spark/TaskContext/addTaskCompletionListener(scala.Function1)
org/apache/spark/sql/execution/joins/ShuffledHashJoinExec/org$apache$spark$sql$execution$joins$ShuffledHashJoinExec$$buildHashedRelation(scala.collection.Iterator)#org/apache/spark/TaskContext/taskMemoryManager()
org/apache/spark/sql/execution/joins/ShuffledHashJoinExec/org$apache$spark$sql$execution$joins$ShuffledHashJoinExec$$buildHashedRelation(scala.collection.Iterator)#org/apache/spark/sql/execution/joins/ShuffledHashJoinExec/longMetric(java.lang.String)
org/apache/spark/sql/execution/joins/ShuffledHashJoinExec/org$apache$spark$sql$execution$joins$ShuffledHashJoinExec$$buildHashedRelation(scala.collection.Iterator)#org/apache/spark/sql/execution/joins/HashedRelation$/apply$default$3()
org/apache/spark/sql/execution/joins/ShuffledHashJoinExec/org$apache$spark$sql$execution$joins$ShuffledHashJoinExec$$buildHashedRelation(scala.collection.Iterator)#org/apache/spark/sql/execution/joins/HashedRelation$/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)
org/apache/spark/sql/execution/joins/ShuffledHashJoinExec/org$apache$spark$sql$execution$joins$ShuffledHashJoinExec$$buildHashedRelation(scala.collection.Iterator)#java/lang/System/nanoTime()
org/apache/spark/sql/execution/joins/ShuffledHashJoinExec/org$apache$spark$sql$execution$joins$ShuffledHashJoinExec$$buildHashedRelation(scala.collection.Iterator)#org/apache/spark/sql/execution/joins/ShuffledHashJoinExec$$anonfun$org$apache$spark$sql$execution$joins$ShuffledHashJoinExec$$buildHashedRelation$1/1(org.apache.spark.sql.execution.joins.ShuffledHashJoinExec,org.apache.spark.sql.execution.joins.HashedRelation)
org/apache/spark/sql/execution/joins/ShuffledHashJoinExec/org$apache$spark$sql$execution$joins$ShuffledHashJoinExec$$buildHashedRelation(scala.collection.Iterator)#org/apache/spark/sql/execution/joins/HashedRelation/estimatedSize()
org/apache/spark/sql/execution/joins/ShuffledHashJoinExec/metrics$lzycompute()#scala/Predef$/Map()
org/apache/spark/sql/execution/joins/ShuffledHashJoinExec/metrics$lzycompute()#scala/Predef$/ArrowAssoc(java.lang.Object)
org/apache/spark/sql/execution/joins/ShuffledHashJoinExec/metrics$lzycompute()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/joins/ShuffledHashJoinExec/metrics$lzycompute()#org/apache/spark/sql/execution/metric/SQLMetrics$/createMetric(org.apache.spark.SparkContext,java.lang.String)
org/apache/spark/sql/execution/joins/ShuffledHashJoinExec/metrics$lzycompute()#org/apache/spark/sql/execution/metric/SQLMetrics$/createSizeMetric(org.apache.spark.SparkContext,java.lang.String)
org/apache/spark/sql/execution/joins/ShuffledHashJoinExec/metrics$lzycompute()#scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/joins/ShuffledHashJoinExec/metrics$lzycompute()#org/apache/spark/sql/execution/metric/SQLMetrics$/createTimingMetric(org.apache.spark.SparkContext,java.lang.String)
org/apache/spark/sql/execution/joins/ShuffledHashJoinExec/metrics$lzycompute()#scala/collection/immutable/Map$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/joins/ShuffledHashJoinExec/metrics$lzycompute()#org/apache/spark/sql/execution/joins/ShuffledHashJoinExec/sparkContext()
org/apache/spark/sql/execution/window/WindowExec/anonfun/windowFrameExpressionFactoryPairs/2/apply(scala.Tuple2)#scala/collection/mutable/Buffer/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/window/WindowExec/anonfun/windowFrameExpressionFactoryPairs/2/apply(scala.Tuple2)#org/apache/spark/sql/execution/window/WindowExec$$anonfun$windowFrameExpressionFactoryPairs$2$$anonfun$5/5(org.apache.spark.sql.execution.window.WindowExec$$anonfun$windowFrameExpressionFactoryPairs$2,int,org.apache.spark.sql.catalyst.expressions.Expression[],int)
org/apache/spark/sql/execution/window/WindowExec/anonfun/windowFrameExpressionFactoryPairs/2/apply(scala.Tuple2)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/window/WindowExec/anonfun/windowFrameExpressionFactoryPairs/2/apply(scala.Tuple2)#scala/Tuple4/_1()
org/apache/spark/sql/execution/window/WindowExec/anonfun/windowFrameExpressionFactoryPairs/2/apply(scala.Tuple2)#scala/Tuple4/_2()
org/apache/spark/sql/execution/window/WindowExec/anonfun/windowFrameExpressionFactoryPairs/2/apply(scala.Tuple2)#org/apache/spark/sql/execution/window/WindowExec$$anonfun$windowFrameExpressionFactoryPairs$2$$anonfun$7/7(org.apache.spark.sql.execution.window.WindowExec$$anonfun$windowFrameExpressionFactoryPairs$2,int,org.apache.spark.sql.catalyst.expressions.Expression[],org.apache.spark.sql.catalyst.expressions.FrameType,int)
org/apache/spark/sql/execution/window/WindowExec/anonfun/windowFrameExpressionFactoryPairs/2/apply(scala.Tuple2)#org/apache/spark/sql/execution/window/WindowExec$$anonfun$windowFrameExpressionFactoryPairs$2$$anonfun$9/9(org.apache.spark.sql.execution.window.WindowExec$$anonfun$windowFrameExpressionFactoryPairs$2,int,org.apache.spark.sql.catalyst.expressions.Expression[])
org/apache/spark/sql/execution/window/WindowExec/anonfun/windowFrameExpressionFactoryPairs/2/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/window/WindowExec/anonfun/windowFrameExpressionFactoryPairs/2/apply(scala.Tuple2)#scala/collection/mutable/Buffer/size()
org/apache/spark/sql/execution/window/WindowExec/anonfun/windowFrameExpressionFactoryPairs/2/apply(scala.Tuple2)#scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)
org/apache/spark/sql/execution/window/WindowExec/anonfun/windowFrameExpressionFactoryPairs/2/apply(scala.Tuple2)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/window/WindowExec/anonfun/windowFrameExpressionFactoryPairs/2/apply(scala.Tuple2)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/window/WindowExec/anonfun/windowFrameExpressionFactoryPairs/2/apply(scala.Tuple2)#scala/Some/x()
org/apache/spark/sql/execution/window/WindowExec/anonfun/windowFrameExpressionFactoryPairs/2/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/window/WindowExec/anonfun/windowFrameExpressionFactoryPairs/2/apply(scala.Tuple2)#org/apache/spark/sql/execution/window/WindowExec$$anonfun$windowFrameExpressionFactoryPairs$2$$anonfun$6/6(org.apache.spark.sql.execution.window.WindowExec$$anonfun$windowFrameExpressionFactoryPairs$2,int,org.apache.spark.sql.catalyst.expressions.Expression[],org.apache.spark.sql.catalyst.expressions.FrameType,int)
org/apache/spark/sql/execution/window/WindowExec/anonfun/windowFrameExpressionFactoryPairs/2/apply(scala.Tuple2)#scala/Tuple4/_3()
org/apache/spark/sql/execution/window/WindowExec/anonfun/windowFrameExpressionFactoryPairs/2/apply(scala.Tuple2)#scala/Tuple2/_2()
org/apache/spark/sql/execution/window/WindowExec/anonfun/windowFrameExpressionFactoryPairs/2/apply(scala.Tuple2)#scala/Tuple4/_4()
org/apache/spark/sql/execution/window/WindowExec/anonfun/windowFrameExpressionFactoryPairs/2/apply(scala.Tuple2)#org/apache/spark/sql/execution/window/WindowExec$$anonfun$windowFrameExpressionFactoryPairs$2$$anonfun$8/8(org.apache.spark.sql.execution.window.WindowExec$$anonfun$windowFrameExpressionFactoryPairs$2,int,org.apache.spark.sql.catalyst.expressions.Expression[],org.apache.spark.sql.catalyst.expressions.FrameType,int,int)
org/apache/spark/sql/catalog/Database/toString()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/catalog/Database/toString()#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/catalog/Database/toString()#org/apache/spark/sql/catalog/Database$$anonfun$toString$2/2(org.apache.spark.sql.catalog.Database)
org/apache/spark/sql/catalog/Database/toString()#org/apache/spark/sql/catalog/Database$$anonfun$toString$1/1(org.apache.spark.sql.catalog.Database)
org/apache/spark/sql/catalog/Database/toString()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/catalog/Database/toString()#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/catalog/Database/toString()#scala/Option/map(scala.Function1)
org/apache/spark/sql/catalog/Database/toString()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/catalog/Database/toString()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/catalog/Database/toString()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/catalog/Database/toString()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/catalog/Database/toString()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#org/apache/spark/sql/execution/metric/SQLMetrics$/TIMING_METRIC()
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/collection/Seq/isEmpty()
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/collection/Seq/apply(int)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/Tuple4/_1()
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/Tuple4/_3()
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/collection/Seq$/unapplySeq(scala.collection.Seq)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#org/apache/spark/sql/execution/metric/SQLMetrics$$anonfun$2/2()
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/collection/Seq$/fill(int,scala.Function0)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#org/apache/spark/sql/execution/metric/SQLMetrics$$anonfun$4/4()
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/collection/Seq/length()
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/Predef$/wrapLongArray(long[])
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#org/apache/spark/sql/execution/metric/SQLMetrics$/SUM_METRIC()
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#java/lang/IllegalStateException/IllegalStateException(java.lang.String)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/collection/Seq/sorted(scala.math.Ordering)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/collection/Seq/filter(scala.Function1)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/Tuple4/_2()
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#org/apache/spark/sql/execution/metric/SQLMetrics$/stringValue(java.lang.String,scala.collection.Seq)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/Some/isEmpty()
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/Tuple4/_4()
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#org/apache/spark/sql/execution/metric/SQLMetrics$/SIZE_METRIC()
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/collection/SeqLike/apply(int)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#java/text/NumberFormat/format(long)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#org/apache/spark/sql/execution/metric/SQLMetrics$$anonfun$1/1()
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#org/apache/spark/sql/execution/metric/SQLMetrics$$anonfun$3/3()
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#java/text/NumberFormat/getIntegerInstance(java.util.Locale)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/Some/get()
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/collection/Seq/sum(scala.math.Numeric)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/Tuple4/Tuple4(java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/metric/SQLMetrics/stringValue(java.lang.String,scala.collection.Seq)#scala/collection/SeqLike/lengthCompare(int)
org/apache/spark/sql/execution/metric/SQLMetrics/postDriverMetricUpdates(org.apache.spark.SparkContext,java.lang.String,scala.collection.Seq)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/metric/SQLMetrics/postDriverMetricUpdates(org.apache.spark.SparkContext,java.lang.String,scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/metric/SQLMetrics/postDriverMetricUpdates(org.apache.spark.SparkContext,java.lang.String,scala.collection.Seq)#org/apache/spark/sql/execution/metric/SQLMetrics$/postDriverMetricUpdates(org.apache.spark.SparkContext,java.lang.String,scala.collection.Seq)
org/apache/spark/sql/execution/metric/SQLMetrics/postDriverMetricUpdates(org.apache.spark.SparkContext,java.lang.String,scala.collection.Seq)#org/apache/spark/SparkContext/listenerBus()
org/apache/spark/sql/execution/metric/SQLMetrics/postDriverMetricUpdates(org.apache.spark.SparkContext,java.lang.String,scala.collection.Seq)#scala/collection/immutable/StringOps/toLong()
org/apache/spark/sql/execution/metric/SQLMetrics/postDriverMetricUpdates(org.apache.spark.SparkContext,java.lang.String,scala.collection.Seq)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/metric/SQLMetrics/postDriverMetricUpdates(org.apache.spark.SparkContext,java.lang.String,scala.collection.Seq)#org/apache/spark/sql/execution/metric/SQLMetrics$$anonfun$postDriverMetricUpdates$1/1()
org/apache/spark/sql/execution/metric/SQLMetrics/postDriverMetricUpdates(org.apache.spark.SparkContext,java.lang.String,scala.collection.Seq)#org/apache/spark/scheduler/LiveListenerBus/post(org.apache.spark.scheduler.SparkListenerEvent)
org/apache/spark/sql/execution/metric/SQLMetrics/postDriverMetricUpdates(org.apache.spark.SparkContext,java.lang.String,scala.collection.Seq)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/SparkPlanInfo/fromSparkPlan(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/SparkPlan/simpleString()
org/apache/spark/sql/execution/SparkPlanInfo/fromSparkPlan(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/SparkPlanInfo$$anonfun$fromSparkPlan$1/1()
org/apache/spark/sql/execution/SparkPlanInfo/fromSparkPlan(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/SparkPlan/subqueries()
org/apache/spark/sql/execution/SparkPlanInfo/fromSparkPlan(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/SparkPlanInfo$/fromSparkPlan(org.apache.spark.sql.execution.SparkPlan)
org/apache/spark/sql/execution/SparkPlanInfo/fromSparkPlan(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/SparkPlan/nodeName()
org/apache/spark/sql/execution/SparkPlanInfo/fromSparkPlan(org.apache.spark.sql.execution.SparkPlan)#scala/collection/immutable/Map/toSeq()
org/apache/spark/sql/execution/SparkPlanInfo/fromSparkPlan(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/SparkPlanInfo/fromSparkPlan(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/SparkPlanInfo/fromSparkPlan(org.apache.spark.sql.execution.SparkPlan)#scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)
org/apache/spark/sql/execution/SparkPlanInfo/fromSparkPlan(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/SparkPlan/children()
org/apache/spark/sql/execution/SparkPlanInfo/fromSparkPlan(org.apache.spark.sql.execution.SparkPlan)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/SparkPlanInfo/fromSparkPlan(org.apache.spark.sql.execution.SparkPlan)#org/apache/spark/sql/execution/SparkPlanInfo$$anonfun$1/1()
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/anonfun/prunePartitions/1/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/anonfun/prunePartitions/1/apply()#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/anonfun/prunePartitions/1/apply()#org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex$$anonfun$prunePartitions$1/apply()
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/anonfun/prunePartitions/1/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/anonfun/prunePartitions/1/apply()#scala/collection/Seq/length()
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/anonfun/prunePartitions/1/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/anonfun/prunePartitions/1/apply()#scala/runtime/BoxesRunTime/boxToDouble(double)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/anonfun/prunePartitions/1/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/anonfun/prunePartitions/1/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/anonfun/prunePartitions/1/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PartitioningAwareFileIndex/anonfun/prunePartitions/1/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/ExternalAppendOnlyUnsafeRowArrayIterator/throwExceptionIfModified()#org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray$ExternalAppendOnlyUnsafeRowArrayIterator/org$apache$spark$sql$execution$ExternalAppendOnlyUnsafeRowArray$ExternalAppendOnlyUnsafeRowArrayIterator$$$outer()
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/ExternalAppendOnlyUnsafeRowArrayIterator/throwExceptionIfModified()#java/util/ConcurrentModificationException/ConcurrentModificationException(java.lang.String)
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/ExternalAppendOnlyUnsafeRowArrayIterator/throwExceptionIfModified()#org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray$ExternalAppendOnlyUnsafeRowArrayIterator/expectedModificationsCount()
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/ExternalAppendOnlyUnsafeRowArrayIterator/throwExceptionIfModified()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/ExternalAppendOnlyUnsafeRowArrayIterator/throwExceptionIfModified()#java/lang/Class/getName()
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/ExternalAppendOnlyUnsafeRowArrayIterator/throwExceptionIfModified()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/ExternalAppendOnlyUnsafeRowArrayIterator/throwExceptionIfModified()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/ExternalAppendOnlyUnsafeRowArrayIterator/throwExceptionIfModified()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/ExternalAppendOnlyUnsafeRowArrayIterator/throwExceptionIfModified()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/ExternalAppendOnlyUnsafeRowArrayIterator/throwExceptionIfModified()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/ExternalAppendOnlyUnsafeRowArray/ExternalAppendOnlyUnsafeRowArrayIterator/throwExceptionIfModified()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/anonfun/13/apply()#java/lang/Object/toString()
org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/anonfun/13/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/anonfun/13/apply()#org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions$/JDBC_BATCH_FETCH_SIZE()
org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/anonfun/13/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/anonfun/13/apply()#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/anonfun/13/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/anonfun/13/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/anonfun/13/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/anonfun/13/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/anonfun/13/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/anonfun/13/apply()#org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions$$anonfun$13/apply()
org/apache/spark/sql/execution/streaming/ProcessingTimeExecutor/anonfun/notifyBatchFallingBehind/1/apply()#org/apache/spark/sql/execution/streaming/ProcessingTimeExecutor$$anonfun$notifyBatchFallingBehind$1/apply()
org/apache/spark/sql/execution/streaming/ProcessingTimeExecutor/anonfun/notifyBatchFallingBehind/1/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/ProcessingTimeExecutor/anonfun/notifyBatchFallingBehind/1/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/streaming/ProcessingTimeExecutor/anonfun/notifyBatchFallingBehind/1/apply()#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/streaming/ProcessingTimeExecutor/anonfun/notifyBatchFallingBehind/1/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/ProcessingTimeExecutor/anonfun/notifyBatchFallingBehind/1/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/streaming/ProcessingTimeExecutor/anonfun/notifyBatchFallingBehind/1/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/ProcessingTimeExecutor/anonfun/notifyBatchFallingBehind/1/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/ProcessingTimeExecutor/anonfun/notifyBatchFallingBehind/1/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/SampleExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#java/lang/String/trim()
org/apache/spark/sql/execution/SampleExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/copyResult_$eq(boolean)
org/apache/spark/sql/execution/SampleExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addNewFunction(java.lang.String,java.lang.String)
org/apache/spark/sql/execution/SampleExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#java/lang/Class/getName()
org/apache/spark/sql/execution/SampleExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/SampleExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/freshName(java.lang.String)
org/apache/spark/sql/execution/SampleExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/SampleExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/SampleExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/SampleExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addMutableState(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/SampleExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/SampleExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/SampleExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/SampleExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/runtime/BoxesRunTime/boxToDouble(double)
org/apache/spark/sql/execution/SampleExec/doConsume(org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext,scala.collection.Seq,org.apache.spark.sql.catalyst.expressions.codegen.ExprCode)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/Dataset/anonfun/toDF/1/apply()#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/Dataset/anonfun/toDF/1/apply()#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/Dataset/anonfun/toDF/1/apply()#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/Dataset/anonfun/toDF/1/apply()#scala/collection/mutable/ArrayOps/mkString(java.lang.String)
org/apache/spark/sql/Dataset/anonfun/toDF/1/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/Dataset/anonfun/toDF/1/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/Dataset/anonfun/toDF/1/apply()#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/Dataset/anonfun/toDF/1/apply()#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/Dataset/anonfun/toDF/1/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/Dataset/anonfun/toDF/1/apply()#org/apache/spark/sql/types/StructType/size()
org/apache/spark/sql/Dataset/anonfun/toDF/1/apply()#org/apache/spark/sql/Dataset$$anonfun$toDF$1/apply()
org/apache/spark/sql/Dataset/anonfun/toDF/1/apply()#org/apache/spark/sql/Dataset$$anonfun$toDF$1$$anonfun$apply$6/6(org.apache.spark.sql.Dataset$$anonfun$toDF$1)
org/apache/spark/sql/Dataset/anonfun/toDF/1/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/Dataset/anonfun/toDF/1/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/Dataset/anonfun/toDF/1/apply()#scala/collection/Seq/size()
org/apache/spark/sql/Dataset/anonfun/toDF/1/apply()#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/Dataset/anonfun/toDF/1/apply()#scala/collection/Seq/mkString(java.lang.String)
org/apache/spark/sql/Dataset/anonfun/toDF/1/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/Dataset/anonfun/toDF/1/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/columnar/STRUCT/extract(java.nio.ByteBuffer)#java/nio/ByteBuffer/position()
org/apache/spark/sql/execution/columnar/STRUCT/extract(java.nio.ByteBuffer)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/pointTo(java.lang.Object,long,int)
org/apache/spark/sql/execution/columnar/STRUCT/extract(java.nio.ByteBuffer)#scala/Predef$/assert(boolean)
org/apache/spark/sql/execution/columnar/STRUCT/extract(java.nio.ByteBuffer)#java/nio/ByteBuffer/arrayOffset()
org/apache/spark/sql/execution/columnar/STRUCT/extract(java.nio.ByteBuffer)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/UnsafeRow(int)
org/apache/spark/sql/execution/columnar/STRUCT/extract(java.nio.ByteBuffer)#java/nio/ByteBuffer/position(int)
org/apache/spark/sql/execution/columnar/STRUCT/extract(java.nio.ByteBuffer)#java/nio/ByteBuffer/array()
org/apache/spark/sql/execution/columnar/STRUCT/extract(java.nio.ByteBuffer)#java/nio/ByteBuffer/hasArray()
org/apache/spark/sql/execution/columnar/STRUCT/extract(java.nio.ByteBuffer)#org/apache/spark/sql/execution/columnar/ByteBufferHelper$/getInt(java.nio.ByteBuffer)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anon/4/anonfun/addBinary/1/apply()#org/apache/parquet/io/api/Binary/length()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anon/4/anonfun/addBinary/1/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anon/4/anonfun/addBinary/1/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anon/4/anonfun/addBinary/1/apply()#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anon/4/anonfun/addBinary/1/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anon/4/anonfun/addBinary/1/apply()#org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter$$anon$4$$anonfun$addBinary$1/apply()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anon/4/anonfun/addBinary/1/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anon/4/anonfun/addBinary/1/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anon/4/anonfun/addBinary/1/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/anon/4/anonfun/addBinary/1/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/command/AlterTableRenamePartitionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/internal/SQLConf/resolver()
org/apache/spark/sql/execution/command/AlterTableRenamePartitionCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/empty()
org/apache/spark/sql/execution/command/AlterTableRenamePartitionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/DDLUtils$/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)
org/apache/spark/sql/execution/command/AlterTableRenamePartitionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/datasources/PartitioningUtils$/normalizePartitionSpec(scala.collection.immutable.Map,scala.collection.Seq,java.lang.String,scala.Function2)
org/apache/spark/sql/execution/command/AlterTableRenamePartitionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()
org/apache/spark/sql/execution/command/AlterTableRenamePartitionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()
org/apache/spark/sql/execution/command/AlterTableRenamePartitionCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/AlterTableRenamePartitionCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/command/AlterTableRenamePartitionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/renamePartitions(org.apache.spark.sql.catalyst.TableIdentifier,scala.collection.Seq,scala.collection.Seq)
org/apache/spark/sql/execution/command/AlterTableRenamePartitionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/AlterTableRenamePartitionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/DDLUtils$/verifyAlterTableType(org.apache.spark.sql.catalyst.catalog.SessionCatalog,org.apache.spark.sql.catalyst.catalog.CatalogTable,boolean)
org/apache/spark/sql/execution/command/AlterTableRenamePartitionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/TableIdentifier/quotedString()
org/apache/spark/sql/execution/RangeExec/anonfun/20/apply(int,scala.collection.Iterator)#scala/math/BigInt/$plus(scala.math.BigInt)
org/apache/spark/sql/execution/RangeExec/anonfun/20/apply(int,scala.collection.Iterator)#org/apache/spark/InterruptibleIterator/InterruptibleIterator(org.apache.spark.TaskContext,scala.collection.Iterator)
org/apache/spark/sql/execution/RangeExec/anonfun/20/apply(int,scala.collection.Iterator)#org/apache/spark/TaskContext$/get()
org/apache/spark/sql/execution/RangeExec/anonfun/20/apply(int,scala.collection.Iterator)#scala/math/BigInt$/long2bigInt(long)
org/apache/spark/sql/execution/RangeExec/anonfun/20/apply(int,scala.collection.Iterator)#scala/math/BigInt/$div(scala.math.BigInt)
org/apache/spark/sql/execution/RangeExec/anonfun/20/apply(int,scala.collection.Iterator)#scala/math/BigInt$/int2bigInt(int)
org/apache/spark/sql/execution/RangeExec/anonfun/20/apply(int,scala.collection.Iterator)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/createFromByteArray(int,int)
org/apache/spark/sql/execution/RangeExec/anonfun/20/apply(int,scala.collection.Iterator)#org/apache/spark/sql/execution/RangeExec$$anonfun$20$$anon$1/1(org.apache.spark.sql.execution.RangeExec$$anonfun$20,long,long,org.apache.spark.sql.catalyst.expressions.UnsafeRow,org.apache.spark.TaskContext)
org/apache/spark/sql/execution/RangeExec/anonfun/20/apply(int,scala.collection.Iterator)#org/apache/spark/sql/execution/RangeExec$$anonfun$20/getSafeMargin$1(scala.math.BigInt)
org/apache/spark/sql/execution/RangeExec/anonfun/20/apply(int,scala.collection.Iterator)#scala/math/BigInt/$times(scala.math.BigInt)
org/apache/spark/sql/execution/RangeExec/anonfun/20/apply(int,scala.collection.Iterator)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/calculateBitSetWidthInBytes(int)
org/apache/spark/sql/execution/RangeExec/anonfun/20/apply(int,scala.collection.Iterator)#org/apache/spark/sql/types/LongType$/defaultSize()
org/apache/spark/sql/execution/ui/SQLListener/anonfun/mergeAccumulatorUpdates/2/apply(scala.Tuple2)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/ui/SQLListener/anonfun/mergeAccumulatorUpdates/2/apply(scala.Tuple2)#org/apache/spark/sql/execution/ui/SQLListener$$anonfun$mergeAccumulatorUpdates$2$$anonfun$apply$11/11(org.apache.spark.sql.execution.ui.SQLListener$$anonfun$mergeAccumulatorUpdates$2)
org/apache/spark/sql/execution/ui/SQLListener/anonfun/mergeAccumulatorUpdates/2/apply(scala.Tuple2)#scala/Predef$/ArrowAssoc(java.lang.Object)
org/apache/spark/sql/execution/ui/SQLListener/anonfun/mergeAccumulatorUpdates/2/apply(scala.Tuple2)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/ui/SQLListener/anonfun/mergeAccumulatorUpdates/2/apply(scala.Tuple2)#scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/ui/SQLListener/anonfun/mergeAccumulatorUpdates/2/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/ui/SQLListener/anonfun/mergeAccumulatorUpdates/2/apply(scala.Tuple2)#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/execution/ui/SQLListener/anonfun/mergeAccumulatorUpdates/2/apply(scala.Tuple2)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/ui/SQLListener/anonfun/mergeAccumulatorUpdates/2/apply(scala.Tuple2)#scala/Tuple2/_1$mcJ$sp()
org/apache/spark/sql/execution/ui/SQLListener/anonfun/mergeAccumulatorUpdates/2/apply(scala.Tuple2)#org/apache/spark/sql/execution/metric/SQLMetrics$/stringValue(java.lang.String,scala.collection.Seq)
org/apache/spark/sql/execution/ui/SQLListener/anonfun/mergeAccumulatorUpdates/2/apply(scala.Tuple2)#scala/Tuple2/_2()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/anonfun/4/anonfun/apply/1/apply()#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/anonfun/4/anonfun/apply/1/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/anonfun/4/anonfun/apply/1/apply()#org/apache/spark/sql/execution/command/AnalyzeColumnCommand$$anonfun$4$$anonfun$apply$1/apply()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/anonfun/4/anonfun/apply/1/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/anonfun/4/anonfun/apply/1/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/anonfun/4/anonfun/apply/1/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/anonfun/4/anonfun/apply/1/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/anonfun/4/anonfun/apply/1/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/anonfun/4/anonfun/apply/1/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/command/AnalyzeColumnCommand/anonfun/4/anonfun/apply/1/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/DataFrameReader/jdbc(java.lang.String,java.lang.String,java.lang.String,long,long,int,java.util.Properties)#scala/collection/immutable/Map$/apply(scala.collection.Seq)
org/apache/spark/sql/DataFrameReader/jdbc(java.lang.String,java.lang.String,java.lang.String,long,long,int,java.util.Properties)#scala/Predef$/Map()
org/apache/spark/sql/DataFrameReader/jdbc(java.lang.String,java.lang.String,java.lang.String,long,long,int,java.util.Properties)#java/lang/Object/toString()
org/apache/spark/sql/DataFrameReader/jdbc(java.lang.String,java.lang.String,java.lang.String,long,long,int,java.util.Properties)#org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions$/JDBC_NUM_PARTITIONS()
org/apache/spark/sql/DataFrameReader/jdbc(java.lang.String,java.lang.String,java.lang.String,long,long,int,java.util.Properties)#scala/Predef$/ArrowAssoc(java.lang.Object)
org/apache/spark/sql/DataFrameReader/jdbc(java.lang.String,java.lang.String,java.lang.String,long,long,int,java.util.Properties)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/DataFrameReader/jdbc(java.lang.String,java.lang.String,java.lang.String,long,long,int,java.util.Properties)#org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions$/JDBC_PARTITION_COLUMN()
org/apache/spark/sql/DataFrameReader/jdbc(java.lang.String,java.lang.String,java.lang.String,long,long,int,java.util.Properties)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/DataFrameReader/jdbc(java.lang.String,java.lang.String,java.lang.String,long,long,int,java.util.Properties)#scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)
org/apache/spark/sql/DataFrameReader/jdbc(java.lang.String,java.lang.String,java.lang.String,long,long,int,java.util.Properties)#scala/collection/mutable/HashMap/$plus$plus$eq(scala.collection.TraversableOnce)
org/apache/spark/sql/DataFrameReader/jdbc(java.lang.String,java.lang.String,java.lang.String,long,long,int,java.util.Properties)#org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions$/JDBC_UPPER_BOUND()
org/apache/spark/sql/DataFrameReader/jdbc(java.lang.String,java.lang.String,java.lang.String,long,long,int,java.util.Properties)#org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions$/JDBC_LOWER_BOUND()
org/apache/spark/sql/DataFrameReader/jdbc(java.lang.String,java.lang.String,java.lang.String,long,long,int,java.util.Properties)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/DataFrameReader/json(org.apache.spark.sql.Dataset)#org/apache/spark/sql/execution/LogicalRDD$/apply$default$4()
org/apache/spark/sql/DataFrameReader/json(org.apache.spark.sql.Dataset)#org/apache/spark/rdd/RDD/mapPartitions(scala.Function1,boolean,scala.reflect.ClassTag)
org/apache/spark/sql/DataFrameReader/json(org.apache.spark.sql.Dataset)#org/apache/spark/rdd/RDD/mapPartitions$default$2()
org/apache/spark/sql/DataFrameReader/json(org.apache.spark.sql.Dataset)#org/apache/spark/sql/Dataset$/ofRows(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/DataFrameReader/json(org.apache.spark.sql.Dataset)#org/apache/spark/sql/DataFrameReader$$anonfun$5/5(org.apache.spark.sql.DataFrameReader,org.apache.spark.sql.catalyst.json.JSONOptions,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.Function2)
org/apache/spark/sql/DataFrameReader/json(org.apache.spark.sql.Dataset)#scala/Predef$/$conforms()
org/apache/spark/sql/DataFrameReader/json(org.apache.spark.sql.Dataset)#org/apache/spark/sql/types/StructType$/apply(scala.collection.Seq)
org/apache/spark/sql/DataFrameReader/json(org.apache.spark.sql.Dataset)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/DataFrameReader/json(org.apache.spark.sql.Dataset)#org/apache/spark/sql/DataFrameReader$$anonfun$3/3(org.apache.spark.sql.DataFrameReader,org.apache.spark.sql.catalyst.json.JSONOptions)
org/apache/spark/sql/DataFrameReader/json(org.apache.spark.sql.Dataset)#org/apache/spark/sql/types/StructType/filterNot(scala.Function1)
org/apache/spark/sql/DataFrameReader/json(org.apache.spark.sql.Dataset)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/DataFrameReader/json(org.apache.spark.sql.Dataset)#org/apache/spark/sql/execution/LogicalRDD$/apply$default$3()
org/apache/spark/sql/DataFrameReader/json(org.apache.spark.sql.Dataset)#org/apache/spark/sql/catalyst/json/JSONOptions/columnNameOfCorruptRecord()
org/apache/spark/sql/DataFrameReader/json(org.apache.spark.sql.Dataset)#org/apache/spark/sql/types/StructType/toAttributes()
org/apache/spark/sql/DataFrameReader/json(org.apache.spark.sql.Dataset)#org/apache/spark/sql/internal/SQLConf/sessionLocalTimeZone()
org/apache/spark/sql/DataFrameReader/json(org.apache.spark.sql.Dataset)#org/apache/spark/sql/DataFrameReader$$anonfun$4/4(org.apache.spark.sql.DataFrameReader)
org/apache/spark/sql/DataFrameReader/json(org.apache.spark.sql.Dataset)#org/apache/spark/sql/catalyst/json/JSONOptions/JSONOptions(scala.collection.immutable.Map,java.lang.String,java.lang.String)
org/apache/spark/sql/DataFrameReader/json(org.apache.spark.sql.Dataset)#org/apache/spark/sql/internal/SQLConf/columnNameOfCorruptRecord()
org/apache/spark/sql/DataFrameReader/json(org.apache.spark.sql.Dataset)#org/apache/spark/sql/DataFrameReader$$anonfun$2/2(org.apache.spark.sql.DataFrameReader,org.apache.spark.sql.Dataset,org.apache.spark.sql.catalyst.json.JSONOptions)
org/apache/spark/sql/DataFrameReader/json(org.apache.spark.sql.Dataset)#scala/collection/mutable/HashMap/toMap(scala.Predef$$less$colon$less)
org/apache/spark/sql/DataFrameReader/jdbc(java.lang.String,java.lang.String,java.lang.String[],java.util.Properties)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/DataFrameReader/jdbc(java.lang.String,java.lang.String,java.lang.String[],java.util.Properties)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/DataFrameReader/jdbc(java.lang.String,java.lang.String,java.lang.String[],java.util.Properties)#org/apache/spark/sql/DataFrameReader$$anonfun$1/1(org.apache.spark.sql.DataFrameReader)
org/apache/spark/sql/DataFrameReader/jdbc(java.lang.String,java.lang.String,java.lang.String[],java.util.Properties)#scala/Predef$/$conforms()
org/apache/spark/sql/DataFrameReader/jdbc(java.lang.String,java.lang.String,java.lang.String[],java.util.Properties)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/DataFrameReader/jdbc(java.lang.String,java.lang.String,java.lang.String[],java.util.Properties)#scala/collection/mutable/HashMap/toMap(scala.Predef$$less$colon$less)
org/apache/spark/sql/DataFrameReader/jdbc(java.lang.String,java.lang.String,java.lang.String[],java.util.Properties)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/DataFrameReader/jdbc(java.lang.String,java.lang.String,java.lang.String[],java.util.Properties)#scala/collection/JavaConverters$/propertiesAsScalaMapConverter(java.util.Properties)
org/apache/spark/sql/DataFrameReader/jdbc(java.lang.String,java.lang.String,java.lang.String[],java.util.Properties)#scala/collection/convert/Decorators$AsScala/asScala()
org/apache/spark/sql/DataFrameReader/jdbc(java.lang.String,java.lang.String,java.lang.String[],java.util.Properties)#scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)
org/apache/spark/sql/DataFrameReader/jdbc(java.lang.String,java.lang.String,java.lang.String[],java.util.Properties)#scala/collection/mutable/ArrayOps/zipWithIndex(scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/DataFrameReader/jdbc(java.lang.String,java.lang.String,java.lang.String[],java.util.Properties)#scala/collection/immutable/Map/$plus$plus(scala.collection.GenTraversableOnce)
org/apache/spark/sql/DataFrameReader/assertNoSpecifiedSchema(java.lang.String)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/DataFrameReader/assertNoSpecifiedSchema(java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/DataFrameReader/assertNoSpecifiedSchema(java.lang.String)#scala/Option/nonEmpty()
org/apache/spark/sql/DataFrameReader/assertNoSpecifiedSchema(java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/DataFrameReader/assertNoSpecifiedSchema(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/DataFrameReader/assertNoSpecifiedSchema(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/DataFrameReader/assertNoSpecifiedSchema(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/DataFrameReader/assertNoSpecifiedSchema(java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/DataFrameReader/assertNoSpecifiedSchema(java.lang.String)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/DataFrameReader/assertNoSpecifiedSchema(java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/DataFrameReader/jdbc(java.lang.String,java.lang.String,java.util.Properties)#org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions$/JDBC_URL()
org/apache/spark/sql/DataFrameReader/jdbc(java.lang.String,java.lang.String,java.util.Properties)#scala/collection/JavaConverters$/propertiesAsScalaMapConverter(java.util.Properties)
org/apache/spark/sql/DataFrameReader/jdbc(java.lang.String,java.lang.String,java.util.Properties)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/DataFrameReader/jdbc(java.lang.String,java.lang.String,java.util.Properties)#scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)
org/apache/spark/sql/DataFrameReader/jdbc(java.lang.String,java.lang.String,java.util.Properties)#scala/collection/mutable/HashMap/$plus$plus$eq(scala.collection.TraversableOnce)
org/apache/spark/sql/DataFrameReader/jdbc(java.lang.String,java.lang.String,java.util.Properties)#org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions$/JDBC_TABLE_NAME()
org/apache/spark/sql/DataFrameReader/jdbc(java.lang.String,java.lang.String,java.util.Properties)#scala/collection/mutable/HashMap/$plus$eq(java.lang.Object,java.lang.Object,scala.collection.Seq)
org/apache/spark/sql/DataFrameReader/jdbc(java.lang.String,java.lang.String,java.util.Properties)#scala/Predef$/ArrowAssoc(java.lang.Object)
org/apache/spark/sql/DataFrameReader/jdbc(java.lang.String,java.lang.String,java.util.Properties)#scala/collection/convert/Decorators$AsScala/asScala()
org/apache/spark/sql/DataFrameReader/csv(org.apache.spark.sql.Dataset)#org/apache/spark/sql/execution/LogicalRDD$/apply$default$4()
org/apache/spark/sql/DataFrameReader/csv(org.apache.spark.sql.Dataset)#org/apache/spark/rdd/RDD/mapPartitions(scala.Function1,boolean,scala.reflect.ClassTag)
org/apache/spark/sql/DataFrameReader/csv(org.apache.spark.sql.Dataset)#org/apache/spark/sql/DataFrameReader$$anonfun$8/8(org.apache.spark.sql.DataFrameReader,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/DataFrameReader/csv(org.apache.spark.sql.Dataset)#org/apache/spark/rdd/RDD/mapPartitions$default$2()
org/apache/spark/sql/DataFrameReader/csv(org.apache.spark.sql.Dataset)#org/apache/spark/sql/types/StructType/toAttributes()
org/apache/spark/sql/DataFrameReader/csv(org.apache.spark.sql.Dataset)#scala/Option/map(scala.Function1)
org/apache/spark/sql/DataFrameReader/csv(org.apache.spark.sql.Dataset)#org/apache/spark/sql/DataFrameReader$$anonfun$10/10(org.apache.spark.sql.DataFrameReader,org.apache.spark.sql.Dataset)
org/apache/spark/sql/DataFrameReader/csv(org.apache.spark.sql.Dataset)#org/apache/spark/sql/Dataset$/ofRows(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/DataFrameReader/csv(org.apache.spark.sql.Dataset)#org/apache/spark/sql/DataFrameReader$$anonfun$11/11(org.apache.spark.sql.DataFrameReader,org.apache.spark.sql.execution.datasources.csv.CSVOptions,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType)
org/apache/spark/sql/DataFrameReader/csv(org.apache.spark.sql.Dataset)#scala/Predef$/$conforms()
org/apache/spark/sql/DataFrameReader/csv(org.apache.spark.sql.Dataset)#org/apache/spark/sql/types/StructType$/apply(scala.collection.Seq)
org/apache/spark/sql/DataFrameReader/csv(org.apache.spark.sql.Dataset)#org/apache/spark/sql/DataFrameReader$$anonfun$7/7(org.apache.spark.sql.DataFrameReader,org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.csv.CSVOptions,scala.Option)
org/apache/spark/sql/DataFrameReader/csv(org.apache.spark.sql.Dataset)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/DataFrameReader/csv(org.apache.spark.sql.Dataset)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/DataFrameReader/csv(org.apache.spark.sql.Dataset)#org/apache/spark/sql/types/StructType/filterNot(scala.Function1)
org/apache/spark/sql/DataFrameReader/csv(org.apache.spark.sql.Dataset)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/DataFrameReader/csv(org.apache.spark.sql.Dataset)#org/apache/spark/sql/execution/LogicalRDD$/apply$default$3()
org/apache/spark/sql/DataFrameReader/csv(org.apache.spark.sql.Dataset)#org/apache/spark/sql/internal/SQLConf/sessionLocalTimeZone()
org/apache/spark/sql/DataFrameReader/csv(org.apache.spark.sql.Dataset)#org/apache/spark/sql/DataFrameReader$$anonfun$9/9(org.apache.spark.sql.DataFrameReader,org.apache.spark.sql.execution.datasources.csv.CSVOptions,org.apache.spark.sql.Dataset)
org/apache/spark/sql/DataFrameReader/csv(org.apache.spark.sql.Dataset)#scala/collection/mutable/ArrayOps/headOption()
org/apache/spark/sql/DataFrameReader/csv(org.apache.spark.sql.Dataset)#org/apache/spark/sql/execution/datasources/csv/CSVUtils$/filterCommentAndEmpty(org.apache.spark.sql.Dataset,org.apache.spark.sql.execution.datasources.csv.CSVOptions)
org/apache/spark/sql/DataFrameReader/csv(org.apache.spark.sql.Dataset)#org/apache/spark/sql/execution/datasources/csv/CSVOptions$/$lessinit$greater$default$3()
org/apache/spark/sql/DataFrameReader/csv(org.apache.spark.sql.Dataset)#scala/collection/mutable/HashMap/toMap(scala.Predef$$less$colon$less)
org/apache/spark/sql/DataFrameReader/load(scala.collection.Seq)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/DataFrameReader/load(scala.collection.Seq)#org/apache/spark/sql/execution/datasources/DataSource$/apply$default$8()
org/apache/spark/sql/DataFrameReader/load(scala.collection.Seq)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/DataFrameReader/load(scala.collection.Seq)#java/lang/String/toLowerCase(java.util.Locale)
org/apache/spark/sql/DataFrameReader/load(scala.collection.Seq)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/DataFrameReader/load(scala.collection.Seq)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/DataFrameReader/load(scala.collection.Seq)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/DataFrameReader/load(scala.collection.Seq)#scala/collection/mutable/HashMap/toMap(scala.Predef$$less$colon$less)
org/apache/spark/sql/DataFrameReader/load(scala.collection.Seq)#org/apache/spark/sql/execution/datasources/DataSource$/apply$default$5()
org/apache/spark/sql/DataFrameReader/load(scala.collection.Seq)#org/apache/spark/sql/execution/command/DDLUtils$/HIVE_PROVIDER()
org/apache/spark/sql/DataFrameReader/load(scala.collection.Seq)#org/apache/spark/sql/execution/datasources/DataSource$/apply$default$6()
org/apache/spark/sql/DataFrameReader/load(scala.collection.Seq)#scala/Predef$/$conforms()
org/apache/spark/sql/DataFrameReader/load(scala.collection.Seq)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/org/apache/spark/sql/execution/datasources/InMemoryFileIndex/listLeafFiles/3/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/FileStatus/isSymlink()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/org/apache/spark/sql/execution/datasources/InMemoryFileIndex/listLeafFiles/3/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/FileSystem/getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/org/apache/spark/sql/execution/datasources/InMemoryFileIndex/listLeafFiles/3/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/FileStatus/getReplication()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/org/apache/spark/sql/execution/datasources/InMemoryFileIndex/listLeafFiles/3/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/FileStatus/getPath()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/org/apache/spark/sql/execution/datasources/InMemoryFileIndex/listLeafFiles/3/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/FileStatus/getBlockSize()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/org/apache/spark/sql/execution/datasources/InMemoryFileIndex/listLeafFiles/3/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/FileStatus/getSymlink()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/org/apache/spark/sql/execution/datasources/InMemoryFileIndex/listLeafFiles/3/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/LocatedFileStatus/setSymlink(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/org/apache/spark/sql/execution/datasources/InMemoryFileIndex/listLeafFiles/3/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/FileStatus/getModificationTime()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/org/apache/spark/sql/execution/datasources/InMemoryFileIndex/listLeafFiles/3/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/LocatedFileStatus/LocatedFileStatus(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.BlockLocation[])
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/org/apache/spark/sql/execution/datasources/InMemoryFileIndex/listLeafFiles/3/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/FileStatus/isDirectory()
org/apache/spark/sql/execution/datasources/InMemoryFileIndex/anonfun/org/apache/spark/sql/execution/datasources/InMemoryFileIndex/listLeafFiles/3/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/FileStatus/getLen()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/latestIterator()#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/latestIterator()#scala/collection/immutable/Set/$plus$plus(scala.collection.GenTraversableOnce)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/latestIterator()#scala/collection/Iterator$/empty()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/latestIterator()#scala/collection/convert/Decorators$AsScala/asScala()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/latestIterator()#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$$anonfun$5/5(org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/latestIterator()#scala/collection/mutable/HashMap/keySet()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/latestIterator()#scala/package$/Iterator()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/latestIterator()#scala/collection/JavaConverters$/asScalaIteratorConverter(java.util.Iterator)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/latestIterator()#java/util/Set/iterator()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/latestIterator()#scala/collection/Iterator/map(scala.Function1)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/latestIterator()#scala/collection/immutable/Set/nonEmpty()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/latestIterator()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/latestIterator()#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$$anonfun$latestIterator$1/1(org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/latestIterator()#scala/collection/TraversableOnce/toSet()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/latestIterator()#scala/collection/immutable/Set/max(scala.math.Ordering)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/latestIterator()#java/util/concurrent/ConcurrentHashMap/entrySet()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/latestIterator()#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$readSnapshotFile(long)#java/util/concurrent/ConcurrentHashMap/put(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$readSnapshotFile(long)#org/spark_project/guava/io/ByteStreams/readFully(java.io.InputStream,byte[],int,int)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$readSnapshotFile(long)#java/util/concurrent/ConcurrentHashMap/ConcurrentHashMap()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$readSnapshotFile(long)#java/io/DataInputStream/close()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$readSnapshotFile(long)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$readSnapshotFile(long)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$readSnapshotFile(long)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$readSnapshotFile(long)#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$readSnapshotFile(long)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$readSnapshotFile(long)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$readSnapshotFile(long)#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$$anonfun$org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$readSnapshotFile$1/1(org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider,long,org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$readSnapshotFile(long)#java/io/IOException/IOException(java.lang.String)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$readSnapshotFile(long)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/UnsafeRow(int)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$readSnapshotFile(long)#java/io/DataInputStream/readInt()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$readSnapshotFile(long)#org/apache/hadoop/fs/FileSystem/open(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$readSnapshotFile(long)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$readSnapshotFile(long)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/pointTo(byte[],int)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/filesForVersion(scala.collection.Seq,long)#scala/Some/x()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/filesForVersion(scala.collection.Seq,long)#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$$anonfun$9/9(org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/filesForVersion(scala.collection.Seq,long)#scala/collection/IterableLike/takeWhile(scala.Function1)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/filesForVersion(scala.collection.Seq,long)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/filesForVersion(scala.collection.Seq,long)#scala/Predef$/require(boolean)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/filesForVersion(scala.collection.Seq,long)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/filesForVersion(scala.collection.Seq,long)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/filesForVersion(scala.collection.Seq,long)#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$$anonfun$filesForVersion$1/1(org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider,long)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/filesForVersion(scala.collection.Seq,long)#scala/collection/TraversableLike/lastOption()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/filesForVersion(scala.collection.Seq,long)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/filesForVersion(scala.collection.Seq,long)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/filesForVersion(scala.collection.Seq,long)#scala/Option$/option2Iterable(scala.Option)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/filesForVersion(scala.collection.Seq,long)#scala/collection/Seq/exists(scala.Function1)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/filesForVersion(scala.collection.Seq,long)#scala/collection/Seq/takeWhile(scala.Function1)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/filesForVersion(scala.collection.Seq,long)#scala/collection/Seq/filter(scala.Function1)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/filesForVersion(scala.collection.Seq,long)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/filesForVersion(scala.collection.Seq,long)#scala/collection/Iterable/toSeq()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/filesForVersion(scala.collection.Seq,long)#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$$anonfun$11/11(org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider,long,org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$StoreFile)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/filesForVersion(scala.collection.Seq,long)#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$$anonfun$2/2(org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider,long,org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$StoreFile,scala.collection.immutable.List)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/filesForVersion(scala.collection.Seq,long)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/filesForVersion(scala.collection.Seq,long)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/filesForVersion(scala.collection.Seq,long)#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$$anonfun$10/10(org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider,long)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/filesForVersion(scala.collection.Seq,long)#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$$anonfun$12/12(org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider,long)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/filesForVersion(scala.collection.Seq,long)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/filesForVersion(scala.collection.Seq,long)#scala/collection/TraversableOnce/toList()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/fetchFiles()#scala/collection/Seq/foreach(scala.Function1)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/fetchFiles()#org/apache/hadoop/fs/FileSystem/listStatus(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/fetchFiles()#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$$anonfun$13/13(org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/fetchFiles()#scala/collection/mutable/HashMap/HashMap()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/fetchFiles()#scala/collection/Seq/sortBy(scala.Function1,scala.math.Ordering)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/fetchFiles()#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$$anonfun$fetchFiles$1/1(org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider,scala.collection.mutable.HashMap)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/fetchFiles()#scala/collection/Seq$/empty()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/fetchFiles()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/fetchFiles()#scala/collection/Iterable/toSeq()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/fetchFiles()#scala/collection/mutable/HashMap/values()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/fetchFiles()#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$$anonfun$fetchFiles$2/2(org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider,scala.collection.Seq)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$updateFromDeltaFile(long,java.util.concurrent.ConcurrentHashMap)#java/util/concurrent/ConcurrentHashMap/put(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$updateFromDeltaFile(long,java.util.concurrent.ConcurrentHashMap)#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$$anonfun$org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$updateFromDeltaFile$1/1(org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider,long,org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$updateFromDeltaFile(long,java.util.concurrent.ConcurrentHashMap)#java/lang/IllegalStateException/IllegalStateException(java.lang.String,java.lang.Throwable)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$updateFromDeltaFile(long,java.util.concurrent.ConcurrentHashMap)#java/io/IOException/IOException(java.lang.String)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$updateFromDeltaFile(long,java.util.concurrent.ConcurrentHashMap)#org/spark_project/guava/io/ByteStreams/readFully(java.io.InputStream,byte[],int,int)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$updateFromDeltaFile(long,java.util.concurrent.ConcurrentHashMap)#java/io/DataInputStream/close()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$updateFromDeltaFile(long,java.util.concurrent.ConcurrentHashMap)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$updateFromDeltaFile(long,java.util.concurrent.ConcurrentHashMap)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$updateFromDeltaFile(long,java.util.concurrent.ConcurrentHashMap)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$updateFromDeltaFile(long,java.util.concurrent.ConcurrentHashMap)#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$updateFromDeltaFile(long,java.util.concurrent.ConcurrentHashMap)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$updateFromDeltaFile(long,java.util.concurrent.ConcurrentHashMap)#java/util/concurrent/ConcurrentHashMap/remove(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$updateFromDeltaFile(long,java.util.concurrent.ConcurrentHashMap)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/UnsafeRow(int)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$updateFromDeltaFile(long,java.util.concurrent.ConcurrentHashMap)#java/io/DataInputStream/readInt()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$updateFromDeltaFile(long,java.util.concurrent.ConcurrentHashMap)#org/apache/hadoop/fs/FileSystem/open(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$updateFromDeltaFile(long,java.util.concurrent.ConcurrentHashMap)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$updateFromDeltaFile(long,java.util.concurrent.ConcurrentHashMap)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/pointTo(byte[],int)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/cleanup()#scala/util/control/NonFatal$/unapply(java.lang.Throwable)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/cleanup()#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$$anonfun$8/8(org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider,org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$StoreFile)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/cleanup()#scala/collection/Seq/last()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/cleanup()#scala/Option/isEmpty()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/cleanup()#scala/collection/Iterable/filter(scala.Function1)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/cleanup()#scala/collection/TraversableOnce/toSeq()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/cleanup()#scala/Option/get()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/cleanup()#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$$anonfun$cleanup$3/3(org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider,org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$StoreFile,scala.collection.Seq)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/cleanup()#scala/collection/mutable/HashMap/keys()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/cleanup()#scala/collection/Seq/filter(scala.Function1)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/cleanup()#scala/collection/Seq/foreach(scala.Function1)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/cleanup()#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$$anonfun$cleanup$4/4(org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/cleanup()#scala/collection/Seq/head()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/cleanup()#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/cleanup()#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$$anonfun$cleanup$2/2(org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/cleanup()#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$$anonfun$cleanup$1/1(org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/cleanup()#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$$anonfun$1/1(org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider,long)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/cleanup()#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$StoreFile/version()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(long,java.util.concurrent.ConcurrentHashMap,org.apache.hadoop.fs.Path)#scala/collection/mutable/HashMap/put(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(long,java.util.concurrent.ConcurrentHashMap,org.apache.hadoop.fs.Path)#java/io/IOException/IOException(java.lang.String)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(long,java.util.concurrent.ConcurrentHashMap,org.apache.hadoop.fs.Path)#org/apache/hadoop/fs/FileSystem/exists(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(long,java.util.concurrent.ConcurrentHashMap,org.apache.hadoop.fs.Path)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(long,java.util.concurrent.ConcurrentHashMap,org.apache.hadoop.fs.Path)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(long,java.util.concurrent.ConcurrentHashMap,org.apache.hadoop.fs.Path)#org/apache/hadoop/fs/FileSystem/delete(org.apache.hadoop.fs.Path,boolean)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(long,java.util.concurrent.ConcurrentHashMap,org.apache.hadoop.fs.Path)#scala/runtime/BoxesRunTime/boxToBoolean(boolean)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(long,java.util.concurrent.ConcurrentHashMap,org.apache.hadoop.fs.Path)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(long,java.util.concurrent.ConcurrentHashMap,org.apache.hadoop.fs.Path)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(long,java.util.concurrent.ConcurrentHashMap,org.apache.hadoop.fs.Path)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(long,java.util.concurrent.ConcurrentHashMap,org.apache.hadoop.fs.Path)#org/apache/hadoop/fs/FileSystem/rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/doSnapshot()#scala/Some/x()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/doSnapshot()#scala/util/control/NonFatal$/unapply(java.lang.Throwable)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/doSnapshot()#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/doSnapshot()#scala/collection/Seq/filter(scala.Function1)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/doSnapshot()#scala/Option/isEmpty()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/doSnapshot()#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$$anonfun$doSnapshot$1/1(org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/doSnapshot()#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/doSnapshot()#scala/collection/Seq/size()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/doSnapshot()#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/doSnapshot()#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$StoreFile/version()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/doSnapshot()#scala/Option/get()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/doSnapshot()#org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider$$anonfun$7/7(org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/doSnapshot()#scala/collection/Seq/last()
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/doSnapshot()#scala/collection/mutable/HashMap/get(java.lang.Object)
org/apache/spark/sql/execution/streaming/state/HDFSBackedStateStoreProvider/doSnapshot()#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/streaming/ConsoleSink/addBatch(long,org.apache.spark.sql.Dataset)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/streaming/ConsoleSink/addBatch(long,org.apache.spark.sql.Dataset)#org/apache/spark/SparkContext/parallelize(scala.collection.Seq,int,scala.reflect.ClassTag)
org/apache/spark/sql/execution/streaming/ConsoleSink/addBatch(long,org.apache.spark.sql.Dataset)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/ConsoleSink/addBatch(long,org.apache.spark.sql.Dataset)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/ConsoleSink/addBatch(long,org.apache.spark.sql.Dataset)#scala/Predef$/println(java.lang.Object)
org/apache/spark/sql/execution/streaming/ConsoleSink/addBatch(long,org.apache.spark.sql.Dataset)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/streaming/ConsoleSink/addBatch(long,org.apache.spark.sql.Dataset)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/ConsoleSink/addBatch(long,org.apache.spark.sql.Dataset)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/ConsoleSink/addBatch(long,org.apache.spark.sql.Dataset)#org/apache/spark/SparkContext/parallelize$default$2()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/addPartitions/1/anonfun/12/anonfun/13/apply(org.apache.spark.sql.execution.command.PartitionStatistics)#scala/collection/immutable/Map$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/addPartitions/1/anonfun/12/anonfun/13/apply(org.apache.spark.sql.execution.command.PartitionStatistics)#scala/Predef$/Map()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/addPartitions/1/anonfun/12/anonfun/13/apply(org.apache.spark.sql.execution.command.PartitionStatistics)#java/lang/Object/toString()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/addPartitions/1/anonfun/12/anonfun/13/apply(org.apache.spark.sql.execution.command.PartitionStatistics)#scala/Predef$/ArrowAssoc(java.lang.Object)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/addPartitions/1/anonfun/12/anonfun/13/apply(org.apache.spark.sql.execution.command.PartitionStatistics)#org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand$$anonfun$addPartitions$1$$anonfun$12/org$apache$spark$sql$execution$command$AlterTableRecoverPartitionsCommand$$anonfun$$anonfun$$$outer()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/addPartitions/1/anonfun/12/anonfun/13/apply(org.apache.spark.sql.execution.command.PartitionStatistics)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/addPartitions/1/anonfun/12/anonfun/13/apply(org.apache.spark.sql.execution.command.PartitionStatistics)#scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/addPartitions/1/anonfun/12/anonfun/13/apply(org.apache.spark.sql.execution.command.PartitionStatistics)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/addPartitions/1/anonfun/12/anonfun/13/apply(org.apache.spark.sql.execution.command.PartitionStatistics)#org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand$$anonfun$addPartitions$1/org$apache$spark$sql$execution$command$AlterTableRecoverPartitionsCommand$$anonfun$$$outer()
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/addPartitions/1/anonfun/12/anonfun/13/apply(org.apache.spark.sql.execution.command.PartitionStatistics)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/AlterTableRecoverPartitionsCommand/anonfun/addPartitions/1/anonfun/12/anonfun/13/apply(org.apache.spark.sql.execution.command.PartitionStatistics)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/stat/StatFunctions/CovarianceCounter/merge(org.apache.spark.sql.execution.stat.StatFunctions$CovarianceCounter)#org/apache/spark/sql/execution/stat/StatFunctions$CovarianceCounter/count()
org/apache/spark/sql/execution/stat/StatFunctions/CovarianceCounter/merge(org.apache.spark.sql.execution.stat.StatFunctions$CovarianceCounter)#org/apache/spark/sql/execution/stat/StatFunctions$CovarianceCounter/count_$eq(long)
org/apache/spark/sql/execution/stat/StatFunctions/CovarianceCounter/merge(org.apache.spark.sql.execution.stat.StatFunctions$CovarianceCounter)#org/apache/spark/sql/execution/stat/StatFunctions$CovarianceCounter/Ck_$eq(double)
org/apache/spark/sql/execution/stat/StatFunctions/CovarianceCounter/merge(org.apache.spark.sql.execution.stat.StatFunctions$CovarianceCounter)#org/apache/spark/sql/execution/stat/StatFunctions$CovarianceCounter/Ck()
org/apache/spark/sql/execution/stat/StatFunctions/CovarianceCounter/merge(org.apache.spark.sql.execution.stat.StatFunctions$CovarianceCounter)#org/apache/spark/sql/execution/stat/StatFunctions$CovarianceCounter/MkX()
org/apache/spark/sql/execution/stat/StatFunctions/CovarianceCounter/merge(org.apache.spark.sql.execution.stat.StatFunctions$CovarianceCounter)#org/apache/spark/sql/execution/stat/StatFunctions$CovarianceCounter/MkX_$eq(double)
org/apache/spark/sql/execution/stat/StatFunctions/CovarianceCounter/merge(org.apache.spark.sql.execution.stat.StatFunctions$CovarianceCounter)#org/apache/spark/sql/execution/stat/StatFunctions$CovarianceCounter/MkY()
org/apache/spark/sql/execution/stat/StatFunctions/CovarianceCounter/merge(org.apache.spark.sql.execution.stat.StatFunctions$CovarianceCounter)#org/apache/spark/sql/execution/stat/StatFunctions$CovarianceCounter/MkY_$eq(double)
org/apache/spark/sql/execution/stat/StatFunctions/CovarianceCounter/merge(org.apache.spark.sql.execution.stat.StatFunctions$CovarianceCounter)#org/apache/spark/sql/execution/stat/StatFunctions$CovarianceCounter/yAvg()
org/apache/spark/sql/execution/stat/StatFunctions/CovarianceCounter/merge(org.apache.spark.sql.execution.stat.StatFunctions$CovarianceCounter)#org/apache/spark/sql/execution/stat/StatFunctions$CovarianceCounter/yAvg_$eq(double)
org/apache/spark/sql/execution/stat/StatFunctions/CovarianceCounter/merge(org.apache.spark.sql.execution.stat.StatFunctions$CovarianceCounter)#org/apache/spark/sql/execution/stat/StatFunctions$CovarianceCounter/xAvg()
org/apache/spark/sql/execution/stat/StatFunctions/CovarianceCounter/merge(org.apache.spark.sql.execution.stat.StatFunctions$CovarianceCounter)#org/apache/spark/sql/execution/stat/StatFunctions$CovarianceCounter/xAvg_$eq(double)
org/apache/spark/sql/execution/stat/StatFunctions/CovarianceCounter/add(double,double)#org/apache/spark/sql/execution/stat/StatFunctions$CovarianceCounter/count()
org/apache/spark/sql/execution/stat/StatFunctions/CovarianceCounter/add(double,double)#org/apache/spark/sql/execution/stat/StatFunctions$CovarianceCounter/count_$eq(long)
org/apache/spark/sql/execution/stat/StatFunctions/CovarianceCounter/add(double,double)#org/apache/spark/sql/execution/stat/StatFunctions$CovarianceCounter/Ck_$eq(double)
org/apache/spark/sql/execution/stat/StatFunctions/CovarianceCounter/add(double,double)#org/apache/spark/sql/execution/stat/StatFunctions$CovarianceCounter/Ck()
org/apache/spark/sql/execution/stat/StatFunctions/CovarianceCounter/add(double,double)#org/apache/spark/sql/execution/stat/StatFunctions$CovarianceCounter/MkX()
org/apache/spark/sql/execution/stat/StatFunctions/CovarianceCounter/add(double,double)#org/apache/spark/sql/execution/stat/StatFunctions$CovarianceCounter/MkX_$eq(double)
org/apache/spark/sql/execution/stat/StatFunctions/CovarianceCounter/add(double,double)#org/apache/spark/sql/execution/stat/StatFunctions$CovarianceCounter/MkY()
org/apache/spark/sql/execution/stat/StatFunctions/CovarianceCounter/add(double,double)#org/apache/spark/sql/execution/stat/StatFunctions$CovarianceCounter/MkY_$eq(double)
org/apache/spark/sql/execution/stat/StatFunctions/CovarianceCounter/add(double,double)#org/apache/spark/sql/execution/stat/StatFunctions$CovarianceCounter/yAvg()
org/apache/spark/sql/execution/stat/StatFunctions/CovarianceCounter/add(double,double)#org/apache/spark/sql/execution/stat/StatFunctions$CovarianceCounter/yAvg_$eq(double)
org/apache/spark/sql/execution/stat/StatFunctions/CovarianceCounter/add(double,double)#org/apache/spark/sql/execution/stat/StatFunctions$CovarianceCounter/xAvg()
org/apache/spark/sql/execution/stat/StatFunctions/CovarianceCounter/add(double,double)#org/apache/spark/sql/execution/stat/StatFunctions$CovarianceCounter/xAvg_$eq(double)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitLoadData/1/apply()#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitLoadData/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$LoadDataContext/tableIdentifier()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitLoadData/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder/visitTableIdentifier(org.apache.spark.sql.catalyst.parser.SqlBaseParser$TableIdentifierContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitLoadData/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$LoadDataContext/partitionSpec()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitLoadData/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitLoadData$1/apply()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitLoadData/1/apply()#org/apache/spark/sql/catalyst/parser/ParserUtils$/string(org.antlr.v4.runtime.Token)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitLoadData/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$LoadDataContext/OVERWRITE()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitLoadData/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$LoadDataContext/LOCAL()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitLoadData/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitLoadData$1$$anonfun$apply$15/15(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitLoadData$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitLoadData/1/apply()#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/deleteMatchingPartitions/2/apply(scala.Tuple2)#scala/Predef$/assert(boolean,scala.Function0)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/deleteMatchingPartitions/2/apply(scala.Tuple2)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/deleteMatchingPartitions/2/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/deleteMatchingPartitions/2/apply(scala.Tuple2)#org/apache/hadoop/fs/Path/Path(java.lang.String)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/deleteMatchingPartitions/2/apply(scala.Tuple2)#org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand$$anonfun$deleteMatchingPartitions$2$$anonfun$apply$1/1(org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$deleteMatchingPartitions$2)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/deleteMatchingPartitions/2/apply(scala.Tuple2)#java/io/IOException/IOException(java.lang.String)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/deleteMatchingPartitions/2/apply(scala.Tuple2)#org/apache/hadoop/fs/FileSystem/exists(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/deleteMatchingPartitions/2/apply(scala.Tuple2)#org/apache/spark/internal/io/FileCommitProtocol/deleteWithJob(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/deleteMatchingPartitions/2/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/deleteMatchingPartitions/2/apply(scala.Tuple2)#scala/collection/immutable/Set/$minus$minus(scala.collection.GenTraversableOnce)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/deleteMatchingPartitions/2/apply(scala.Tuple2)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/deleteMatchingPartitions/2/apply(scala.Tuple2)#scala/Tuple2/_2()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/deleteMatchingPartitions/2/apply(scala.Tuple2)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/deleteMatchingPartitions/2/apply(scala.Tuple2)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/deleteMatchingPartitions/2/apply(scala.Tuple2)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/deleteMatchingPartitions/2/apply(scala.Tuple2)#scala/collection/immutable/Map/toSet()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/deleteMatchingPartitions/2/apply(scala.Tuple2)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/deleteMatchingPartitions/2/apply(scala.Tuple2)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/InsertIntoHadoopFsRelationCommand/anonfun/deleteMatchingPartitions/2/apply(scala.Tuple2)#scala/collection/SetLike/isEmpty()
org/apache/spark/sql/execution/python/ExtractPythonUDFs/anonfun/5/anonfun/7/apply(scala.Tuple2)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/anonfun/5/anonfun/7/apply(scala.Tuple2)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/anonfun/5/anonfun/7/apply(scala.Tuple2)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/anonfun/5/anonfun/7/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/anonfun/5/anonfun/7/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/AttributeReference$/apply$default$3()
org/apache/spark/sql/execution/python/ExtractPythonUDFs/anonfun/5/anonfun/7/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/AttributeReference$/apply$default$4()
org/apache/spark/sql/execution/python/ExtractPythonUDFs/anonfun/5/anonfun/7/apply(scala.Tuple2)#scala/Tuple2/_2$mcI$sp()
org/apache/spark/sql/execution/python/ExtractPythonUDFs/anonfun/5/anonfun/7/apply(scala.Tuple2)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/anonfun/5/anonfun/7/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/AttributeReference$/apply$default$5(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.types.Metadata)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/anonfun/5/anonfun/7/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/AttributeReference/AttributeReference(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.types.Metadata,org.apache.spark.sql.catalyst.expressions.ExprId,scala.Option,java.lang.Boolean)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/anonfun/5/anonfun/7/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/AttributeReference$/apply$default$6(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.types.Metadata)
org/apache/spark/sql/execution/python/ExtractPythonUDFs/anonfun/5/anonfun/7/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/python/ExtractPythonUDFs/anonfun/5/anonfun/7/apply(scala.Tuple2)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/python/ExtractPythonUDFs/anonfun/5/anonfun/7/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/AttributeReference$/apply$default$7(java.lang.String,org.apache.spark.sql.types.DataType,boolean,org.apache.spark.sql.types.Metadata)
org/apache/spark/sql/execution/datasources/DataSource/anonfun/4/anonfun/apply/9/apply()#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/DataSource/anonfun/4/anonfun/apply/9/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/DataSource/anonfun/4/anonfun/apply/9/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/DataSource/anonfun/4/anonfun/apply/9/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/DataSource/anonfun/4/anonfun/apply/9/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/DataSource/anonfun/4/anonfun/apply/9/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/anonfun/4/anonfun/apply/9/apply()#org/apache/spark/sql/execution/datasources/DataSource$$anonfun$4$$anonfun$apply$9/apply()
org/apache/spark/sql/execution/datasources/DataSource/anonfun/4/anonfun/apply/9/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSource/anonfun/4/anonfun/apply/9/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/DataSource/anonfun/4/anonfun/apply/9/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/DataSource/anonfun/4/anonfun/apply/9/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/DataSource/anonfun/4/anonfun/apply/9/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/DataSource/anonfun/4/anonfun/apply/9/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/streaming/StreamMetadata/read(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/streaming/StreamMetadata/read(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#scala/util/control/NonFatal$/unapply(java.lang.Throwable)
org/apache/spark/sql/execution/streaming/StreamMetadata/read(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/commons/io/IOUtils/closeQuietly(java.io.InputStream)
org/apache/spark/sql/execution/streaming/StreamMetadata/read(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/fs/FileSystem/exists(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/streaming/StreamMetadata/read(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/json4s/jackson/Serialization$/read(java.io.Reader,org.json4s.Formats,scala.reflect.Manifest)
org/apache/spark/sql/execution/streaming/StreamMetadata/read(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/streaming/StreamMetadata$/logError(scala.Function0,java.lang.Throwable)
org/apache/spark/sql/execution/streaming/StreamMetadata/read(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#java/io/InputStreamReader/InputStreamReader(java.io.InputStream,java.nio.charset.Charset)
org/apache/spark/sql/execution/streaming/StreamMetadata/read(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamMetadata/read(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/fs/FileSystem/open(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/streaming/StreamMetadata/read(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/streaming/StreamMetadata$/format()
org/apache/spark/sql/execution/streaming/StreamMetadata/read(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#scala/Option/get()
org/apache/spark/sql/execution/streaming/StreamMetadata/read(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#scala/reflect/ManifestFactory$/classType(java.lang.Class)
org/apache/spark/sql/execution/streaming/StreamMetadata/read(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/streaming/StreamMetadata$/read(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/streaming/StreamMetadata/read(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#scala/Option/isEmpty()
org/apache/spark/sql/execution/streaming/StreamMetadata/read(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/streaming/StreamMetadata$$anonfun$read$1/1(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/streaming/StreamMetadata/write(org.apache.spark.sql.execution.streaming.StreamMetadata,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/streaming/StreamMetadata/write(org.apache.spark.sql.execution.streaming.StreamMetadata,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#scala/util/control/NonFatal$/unapply(java.lang.Throwable)
org/apache/spark/sql/execution/streaming/StreamMetadata/write(org.apache.spark.sql.execution.streaming.StreamMetadata,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/streaming/StreamMetadata$/write(org.apache.spark.sql.execution.streaming.StreamMetadata,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/streaming/StreamMetadata/write(org.apache.spark.sql.execution.streaming.StreamMetadata,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/fs/FileSystem/create(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/streaming/StreamMetadata/write(org.apache.spark.sql.execution.streaming.StreamMetadata,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#scala/Option/isEmpty()
org/apache/spark/sql/execution/streaming/StreamMetadata/write(org.apache.spark.sql.execution.streaming.StreamMetadata,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/streaming/StreamMetadata$/logError(scala.Function0,java.lang.Throwable)
org/apache/spark/sql/execution/streaming/StreamMetadata/write(org.apache.spark.sql.execution.streaming.StreamMetadata,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#java/io/OutputStreamWriter/OutputStreamWriter(java.io.OutputStream)
org/apache/spark/sql/execution/streaming/StreamMetadata/write(org.apache.spark.sql.execution.streaming.StreamMetadata,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/streaming/StreamMetadata$/format()
org/apache/spark/sql/execution/streaming/StreamMetadata/write(org.apache.spark.sql.execution.streaming.StreamMetadata,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#scala/Option/get()
org/apache/spark/sql/execution/streaming/StreamMetadata/write(org.apache.spark.sql.execution.streaming.StreamMetadata,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#java/io/OutputStreamWriter/close()
org/apache/spark/sql/execution/streaming/StreamMetadata/write(org.apache.spark.sql.execution.streaming.StreamMetadata,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/streaming/StreamMetadata$$anonfun$write$1/1(org.apache.spark.sql.execution.streaming.StreamMetadata,org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/streaming/StreamMetadata/write(org.apache.spark.sql.execution.streaming.StreamMetadata,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/json4s/jackson/Serialization$/write(java.lang.Object,java.io.Writer,org.json4s.Formats)
org/apache/spark/sql/execution/streaming/StreamMetadata/write(org.apache.spark.sql.execution.streaming.StreamMetadata,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/commons/io/IOUtils/closeQuietly(java.io.OutputStream)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/binaryToSQLTimestamp(org.apache.parquet.io.api.Binary)#org/apache/parquet/io/api/Binary/toByteBuffer()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/binaryToSQLTimestamp(org.apache.parquet.io.api.Binary)#org/apache/parquet/io/api/Binary/length()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/binaryToSQLTimestamp(org.apache.parquet.io.api.Binary)#scala/Predef$/assert(boolean,scala.Function0)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/binaryToSQLTimestamp(org.apache.parquet.io.api.Binary)#java/nio/ByteBuffer/order(java.nio.ByteOrder)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/binaryToSQLTimestamp(org.apache.parquet.io.api.Binary)#java/nio/ByteBuffer/getLong()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/binaryToSQLTimestamp(org.apache.parquet.io.api.Binary)#java/nio/ByteBuffer/getInt()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/binaryToSQLTimestamp(org.apache.parquet.io.api.Binary)#org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter$$anonfun$binaryToSQLTimestamp$1/1(org.apache.parquet.io.api.Binary)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/binaryToSQLTimestamp(org.apache.parquet.io.api.Binary)#org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter$/binaryToSQLTimestamp(org.apache.parquet.io.api.Binary)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/binaryToSQLTimestamp(org.apache.parquet.io.api.Binary)#org/apache/spark/sql/catalyst/util/DateTimeUtils$/fromJulianDay(int,long)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetRowConverter$$newConverter(org.apache.parquet.schema.Type,org.apache.spark.sql.types.DataType,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)#org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter$$anon$4/4(org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetRowConverter$$newConverter(org.apache.parquet.schema.Type,org.apache.spark.sql.types.DataType,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)#org/apache/spark/sql/types/DecimalType/scale()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetRowConverter$$newConverter(org.apache.parquet.schema.Type,org.apache.spark.sql.types.DataType,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)#org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter$$anon$5/5(org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetRowConverter$$newConverter(org.apache.parquet.schema.Type,org.apache.spark.sql.types.DataType,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)#org/apache/parquet/schema/Type/getOriginalType()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetRowConverter$$newConverter(org.apache.parquet.schema.Type,org.apache.spark.sql.types.DataType,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)#org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter$ParquetArrayConverter/ParquetArrayConverter(org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter,org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.ArrayType,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetRowConverter$$newConverter(org.apache.parquet.schema.Type,org.apache.spark.sql.types.DataType,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)#org/apache/spark/sql/types/DecimalType/json()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetRowConverter$$newConverter(org.apache.parquet.schema.Type,org.apache.spark.sql.types.DataType,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)#org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter$ParquetMapConverter/ParquetMapConverter(org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter,org.apache.parquet.schema.GroupType,org.apache.spark.sql.types.MapType,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetRowConverter$$newConverter(org.apache.parquet.schema.Type,org.apache.spark.sql.types.DataType,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)#org/apache/parquet/schema/Type/asGroupType()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetRowConverter$$newConverter(org.apache.parquet.schema.Type,org.apache.spark.sql.types.DataType,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)#org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter$ParquetStringConverter/ParquetStringConverter(org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetRowConverter$$newConverter(org.apache.parquet.schema.Type,org.apache.spark.sql.types.DataType,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetRowConverter$$newConverter(org.apache.parquet.schema.Type,org.apache.spark.sql.types.DataType,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)#org/apache/spark/sql/types/DataType/json()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetRowConverter$$newConverter(org.apache.parquet.schema.Type,org.apache.spark.sql.types.DataType,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetRowConverter$$newConverter(org.apache.parquet.schema.Type,org.apache.spark.sql.types.DataType,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)#org/apache/parquet/schema/Type/asPrimitiveType()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetRowConverter$$newConverter(org.apache.parquet.schema.Type,org.apache.spark.sql.types.DataType,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)#org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter$$anon$1/1(org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetRowConverter$$newConverter(org.apache.parquet.schema.Type,org.apache.spark.sql.types.DataType,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)#org/apache/parquet/schema/Type/isPrimitive()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetRowConverter$$newConverter(org.apache.parquet.schema.Type,org.apache.spark.sql.types.DataType,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)#org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter$ParquetLongDictionaryAwareDecimalConverter/ParquetLongDictionaryAwareDecimalConverter(org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter,int,int,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetRowConverter$$newConverter(org.apache.parquet.schema.Type,org.apache.spark.sql.types.DataType,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetRowConverter$$newConverter(org.apache.parquet.schema.Type,org.apache.spark.sql.types.DataType,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)#org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter$ParquetBinaryDictionaryAwareDecimalConverter/ParquetBinaryDictionaryAwareDecimalConverter(org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter,int,int,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetRowConverter$$newConverter(org.apache.parquet.schema.Type,org.apache.spark.sql.types.DataType,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)#org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter$$anon$6/6(org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetRowConverter$$newConverter(org.apache.parquet.schema.Type,org.apache.spark.sql.types.DataType,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetRowConverter$$newConverter(org.apache.parquet.schema.Type,org.apache.spark.sql.types.DataType,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetRowConverter$$newConverter(org.apache.parquet.schema.Type,org.apache.spark.sql.types.DataType,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)#org/apache/spark/sql/types/ArrayType/elementType()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetRowConverter$$newConverter(org.apache.parquet.schema.Type,org.apache.spark.sql.types.DataType,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)#org/apache/spark/sql/types/DecimalType/precision()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetRowConverter$$newConverter(org.apache.parquet.schema.Type,org.apache.spark.sql.types.DataType,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetRowConverter$$newConverter(org.apache.parquet.schema.Type,org.apache.spark.sql.types.DataType,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)#org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter$RepeatedGroupConverter/RepeatedGroupConverter(org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter,org.apache.parquet.schema.Type,org.apache.spark.sql.types.DataType,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetRowConverter$$newConverter(org.apache.parquet.schema.Type,org.apache.spark.sql.types.DataType,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)#java/lang/RuntimeException/RuntimeException(java.lang.String)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetRowConverter$$newConverter(org.apache.parquet.schema.Type,org.apache.spark.sql.types.DataType,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)#org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter$RepeatedPrimitiveConverter/RepeatedPrimitiveConverter(org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter,org.apache.parquet.schema.Type,org.apache.spark.sql.types.DataType,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetRowConverter$$newConverter(org.apache.parquet.schema.Type,org.apache.spark.sql.types.DataType,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)#org/apache/parquet/schema/PrimitiveType/getPrimitiveTypeName()
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetRowConverter$$newConverter(org.apache.parquet.schema.Type,org.apache.spark.sql.types.DataType,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)#org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter$$anon$2/2(org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetRowConverter$$newConverter(org.apache.parquet.schema.Type,org.apache.spark.sql.types.DataType,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)#org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter$ParquetIntDictionaryAwareDecimalConverter/ParquetIntDictionaryAwareDecimalConverter(org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter,int,int,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetRowConverter$$newConverter(org.apache.parquet.schema.Type,org.apache.spark.sql.types.DataType,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetRowConverter$$newConverter(org.apache.parquet.schema.Type,org.apache.spark.sql.types.DataType,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)#org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter$$anon$3/3(org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)
org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter/org$apache$spark$sql$execution$datasources$parquet$ParquetRowConverter$$newConverter(org.apache.parquet.schema.Type,org.apache.spark.sql.types.DataType,org.apache.spark.sql.execution.datasources.parquet.ParentContainerUpdater)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/DataSourceScanExec/class/simpleString(org.apache.spark.sql.execution.DataSourceScanExec)#org/apache/spark/util/Utils$/truncatedString$default$5()
org/apache/spark/sql/execution/DataSourceScanExec/class/simpleString(org.apache.spark.sql.execution.DataSourceScanExec)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/DataSourceScanExec/class/simpleString(org.apache.spark.sql.execution.DataSourceScanExec)#org/apache/spark/sql/execution/DataSourceScanExec$$anonfun$4/4(org.apache.spark.sql.execution.DataSourceScanExec)
org/apache/spark/sql/execution/DataSourceScanExec/class/simpleString(org.apache.spark.sql.execution.DataSourceScanExec)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/DataSourceScanExec/class/simpleString(org.apache.spark.sql.execution.DataSourceScanExec)#scala/collection/immutable/Map/toSeq()
org/apache/spark/sql/execution/DataSourceScanExec/class/simpleString(org.apache.spark.sql.execution.DataSourceScanExec)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/DataSourceScanExec/class/simpleString(org.apache.spark.sql.execution.DataSourceScanExec)#scala/collection/Seq/sorted(scala.math.Ordering)
org/apache/spark/sql/execution/DataSourceScanExec/class/simpleString(org.apache.spark.sql.execution.DataSourceScanExec)#org/apache/spark/sql/catalyst/plans/QueryPlan/output()
org/apache/spark/sql/execution/DataSourceScanExec/class/simpleString(org.apache.spark.sql.execution.DataSourceScanExec)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/DataSourceScanExec/class/simpleString(org.apache.spark.sql.execution.DataSourceScanExec)#scala/math/Ordering$/Tuple2(scala.math.Ordering,scala.math.Ordering)
org/apache/spark/sql/execution/DataSourceScanExec/class/simpleString(org.apache.spark.sql.execution.DataSourceScanExec)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/DataSourceScanExec/class/simpleString(org.apache.spark.sql.execution.DataSourceScanExec)#org/apache/spark/util/Utils$/truncatedString(scala.collection.Seq,java.lang.String,java.lang.String,java.lang.String,int)
org/apache/spark/sql/execution/DataSourceScanExec/class/simpleString(org.apache.spark.sql.execution.DataSourceScanExec)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/streaming/DataStreamWriter/anonfun/org/apache/spark/sql/streaming/DataStreamWriter/normalize/2/apply()#org/apache/spark/sql/streaming/DataStreamWriter$$anonfun$org$apache$spark$sql$streaming$DataStreamWriter$$normalize$2/apply()
org/apache/spark/sql/streaming/DataStreamWriter/anonfun/org/apache/spark/sql/streaming/DataStreamWriter/normalize/2/apply()#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/streaming/DataStreamWriter/anonfun/org/apache/spark/sql/streaming/DataStreamWriter/normalize/2/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/streaming/DataStreamWriter/anonfun/org/apache/spark/sql/streaming/DataStreamWriter/normalize/2/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/streaming/DataStreamWriter/anonfun/org/apache/spark/sql/streaming/DataStreamWriter/normalize/2/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/streaming/DataStreamWriter/anonfun/org/apache/spark/sql/streaming/DataStreamWriter/normalize/2/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/streaming/DataStreamWriter/anonfun/org/apache/spark/sql/streaming/DataStreamWriter/normalize/2/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/streaming/DataStreamWriter/anonfun/org/apache/spark/sql/streaming/DataStreamWriter/normalize/2/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/streaming/DataStreamWriter/anonfun/org/apache/spark/sql/streaming/DataStreamWriter/normalize/2/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/streaming/DataStreamWriter/anonfun/org/apache/spark/sql/streaming/DataStreamWriter/normalize/2/apply()#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/streaming/DataStreamWriter/anonfun/org/apache/spark/sql/streaming/DataStreamWriter/normalize/2/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/streaming/DataStreamWriter/anonfun/org/apache/spark/sql/streaming/DataStreamWriter/normalize/2/apply()#scala/collection/Seq/mkString(java.lang.String)
org/apache/spark/sql/streaming/DataStreamWriter/anonfun/org/apache/spark/sql/streaming/DataStreamWriter/normalize/2/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/streaming/DataStreamWriter/anonfun/org/apache/spark/sql/streaming/DataStreamWriter/normalize/2/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatSerde/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitRowFormatSerde$1/apply()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatSerde/1/apply()#org/apache/spark/sql/catalyst/parser/ParserUtils$/string(org.antlr.v4.runtime.Token)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatSerde/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$2()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatSerde/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$5()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatSerde/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy(scala.Option,scala.Option,scala.Option,scala.Option,boolean,scala.collection.immutable.Map)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatSerde/1/apply()#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatSerde/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$RowFormatSerdeContext/tablePropertyList()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatSerde/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat$/empty()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatSerde/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitRowFormatSerde$1$$anonfun$44/44(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitRowFormatSerde$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatSerde/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitRowFormatSerde$1$$anonfun$45/45(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitRowFormatSerde$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatSerde/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$1()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatSerde/1/apply()#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatSerde/1/apply()#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitRowFormatSerde/1/apply()#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/copy$default$3()
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/appendValue(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.types.DataType,java.lang.Object)#org/apache/spark/sql/types/Decimal/toUnscaledLong()
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/appendValue(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.types.DataType,java.lang.Object)#org/apache/spark/sql/types/Decimal/MAX_INT_DIGITS()
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/appendValue(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.types.DataType,java.lang.Object)#org/apache/spark/sql/types/Decimal/MAX_LONG_DIGITS()
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/appendValue(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.types.DataType,java.lang.Object)#org/apache/spark/sql/catalyst/util/DateTimeUtils/fromJavaDate(java.sql.Date)
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/appendValue(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.types.DataType,java.lang.Object)#java/lang/Boolean/booleanValue()
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/appendValue(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.types.DataType,java.lang.Object)#java/lang/StringBuilder/toString()
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/appendValue(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.types.DataType,java.lang.Object)#java/lang/Integer/intValue()
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/appendValue(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.types.DataType,java.lang.Object)#org/apache/spark/sql/types/DecimalType/scale()
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/appendValue(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.types.DataType,java.lang.Object)#org/apache/spark/sql/types/Decimal/toJavaBigDecimal()
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/appendValue(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.types.DataType,java.lang.Object)#java/lang/StringBuilder/append(java.lang.String)
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/appendValue(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.types.DataType,java.lang.Object)#java/lang/Byte/byteValue()
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/appendValue(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.types.DataType,java.lang.Object)#java/lang/Double/doubleValue()
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/appendValue(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.types.DataType,java.lang.Object)#java/lang/Long/longValue()
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/appendValue(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.types.DataType,java.lang.Object)#java/lang/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/appendValue(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.types.DataType,java.lang.Object)#java/math/BigDecimal/unscaledValue()
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/appendValue(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.types.DataType,java.lang.Object)#org/apache/spark/sql/types/DecimalType/precision()
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/appendValue(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.types.DataType,java.lang.Object)#java/lang/UnsupportedOperationException/UnsupportedOperationException(java.lang.String)
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/appendValue(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.types.DataType,java.lang.Object)#java/lang/Short/shortValue()
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/appendValue(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.types.DataType,java.lang.Object)#java/lang/Float/floatValue()
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/appendValue(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.types.DataType,java.lang.Object)#java/lang/String/getBytes(java.nio.charset.Charset)
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/appendValue(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.types.DataType,java.lang.Object)#java/math/BigInteger/toByteArray()
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/appendValue(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.types.DataType,java.lang.Object)#java/lang/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/appendValue(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.types.DataType,java.lang.Object)#org/apache/spark/sql/types/Decimal/apply(java.math.BigDecimal,int,int)
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/populate(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/catalyst/InternalRow/getUTF8String(int)
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/populate(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/types/Decimal/toUnscaledLong()
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/populate(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/types/Decimal/MAX_INT_DIGITS()
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/populate(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/catalyst/InternalRow/getBoolean(int)
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/populate(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/catalyst/InternalRow/getDouble(int)
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/populate(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.catalyst.InternalRow,int)#java/math/BigDecimal/unscaledValue()
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/populate(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/types/DecimalType/precision()
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/populate(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.catalyst.InternalRow,int)#java/math/BigInteger/toByteArray()
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/populate(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/catalyst/InternalRow/getByte(int)
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/populate(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/catalyst/InternalRow/getDecimal(int,int,int)
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/populate(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/types/DecimalType/scale()
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/populate(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/catalyst/InternalRow/getInt(int)
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/populate(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/unsafe/types/UTF8String/getBytes()
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/populate(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/types/Decimal/toJavaBigDecimal()
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/populate(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/catalyst/InternalRow/getLong(int)
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/populate(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/types/Decimal/MAX_LONG_DIGITS()
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/populate(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/catalyst/InternalRow/get(int,org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/populate(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/catalyst/InternalRow/getShort(int)
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/populate(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/catalyst/InternalRow/getFloat(int)
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/populate(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.catalyst.InternalRow,int)#org/apache/spark/sql/catalyst/InternalRow/isNullAt(int)
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/appendValue(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.types.DataType,org.apache.spark.sql.Row,int)#java/util/Iterator/hasNext()
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/appendValue(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.types.DataType,org.apache.spark.sql.Row,int)#org/apache/spark/sql/Row/getList(int)
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/appendValue(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.types.DataType,org.apache.spark.sql.Row,int)#org/apache/spark/sql/types/StructField/dataType()
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/appendValue(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.types.DataType,org.apache.spark.sql.Row,int)#java/util/List/size()
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/appendValue(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.types.DataType,org.apache.spark.sql.Row,int)#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/appendValue(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.types.DataType,org.apache.spark.sql.Row,int)#org/apache/spark/sql/types/ArrayType/elementType()
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/appendValue(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.types.DataType,org.apache.spark.sql.Row,int)#java/util/List/iterator()
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/appendValue(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.types.DataType,org.apache.spark.sql.Row,int)#org/apache/spark/sql/Row/get(int)
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/appendValue(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.types.DataType,org.apache.spark.sql.Row,int)#java/util/Iterator/next()
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/appendValue(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.types.DataType,org.apache.spark.sql.Row,int)#org/apache/spark/sql/Row/getStruct(int)
org/apache/spark/sql/execution/vectorized/ColumnVectorUtils/appendValue(org.apache.spark.sql.execution.vectorized.ColumnVector,org.apache.spark.sql.types.DataType,org.apache.spark.sql.Row,int)#org/apache/spark/sql/Row/isNullAt(int)
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/collectStatisticalData/3/apply(scala.Tuple2)#org/apache/spark/sql/execution/stat/StatFunctions$$anonfun$collectStatisticalData$3$$anonfun$apply$4/4(org.apache.spark.sql.execution.stat.StatFunctions$$anonfun$collectStatisticalData$3,java.lang.String)
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/collectStatisticalData/3/apply(scala.Tuple2)#org/apache/spark/sql/types/StructField/dataType()
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/collectStatisticalData/3/apply(scala.Tuple2)#scala/Tuple2/_2()
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/collectStatisticalData/3/apply(scala.Tuple2)#scala/Option/nonEmpty()
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/collectStatisticalData/3/apply(scala.Tuple2)#scala/Predef$/require(boolean,scala.Function0)
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/collectStatisticalData/3/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/collectStatisticalData/3/apply(scala.Tuple2)#scala/Option/get()
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/collectStatisticalData/3/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/stat/StatFunctions/anonfun/collectStatisticalData/3/apply(scala.Tuple2)#org/apache/spark/sql/execution/stat/StatFunctions$$anonfun$collectStatisticalData$3$$anonfun$apply$5/5(org.apache.spark.sql.execution.stat.StatFunctions$$anonfun$collectStatisticalData$3,scala.Option)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/org$apache$spark$sql$execution$streaming$FlatMapGroupsWithStateExec$StateStoreUpdater$$callFunctionAndUpdateState(org.apache.spark.sql.catalyst.expressions.UnsafeRow,scala.collection.Iterator,scala.Option,boolean)#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/org$apache$spark$sql$execution$streaming$FlatMapGroupsWithStateExec$StateStoreUpdater$$callFunctionAndUpdateState(org.apache.spark.sql.catalyst.expressions.UnsafeRow,scala.collection.Iterator,scala.Option,boolean)#org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec$StateStoreUpdater/getKeyObj()
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/org$apache$spark$sql$execution$streaming$FlatMapGroupsWithStateExec$StateStoreUpdater$$callFunctionAndUpdateState(org.apache.spark.sql.catalyst.expressions.UnsafeRow,scala.collection.Iterator,scala.Option,boolean)#org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec$StateStoreUpdater/org$apache$spark$sql$execution$streaming$FlatMapGroupsWithStateExec$StateStoreUpdater$$$outer()
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/org$apache$spark$sql$execution$streaming$FlatMapGroupsWithStateExec$StateStoreUpdater$$callFunctionAndUpdateState(org.apache.spark.sql.catalyst.expressions.UnsafeRow,scala.collection.Iterator,scala.Option,boolean)#org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec$StateStoreUpdater$$anonfun$7/7(org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExec$StateStoreUpdater)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/org$apache$spark$sql$execution$streaming$FlatMapGroupsWithStateExec$StateStoreUpdater$$callFunctionAndUpdateState(org.apache.spark.sql.catalyst.expressions.UnsafeRow,scala.collection.Iterator,scala.Option,boolean)#org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec$StateStoreUpdater$$anonfun$6/6(org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExec$StateStoreUpdater)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/org$apache$spark$sql$execution$streaming$FlatMapGroupsWithStateExec$StateStoreUpdater$$callFunctionAndUpdateState(org.apache.spark.sql.catalyst.expressions.UnsafeRow,scala.collection.Iterator,scala.Option,boolean)#org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec$StateStoreUpdater$$anonfun$org$apache$spark$sql$execution$streaming$FlatMapGroupsWithStateExec$StateStoreUpdater$$callFunctionAndUpdateState$1/1(org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExec$StateStoreUpdater,org.apache.spark.sql.catalyst.expressions.UnsafeRow,scala.Option,org.apache.spark.sql.execution.streaming.GroupStateImpl)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/org$apache$spark$sql$execution$streaming$FlatMapGroupsWithStateExec$StateStoreUpdater$$callFunctionAndUpdateState(org.apache.spark.sql.catalyst.expressions.UnsafeRow,scala.collection.Iterator,scala.Option,boolean)#org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec$StateStoreUpdater$$anonfun$1/1(org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExec$StateStoreUpdater)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/org$apache$spark$sql$execution$streaming$FlatMapGroupsWithStateExec$StateStoreUpdater$$callFunctionAndUpdateState(org.apache.spark.sql.catalyst.expressions.UnsafeRow,scala.collection.Iterator,scala.Option,boolean)#scala/collection/Iterator/map(scala.Function1)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/org$apache$spark$sql$execution$streaming$FlatMapGroupsWithStateExec$StateStoreUpdater$$callFunctionAndUpdateState(org.apache.spark.sql.catalyst.expressions.UnsafeRow,scala.collection.Iterator,scala.Option,boolean)#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/org$apache$spark$sql$execution$streaming$FlatMapGroupsWithStateExec$StateStoreUpdater$$callFunctionAndUpdateState(org.apache.spark.sql.catalyst.expressions.UnsafeRow,scala.collection.Iterator,scala.Option,boolean)#scala/Function3/apply(java.lang.Object,java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/org$apache$spark$sql$execution$streaming$FlatMapGroupsWithStateExec$StateStoreUpdater$$callFunctionAndUpdateState(org.apache.spark.sql.catalyst.expressions.UnsafeRow,scala.collection.Iterator,scala.Option,boolean)#org/apache/spark/util/CompletionIterator$/apply(scala.collection.Iterator,scala.Function0)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/org$apache$spark$sql$execution$streaming$FlatMapGroupsWithStateExec$StateStoreUpdater$$callFunctionAndUpdateState(org.apache.spark.sql.catalyst.expressions.UnsafeRow,scala.collection.Iterator,scala.Option,boolean)#org/apache/spark/sql/execution/streaming/GroupStateImpl$/createForStreaming(scala.Option,long,long,org.apache.spark.sql.streaming.GroupStateTimeout,boolean)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/org$apache$spark$sql$execution$streaming$FlatMapGroupsWithStateExec$StateStoreUpdater$$callFunctionAndUpdateState(org.apache.spark.sql.catalyst.expressions.UnsafeRow,scala.collection.Iterator,scala.Option,boolean)#org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec$StateStoreUpdater/getStateObj(scala.Option)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/org$apache$spark$sql$execution$streaming$FlatMapGroupsWithStateExec$StateStoreUpdater$$callFunctionAndUpdateState(org.apache.spark.sql.catalyst.expressions.UnsafeRow,scala.collection.Iterator,scala.Option,boolean)#org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec$StateStoreUpdater$$anonfun$2/2(org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExec$StateStoreUpdater)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/org$apache$spark$sql$execution$streaming$FlatMapGroupsWithStateExec$StateStoreUpdater$$callFunctionAndUpdateState(org.apache.spark.sql.catalyst.expressions.UnsafeRow,scala.collection.Iterator,scala.Option,boolean)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/org$apache$spark$sql$execution$streaming$FlatMapGroupsWithStateExec$StateStoreUpdater$$onIteratorCompletion$1(org.apache.spark.sql.catalyst.expressions.UnsafeRow,scala.Option,org.apache.spark.sql.execution.streaming.GroupStateImpl)#java/lang/IllegalStateException/IllegalStateException(java.lang.String)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/org$apache$spark$sql$execution$streaming$FlatMapGroupsWithStateExec$StateStoreUpdater$$onIteratorCompletion$1(org.apache.spark.sql.catalyst.expressions.UnsafeRow,scala.Option,org.apache.spark.sql.execution.streaming.GroupStateImpl)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/org$apache$spark$sql$execution$streaming$FlatMapGroupsWithStateExec$StateStoreUpdater$$onIteratorCompletion$1(org.apache.spark.sql.catalyst.expressions.UnsafeRow,scala.Option,org.apache.spark.sql.execution.streaming.GroupStateImpl)#org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec$StateStoreUpdater/numUpdatedStateRows()
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/org$apache$spark$sql$execution$streaming$FlatMapGroupsWithStateExec$StateStoreUpdater$$onIteratorCompletion$1(org.apache.spark.sql.catalyst.expressions.UnsafeRow,scala.Option,org.apache.spark.sql.execution.streaming.GroupStateImpl)#org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec$StateStoreUpdater/getTimeoutTimestamp(org.apache.spark.sql.catalyst.expressions.UnsafeRow)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/org$apache$spark$sql$execution$streaming$FlatMapGroupsWithStateExec$StateStoreUpdater$$onIteratorCompletion$1(org.apache.spark.sql.catalyst.expressions.UnsafeRow,scala.Option,org.apache.spark.sql.execution.streaming.GroupStateImpl)#org/apache/spark/sql/execution/streaming/GroupStateImpl$/NO_TIMESTAMP()
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/org$apache$spark$sql$execution$streaming$FlatMapGroupsWithStateExec$StateStoreUpdater$$onIteratorCompletion$1(org.apache.spark.sql.catalyst.expressions.UnsafeRow,scala.Option,org.apache.spark.sql.execution.streaming.GroupStateImpl)#scala/Some/x()
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/org$apache$spark$sql$execution$streaming$FlatMapGroupsWithStateExec$StateStoreUpdater$$onIteratorCompletion$1(org.apache.spark.sql.catalyst.expressions.UnsafeRow,scala.Option,org.apache.spark.sql.execution.streaming.GroupStateImpl)#org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec$StateStoreUpdater/getStateRow(java.lang.Object)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/org$apache$spark$sql$execution$streaming$FlatMapGroupsWithStateExec$StateStoreUpdater$$onIteratorCompletion$1(org.apache.spark.sql.catalyst.expressions.UnsafeRow,scala.Option,org.apache.spark.sql.execution.streaming.GroupStateImpl)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/copy()
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/org$apache$spark$sql$execution$streaming$FlatMapGroupsWithStateExec$StateStoreUpdater$$onIteratorCompletion$1(org.apache.spark.sql.catalyst.expressions.UnsafeRow,scala.Option,org.apache.spark.sql.execution.streaming.GroupStateImpl)#scala/Option/orNull(scala.Predef$$less$colon$less)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/org$apache$spark$sql$execution$streaming$FlatMapGroupsWithStateExec$StateStoreUpdater$$onIteratorCompletion$1(org.apache.spark.sql.catalyst.expressions.UnsafeRow,scala.Option,org.apache.spark.sql.execution.streaming.GroupStateImpl)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/org$apache$spark$sql$execution$streaming$FlatMapGroupsWithStateExec$StateStoreUpdater$$onIteratorCompletion$1(org.apache.spark.sql.catalyst.expressions.UnsafeRow,scala.Option,org.apache.spark.sql.execution.streaming.GroupStateImpl)#org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec$StateStoreUpdater/setTimeoutTimestamp(org.apache.spark.sql.catalyst.expressions.UnsafeRow,long)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/org$apache$spark$sql$execution$streaming$FlatMapGroupsWithStateExec$StateStoreUpdater$$onIteratorCompletion$1(org.apache.spark.sql.catalyst.expressions.UnsafeRow,scala.Option,org.apache.spark.sql.execution.streaming.GroupStateImpl)#scala/Predef$/$conforms()
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/updateStateForTimedOutKeys()#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/updateStateForTimedOutKeys()#java/lang/IllegalStateException/IllegalStateException(java.lang.String)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/updateStateForTimedOutKeys()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/updateStateForTimedOutKeys()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/updateStateForTimedOutKeys()#scala/package$/Iterator()
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/updateStateForTimedOutKeys()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/updateStateForTimedOutKeys()#org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec$StateStoreUpdater$$anonfun$updateStateForTimedOutKeys$1/1(org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExec$StateStoreUpdater)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/updateStateForTimedOutKeys()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/updateStateForTimedOutKeys()#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/updateStateForTimedOutKeys()#org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec$StateStoreUpdater/org$apache$spark$sql$execution$streaming$FlatMapGroupsWithStateExec$StateStoreUpdater$$$outer()
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/updateStateForTimedOutKeys()#scala/collection/Iterator$/empty()
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/updateStateForTimedOutKeys()#scala/collection/Iterator/flatMap(scala.Function1)
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/updateStateForTimedOutKeys()#scala/Option/get()
org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec/StateStoreUpdater/updateStateForTimedOutKeys()#org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec$StateStoreUpdater$$anonfun$5/5(org.apache.spark.sql.execution.streaming.FlatMapGroupsWithStateExec$StateStoreUpdater,long)
org/apache/spark/sql/execution/streaming/FileStreamSink/hasMetadata(scala.collection.Seq,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/streaming/FileStreamSink/hasMetadata(scala.collection.Seq,org.apache.hadoop.conf.Configuration)#scala/util/control/NonFatal$/unapply(java.lang.Throwable)
org/apache/spark/sql/execution/streaming/FileStreamSink/hasMetadata(scala.collection.Seq,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/fs/Path/Path(org.apache.hadoop.fs.Path,java.lang.String)
org/apache/spark/sql/execution/streaming/FileStreamSink/hasMetadata(scala.collection.Seq,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/fs/FileSystem/exists(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/streaming/FileStreamSink/hasMetadata(scala.collection.Seq,org.apache.hadoop.conf.Configuration)#scala/Some/isEmpty()
org/apache/spark/sql/execution/streaming/FileStreamSink/hasMetadata(scala.collection.Seq,org.apache.hadoop.conf.Configuration)#scala/collection/Seq$/unapplySeq(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/FileStreamSink/hasMetadata(scala.collection.Seq,org.apache.hadoop.conf.Configuration)#scala/collection/SeqLike/apply(int)
org/apache/spark/sql/execution/streaming/FileStreamSink/hasMetadata(scala.collection.Seq,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/streaming/FileStreamSink$/metadataDir()
org/apache/spark/sql/execution/streaming/FileStreamSink/hasMetadata(scala.collection.Seq,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/streaming/FileStreamSink$/hasMetadata(scala.collection.Seq,org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/streaming/FileStreamSink/hasMetadata(scala.collection.Seq,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/streaming/FileStreamSink$/logWarning(scala.Function0)
org/apache/spark/sql/execution/streaming/FileStreamSink/hasMetadata(scala.collection.Seq,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/fs/Path/Path(java.lang.String)
org/apache/spark/sql/execution/streaming/FileStreamSink/hasMetadata(scala.collection.Seq,org.apache.hadoop.conf.Configuration)#scala/Some/get()
org/apache/spark/sql/execution/streaming/FileStreamSink/hasMetadata(scala.collection.Seq,org.apache.hadoop.conf.Configuration)#scala/collection/SeqLike/lengthCompare(int)
org/apache/spark/sql/execution/streaming/FileStreamSink/hasMetadata(scala.collection.Seq,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/streaming/FileStreamSink$$anonfun$hasMetadata$1/1()
org/apache/spark/sql/execution/streaming/FileStreamSink/hasMetadata(scala.collection.Seq,org.apache.hadoop.conf.Configuration)#scala/Option/isEmpty()
org/apache/spark/sql/execution/streaming/FileStreamSink/ancestorIsMetadataDirectory(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/fs/Path/getParent()
org/apache/spark/sql/execution/streaming/FileStreamSink/ancestorIsMetadataDirectory(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/fs/Path/getName()
org/apache/spark/sql/execution/streaming/FileStreamSink/ancestorIsMetadataDirectory(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/streaming/FileStreamSink/ancestorIsMetadataDirectory(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/streaming/FileStreamSink/ancestorIsMetadataDirectory(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/fs/FileSystem/getUri()
org/apache/spark/sql/execution/streaming/FileStreamSink/ancestorIsMetadataDirectory(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/streaming/FileStreamSink$/metadataDir()
org/apache/spark/sql/execution/streaming/FileStreamSink/ancestorIsMetadataDirectory(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/streaming/FileStreamSink$/ancestorIsMetadataDirectory(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/streaming/FileStreamSink/ancestorIsMetadataDirectory(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/fs/Path/makeQualified(java.net.URI,org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/streaming/FileStreamSink/ancestorIsMetadataDirectory(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/fs/FileSystem/getWorkingDirectory()
org/apache/spark/sql/execution/streaming/FileStreamSink/addBatch(long,org.apache.spark.sql.Dataset)#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/streaming/FileStreamSink/addBatch(long,org.apache.spark.sql.Dataset)#org/apache/spark/internal/io/FileCommitProtocol$/instantiate(java.lang.String,java.lang.String,java.lang.String,boolean)
org/apache/spark/sql/execution/streaming/FileStreamSink/addBatch(long,org.apache.spark.sql.Dataset)#scala/collection/immutable/Map$/empty()
org/apache/spark/sql/execution/streaming/FileStreamSink/addBatch(long,org.apache.spark.sql.Dataset)#org/apache/spark/sql/execution/streaming/FileStreamSink$$anonfun$addBatch$4/4(org.apache.spark.sql.execution.streaming.FileStreamSink)
org/apache/spark/sql/execution/streaming/FileStreamSink/addBatch(long,org.apache.spark.sql.Dataset)#org/apache/spark/sql/execution/datasources/FileFormatWriter$/write(org.apache.spark.sql.SparkSession,org.apache.spark.sql.execution.QueryExecution,org.apache.spark.sql.execution.datasources.FileFormat,org.apache.spark.internal.io.FileCommitProtocol,org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec,org.apache.hadoop.conf.Configuration,scala.collection.Seq,scala.Option,scala.Function1,scala.collection.immutable.Map)
org/apache/spark/sql/execution/streaming/FileStreamSink/addBatch(long,org.apache.spark.sql.Dataset)#org/apache/spark/sql/execution/streaming/FileStreamSink$$anonfun$addBatch$2/2(org.apache.spark.sql.execution.streaming.FileStreamSink)
org/apache/spark/sql/execution/streaming/FileStreamSink/addBatch(long,org.apache.spark.sql.Dataset)#scala/Predef$/Map()
org/apache/spark/sql/execution/streaming/FileStreamSink/addBatch(long,org.apache.spark.sql.Dataset)#java/lang/Object/toString()
org/apache/spark/sql/execution/streaming/FileStreamSink/addBatch(long,org.apache.spark.sql.Dataset)#org/apache/spark/sql/execution/streaming/FileStreamSink$$anonfun$addBatch$1/1(org.apache.spark.sql.execution.streaming.FileStreamSink)
org/apache/spark/sql/execution/streaming/FileStreamSink/addBatch(long,org.apache.spark.sql.Dataset)#org/apache/spark/sql/internal/SQLConf/streamingFileCommitProtocolClass()
org/apache/spark/sql/execution/streaming/FileStreamSink/addBatch(long,org.apache.spark.sql.Dataset)#org/apache/spark/sql/execution/streaming/FileStreamSink$$anonfun$addBatch$3/3(org.apache.spark.sql.execution.streaming.FileStreamSink,long)
org/apache/spark/sql/execution/streaming/FileStreamSink/addBatch(long,org.apache.spark.sql.Dataset)#org/apache/spark/sql/execution/datasources/FileFormatWriter$OutputSpec/OutputSpec(java.lang.String,scala.collection.immutable.Map)
org/apache/spark/sql/execution/streaming/FileStreamSink/addBatch(long,org.apache.spark.sql.Dataset)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/streaming/FileStreamSink/addBatch(long,org.apache.spark.sql.Dataset)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/streaming/FileStreamSink/addBatch(long,org.apache.spark.sql.Dataset)#org/apache/spark/sql/execution/streaming/FileStreamSinkLog/getLatest()
org/apache/spark/sql/execution/streaming/FileStreamSink/addBatch(long,org.apache.spark.sql.Dataset)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/exchange/ShuffleExchange/prepareShuffleDependency(org.apache.spark.rdd.RDD,scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning,org.apache.spark.serializer.Serializer)#scala/reflect/ClassTag$/Int()
org/apache/spark/sql/execution/exchange/ShuffleExchange/prepareShuffleDependency(org.apache.spark.rdd.RDD,scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning,org.apache.spark.serializer.Serializer)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/exchange/ShuffleExchange/prepareShuffleDependency(org.apache.spark.rdd.RDD,scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning,org.apache.spark.serializer.Serializer)#org/apache/spark/ShuffleDependency$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/exchange/ShuffleExchange/prepareShuffleDependency(org.apache.spark.rdd.RDD,scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning,org.apache.spark.serializer.Serializer)#org/apache/spark/ShuffleDependency$/$lessinit$greater$default$6()
org/apache/spark/sql/execution/exchange/ShuffleExchange/prepareShuffleDependency(org.apache.spark.rdd.RDD,scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning,org.apache.spark.serializer.Serializer)#org/apache/spark/sql/catalyst/plans/physical/HashPartitioning/numPartitions()
org/apache/spark/sql/execution/exchange/ShuffleExchange/prepareShuffleDependency(org.apache.spark.rdd.RDD,scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning,org.apache.spark.serializer.Serializer)#org/apache/spark/sql/catalyst/plans/physical/RoundRobinPartitioning/numPartitions()
org/apache/spark/sql/execution/exchange/ShuffleExchange/prepareShuffleDependency(org.apache.spark.rdd.RDD,scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning,org.apache.spark.serializer.Serializer)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/exchange/ShuffleExchange/prepareShuffleDependency(org.apache.spark.rdd.RDD,scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning,org.apache.spark.serializer.Serializer)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/exchange/ShuffleExchange/prepareShuffleDependency(org.apache.spark.rdd.RDD,scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning,org.apache.spark.serializer.Serializer)#org/apache/spark/rdd/RDD/mapPartitionsInternal(scala.Function1,boolean,scala.reflect.ClassTag)
org/apache/spark/sql/execution/exchange/ShuffleExchange/prepareShuffleDependency(org.apache.spark.rdd.RDD,scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning,org.apache.spark.serializer.Serializer)#org/apache/spark/sql/execution/exchange/ShuffleExchange$/needToCopyObjectsBeforeShuffle(org.apache.spark.Partitioner,org.apache.spark.serializer.Serializer)
org/apache/spark/sql/execution/exchange/ShuffleExchange/prepareShuffleDependency(org.apache.spark.rdd.RDD,scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning,org.apache.spark.serializer.Serializer)#org/apache/spark/rdd/RDD/mapPartitionsInternal$default$2()
org/apache/spark/sql/execution/exchange/ShuffleExchange/prepareShuffleDependency(org.apache.spark.rdd.RDD,scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning,org.apache.spark.serializer.Serializer)#org/apache/spark/ShuffleDependency$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/exchange/ShuffleExchange/prepareShuffleDependency(org.apache.spark.rdd.RDD,scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning,org.apache.spark.serializer.Serializer)#org/apache/spark/sql/catalyst/plans/physical/RangePartitioning/numPartitions()
org/apache/spark/sql/execution/exchange/ShuffleExchange/prepareShuffleDependency(org.apache.spark.rdd.RDD,scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning,org.apache.spark.serializer.Serializer)#org/apache/spark/HashPartitioner/HashPartitioner(int)
org/apache/spark/sql/execution/exchange/ShuffleExchange/prepareShuffleDependency(org.apache.spark.rdd.RDD,scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning,org.apache.spark.serializer.Serializer)#org/apache/spark/sql/execution/exchange/ShuffleExchange$$anonfun$3/3(scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning,org.apache.spark.Partitioner)
org/apache/spark/sql/execution/exchange/ShuffleExchange/prepareShuffleDependency(org.apache.spark.rdd.RDD,scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning,org.apache.spark.serializer.Serializer)#org/apache/spark/Partitioner/numPartitions()
org/apache/spark/sql/execution/exchange/ShuffleExchange/prepareShuffleDependency(org.apache.spark.rdd.RDD,scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning,org.apache.spark.serializer.Serializer)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/exchange/ShuffleExchange/prepareShuffleDependency(org.apache.spark.rdd.RDD,scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning,org.apache.spark.serializer.Serializer)#org/apache/spark/sql/execution/exchange/ShuffleExchange$$anonfun$2/2(scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning,org.apache.spark.Partitioner)
org/apache/spark/sql/execution/exchange/ShuffleExchange/prepareShuffleDependency(org.apache.spark.rdd.RDD,scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning,org.apache.spark.serializer.Serializer)#org/apache/spark/sql/catalyst/plans/physical/RangePartitioning/ordering()
org/apache/spark/sql/execution/exchange/ShuffleExchange/prepareShuffleDependency(org.apache.spark.rdd.RDD,scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning,org.apache.spark.serializer.Serializer)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/exchange/ShuffleExchange/prepareShuffleDependency(org.apache.spark.rdd.RDD,scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning,org.apache.spark.serializer.Serializer)#org/apache/spark/sql/execution/exchange/ShuffleExchange$$anon$2/2()
org/apache/spark/sql/execution/exchange/ShuffleExchange/prepareShuffleDependency(org.apache.spark.rdd.RDD,scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning,org.apache.spark.serializer.Serializer)#org/apache/spark/RangePartitioner/RangePartitioner(int,org.apache.spark.rdd.RDD,boolean,scala.math.Ordering,scala.reflect.ClassTag)
org/apache/spark/sql/execution/exchange/ShuffleExchange/prepareShuffleDependency(org.apache.spark.rdd.RDD,scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning,org.apache.spark.serializer.Serializer)#org/apache/spark/sql/execution/exchange/ShuffleExchange$$anonfun$1/1()
org/apache/spark/sql/execution/exchange/ShuffleExchange/prepareShuffleDependency(org.apache.spark.rdd.RDD,scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning,org.apache.spark.serializer.Serializer)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/exchange/ShuffleExchange/prepareShuffleDependency(org.apache.spark.rdd.RDD,scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning,org.apache.spark.serializer.Serializer)#scala/sys/package$/error(java.lang.String)
org/apache/spark/sql/execution/exchange/ShuffleExchange/prepareShuffleDependency(org.apache.spark.rdd.RDD,scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning,org.apache.spark.serializer.Serializer)#org/apache/spark/sql/execution/exchange/ShuffleExchange$$anon$1/1(int)
org/apache/spark/sql/execution/exchange/ShuffleExchange/prepareShuffleDependency(org.apache.spark.rdd.RDD,scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning,org.apache.spark.serializer.Serializer)#org/apache/spark/sql/catalyst/expressions/codegen/LazilyGeneratedOrdering/LazilyGeneratedOrdering(scala.collection.Seq,scala.collection.Seq)
org/apache/spark/sql/execution/exchange/ShuffleExchange/prepareShuffleDependency(org.apache.spark.rdd.RDD,scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning,org.apache.spark.serializer.Serializer)#org/apache/spark/ShuffleDependency/ShuffleDependency(org.apache.spark.rdd.RDD,org.apache.spark.Partitioner,org.apache.spark.serializer.Serializer,scala.Option,scala.Option,boolean,scala.reflect.ClassTag,scala.reflect.ClassTag,scala.reflect.ClassTag)
org/apache/spark/sql/execution/exchange/ShuffleExchange/org$apache$spark$sql$execution$exchange$ShuffleExchange$$getPartitionKeyExtractor$1(scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning)#org/apache/spark/sql/catalyst/expressions/UnsafeProjection$/create(scala.collection.Seq,scala.collection.Seq)
org/apache/spark/sql/execution/exchange/ShuffleExchange/org$apache$spark$sql$execution$exchange$ShuffleExchange$$getPartitionKeyExtractor$1(scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning)#org/apache/spark/TaskContext$/get()
org/apache/spark/sql/execution/exchange/ShuffleExchange/org$apache$spark$sql$execution$exchange$ShuffleExchange$$getPartitionKeyExtractor$1(scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/exchange/ShuffleExchange/org$apache$spark$sql$execution$exchange$ShuffleExchange$$getPartitionKeyExtractor$1(scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/exchange/ShuffleExchange/org$apache$spark$sql$execution$exchange$ShuffleExchange$$getPartitionKeyExtractor$1(scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning)#org/apache/spark/TaskContext/partitionId()
org/apache/spark/sql/execution/exchange/ShuffleExchange/org$apache$spark$sql$execution$exchange$ShuffleExchange$$getPartitionKeyExtractor$1(scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning)#org/apache/spark/sql/execution/exchange/ShuffleExchange$$anonfun$org$apache$spark$sql$execution$exchange$ShuffleExchange$$getPartitionKeyExtractor$1$3/3()
org/apache/spark/sql/execution/exchange/ShuffleExchange/org$apache$spark$sql$execution$exchange$ShuffleExchange$$getPartitionKeyExtractor$1(scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning)#scala/runtime/IntRef/create(int)
org/apache/spark/sql/execution/exchange/ShuffleExchange/org$apache$spark$sql$execution$exchange$ShuffleExchange$$getPartitionKeyExtractor$1(scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/exchange/ShuffleExchange/org$apache$spark$sql$execution$exchange$ShuffleExchange$$getPartitionKeyExtractor$1(scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning)#scala/sys/package$/error(java.lang.String)
org/apache/spark/sql/execution/exchange/ShuffleExchange/org$apache$spark$sql$execution$exchange$ShuffleExchange$$getPartitionKeyExtractor$1(scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning)#org/apache/spark/sql/execution/exchange/ShuffleExchange$$anonfun$org$apache$spark$sql$execution$exchange$ShuffleExchange$$getPartitionKeyExtractor$1$2/2(org.apache.spark.sql.catalyst.expressions.UnsafeProjection)
org/apache/spark/sql/execution/exchange/ShuffleExchange/org$apache$spark$sql$execution$exchange$ShuffleExchange$$getPartitionKeyExtractor$1(scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/exchange/ShuffleExchange/org$apache$spark$sql$execution$exchange$ShuffleExchange$$getPartitionKeyExtractor$1(scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/exchange/ShuffleExchange/org$apache$spark$sql$execution$exchange$ShuffleExchange$$getPartitionKeyExtractor$1(scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning)#scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)
org/apache/spark/sql/execution/exchange/ShuffleExchange/org$apache$spark$sql$execution$exchange$ShuffleExchange$$getPartitionKeyExtractor$1(scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning)#org/apache/spark/sql/execution/exchange/ShuffleExchange$$anonfun$org$apache$spark$sql$execution$exchange$ShuffleExchange$$getPartitionKeyExtractor$1$1/1(scala.runtime.IntRef)
org/apache/spark/sql/execution/exchange/ShuffleExchange/org$apache$spark$sql$execution$exchange$ShuffleExchange$$getPartitionKeyExtractor$1(scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning)#java/util/Random/nextInt(int)
org/apache/spark/sql/execution/exchange/ShuffleExchange/org$apache$spark$sql$execution$exchange$ShuffleExchange$$getPartitionKeyExtractor$1(scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning)#java/util/Random/Random(long)
org/apache/spark/sql/execution/exchange/ShuffleExchange/org$apache$spark$sql$execution$exchange$ShuffleExchange$$getPartitionKeyExtractor$1(scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning)#org/apache/spark/sql/catalyst/plans/physical/RoundRobinPartitioning/numPartitions()
org/apache/spark/sql/execution/exchange/ShuffleExchange/org$apache$spark$sql$execution$exchange$ShuffleExchange$$getPartitionKeyExtractor$1(scala.collection.Seq,org.apache.spark.sql.catalyst.plans.physical.Partitioning)#org/apache/spark/sql/catalyst/plans/physical/HashPartitioning/partitionIdExpression()
org/apache/spark/sql/functions/udf(scala.Function4,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/runtime/package$/universe()
org/apache/spark/sql/functions/udf(scala.Function4,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$/udf(scala.Function4,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function4,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try/toOption()
org/apache/spark/sql/functions/udf(scala.Function4,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$$anonfun$7/7(scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function4,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/api/TypeTags/typeTag(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function4,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function4,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)
org/apache/spark/sql/functions/udf(scala.Function4,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()
org/apache/spark/sql/functions/udf(scala.Function4,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try$/apply(scala.Function0)
org/apache/spark/sql/functions/udf(scala.Function1,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/runtime/package$/universe()
org/apache/spark/sql/functions/udf(scala.Function1,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try/toOption()
org/apache/spark/sql/functions/udf(scala.Function1,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/api/TypeTags/typeTag(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function1,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function1,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$$anonfun$4/4(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function1,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)
org/apache/spark/sql/functions/udf(scala.Function1,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$/udf(scala.Function1,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function1,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()
org/apache/spark/sql/functions/udf(scala.Function1,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try$/apply(scala.Function0)
org/apache/spark/sql/functions/udf(scala.Function6,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/runtime/package$/universe()
org/apache/spark/sql/functions/udf(scala.Function6,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$$anonfun$9/9(scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function6,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/api/TypeTags/typeTag(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function6,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function6,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)
org/apache/spark/sql/functions/udf(scala.Function6,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$/udf(scala.Function6,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function6,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try/toOption()
org/apache/spark/sql/functions/udf(scala.Function6,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()
org/apache/spark/sql/functions/udf(scala.Function6,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try$/apply(scala.Function0)
org/apache/spark/sql/functions/json_tuple(org.apache.spark.sql.Column,scala.collection.Seq)#org/apache/spark/sql/functions$/json_tuple(org.apache.spark.sql.Column,scala.collection.Seq)
org/apache/spark/sql/functions/json_tuple(org.apache.spark.sql.Column,scala.collection.Seq)#org/apache/spark/sql/functions$$anonfun$json_tuple$2/2()
org/apache/spark/sql/functions/json_tuple(org.apache.spark.sql.Column,scala.collection.Seq)#scala/Predef$/require(boolean,scala.Function0)
org/apache/spark/sql/functions/json_tuple(org.apache.spark.sql.Column,scala.collection.Seq)#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/functions/json_tuple(org.apache.spark.sql.Column,scala.collection.Seq)#org/apache/spark/sql/functions$/withExpr(org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/functions/json_tuple(org.apache.spark.sql.Column,scala.collection.Seq)#scala/collection/SeqLike/$plus$colon(java.lang.Object,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/functions/json_tuple(org.apache.spark.sql.Column,scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/functions/json_tuple(org.apache.spark.sql.Column,scala.collection.Seq)#org/apache/spark/sql/functions$$anonfun$json_tuple$1/1()
org/apache/spark/sql/functions/json_tuple(org.apache.spark.sql.Column,scala.collection.Seq)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/functions/json_tuple(org.apache.spark.sql.Column,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/JsonTuple/JsonTuple(scala.collection.Seq)
org/apache/spark/sql/functions/udf(scala.Function8,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/runtime/package$/universe()
org/apache/spark/sql/functions/udf(scala.Function8,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try/toOption()
org/apache/spark/sql/functions/udf(scala.Function8,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$/udf(scala.Function8,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function8,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/api/TypeTags/typeTag(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function8,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function8,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$$anonfun$11/11(scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function8,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)
org/apache/spark/sql/functions/udf(scala.Function8,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()
org/apache/spark/sql/functions/udf(scala.Function8,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try$/apply(scala.Function0)
org/apache/spark/sql/functions/sha2(org.apache.spark.sql.Column,int)#org/apache/spark/sql/functions$/lit(java.lang.Object)
org/apache/spark/sql/functions/sha2(org.apache.spark.sql.Column,int)#org/apache/spark/sql/functions$$anonfun$sha2$1/1(int)
org/apache/spark/sql/functions/sha2(org.apache.spark.sql.Column,int)#scala/Predef$/require(boolean,scala.Function0)
org/apache/spark/sql/functions/sha2(org.apache.spark.sql.Column,int)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/functions/sha2(org.apache.spark.sql.Column,int)#org/apache/spark/sql/catalyst/expressions/Sha2/Sha2(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/functions/sha2(org.apache.spark.sql.Column,int)#org/apache/spark/sql/functions$/withExpr(org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/functions/sha2(org.apache.spark.sql.Column,int)#org/apache/spark/sql/functions$/sha2(org.apache.spark.sql.Column,int)
org/apache/spark/sql/functions/sha2(org.apache.spark.sql.Column,int)#scala/collection/SeqLike/contains(java.lang.Object)
org/apache/spark/sql/functions/sha2(org.apache.spark.sql.Column,int)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/functions/sha2(org.apache.spark.sql.Column,int)#scala/Predef$/wrapIntArray(int[])
org/apache/spark/sql/functions/udf(scala.Function10,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/runtime/package$/universe()
org/apache/spark/sql/functions/udf(scala.Function10,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try/toOption()
org/apache/spark/sql/functions/udf(scala.Function10,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$/udf(scala.Function10,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function10,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/api/TypeTags/typeTag(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function10,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function10,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$$anonfun$13/13(scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function10,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)
org/apache/spark/sql/functions/udf(scala.Function10,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()
org/apache/spark/sql/functions/udf(scala.Function10,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try$/apply(scala.Function0)
org/apache/spark/sql/functions/least(scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/functions/least(scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/Least/Least(scala.collection.Seq)
org/apache/spark/sql/functions/least(scala.collection.Seq)#scala/Predef$/require(boolean,scala.Function0)
org/apache/spark/sql/functions/least(scala.collection.Seq)#org/apache/spark/sql/functions$/least(scala.collection.Seq)
org/apache/spark/sql/functions/least(scala.collection.Seq)#org/apache/spark/sql/functions$$anonfun$least$1/1()
org/apache/spark/sql/functions/least(scala.collection.Seq)#org/apache/spark/sql/functions$/withExpr(org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/functions/least(scala.collection.Seq)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/functions/least(scala.collection.Seq)#scala/collection/Seq/length()
org/apache/spark/sql/functions/least(scala.collection.Seq)#org/apache/spark/sql/functions$$anonfun$least$2/2()
org/apache/spark/sql/functions/udf(scala.Function9,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/runtime/package$/universe()
org/apache/spark/sql/functions/udf(scala.Function9,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try/toOption()
org/apache/spark/sql/functions/udf(scala.Function9,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/api/TypeTags/typeTag(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function9,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$$anonfun$12/12(scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function9,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)
org/apache/spark/sql/functions/udf(scala.Function9,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()
org/apache/spark/sql/functions/udf(scala.Function9,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try$/apply(scala.Function0)
org/apache/spark/sql/functions/udf(scala.Function9,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$/udf(scala.Function9,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function9,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function7,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/runtime/package$/universe()
org/apache/spark/sql/functions/udf(scala.Function7,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try/toOption()
org/apache/spark/sql/functions/udf(scala.Function7,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/api/TypeTags/typeTag(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function7,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$$anonfun$10/10(scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function7,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function7,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$/udf(scala.Function7,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function7,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)
org/apache/spark/sql/functions/udf(scala.Function7,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()
org/apache/spark/sql/functions/udf(scala.Function7,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try$/apply(scala.Function0)
org/apache/spark/sql/functions/greatest(scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/functions/greatest(scala.collection.Seq)#org/apache/spark/sql/functions$$anonfun$greatest$1/1()
org/apache/spark/sql/functions/greatest(scala.collection.Seq)#org/apache/spark/sql/functions$$anonfun$greatest$2/2()
org/apache/spark/sql/functions/greatest(scala.collection.Seq)#scala/Predef$/require(boolean,scala.Function0)
org/apache/spark/sql/functions/greatest(scala.collection.Seq)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/functions/greatest(scala.collection.Seq)#org/apache/spark/sql/functions$/greatest(scala.collection.Seq)
org/apache/spark/sql/functions/greatest(scala.collection.Seq)#org/apache/spark/sql/functions$/withExpr(org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/functions/greatest(scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/Greatest/Greatest(scala.collection.Seq)
org/apache/spark/sql/functions/greatest(scala.collection.Seq)#scala/collection/Seq/length()
org/apache/spark/sql/functions/udf(scala.Function0,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/runtime/package$/universe()
org/apache/spark/sql/functions/udf(scala.Function0,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try/toOption()
org/apache/spark/sql/functions/udf(scala.Function0,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/api/TypeTags/typeTag(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function0,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()
org/apache/spark/sql/functions/udf(scala.Function0,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try$/apply(scala.Function0)
org/apache/spark/sql/functions/udf(scala.Function0,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function0,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$$anonfun$3/3()
org/apache/spark/sql/functions/udf(scala.Function0,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)
org/apache/spark/sql/functions/udf(scala.Function0,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$/udf(scala.Function0,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function5,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/runtime/package$/universe()
org/apache/spark/sql/functions/udf(scala.Function5,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try/toOption()
org/apache/spark/sql/functions/udf(scala.Function5,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$$anonfun$8/8(scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function5,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/api/TypeTags/typeTag(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function5,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function5,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$/udf(scala.Function5,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function5,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)
org/apache/spark/sql/functions/udf(scala.Function5,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()
org/apache/spark/sql/functions/udf(scala.Function5,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try$/apply(scala.Function0)
org/apache/spark/sql/functions/udf(scala.Function2,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try/toOption()
org/apache/spark/sql/functions/udf(scala.Function2,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$$anonfun$5/5(scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function2,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/api/TypeTags/typeTag(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function2,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function2,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)
org/apache/spark/sql/functions/udf(scala.Function2,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$/udf(scala.Function2,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function2,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/runtime/package$/universe()
org/apache/spark/sql/functions/udf(scala.Function2,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()
org/apache/spark/sql/functions/udf(scala.Function2,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try$/apply(scala.Function0)
org/apache/spark/sql/functions/udf(scala.Function3,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/runtime/package$/universe()
org/apache/spark/sql/functions/udf(scala.Function3,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$$anonfun$6/6(scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function3,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try/toOption()
org/apache/spark/sql/functions/udf(scala.Function3,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/reflect/api/TypeTags/typeTag(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function3,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$/schemaFor(scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function3,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/functions$/udf(scala.Function3,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)
org/apache/spark/sql/functions/udf(scala.Function3,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/expressions/UserDefinedFunction$/apply(java.lang.Object,org.apache.spark.sql.types.DataType,scala.Option)
org/apache/spark/sql/functions/udf(scala.Function3,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#org/apache/spark/sql/catalyst/ScalaReflection$Schema/dataType()
org/apache/spark/sql/functions/udf(scala.Function3,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag,scala.reflect.api.TypeTags$TypeTag)#scala/util/Try$/apply(scala.Function0)
org/apache/spark/sql/execution/datasources/FileFormatWriter/WriteJobDescription/anonfun/2/apply()#org/apache/spark/sql/execution/datasources/FileFormatWriter$WriteJobDescription/dataColumns()
org/apache/spark/sql/execution/datasources/FileFormatWriter/WriteJobDescription/anonfun/2/apply()#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/datasources/FileFormatWriter/WriteJobDescription/anonfun/2/apply()#org/apache/spark/sql/execution/datasources/FileFormatWriter$WriteJobDescription/allColumns()
org/apache/spark/sql/execution/datasources/FileFormatWriter/WriteJobDescription/anonfun/2/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/FileFormatWriter/WriteJobDescription/anonfun/2/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/FileFormatWriter/WriteJobDescription/anonfun/2/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/FileFormatWriter/WriteJobDescription/anonfun/2/apply()#org/apache/spark/sql/execution/datasources/FileFormatWriter$WriteJobDescription$$anonfun$2/apply()
org/apache/spark/sql/execution/datasources/FileFormatWriter/WriteJobDescription/anonfun/2/apply()#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/datasources/FileFormatWriter/WriteJobDescription/anonfun/2/apply()#org/apache/spark/sql/execution/datasources/FileFormatWriter$WriteJobDescription/partitionColumns()
org/apache/spark/sql/execution/datasources/FileFormatWriter/WriteJobDescription/anonfun/2/apply()#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/datasources/FileFormatWriter/WriteJobDescription/anonfun/2/apply()#scala/collection/Seq/mkString(java.lang.String)
org/apache/spark/sql/execution/datasources/FileFormatWriter/WriteJobDescription/anonfun/2/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/writeTempBatch(java.lang.Object)#org/apache/hadoop/fs/Path/Path(org.apache.hadoop.fs.Path,java.lang.String)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/writeTempBatch(java.lang.Object)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/writeTempBatch(java.lang.Object)#java/util/UUID/randomUUID()
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/writeTempBatch(java.lang.Object)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/writeTempBatch(java.lang.Object)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/writeTempBatch(java.lang.Object)#org/apache/spark/sql/execution/streaming/HDFSMetadataLog$FileManager/create(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/writeTempBatch(java.lang.Object)#java/util/UUID/toString()
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/writeTempBatch(java.lang.Object)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/writeTempBatch(java.lang.Object)#org/apache/commons/io/IOUtils/closeQuietly(java.io.OutputStream)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/writeTempBatch(java.lang.Object)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/get(long)#org/apache/commons/io/IOUtils/closeQuietly(java.io.InputStream)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/get(long)#org/apache/spark/sql/execution/streaming/HDFSMetadataLog$FileManager/open(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/get(long)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/get(long)#org/apache/spark/sql/execution/streaming/HDFSMetadataLog$$anonfun$get$1/1(org.apache.spark.sql.execution.streaming.HDFSMetadataLog,org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/get(long)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/get(long)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/get(long)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/get(long)#org/apache/spark/sql/execution/streaming/HDFSMetadataLog$FileManager/exists(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/get(long)#java/lang/IllegalStateException/getMessage()
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/get(long)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/get(long)#java/lang/IllegalStateException/IllegalStateException(java.lang.String,java.lang.Throwable)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/getLatest()#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/getLatest()#org/apache/spark/sql/execution/streaming/HDFSMetadataLog$$anonfun$5/5(org.apache.spark.sql.execution.streaming.HDFSMetadataLog)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/getLatest()#scala/runtime/NonLocalReturnControl/value()
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/getLatest()#scala/collection/mutable/ArrayOps/foreach(scala.Function1)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/getLatest()#org/apache/spark/sql/execution/streaming/HDFSMetadataLog$FileManager/list(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/getLatest()#scala/collection/mutable/ArrayOps/sorted(scala.math.Ordering)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/getLatest()#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/getLatest()#scala/Predef$/longArrayOps(long[])
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/getLatest()#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/getLatest()#scala/collection/mutable/ArrayOps/reverse()
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/getLatest()#org/apache/spark/sql/execution/streaming/HDFSMetadataLog$$anonfun$getLatest$1/1(org.apache.spark.sql.execution.streaming.HDFSMetadataLog,java.lang.Object)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/getLatest()#scala/runtime/NonLocalReturnControl/key()
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/getLatest()#scala/reflect/ClassTag$/Long()
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/getLatest()#java/lang/Object/Object()
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/parseVersion(java.lang.String,int)#scala/collection/immutable/StringOps$/apply$extension(java.lang.String,int)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/parseVersion(java.lang.String,int)#java/lang/String/length()
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/parseVersion(java.lang.String,int)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/parseVersion(java.lang.String,int)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/parseVersion(java.lang.String,int)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/parseVersion(java.lang.String,int)#java/lang/IllegalStateException/IllegalStateException(java.lang.String)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/parseVersion(java.lang.String,int)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/parseVersion(java.lang.String,int)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/parseVersion(java.lang.String,int)#scala/collection/immutable/StringOps/toInt()
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/parseVersion(java.lang.String,int)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/parseVersion(java.lang.String,int)#java/lang/String/substring(int,int)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/parseVersion(java.lang.String,int)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/parseVersion(java.lang.String,int)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/parseVersion(java.lang.String,int)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/parseVersion(java.lang.String,int)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/org$apache$spark$sql$execution$streaming$HDFSMetadataLog$$writeBatch(long,java.lang.Object)#org/apache/hadoop/fs/Path/getName()
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/org$apache$spark$sql$execution$streaming$HDFSMetadataLog$$writeBatch(long,java.lang.Object)#org/apache/hadoop/fs/Path/Path(org.apache.hadoop.fs.Path,java.lang.String)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/org$apache$spark$sql$execution$streaming$HDFSMetadataLog$$writeBatch(long,java.lang.Object)#org/apache/spark/sql/execution/streaming/HDFSMetadataLog$$anonfun$3/3(org.apache.spark.sql.execution.streaming.HDFSMetadataLog,long)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/org$apache$spark$sql$execution$streaming$HDFSMetadataLog$$writeBatch(long,java.lang.Object)#java/util/ConcurrentModificationException/ConcurrentModificationException(java.lang.String,java.lang.Throwable)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/org$apache$spark$sql$execution$streaming$HDFSMetadataLog$$writeBatch(long,java.lang.Object)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/org$apache$spark$sql$execution$streaming$HDFSMetadataLog$$writeBatch(long,java.lang.Object)#org/apache/spark/sql/execution/streaming/HDFSMetadataLog$FileManager/rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/org$apache$spark$sql$execution$streaming$HDFSMetadataLog$$writeBatch(long,java.lang.Object)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/org$apache$spark$sql$execution$streaming$HDFSMetadataLog$$writeBatch(long,java.lang.Object)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/org$apache$spark$sql$execution$streaming$HDFSMetadataLog$$writeBatch(long,java.lang.Object)#org/apache/spark/sql/execution/streaming/HDFSMetadataLog$FileManager/exists(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/org$apache$spark$sql$execution$streaming$HDFSMetadataLog$$writeBatch(long,java.lang.Object)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/org$apache$spark$sql$execution$streaming$HDFSMetadataLog$$writeBatch(long,java.lang.Object)#org/apache/hadoop/fs/Path/getParent()
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/org$apache$spark$sql$execution$streaming$HDFSMetadataLog$$writeBatch(long,java.lang.Object)#org/apache/spark/sql/execution/streaming/HDFSMetadataLog$FileManager/delete(org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/org$apache$spark$sql$execution$streaming$HDFSMetadataLog$$writeBatch(long,java.lang.Object)#org/apache/spark/sql/execution/streaming/HDFSMetadataLog$$anonfun$org$apache$spark$sql$execution$streaming$HDFSMetadataLog$$writeBatch$1/1(org.apache.spark.sql.execution.streaming.HDFSMetadataLog,long)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/org$apache$spark$sql$execution$streaming$HDFSMetadataLog$$writeBatch(long,java.lang.Object)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/purge(long)#org/apache/spark/sql/execution/streaming/HDFSMetadataLog$$anonfun$purge$1/1(org.apache.spark.sql.execution.streaming.HDFSMetadataLog,long)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/purge(long)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/purge(long)#org/apache/spark/sql/execution/streaming/HDFSMetadataLog$$anonfun$purge$2/2(org.apache.spark.sql.execution.streaming.HDFSMetadataLog)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/purge(long)#org/apache/spark/sql/execution/streaming/HDFSMetadataLog$$anonfun$6/6(org.apache.spark.sql.execution.streaming.HDFSMetadataLog)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/purge(long)#scala/collection/generic/FilterMonadic/foreach(scala.Function1)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/purge(long)#org/apache/spark/sql/execution/streaming/HDFSMetadataLog$FileManager/list(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/purge(long)#scala/collection/mutable/ArrayOps/withFilter(scala.Function1)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/purge(long)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/purge(long)#scala/reflect/ClassTag$/Long()
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/purge(long)#scala/Predef$/longArrayOps(long[])
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/purge(long)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/get(scala.Option,scala.Option)#org/apache/spark/sql/execution/streaming/HDFSMetadataLog$$anonfun$1/1(org.apache.spark.sql.execution.streaming.HDFSMetadataLog,scala.Option,scala.Option)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/get(scala.Option,scala.Option)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/get(scala.Option,scala.Option)#org/apache/spark/sql/execution/streaming/HDFSMetadataLog$$anonfun$4/4(org.apache.spark.sql.execution.streaming.HDFSMetadataLog)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/get(scala.Option,scala.Option)#scala/collection/mutable/ArrayOps/sorted(scala.math.Ordering)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/get(scala.Option,scala.Option)#org/apache/spark/sql/execution/streaming/HDFSMetadataLog$$anonfun$get$3/3(org.apache.spark.sql.execution.streaming.HDFSMetadataLog)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/get(scala.Option,scala.Option)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/get(scala.Option,scala.Option)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/get(scala.Option,scala.Option)#scala/Predef$/longArrayOps(long[])
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/get(scala.Option,scala.Option)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/get(scala.Option,scala.Option)#org/apache/spark/sql/execution/streaming/HDFSMetadataLog$$anonfun$get$2/2(org.apache.spark.sql.execution.streaming.HDFSMetadataLog)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/get(scala.Option,scala.Option)#org/apache/spark/sql/execution/streaming/HDFSMetadataLog$FileManager/list(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/get(scala.Option,scala.Option)#org/apache/spark/sql/execution/streaming/HDFSMetadataLog$$anonfun$get$4/4(org.apache.spark.sql.execution.streaming.HDFSMetadataLog)
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/get(scala.Option,scala.Option)#scala/reflect/ClassTag$/Long()
org/apache/spark/sql/execution/streaming/HDFSMetadataLog/get(scala.Option,scala.Option)#scala/collection/mutable/ArrayOps/filter(scala.Function1)
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#scala/collection/mutable/Buffer/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#scala/Option/foreach(scala.Function1)
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#scala/Option$/option2Iterable(scala.Option)
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#scala/Option/isDefined()
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#scala/collection/mutable/Buffer/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#org/apache/spark/sql/execution/window/AggregateProcessor$$anonfun$1/1()
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#org/apache/spark/sql/execution/window/AggregateProcessor$$anonfun$2/2()
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#org/apache/spark/sql/execution/window/AggregateProcessor$$anonfun$apply$3/3(scala.collection.Seq,scala.collection.mutable.Buffer,scala.collection.mutable.Buffer,scala.collection.mutable.Buffer,scala.collection.mutable.Buffer,scala.collection.mutable.Buffer)
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#org/apache/spark/sql/execution/window/AggregateProcessor$$anonfun$3/3()
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#scala/collection/mutable/Buffer$/fill(int,scala.Function0)
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#scala/collection/mutable/ArrayOps/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#scala/collection/mutable/Buffer$/canBuildFrom()
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#scala/collection/mutable/Buffer$/empty()
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#scala/collection/Iterable/toSeq()
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#org/apache/spark/sql/execution/window/AggregateProcessor$/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#scala/Function2/apply(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#scala/collection/mutable/ArrayOps/headOption()
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#scala/collection/mutable/ArrayOps/foreach(scala.Function1)
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#org/apache/spark/sql/execution/window/AggregateProcessor$$anonfun$apply$2/2(scala.collection.mutable.Buffer,scala.collection.mutable.Buffer,scala.collection.mutable.Buffer)
org/apache/spark/sql/execution/window/AggregateProcessor/apply(org.apache.spark.sql.catalyst.expressions.Expression[],int,scala.collection.Seq,scala.Function2)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/anonfun/24/apply(scala.Tuple2)#org/apache/spark/sql/types/DataType/sameType(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/anonfun/24/apply(scala.Tuple2)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/anonfun/24/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/Alias/Alias(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String,org.apache.spark.sql.catalyst.expressions.ExprId,scala.Option,scala.Option,java.lang.Boolean)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/anonfun/24/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/anonfun/24/apply(scala.Tuple2)#scala/Tuple2/_2()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/anonfun/24/apply(scala.Tuple2)#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/anonfun/24/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$6(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/anonfun/24/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/Attribute/dataType()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/anonfun/24/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$4(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/anonfun/24/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$3(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/anonfun/24/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/anonfun/24/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/Attribute/name()
org/apache/spark/sql/execution/datasources/PreprocessTableInsertion/anonfun/24/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/Attribute/metadata()
org/apache/spark/sql/execution/vectorized/ColumnVector/Array/array()#org/apache/spark/sql/types/DecimalType/scale()
org/apache/spark/sql/execution/vectorized/ColumnVector/Array/array()#org/apache/spark/sql/execution/vectorized/ColumnVector$Array/getUTF8String(int)
org/apache/spark/sql/execution/vectorized/ColumnVector/Array/array()#java/lang/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/vectorized/ColumnVector/Array/array()#java/lang/Integer/valueOf(int)
org/apache/spark/sql/execution/vectorized/ColumnVector/Array/array()#org/apache/spark/unsafe/types/UTF8String/toString()
org/apache/spark/sql/execution/vectorized/ColumnVector/Array/array()#java/lang/StringBuilder/toString()
org/apache/spark/sql/execution/vectorized/ColumnVector/Array/array()#java/lang/Double/valueOf(double)
org/apache/spark/sql/execution/vectorized/ColumnVector/Array/array()#java/lang/Long/valueOf(long)
org/apache/spark/sql/execution/vectorized/ColumnVector/Array/array()#java/lang/Byte/valueOf(byte)
org/apache/spark/sql/execution/vectorized/ColumnVector/Array/array()#org/apache/spark/sql/execution/vectorized/ColumnVector$Array/getInterval(int)
org/apache/spark/sql/execution/vectorized/ColumnVector/Array/array()#java/lang/StringBuilder/append(java.lang.String)
org/apache/spark/sql/execution/vectorized/ColumnVector/Array/array()#java/lang/Short/valueOf(short)
org/apache/spark/sql/execution/vectorized/ColumnVector/Array/array()#java/lang/Float/valueOf(float)
org/apache/spark/sql/execution/vectorized/ColumnVector/Array/array()#java/lang/Boolean/valueOf(boolean)
org/apache/spark/sql/execution/vectorized/ColumnVector/Array/array()#org/apache/spark/sql/execution/vectorized/ColumnVector$Array/getDecimal(int,int,int)
org/apache/spark/sql/execution/vectorized/ColumnVector/Array/array()#org/apache/spark/sql/types/DecimalType/precision()
org/apache/spark/sql/execution/vectorized/ColumnVector/Array/array()#java/lang/UnsupportedOperationException/UnsupportedOperationException(java.lang.String)
org/apache/spark/sql/execution/vectorized/ColumnVector/Array/array()#java/lang/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/FileFormatWriter/anonfun/write/1/apply$mcV$sp()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/FileFormatWriter/anonfun/write/1/apply$mcV$sp()#org/apache/spark/internal/io/FileCommitProtocol/commitJob(org.apache.hadoop.mapreduce.JobContext,scala.collection.Seq)
org/apache/spark/sql/execution/datasources/FileFormatWriter/anonfun/write/1/apply$mcV$sp()#org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$4/4(org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1)
org/apache/spark/sql/execution/datasources/FileFormatWriter/anonfun/write/1/apply$mcV$sp()#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/FileFormatWriter/anonfun/write/1/apply$mcV$sp()#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/FileFormatWriter/anonfun/write/1/apply$mcV$sp()#scala/collection/mutable/ArrayOps/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/FileFormatWriter/anonfun/write/1/apply$mcV$sp()#org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$2/2(org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1,org.apache.spark.sql.execution.datasources.FileFormatWriter$WriteTaskResult[])
org/apache/spark/sql/execution/datasources/FileFormatWriter/anonfun/write/1/apply$mcV$sp()#scala/runtime/RichInt$/until$extension0(int,int)
org/apache/spark/sql/execution/datasources/FileFormatWriter/anonfun/write/1/apply$mcV$sp()#org/apache/spark/SparkException/SparkException(java.lang.String,java.lang.Throwable)
org/apache/spark/sql/execution/datasources/FileFormatWriter/anonfun/write/1/apply$mcV$sp()#scala/Predef$/intWrapper(int)
org/apache/spark/sql/execution/datasources/FileFormatWriter/anonfun/write/1/apply$mcV$sp()#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/FileFormatWriter/anonfun/write/1/apply$mcV$sp()#org/apache/spark/sql/execution/datasources/FileFormatWriter$/logError(scala.Function0,java.lang.Throwable)
org/apache/spark/sql/execution/datasources/FileFormatWriter/anonfun/write/1/apply$mcV$sp()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/FileFormatWriter/anonfun/write/1/apply$mcV$sp()#org/apache/spark/rdd/RDD/partitions()
org/apache/spark/sql/execution/datasources/FileFormatWriter/anonfun/write/1/apply$mcV$sp()#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/FileFormatWriter/anonfun/write/1/apply$mcV$sp()#org/apache/spark/internal/io/FileCommitProtocol/setupJob(org.apache.hadoop.mapreduce.JobContext)
org/apache/spark/sql/execution/datasources/FileFormatWriter/anonfun/write/1/apply$mcV$sp()#org/apache/spark/SparkContext/runJob(org.apache.spark.rdd.RDD,scala.Function2,scala.collection.Seq,scala.Function2,scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/FileFormatWriter/anonfun/write/1/apply$mcV$sp()#org/apache/spark/sql/execution/SortExec/execute()
org/apache/spark/sql/execution/datasources/FileFormatWriter/anonfun/write/1/apply$mcV$sp()#org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$1/1(org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1)
org/apache/spark/sql/execution/datasources/FileFormatWriter/anonfun/write/1/apply$mcV$sp()#org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$write$1$$anonfun$12/12(org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1)
org/apache/spark/sql/execution/datasources/FileFormatWriter/anonfun/write/1/apply$mcV$sp()#org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$write$1$$anonfun$13/13(org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1)
org/apache/spark/sql/execution/datasources/FileFormatWriter/anonfun/write/1/apply$mcV$sp()#org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$write$1$$anonfun$14/14(org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1)
org/apache/spark/sql/execution/datasources/FileFormatWriter/anonfun/write/1/apply$mcV$sp()#org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$write$1$$anonfun$15/15(org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1)
org/apache/spark/sql/execution/datasources/FileFormatWriter/anonfun/write/1/apply$mcV$sp()#org/apache/spark/sql/execution/datasources/FileFormatWriter$$anonfun$write$1$$anonfun$apply$mcV$sp$3/3(org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1)
org/apache/spark/sql/execution/datasources/FileFormatWriter/anonfun/write/1/apply$mcV$sp()#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/execution/datasources/FileFormatWriter/anonfun/write/1/apply$mcV$sp()#org/apache/spark/sql/execution/SortExec$/apply$default$4()
org/apache/spark/sql/execution/datasources/FileFormatWriter/anonfun/write/1/apply$mcV$sp()#org/apache/spark/internal/io/FileCommitProtocol/abortJob(org.apache.hadoop.mapreduce.JobContext)
org/apache/spark/sql/execution/datasources/FileFormatWriter/anonfun/write/1/apply$mcV$sp()#scala/collection/mutable/ArrayOps/distinct()
org/apache/spark/sql/execution/datasources/FileFormatWriter/anonfun/write/1/apply$mcV$sp()#org/apache/spark/sql/execution/datasources/FileFormatWriter$/logInfo(scala.Function0)
org/apache/spark/sql/execution/datasources/FileFormatWriter/anonfun/write/1/apply$mcV$sp()#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/debug/package/DebugExec/dumpStats()#org/apache/spark/util/LongAccumulator/value()
org/apache/spark/sql/execution/debug/package/DebugExec/dumpStats()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/debug/package/DebugExec/dumpStats()#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/debug/package/DebugExec/dumpStats()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/debug/package/DebugExec/dumpStats()#scala/collection/IterableLike/foreach(scala.Function1)
org/apache/spark/sql/execution/debug/package/DebugExec/dumpStats()#scala/collection/Seq/zip(scala.collection.GenIterable,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/debug/package/DebugExec/dumpStats()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/debug/package/DebugExec/dumpStats()#org/apache/spark/sql/execution/debug/package$DebugExec/child()
org/apache/spark/sql/execution/debug/package/DebugExec/dumpStats()#org/apache/spark/sql/execution/debug/package$DebugExec$$anonfun$dumpStats$1/1(org.apache.spark.sql.execution.debug.package$DebugExec)
org/apache/spark/sql/execution/debug/package/DebugExec/dumpStats()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/debug/package/DebugExec/dumpStats()#org/apache/spark/sql/execution/debug/package$/org$apache$spark$sql$execution$debug$package$$debugPrint(java.lang.String)
org/apache/spark/sql/execution/debug/package/DebugExec/dumpStats()#org/apache/spark/sql/execution/debug/package$DebugExec/tupleCount()
org/apache/spark/sql/execution/debug/package/DebugExec/dumpStats()#org/apache/spark/sql/execution/SparkPlan/simpleString()
org/apache/spark/sql/execution/debug/package/DebugExec/dumpStats()#org/apache/spark/sql/execution/debug/package$DebugExec/columnStats()
org/apache/spark/sql/execution/debug/package/DebugExec/dumpStats()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/anonfun/execute/2/apply(org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/sql/catalyst/expressions/UnsafeProjection/apply(org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/anonfun/execute/2/apply(org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/sql/execution/datasources/FileFormatWriter$DynamicPartitionWriteTask/org$apache$spark$sql$execution$datasources$FileFormatWriter$DynamicPartitionWriteTask$$newOutputWriter(org.apache.spark.sql.catalyst.InternalRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,int,scala.collection.mutable.Set)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/anonfun/execute/2/apply(org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/sql/execution/datasources/FileFormatWriter$/org$apache$spark$sql$execution$datasources$FileFormatWriter$$MAX_FILE_COUNTER()
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/anonfun/execute/2/apply(org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/sql/execution/datasources/FileFormatWriter$/logDebug(scala.Function0)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/anonfun/execute/2/apply(org.apache.spark.sql.catalyst.InternalRow)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/anonfun/execute/2/apply(org.apache.spark.sql.catalyst.InternalRow)#scala/Predef$/assert(boolean,scala.Function0)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/anonfun/execute/2/apply(org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/sql/execution/datasources/FileFormatWriter$WriteJobDescription/maxRecordsPerFile()
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/anonfun/execute/2/apply(org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/sql/execution/datasources/FileFormatWriter$DynamicPartitionWriteTask$$anonfun$execute$2$$anonfun$apply$6/6(org.apache.spark.sql.execution.datasources.FileFormatWriter$DynamicPartitionWriteTask$$anonfun$execute$2)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/anonfun/execute/2/apply(org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/sql/execution/datasources/FileFormatWriter$DynamicPartitionWriteTask/org$apache$spark$sql$execution$datasources$FileFormatWriter$DynamicPartitionWriteTask$$currentWriter()
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/anonfun/execute/2/apply(org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/sql/catalyst/expressions/UnsafeRow/copy()
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/anonfun/execute/2/apply(org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/sql/execution/datasources/FileFormatWriter$DynamicPartitionWriteTask$$anonfun$execute$2$$anonfun$apply$7/7(org.apache.spark.sql.execution.datasources.FileFormatWriter$DynamicPartitionWriteTask$$anonfun$execute$2)
org/apache/spark/sql/execution/datasources/FileFormatWriter/DynamicPartitionWriteTask/anonfun/execute/2/apply(org.apache.spark.sql.catalyst.InternalRow)#org/apache/spark/sql/execution/datasources/FileFormatWriter$DynamicPartitionWriteTask/releaseResources()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDropTablePartitions/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$DropTablePartitionsContext/partitionSpec()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDropTablePartitions/1/apply()#scala/collection/convert/Decorators$AsScala/asScala()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDropTablePartitions/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitDropTablePartitions$1$$anonfun$apply$24/24(org.apache.spark.sql.execution.SparkSqlAstBuilder$$anonfun$visitDropTablePartitions$1)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDropTablePartitions/1/apply()#scala/collection/JavaConverters$/asScalaBufferConverter(java.util.List)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDropTablePartitions/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$DropTablePartitionsContext/tableIdentifier()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDropTablePartitions/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$DropTablePartitionsContext/EXISTS()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDropTablePartitions/1/apply()#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDropTablePartitions/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder/visitTableIdentifier(org.apache.spark.sql.catalyst.parser.SqlBaseParser$TableIdentifierContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDropTablePartitions/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitDropTablePartitions$1/apply()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDropTablePartitions/1/apply()#scala/collection/mutable/Buffer$/canBuildFrom()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDropTablePartitions/1/apply()#org/apache/spark/sql/catalyst/parser/ParserUtils$/operationNotAllowed(java.lang.String,org.antlr.v4.runtime.ParserRuleContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDropTablePartitions/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$DropTablePartitionsContext/PURGE()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitDropTablePartitions/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$DropTablePartitionsContext/VIEW()
org/apache/spark/sql/execution/joins/CartesianProductExec/doExecute()#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/joins/CartesianProductExec/doExecute()#org/apache/spark/sql/execution/joins/UnsafeCartesianRDD/mapPartitionsWithIndexInternal$default$2()
org/apache/spark/sql/execution/joins/CartesianProductExec/doExecute()#org/apache/spark/sql/execution/joins/CartesianProductExec/sqlContext()
org/apache/spark/sql/execution/joins/CartesianProductExec/doExecute()#scala/collection/Seq/size()
org/apache/spark/sql/execution/joins/CartesianProductExec/doExecute()#org/apache/spark/sql/internal/SQLConf/cartesianProductExecBufferSpillThreshold()
org/apache/spark/sql/execution/joins/CartesianProductExec/doExecute()#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/joins/CartesianProductExec/doExecute()#org/apache/spark/sql/execution/joins/CartesianProductExec/longMetric(java.lang.String)
org/apache/spark/sql/execution/joins/CartesianProductExec/doExecute()#org/apache/spark/sql/execution/joins/CartesianProductExec$$anonfun$doExecute$1/1(org.apache.spark.sql.execution.joins.CartesianProductExec,org.apache.spark.sql.execution.metric.SQLMetric)
org/apache/spark/sql/execution/joins/CartesianProductExec/doExecute()#org/apache/spark/sql/execution/joins/UnsafeCartesianRDD/mapPartitionsWithIndexInternal(scala.Function2,boolean,scala.reflect.ClassTag)
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createSetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createSetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils$$anonfun$createSetters$2/2(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils)
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createSetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils$$anonfun$createSetters$3/3(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils)
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createSetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils$$anonfun$createSetters$4/4(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils)
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createSetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils$$anonfun$createSetters$5/5(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils)
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createSetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils$$anonfun$createSetters$6/6(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils)
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createSetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils$$anonfun$createSetters$7/7(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils)
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createSetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createSetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils$$anonfun$createSetters$10/10(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils)
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createSetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils$$anonfun$createSetters$11/11(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils)
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createSetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils$$anonfun$createSetters$12/12(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils)
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createSetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createSetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createSetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils$$anonfun$createSetters$1/1(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils)
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createSetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createSetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createSetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils$$anonfun$createSetters$9/9(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,int)
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createSetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils$$anonfun$createSetters$8/8(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils)
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createSetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils$$anonfun$2/2(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils)
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createSetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/types/DecimalType/precision()
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createGetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createGetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils$$anonfun$createGetters$8/8(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils)
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createGetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils$$anonfun$1/1(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils)
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createGetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/types/StructType/fields()
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createGetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/types/DecimalType/precision()
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createGetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils$$anonfun$createGetters$1/1(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils)
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createGetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils$$anonfun$createGetters$3/3(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils)
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createGetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/types/DecimalType/scale()
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createGetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils$$anonfun$createGetters$4/4(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils)
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createGetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils$$anonfun$createGetters$12/12(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createGetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils$$anonfun$createGetters$5/5(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils)
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createGetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createGetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createGetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils$$anonfun$createGetters$6/6(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils)
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createGetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils$$anonfun$createGetters$9/9(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,int,int)
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createGetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils$$anonfun$createGetters$7/7(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils)
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createGetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createGetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils$$anonfun$createGetters$10/10(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils)
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createGetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils$$anonfun$createGetters$11/11(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils)
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createGetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils$$anonfun$createGetters$2/2(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils)
org/apache/spark/sql/execution/aggregate/BufferSetterGetterUtils/class/createGetters(org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils,org.apache.spark.sql.types.StructType)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinatorRef/forDriver(org.apache.spark.SparkEnv)#org/apache/spark/sql/execution/streaming/state/StateStoreCoordinatorRef$/logDebug(scala.Function0)
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinatorRef/forDriver(org.apache.spark.SparkEnv)#org/apache/spark/sql/execution/streaming/state/StateStoreCoordinatorRef$/endpointName()
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinatorRef/forDriver(org.apache.spark.SparkEnv)#org/apache/spark/sql/execution/streaming/state/StateStoreCoordinatorRef$/forDriver(org.apache.spark.SparkEnv)
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinatorRef/forDriver(org.apache.spark.SparkEnv)#org/apache/spark/sql/execution/streaming/state/StateStoreCoordinatorRef$$anonfun$forDriver$1/1()
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinatorRef/forDriver(org.apache.spark.SparkEnv)#org/apache/spark/sql/execution/streaming/state/StateStoreCoordinatorRef$/logInfo(scala.Function0)
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinatorRef/forDriver(org.apache.spark.SparkEnv)#org/apache/spark/sql/execution/streaming/state/StateStoreCoordinatorRef$$anonfun$forDriver$2/2()
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinatorRef/forDriver(org.apache.spark.SparkEnv)#org/apache/spark/SparkEnv/rpcEnv()
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinatorRef/forDriver(org.apache.spark.SparkEnv)#org/apache/spark/SparkEnv/conf()
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinatorRef/forDriver(org.apache.spark.SparkEnv)#org/apache/spark/rpc/RpcEnv/setupEndpoint(java.lang.String,org.apache.spark.rpc.RpcEndpoint)
org/apache/spark/sql/execution/streaming/state/StateStoreCoordinatorRef/forDriver(org.apache.spark.SparkEnv)#org/apache/spark/util/RpcUtils$/makeDriverRef(java.lang.String,org.apache.spark.SparkConf,org.apache.spark.rpc.RpcEnv)
org/apache/spark/sql/execution/datasources/CatalogFileIndex/filterPartitions(scala.collection.Seq)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/listPartitionsByFilter(org.apache.spark.sql.catalyst.TableIdentifier,scala.collection.Seq)
org/apache/spark/sql/execution/datasources/CatalogFileIndex/filterPartitions(scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/CatalogFileIndex/filterPartitions(scala.collection.Seq)#java/lang/System/nanoTime()
org/apache/spark/sql/execution/datasources/CatalogFileIndex/filterPartitions(scala.collection.Seq)#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/datasources/CatalogFileIndex/filterPartitions(scala.collection.Seq)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/datasources/CatalogFileIndex/filterPartitions(scala.collection.Seq)#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/execution/datasources/CatalogFileIndex/filterPartitions(scala.collection.Seq)#org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()
org/apache/spark/sql/execution/datasources/CatalogFileIndex/filterPartitions(scala.collection.Seq)#org/apache/spark/sql/catalyst/catalog/CatalogTable/storage()
org/apache/spark/sql/execution/datasources/CatalogFileIndex/filterPartitions(scala.collection.Seq)#org/apache/spark/sql/catalyst/catalog/CatalogStorageFormat/properties()
org/apache/spark/sql/execution/datasources/CatalogFileIndex/filterPartitions(scala.collection.Seq)#org/apache/hadoop/fs/Path/Path(java.net.URI)
org/apache/spark/sql/execution/datasources/CatalogFileIndex/filterPartitions(scala.collection.Seq)#org/apache/spark/sql/execution/datasources/InMemoryFileIndex$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/CatalogFileIndex/filterPartitions(scala.collection.Seq)#org/apache/spark/sql/execution/datasources/CatalogFileIndex$$anonfun$2/2(org.apache.spark.sql.execution.datasources.CatalogFileIndex)
org/apache/spark/sql/execution/datasources/CatalogFileIndex/filterPartitions(scala.collection.Seq)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/CatalogFileIndex/filterPartitions(scala.collection.Seq)#org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()
org/apache/spark/sql/execution/datasources/CatalogFileIndex/filterPartitions(scala.collection.Seq)#scala/Option/get()
org/apache/spark/sql/execution/vectorized/OnHeapColumnVector/reserveInternal(int)#org/apache/spark/sql/types/DecimalType/is32BitDecimalType(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/vectorized/OnHeapColumnVector/reserveInternal(int)#org/apache/spark/sql/types/DecimalType/is64BitDecimalType(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/vectorized/OnHeapColumnVector/reserveInternal(int)#org/apache/spark/sql/types/DecimalType/isByteArrayDecimalType(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/vectorized/OnHeapColumnVector/reserveInternal(int)#java/lang/StringBuilder/append(java.lang.String)
org/apache/spark/sql/execution/vectorized/OnHeapColumnVector/reserveInternal(int)#java/lang/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/vectorized/OnHeapColumnVector/reserveInternal(int)#java/lang/RuntimeException/RuntimeException(java.lang.String)
org/apache/spark/sql/execution/vectorized/OnHeapColumnVector/reserveInternal(int)#java/lang/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/vectorized/OnHeapColumnVector/reserveInternal(int)#java/lang/System/arraycopy(java.lang.Object,int,java.lang.Object,int,int)
org/apache/spark/sql/execution/vectorized/OnHeapColumnVector/reserveInternal(int)#java/lang/StringBuilder/toString()
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$collect$1(java.lang.String,org.apache.spark.sql.catalyst.expressions.SpecifiedWindowFrame,org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.mutable.Map)#org/apache/spark/sql/catalyst/expressions/SpecifiedWindowFrame/frameType()
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$collect$1(java.lang.String,org.apache.spark.sql.catalyst.expressions.SpecifiedWindowFrame,org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.mutable.Map)#scala/Tuple2/_1()
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$collect$1(java.lang.String,org.apache.spark.sql.catalyst.expressions.SpecifiedWindowFrame,org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.mutable.Map)#org/apache/spark/sql/catalyst/expressions/SpecifiedWindowFrame/frameStart()
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$collect$1(java.lang.String,org.apache.spark.sql.catalyst.expressions.SpecifiedWindowFrame,org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.mutable.Map)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$collect$1(java.lang.String,org.apache.spark.sql.catalyst.expressions.SpecifiedWindowFrame,org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.mutable.Map)#scala/collection/mutable/Buffer/$plus$eq(java.lang.Object)
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$collect$1(java.lang.String,org.apache.spark.sql.catalyst.expressions.SpecifiedWindowFrame,org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.mutable.Map)#scala/collection/mutable/Map/getOrElseUpdate(java.lang.Object,scala.Function0)
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$collect$1(java.lang.String,org.apache.spark.sql.catalyst.expressions.SpecifiedWindowFrame,org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.mutable.Map)#org/apache/spark/sql/catalyst/expressions/FrameBoundary$/apply(org.apache.spark.sql.catalyst.expressions.FrameBoundary)
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$collect$1(java.lang.String,org.apache.spark.sql.catalyst.expressions.SpecifiedWindowFrame,org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.mutable.Map)#org/apache/spark/sql/execution/window/WindowExec$$anonfun$4/4(org.apache.spark.sql.execution.window.WindowExec)
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$collect$1(java.lang.String,org.apache.spark.sql.catalyst.expressions.SpecifiedWindowFrame,org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.mutable.Map)#org/apache/spark/sql/catalyst/expressions/SpecifiedWindowFrame/frameEnd()
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$collect$1(java.lang.String,org.apache.spark.sql.catalyst.expressions.SpecifiedWindowFrame,org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.mutable.Map)#scala/Tuple2/_2()
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$collect$1(java.lang.String,org.apache.spark.sql.catalyst.expressions.SpecifiedWindowFrame,org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.mutable.Map)#scala/Tuple4/Tuple4(java.lang.Object,java.lang.Object,java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$collect$1(java.lang.String,org.apache.spark.sql.catalyst.expressions.SpecifiedWindowFrame,org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.mutable.Map)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createBoundOrdering(org.apache.spark.sql.catalyst.expressions.FrameType,int)#scala/collection/Seq/zipWithIndex(scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createBoundOrdering(org.apache.spark.sql.catalyst.expressions.FrameType,int)#org/apache/spark/sql/execution/window/WindowExec$$anonfun$1/1(org.apache.spark.sql.execution.window.WindowExec)
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createBoundOrdering(org.apache.spark.sql.catalyst.expressions.FrameType,int)#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createBoundOrdering(org.apache.spark.sql.catalyst.expressions.FrameType,int)#scala/Tuple3/_1()
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createBoundOrdering(org.apache.spark.sql.catalyst.expressions.FrameType,int)#scala/collection/Seq/size()
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createBoundOrdering(org.apache.spark.sql.catalyst.expressions.FrameType,int)#org/apache/spark/sql/catalyst/expressions/SortOrder/child()
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createBoundOrdering(org.apache.spark.sql.catalyst.expressions.FrameType,int)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createBoundOrdering(org.apache.spark.sql.catalyst.expressions.FrameType,int)#org/apache/spark/sql/execution/window/WindowExec/newMutableProjection(scala.collection.Seq,scala.collection.Seq,boolean)
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createBoundOrdering(org.apache.spark.sql.catalyst.expressions.FrameType,int)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createBoundOrdering(org.apache.spark.sql.catalyst.expressions.FrameType,int)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createBoundOrdering(org.apache.spark.sql.catalyst.expressions.FrameType,int)#scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createBoundOrdering(org.apache.spark.sql.catalyst.expressions.FrameType,int)#org/apache/spark/sql/execution/window/WindowExec$$anonfun$3/3(org.apache.spark.sql.execution.window.WindowExec)
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createBoundOrdering(org.apache.spark.sql.catalyst.expressions.FrameType,int)#org/apache/spark/sql/catalyst/expressions/Cast/Cast(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.types.DataType,scala.Option)
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createBoundOrdering(org.apache.spark.sql.catalyst.expressions.FrameType,int)#scala/Tuple3/_2()
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createBoundOrdering(org.apache.spark.sql.catalyst.expressions.FrameType,int)#org/apache/spark/sql/catalyst/expressions/Literal$/create(java.lang.Object,org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createBoundOrdering(org.apache.spark.sql.catalyst.expressions.FrameType,int)#scala/Function0/apply()
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createBoundOrdering(org.apache.spark.sql.catalyst.expressions.FrameType,int)#org/apache/spark/sql/catalyst/expressions/Cast$/apply$default$3()
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createBoundOrdering(org.apache.spark.sql.catalyst.expressions.FrameType,int)#scala/Tuple3/_3()
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createBoundOrdering(org.apache.spark.sql.catalyst.expressions.FrameType,int)#org/apache/spark/sql/execution/window/WindowExec$$anonfun$2/2(org.apache.spark.sql.execution.window.WindowExec,scala.collection.Seq)
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createBoundOrdering(org.apache.spark.sql.catalyst.expressions.FrameType,int)#org/apache/spark/sql/execution/window/WindowExec/newMutableProjection$default$3()
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createBoundOrdering(org.apache.spark.sql.catalyst.expressions.FrameType,int)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createBoundOrdering(org.apache.spark.sql.catalyst.expressions.FrameType,int)#org/apache/spark/sql/catalyst/expressions/Add/Add(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createBoundOrdering(org.apache.spark.sql.catalyst.expressions.FrameType,int)#scala/collection/Seq/head()
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createBoundOrdering(org.apache.spark.sql.catalyst.expressions.FrameType,int)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createBoundOrdering(org.apache.spark.sql.catalyst.expressions.FrameType,int)#scala/Tuple3/Tuple3(java.lang.Object,java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createBoundOrdering(org.apache.spark.sql.catalyst.expressions.FrameType,int)#org/apache/spark/sql/catalyst/expressions/Expression/dataType()
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createBoundOrdering(org.apache.spark.sql.catalyst.expressions.FrameType,int)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createBoundOrdering(org.apache.spark.sql.catalyst.expressions.FrameType,int)#scala/sys/package$/error(java.lang.String)
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createBoundOrdering(org.apache.spark.sql.catalyst.expressions.FrameType,int)#org/apache/spark/sql/catalyst/expressions/SortOrder/direction()
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createBoundOrdering(org.apache.spark.sql.catalyst.expressions.FrameType,int)#org/apache/spark/sql/execution/window/WindowExec/newOrdering(scala.collection.Seq,scala.collection.Seq)
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createResultProjection(scala.collection.Seq)#scala/collection/Seq/zipWithIndex(scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createResultProjection(scala.collection.Seq)#org/apache/spark/sql/execution/window/WindowExec$$anonfun$11/11(org.apache.spark.sql.execution.window.WindowExec,scala.collection.immutable.Map)
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createResultProjection(scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/UnsafeProjection$/create(scala.collection.Seq,scala.collection.Seq)
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createResultProjection(scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createResultProjection(scala.collection.Seq)#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createResultProjection(scala.collection.Seq)#scala/collection/Seq/zip(scala.collection.GenIterable,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createResultProjection(scala.collection.Seq)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createResultProjection(scala.collection.Seq)#scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createResultProjection(scala.collection.Seq)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createResultProjection(scala.collection.Seq)#org/apache/spark/sql/execution/window/WindowExec$$anonfun$10/10(org.apache.spark.sql.execution.window.WindowExec)
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createResultProjection(scala.collection.Seq)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/window/WindowExec/org$apache$spark$sql$execution$window$WindowExec$$createResultProjection(scala.collection.Seq)#scala/Predef$/$conforms()
org/apache/spark/sql/execution/window/WindowExec/doExecute()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/window/WindowExec/doExecute()#org/apache/spark/sql/execution/window/WindowExec/sqlContext()
org/apache/spark/sql/execution/window/WindowExec/doExecute()#scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/window/WindowExec/doExecute()#scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/window/WindowExec/doExecute()#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/window/WindowExec/doExecute()#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/window/WindowExec/doExecute()#org/apache/spark/sql/internal/SQLConf/windowExecBufferSpillThreshold()
org/apache/spark/sql/execution/window/WindowExec/doExecute()#org/apache/spark/rdd/RDD/mapPartitions(scala.Function1,boolean,scala.reflect.ClassTag)
org/apache/spark/sql/execution/window/WindowExec/doExecute()#org/apache/spark/sql/execution/window/WindowExec$$anonfun$14/14(org.apache.spark.sql.execution.window.WindowExec,scala.collection.Seq,scala.Function1[],int)
org/apache/spark/sql/execution/window/WindowExec/doExecute()#org/apache/spark/rdd/RDD/mapPartitions$default$2()
org/apache/spark/sql/execution/window/WindowExec/doExecute()#org/apache/spark/sql/execution/window/WindowExec$$anonfun$12/12(org.apache.spark.sql.execution.window.WindowExec)
org/apache/spark/sql/execution/window/WindowExec/doExecute()#org/apache/spark/sql/execution/window/WindowExec$$anonfun$13/13(org.apache.spark.sql.execution.window.WindowExec)
org/apache/spark/sql/execution/datasources/CodecStreams/createOutputStream(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)#org/apache/spark/sql/execution/datasources/CodecStreams$$anonfun$createOutputStream$1/1(java.io.OutputStream)
org/apache/spark/sql/execution/datasources/CodecStreams/createOutputStream(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)#org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/datasources/CodecStreams/createOutputStream(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)#org/apache/spark/sql/execution/datasources/CodecStreams$$anonfun$createOutputStream$2/2(java.io.OutputStream)
org/apache/spark/sql/execution/datasources/CodecStreams/createOutputStream(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)#org/apache/spark/sql/execution/datasources/CodecStreams$/createOutputStream(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)
org/apache/spark/sql/execution/datasources/CodecStreams/createOutputStream(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)#org/apache/hadoop/fs/FileSystem/create(org.apache.hadoop.fs.Path,boolean)
org/apache/spark/sql/execution/datasources/CodecStreams/createOutputStream(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/datasources/CodecStreams/createOutputStream(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/datasources/CodecStreams/createOutputStream(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)#org/apache/spark/sql/execution/datasources/CodecStreams$/getCompressionCodec(org.apache.hadoop.mapreduce.JobContext,scala.Option)
org/apache/spark/sql/execution/datasources/CodecStreams/createOutputStream(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)#org/apache/hadoop/mapreduce/JobContext/getConfiguration()
org/apache/spark/sql/execution/datasources/CodecStreams/createOutputStream(org.apache.hadoop.mapreduce.JobContext,org.apache.hadoop.fs.Path)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#org/apache/spark/sql/execution/streaming/FileStreamSourceLog$$anonfun$2/2(org.apache.spark.sql.execution.streaming.FileStreamSourceLog)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#org/apache/spark/sql/execution/streaming/FileStreamSourceLog$$anonfun$3/3(org.apache.spark.sql.execution.streaming.FileStreamSourceLog)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#scala/collection/immutable/IndexedSeq/nonEmpty()
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#scala/collection/Seq/apply(int)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#org/apache/spark/sql/execution/streaming/FileStreamSourceLog/getLatest()
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#org/apache/spark/sql/execution/streaming/FileStreamSourceLog$$anonfun$5/5(org.apache.spark.sql.execution.streaming.FileStreamSourceLog)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#scala/Tuple2/_1()
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#org/apache/spark/sql/execution/streaming/FileStreamSourceLog$$anonfun$8/8(org.apache.spark.sql.execution.streaming.FileStreamSourceLog)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#org/apache/spark/sql/execution/streaming/FileStreamSourceLog$$anonfun$9/9(org.apache.spark.sql.execution.streaming.FileStreamSourceLog)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#scala/collection/immutable/IndexedSeq$/canBuildFrom()
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#org/apache/spark/sql/execution/streaming/CompactibleFileStreamLog$/getAllValidBatches(long,long)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#scala/runtime/BoxesRunTime/unboxToLong(java.lang.Object)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#org/apache/spark/sql/execution/streaming/FileStreamSourceLog$$anonfun$1/1(org.apache.spark.sql.execution.streaming.FileStreamSourceLog)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#scala/Predef$/longWrapper(long)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#scala/Option/foreach(scala.Function1)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#scala/Predef$/Map()
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#scala/collection/TraversableLike/partition(scala.Function1)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#scala/collection/immutable/NumericRange$Inclusive/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#scala/Tuple2/_2()
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#org/apache/spark/sql/execution/streaming/FileStreamSourceLog$$anonfun$6/6(org.apache.spark.sql.execution.streaming.FileStreamSourceLog)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#org/apache/spark/sql/execution/streaming/FileStreamSourceLog$$anonfun$13/13(org.apache.spark.sql.execution.streaming.FileStreamSourceLog,scala.collection.mutable.HashMap)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#org/apache/spark/sql/execution/streaming/FileStreamSourceLog$$anonfun$12/12(org.apache.spark.sql.execution.streaming.FileStreamSourceLog,scala.collection.mutable.HashMap)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#scala/Option/orElse(scala.Function0)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#scala/collection/immutable/IndexedSeq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#scala/collection/TraversableLike/filter(scala.Function1)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#org/apache/spark/sql/execution/streaming/FileStreamSourceLog/logWarning(scala.Function0)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#org/apache/spark/sql/execution/streaming/FileStreamSourceLog$$anonfun$get$1/1(org.apache.spark.sql.execution.streaming.FileStreamSourceLog)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#org/apache/spark/sql/execution/streaming/FileStreamSourceLog/compactInterval()
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#org/apache/spark/sql/execution/streaming/FileStreamSourceLog$$anonfun$get$2/2(org.apache.spark.sql.execution.streaming.FileStreamSourceLog)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#scala/collection/TraversableOnce/toMap(scala.Predef$$less$colon$less)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#scala/runtime/RichLong/RichLong(long)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#scala/Predef$/$conforms()
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#scala/collection/immutable/Map$/empty()
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#scala/runtime/RichLong/to(java.lang.Object)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#scala/collection/mutable/ArrayOps/sortBy(scala.Function1,scala.math.Ordering)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#org/apache/spark/sql/execution/streaming/FileStreamSourceLog$$anonfun$14/14(org.apache.spark.sql.execution.streaming.FileStreamSourceLog)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#scala/collection/mutable/HashMap/HashMap()
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#org/apache/spark/sql/execution/streaming/FileStreamSourceLog$$anonfun$10/10(org.apache.spark.sql.execution.streaming.FileStreamSourceLog)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#org/apache/spark/sql/execution/streaming/FileStreamSourceLog$$anonfun$11/11(org.apache.spark.sql.execution.streaming.FileStreamSourceLog)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#scala/collection/immutable/IndexedSeq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/streaming/FileStreamSourceLog/get(scala.Option,scala.Option)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/joins/HashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#org/apache/spark/sql/execution/joins/UnsafeHashedRelation$/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)
org/apache/spark/sql/execution/joins/HashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/joins/HashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#org/apache/spark/sql/execution/joins/LongHashedRelation$/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)
org/apache/spark/sql/execution/joins/HashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#scala/Option$/apply(java.lang.Object)
org/apache/spark/sql/execution/joins/HashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#scala/collection/Seq/head()
org/apache/spark/sql/execution/joins/HashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#org/apache/spark/sql/catalyst/expressions/Expression/dataType()
org/apache/spark/sql/execution/joins/HashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#org/apache/spark/sql/execution/joins/HashedRelation$$anonfun$3/3()
org/apache/spark/sql/execution/joins/HashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#scala/collection/Seq/length()
org/apache/spark/sql/execution/joins/HashedRelation/apply(scala.collection.Iterator,scala.collection.Seq,int,org.apache.spark.memory.TaskMemoryManager)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/window/OffsetWindowFunctionFrame/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.OffsetWindowFunction)#org/apache/spark/sql/catalyst/expressions/BindReferences$/bindReference$default$3()
org/apache/spark/sql/execution/window/OffsetWindowFunctionFrame/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.OffsetWindowFunction)#org/apache/spark/sql/catalyst/expressions/OffsetWindowFunction/default()
org/apache/spark/sql/execution/window/OffsetWindowFunctionFrame/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.OffsetWindowFunction)#org/apache/spark/sql/catalyst/expressions/Expression/eval(org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/window/OffsetWindowFunctionFrame/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.OffsetWindowFunction)#org/apache/spark/sql/catalyst/expressions/Literal$/create(java.lang.Object,org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/window/OffsetWindowFunctionFrame/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.OffsetWindowFunction)#org/apache/spark/sql/catalyst/expressions/Expression/foldable()
org/apache/spark/sql/execution/window/OffsetWindowFunctionFrame/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.OffsetWindowFunction)#org/apache/spark/sql/catalyst/expressions/OffsetWindowFunction/dataType()
org/apache/spark/sql/execution/window/OffsetWindowFunctionFrame/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.OffsetWindowFunction)#org/apache/spark/sql/catalyst/expressions/BindReferences$/bindReference(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.package$AttributeSeq,boolean)
org/apache/spark/sql/execution/window/OffsetWindowFunctionFrame/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.OffsetWindowFunction)#org/apache/spark/sql/catalyst/expressions/package$/AttributeSeq(scala.collection.Seq)
org/apache/spark/sql/execution/window/OffsetWindowFunctionFrame/anonfun/6/apply(org.apache.spark.sql.catalyst.expressions.OffsetWindowFunction)#org/apache/spark/sql/catalyst/expressions/Expression/eval$default$1()
org/apache/spark/sql/DataFrameNaFunctions/anonfun/fillMap/1/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/DataFrameNaFunctions/anonfun/fillMap/1/apply(scala.Tuple2)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/DataFrameNaFunctions/anonfun/fillMap/1/apply(scala.Tuple2)#java/lang/Class/getName()
org/apache/spark/sql/DataFrameNaFunctions/anonfun/fillMap/1/apply(scala.Tuple2)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/DataFrameNaFunctions/anonfun/fillMap/1/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/DataFrameNaFunctions/anonfun/fillMap/1/apply(scala.Tuple2)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/DataFrameNaFunctions/anonfun/fillMap/1/apply(scala.Tuple2)#java/lang/Object/getClass()
org/apache/spark/sql/DataFrameNaFunctions/anonfun/fillMap/1/apply(scala.Tuple2)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/DataFrameNaFunctions/anonfun/fillMap/1/apply(scala.Tuple2)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/DataFrameNaFunctions/anonfun/fillMap/1/apply(scala.Tuple2)#scala/Tuple2/_2()
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/1/apply(scala.Tuple2)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/1/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/1/apply(scala.Tuple2)#scala/Option/isEmpty()
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/1/apply(scala.Tuple2)#org/apache/spark/sql/types/Decimal$/MAX_LONG_DIGITS()
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/1/apply(scala.Tuple2)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/1/apply(scala.Tuple2)#java/lang/Class/getName()
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/1/apply(scala.Tuple2)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/1/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/freshName(java.lang.String)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/1/apply(scala.Tuple2)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/1/apply(scala.Tuple2)#java/lang/Object/getClass()
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/1/apply(scala.Tuple2)#org/apache/spark/sql/types/DecimalType$Fixed$/unapply(org.apache.spark.sql.types.DecimalType)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/1/apply(scala.Tuple2)#scala/Tuple2/_2$mcI$sp()
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addMutableState(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/isPrimitiveType(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/1/apply(scala.Tuple2)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/1/apply(scala.Tuple2)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/1/apply(scala.Tuple2)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/1/apply(scala.Tuple2)#org/apache/spark/sql/types/DecimalType/precision()
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/1/apply(scala.Tuple2)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/1/apply(scala.Tuple2)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/1/apply(scala.Tuple2)#scala/Tuple2/_1$mcI$sp()
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/1/apply(scala.Tuple2)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/1/apply(scala.Tuple2)#scala/Option/get()
org/apache/spark/sql/execution/window/WindowExec/anonfun/windowFrameExpressionFactoryPairs/1/anonfun/apply/1/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/execution/window/WindowExec$$anonfun$windowFrameExpressionFactoryPairs$1/org$apache$spark$sql$execution$window$WindowExec$$anonfun$$$outer()
org/apache/spark/sql/execution/window/WindowExec/anonfun/windowFrameExpressionFactoryPairs/1/anonfun/apply/1/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/WindowSpecDefinition/frameSpecification()
org/apache/spark/sql/execution/window/WindowExec/anonfun/windowFrameExpressionFactoryPairs/1/anonfun/apply/1/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/window/WindowExec/anonfun/windowFrameExpressionFactoryPairs/1/anonfun/apply/1/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/WindowExpression/windowSpec()
org/apache/spark/sql/execution/window/WindowExec/anonfun/windowFrameExpressionFactoryPairs/1/anonfun/apply/1/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/window/WindowExec/anonfun/windowFrameExpressionFactoryPairs/1/anonfun/apply/1/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/window/WindowExec/anonfun/windowFrameExpressionFactoryPairs/1/anonfun/apply/1/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/sys/package$/error(java.lang.String)
org/apache/spark/sql/execution/window/WindowExec/anonfun/windowFrameExpressionFactoryPairs/1/anonfun/apply/1/apply(org.apache.spark.sql.catalyst.expressions.Expression)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/window/WindowExec/anonfun/windowFrameExpressionFactoryPairs/1/anonfun/apply/1/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/aggregate/AggregateExpression/aggregateFunction()
org/apache/spark/sql/execution/window/WindowExec/anonfun/windowFrameExpressionFactoryPairs/1/anonfun/apply/1/apply(org.apache.spark.sql.catalyst.expressions.Expression)#org/apache/spark/sql/catalyst/expressions/WindowExpression/windowFunction()
org/apache/spark/sql/DataFrameNaFunctions/anonfun/5/anonfun/apply/2/apply(scala.Tuple2)#org/apache/spark/sql/DataFrameNaFunctions$$anonfun$5/org$apache$spark$sql$DataFrameNaFunctions$$anonfun$$$outer()
org/apache/spark/sql/DataFrameNaFunctions/anonfun/5/anonfun/apply/2/apply(scala.Tuple2)#scala/Tuple2/_2()
org/apache/spark/sql/DataFrameNaFunctions/anonfun/5/anonfun/apply/2/apply(scala.Tuple2)#scala/Predef$/Float2float(java.lang.Float)
org/apache/spark/sql/DataFrameNaFunctions/anonfun/5/anonfun/apply/2/apply(scala.Tuple2)#scala/Predef$/Double2double(java.lang.Double)
org/apache/spark/sql/DataFrameNaFunctions/anonfun/5/anonfun/apply/2/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/DataFrameNaFunctions/anonfun/5/anonfun/apply/2/apply(scala.Tuple2)#java/lang/Boolean/booleanValue()
org/apache/spark/sql/DataFrameNaFunctions/anonfun/5/anonfun/apply/2/apply(scala.Tuple2)#scala/Predef$/Long2long(java.lang.Long)
org/apache/spark/sql/DataFrameNaFunctions/anonfun/5/anonfun/apply/2/apply(scala.Tuple2)#scala/runtime/BoxesRunTime/boxToFloat(float)
org/apache/spark/sql/DataFrameNaFunctions/anonfun/5/anonfun/apply/2/apply(scala.Tuple2)#scala/runtime/BoxesRunTime/boxToBoolean(boolean)
org/apache/spark/sql/DataFrameNaFunctions/anonfun/5/anonfun/apply/2/apply(scala.Tuple2)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/DataFrameNaFunctions/anonfun/5/anonfun/apply/2/apply(scala.Tuple2)#scala/runtime/BoxesRunTime/boxToDouble(double)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/checkDuplication(scala.collection.Seq,java.lang.String)#org/apache/spark/sql/execution/datasources/PreprocessTableCreation$$anonfun$1/1(org.apache.spark.sql.execution.datasources.PreprocessTableCreation)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/checkDuplication(scala.collection.Seq,java.lang.String)#scala/collection/SeqLike/length()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/checkDuplication(scala.collection.Seq,java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/checkDuplication(scala.collection.Seq,java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/checkDuplication(scala.collection.Seq,java.lang.String)#scala/collection/immutable/Iterable$/canBuildFrom()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/checkDuplication(scala.collection.Seq,java.lang.String)#scala/collection/Seq/length()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/checkDuplication(scala.collection.Seq,java.lang.String)#scala/collection/immutable/Iterable/mkString(java.lang.String)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/checkDuplication(scala.collection.Seq,java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/checkDuplication(scala.collection.Seq,java.lang.String)#scala/collection/Seq/distinct()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/checkDuplication(scala.collection.Seq,java.lang.String)#org/apache/spark/sql/execution/datasources/PreprocessTableCreation$$anonfun$20/20(org.apache.spark.sql.execution.datasources.PreprocessTableCreation)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/checkDuplication(scala.collection.Seq,java.lang.String)#scala/collection/Seq/groupBy(scala.Function1)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/checkDuplication(scala.collection.Seq,java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/checkDuplication(scala.collection.Seq,java.lang.String)#scala/collection/immutable/Map/collect(scala.PartialFunction,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeBucketSpec(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/Some/x()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeBucketSpec(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/types/StructType/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeBucketSpec(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/internal/SQLConf/resolver()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeBucketSpec(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/BucketSpec/sortColumnNames()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeBucketSpec(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogUtils$/normalizeBucketSpec(java.lang.String,scala.collection.Seq,org.apache.spark.sql.catalyst.catalog.BucketSpec,scala.Function2)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeBucketSpec(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/collection/IterableLike/foreach(scala.Function1)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeBucketSpec(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/execution/datasources/PreprocessTableCreation$$anonfun$normalizeBucketSpec$1/1(org.apache.spark.sql.execution.datasources.PreprocessTableCreation,org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeBucketSpec(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/execution/datasources/PreprocessTableCreation$$anonfun$normalizeBucketSpec$3/3(org.apache.spark.sql.execution.datasources.PreprocessTableCreation)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeBucketSpec(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeBucketSpec(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/bucketSpec()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeBucketSpec(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/execution/datasources/PreprocessTableCreation$$anonfun$19/19(org.apache.spark.sql.execution.datasources.PreprocessTableCreation)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeBucketSpec(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeBucketSpec(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeBucketSpec(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/execution/datasources/PreprocessTableCreation$$anonfun$normalizeBucketSpec$2/2(org.apache.spark.sql.execution.datasources.PreprocessTableCreation)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeBucketSpec(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/TableIdentifier/unquotedString()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeBucketSpec(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeBucketSpec(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeBucketSpec(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/BucketSpec/bucketColumnNames()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeBucketSpec(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizeBucketSpec(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/org$apache$spark$sql$execution$datasources$PreprocessTableCreation$$normalizeCatalogTable(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/types/StructType/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/org$apache$spark$sql$execution$datasources$PreprocessTableCreation$$normalizeCatalogTable(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$2()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/org$apache$spark$sql$execution$datasources$PreprocessTableCreation$$normalizeCatalogTable(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$13()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/org$apache$spark$sql$execution$datasources$PreprocessTableCreation$$normalizeCatalogTable(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/org$apache$spark$sql$execution$datasources$PreprocessTableCreation$$normalizeCatalogTable(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/Option/foreach(scala.Function1)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/org$apache$spark$sql$execution$datasources$PreprocessTableCreation$$normalizeCatalogTable(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/org$apache$spark$sql$execution$datasources$PreprocessTableCreation$$normalizeCatalogTable(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$1()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/org$apache$spark$sql$execution$datasources$PreprocessTableCreation$$normalizeCatalogTable(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$3()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/org$apache$spark$sql$execution$datasources$PreprocessTableCreation$$normalizeCatalogTable(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$10()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/org$apache$spark$sql$execution$datasources$PreprocessTableCreation$$normalizeCatalogTable(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$4()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/org$apache$spark$sql$execution$datasources$PreprocessTableCreation$$normalizeCatalogTable(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/org$apache$spark$sql$execution$datasources$PreprocessTableCreation$$normalizeCatalogTable(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$11()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/org$apache$spark$sql$execution$datasources$PreprocessTableCreation$$normalizeCatalogTable(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$5()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/org$apache$spark$sql$execution$datasources$PreprocessTableCreation$$normalizeCatalogTable(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$12()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/org$apache$spark$sql$execution$datasources$PreprocessTableCreation$$normalizeCatalogTable(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy(org.apache.spark.sql.catalyst.TableIdentifier,org.apache.spark.sql.catalyst.catalog.CatalogTableType,org.apache.spark.sql.catalyst.catalog.CatalogStorageFormat,org.apache.spark.sql.types.StructType,scala.Option,scala.collection.Seq,scala.Option,java.lang.String,long,long,scala.collection.immutable.Map,scala.Option,scala.Option,scala.Option,scala.collection.Seq,boolean,boolean)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/org$apache$spark$sql$execution$datasources$PreprocessTableCreation$$normalizeCatalogTable(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/internal/SQLConf/caseSensitiveAnalysis()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/org$apache$spark$sql$execution$datasources$PreprocessTableCreation$$normalizeCatalogTable(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$14()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/org$apache$spark$sql$execution$datasources$PreprocessTableCreation$$normalizeCatalogTable(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/org$apache$spark$sql$execution$datasources$PreprocessTableCreation$$normalizeCatalogTable(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$15()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/org$apache$spark$sql$execution$datasources$PreprocessTableCreation$$normalizeCatalogTable(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$8()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/org$apache$spark$sql$execution$datasources$PreprocessTableCreation$$normalizeCatalogTable(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$16()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/org$apache$spark$sql$execution$datasources$PreprocessTableCreation$$normalizeCatalogTable(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$9()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/org$apache$spark$sql$execution$datasources$PreprocessTableCreation$$normalizeCatalogTable(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/copy$default$17()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/org$apache$spark$sql$execution$datasources$PreprocessTableCreation$$normalizeCatalogTable(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/execution/datasources/PreprocessTableCreation$$anonfun$16/16(org.apache.spark.sql.execution.datasources.PreprocessTableCreation)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/org$apache$spark$sql$execution$datasources$PreprocessTableCreation$$normalizeCatalogTable(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/execution/datasources/PreprocessTableCreation$$anonfun$17/17(org.apache.spark.sql.execution.datasources.PreprocessTableCreation)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/org$apache$spark$sql$execution$datasources$PreprocessTableCreation$$normalizeCatalogTable(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/execution/datasources/PreprocessTableCreation$$anonfun$org$apache$spark$sql$execution$datasources$PreprocessTableCreation$$normalizeCatalogTable$1/1(org.apache.spark.sql.execution.datasources.PreprocessTableCreation,scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/org$apache$spark$sql$execution$datasources$PreprocessTableCreation$$normalizeCatalogTable(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizePartitionColumns(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/types/StructType/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizePartitionColumns(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/internal/SQLConf/resolver()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizePartitionColumns(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/collection/IterableLike/foreach(scala.Function1)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizePartitionColumns(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/TableIdentifier/unquotedString()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizePartitionColumns(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/execution/datasources/PreprocessTableCreation$$anonfun$normalizePartitionColumns$1/1(org.apache.spark.sql.execution.datasources.PreprocessTableCreation,scala.collection.Seq)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizePartitionColumns(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/collection/TraversableLike/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizePartitionColumns(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/collection/Seq/length()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizePartitionColumns(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizePartitionColumns(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/execution/datasources/PreprocessTableCreation$$anonfun$18/18(org.apache.spark.sql.execution.datasources.PreprocessTableCreation)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizePartitionColumns(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogUtils$/normalizePartCols(java.lang.String,scala.collection.Seq,scala.collection.Seq,scala.Function2)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizePartitionColumns(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/execution/datasources/PreprocessTableCreation$$anonfun$normalizePartitionColumns$2/2(org.apache.spark.sql.execution.datasources.PreprocessTableCreation)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizePartitionColumns(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/types/StructType/length()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizePartitionColumns(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/execution/datasources/PreprocessTableCreation$$anonfun$normalizePartitionColumns$3/3(org.apache.spark.sql.execution.datasources.PreprocessTableCreation)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizePartitionColumns(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/types/StructType/filter(scala.Function1)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizePartitionColumns(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/execution/command/DDLUtils$/isHiveTable(org.apache.spark.sql.catalyst.catalog.CatalogTable)
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizePartitionColumns(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizePartitionColumns(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()
org/apache/spark/sql/execution/datasources/PreprocessTableCreation/normalizePartitionColumns(org.apache.spark.sql.types.StructType,org.apache.spark.sql.catalyst.catalog.CatalogTable)#org/apache/spark/sql/types/StructType/nonEmpty()
org/apache/spark/sql/execution/command/DropFunctionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/dropFunction(org.apache.spark.sql.catalyst.FunctionIdentifier,boolean)
org/apache/spark/sql/execution/command/DropFunctionCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/empty()
org/apache/spark/sql/execution/command/DropFunctionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/command/DropFunctionCommand/run(org.apache.spark.sql.SparkSession)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/DropFunctionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/command/DropFunctionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/command/DropFunctionCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/command/DropFunctionCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/isDefined()
org/apache/spark/sql/execution/command/DropFunctionCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/DropFunctionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/analysis/SimpleFunctionRegistry/functionExists(java.lang.String)
org/apache/spark/sql/execution/command/DropFunctionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/FunctionIdentifier/FunctionIdentifier(java.lang.String,scala.Option)
org/apache/spark/sql/execution/command/DropFunctionCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/command/DropFunctionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/command/DropFunctionCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/DropFunctionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/analysis/FunctionRegistry$/builtin()
org/apache/spark/sql/execution/command/DropFunctionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/command/DropFunctionCommand/run(org.apache.spark.sql.SparkSession)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/DropFunctionCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/command/DropFunctionCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/get()
org/apache/spark/sql/execution/command/DropFunctionCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/dropTempFunction(java.lang.String,boolean)
org/apache/spark/sql/api/r/SQLUtils/setSparkContextSessionConf(org.apache.spark.sql.SparkSession,java.util.Map)#scala/collection/TraversableLike/withFilter(scala.Function1)
org/apache/spark/sql/api/r/SQLUtils/setSparkContextSessionConf(org.apache.spark.sql.SparkSession,java.util.Map)#scala/collection/JavaConverters$/mapAsScalaMapConverter(java.util.Map)
org/apache/spark/sql/api/r/SQLUtils/setSparkContextSessionConf(org.apache.spark.sql.SparkSession,java.util.Map)#scala/collection/convert/Decorators$AsScala/asScala()
org/apache/spark/sql/api/r/SQLUtils/setSparkContextSessionConf(org.apache.spark.sql.SparkSession,java.util.Map)#scala/collection/generic/FilterMonadic/foreach(scala.Function1)
org/apache/spark/sql/api/r/SQLUtils/setSparkContextSessionConf(org.apache.spark.sql.SparkSession,java.util.Map)#org/apache/spark/sql/api/r/SQLUtils$$anonfun$setSparkContextSessionConf$1/1()
org/apache/spark/sql/api/r/SQLUtils/setSparkContextSessionConf(org.apache.spark.sql.SparkSession,java.util.Map)#org/apache/spark/sql/api/r/SQLUtils$$anonfun$setSparkContextSessionConf$4/4(org.apache.spark.sql.SparkSession)
org/apache/spark/sql/api/r/SQLUtils/setSparkContextSessionConf(org.apache.spark.sql.SparkSession,java.util.Map)#org/apache/spark/sql/api/r/SQLUtils$$anonfun$setSparkContextSessionConf$3/3()
org/apache/spark/sql/api/r/SQLUtils/setSparkContextSessionConf(org.apache.spark.sql.SparkSession,java.util.Map)#org/apache/spark/sql/api/r/SQLUtils$/setSparkContextSessionConf(org.apache.spark.sql.SparkSession,java.util.Map)
org/apache/spark/sql/api/r/SQLUtils/setSparkContextSessionConf(org.apache.spark.sql.SparkSession,java.util.Map)#org/apache/spark/sql/api/r/SQLUtils$$anonfun$setSparkContextSessionConf$2/2(org.apache.spark.sql.SparkSession)
org/apache/spark/sql/api/r/SQLUtils/getOrCreateSparkSession(org.apache.spark.api.java.JavaSparkContext,java.util.Map,boolean)#org/apache/spark/SparkConf/get(java.lang.String,java.lang.String)
org/apache/spark/sql/api/r/SQLUtils/getOrCreateSparkSession(org.apache.spark.api.java.JavaSparkContext,java.util.Map,boolean)#org/apache/spark/sql/SparkSession$/builder()
org/apache/spark/sql/api/r/SQLUtils/getOrCreateSparkSession(org.apache.spark.api.java.JavaSparkContext,java.util.Map,boolean)#org/apache/spark/sql/api/r/SQLUtils$$anonfun$3/3()
org/apache/spark/sql/api/r/SQLUtils/getOrCreateSparkSession(org.apache.spark.api.java.JavaSparkContext,java.util.Map,boolean)#org/apache/spark/sql/SparkSession$/hiveClassesArePresent()
org/apache/spark/sql/api/r/SQLUtils/getOrCreateSparkSession(org.apache.spark.api.java.JavaSparkContext,java.util.Map,boolean)#java/lang/String/toLowerCase(java.util.Locale)
org/apache/spark/sql/api/r/SQLUtils/getOrCreateSparkSession(org.apache.spark.api.java.JavaSparkContext,java.util.Map,boolean)#org/apache/spark/sql/api/r/SQLUtils$/getOrCreateSparkSession(org.apache.spark.api.java.JavaSparkContext,java.util.Map,boolean)
org/apache/spark/sql/api/r/SQLUtils/getOrCreateSparkSession(org.apache.spark.api.java.JavaSparkContext,java.util.Map,boolean)#org/apache/spark/SparkContext/conf()
org/apache/spark/sql/api/r/SQLUtils/getOrCreateSparkSession(org.apache.spark.api.java.JavaSparkContext,java.util.Map,boolean)#org/apache/spark/sql/api/r/SQLUtils$/logWarning(scala.Function0)
org/apache/spark/sql/api/r/SQLUtils/getOrCreateSparkSession(org.apache.spark.api.java.JavaSparkContext,java.util.Map,boolean)#org/apache/spark/sql/internal/StaticSQLConf$/CATALOG_IMPLEMENTATION()
org/apache/spark/sql/api/r/SQLUtils/getOrCreateSparkSession(org.apache.spark.api.java.JavaSparkContext,java.util.Map,boolean)#org/apache/spark/sql/SparkSession$Builder/getOrCreate()
org/apache/spark/sql/api/r/SQLUtils/getOrCreateSparkSession(org.apache.spark.api.java.JavaSparkContext,java.util.Map,boolean)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/api/r/SQLUtils/getOrCreateSparkSession(org.apache.spark.api.java.JavaSparkContext,java.util.Map,boolean)#org/apache/spark/sql/api/r/SQLUtils$/withHiveExternalCatalog(org.apache.spark.SparkContext)
org/apache/spark/sql/api/r/SQLUtils/getOrCreateSparkSession(org.apache.spark.api.java.JavaSparkContext,java.util.Map,boolean)#org/apache/spark/sql/api/r/SQLUtils$/setSparkContextSessionConf(org.apache.spark.sql.SparkSession,java.util.Map)
org/apache/spark/sql/api/r/SQLUtils/getOrCreateSparkSession(org.apache.spark.api.java.JavaSparkContext,java.util.Map,boolean)#org/apache/spark/sql/SparkSession$Builder/sparkContext(org.apache.spark.SparkContext)
org/apache/spark/sql/api/r/SQLUtils/getOrCreateSparkSession(org.apache.spark.api.java.JavaSparkContext,java.util.Map,boolean)#org/apache/spark/api/java/JavaSparkContext/sc()
org/apache/spark/sql/api/r/SQLUtils/getOrCreateSparkSession(org.apache.spark.api.java.JavaSparkContext,java.util.Map,boolean)#org/apache/spark/internal/config/ConfigEntry/key()
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#org/apache/spark/sql/types/MapType$/apply(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#scala/util/matching/Regex/unapplySeq(java.lang.CharSequence)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#scala/collection/LinearSeqOptimized/lengthCompare(int)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#org/apache/spark/sql/types/ArrayType$/apply(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#org/apache/spark/sql/api/r/SQLUtils$/getSQLDataType(java.lang.String)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#scala/Option/get()
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#java/lang/String/split(java.lang.String)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#org/apache/spark/sql/api/r/SQLUtils$/org$apache$spark$sql$api$r$SQLUtils$$RegexContext(scala.StringContext)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#org/apache/spark/sql/api/r/SQLUtils$$anonfun$4/4(java.lang.String)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#scala/collection/immutable/StringOps$/apply$extension(java.lang.String,int)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#scala/collection/LinearSeqOptimized/apply(int)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#org/apache/spark/sql/api/r/SQLUtils$RegexContext/r()
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#java/lang/String/length()
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#scala/Option/isEmpty()
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/api/r/SQLUtils/getSQLDataType(java.lang.String)#org/apache/spark/sql/api/r/SQLUtils$/createStructType(scala.collection.Seq)
org/apache/spark/sql/api/r/SQLUtils/getTableNames(org.apache.spark.sql.SparkSession,java.lang.String)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/api/r/SQLUtils/getTableNames(org.apache.spark.sql.SparkSession,java.lang.String)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/api/r/SQLUtils/getTableNames(org.apache.spark.sql.SparkSession,java.lang.String)#org/apache/spark/sql/api/r/SQLUtils$/getTableNames(org.apache.spark.sql.SparkSession,java.lang.String)
org/apache/spark/sql/api/r/SQLUtils/getTableNames(org.apache.spark.sql.SparkSession,java.lang.String)#org/apache/spark/sql/api/r/SQLUtils$$anonfun$getTableNames$1/1()
org/apache/spark/sql/api/r/SQLUtils/getTableNames(org.apache.spark.sql.SparkSession,java.lang.String)#scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/api/r/SQLUtils/getTableNames(org.apache.spark.sql.SparkSession,java.lang.String)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/api/r/SQLUtils/getTableNames(org.apache.spark.sql.SparkSession,java.lang.String)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/api/r/SQLUtils/getTableNames(org.apache.spark.sql.SparkSession,java.lang.String)#scala/collection/immutable/StringOps/nonEmpty()
org/apache/spark/sql/api/r/SQLUtils/getTableNames(org.apache.spark.sql.SparkSession,java.lang.String)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/api/r/SQLUtils/getTableNames(org.apache.spark.sql.SparkSession,java.lang.String)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/listTables(java.lang.String)
org/apache/spark/sql/api/r/SQLUtils/getTableNames(org.apache.spark.sql.SparkSession,java.lang.String)#java/lang/String/trim()
org/apache/spark/sql/api/r/SQLUtils/getTables(org.apache.spark.sql.SparkSession,java.lang.String)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/api/r/SQLUtils/getTables(org.apache.spark.sql.SparkSession,java.lang.String)#org/apache/spark/sql/execution/command/ShowTablesCommand$/apply$default$3()
org/apache/spark/sql/api/r/SQLUtils/getTables(org.apache.spark.sql.SparkSession,java.lang.String)#org/apache/spark/sql/execution/command/ShowTablesCommand$/apply$default$4()
org/apache/spark/sql/api/r/SQLUtils/getTables(org.apache.spark.sql.SparkSession,java.lang.String)#java/lang/String/trim()
org/apache/spark/sql/api/r/SQLUtils/getTables(org.apache.spark.sql.SparkSession,java.lang.String)#org/apache/spark/sql/Dataset$/ofRows(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/api/r/SQLUtils/getTables(org.apache.spark.sql.SparkSession,java.lang.String)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/api/r/SQLUtils/getTables(org.apache.spark.sql.SparkSession,java.lang.String)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/api/r/SQLUtils/getTables(org.apache.spark.sql.SparkSession,java.lang.String)#org/apache/spark/sql/api/r/SQLUtils$/getTables(org.apache.spark.sql.SparkSession,java.lang.String)
org/apache/spark/sql/api/r/SQLUtils/getTables(org.apache.spark.sql.SparkSession,java.lang.String)#scala/collection/immutable/StringOps/nonEmpty()
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateResultProjection()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateResultProjection()#scala/Function2/apply(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateResultProjection()#org/apache/spark/sql/execution/aggregate/AggregationIterator$$anonfun$generateResultProjection$3/3(org.apache.spark.sql.execution.aggregate.AggregationIterator,org.apache.spark.sql.catalyst.expressions.UnsafeProjection)
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateResultProjection()#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateResultProjection()#org/apache/spark/sql/catalyst/expressions/SpecificInternalRow/SpecificInternalRow(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateResultProjection()#org/apache/spark/sql/catalyst/expressions/package$MutableProjection/target(org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateResultProjection()#org/apache/spark/sql/catalyst/expressions/JoinedRow/JoinedRow()
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateResultProjection()#org/apache/spark/sql/execution/aggregate/AggregationIterator$$anonfun$generateResultProjection$2/2(org.apache.spark.sql.execution.aggregate.AggregationIterator,org.apache.spark.sql.catalyst.expressions.JoinedRow,org.apache.spark.sql.catalyst.expressions.UnsafeProjection,org.apache.spark.sql.catalyst.expressions.aggregate.TypedImperativeAggregate[])
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateResultProjection()#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateResultProjection()#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateResultProjection()#scala/collection/mutable/ArrayOps/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateResultProjection()#scala/collection/Seq/contains(java.lang.Object)
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateResultProjection()#org/apache/spark/sql/execution/aggregate/AggregationIterator$$anonfun$generateResultProjection$1/1(org.apache.spark.sql.execution.aggregate.AggregationIterator,org.apache.spark.sql.catalyst.expressions.JoinedRow,org.apache.spark.sql.catalyst.expressions.SpecificInternalRow,org.apache.spark.sql.catalyst.expressions.package$MutableProjection,org.apache.spark.sql.catalyst.expressions.UnsafeProjection)
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateResultProjection()#org/apache/spark/sql/catalyst/expressions/UnsafeProjection$/create(scala.collection.Seq,scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateResultProjection()#org/apache/spark/sql/execution/aggregate/AggregationIterator$$anonfun$2/2(org.apache.spark.sql.execution.aggregate.AggregationIterator)
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateResultProjection()#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateResultProjection()#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateResultProjection()#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateResultProjection()#scala/collection/mutable/ArrayOps/collect(scala.PartialFunction,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateResultProjection()#scala/collection/SeqLike/distinct()
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateResultProjection()#org/apache/spark/sql/execution/aggregate/AggregationIterator$$anonfun$11/11(org.apache.spark.sql.execution.aggregate.AggregationIterator)
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateResultProjection()#org/apache/spark/sql/execution/aggregate/AggregationIterator$$anonfun$12/12(org.apache.spark.sql.execution.aggregate.AggregationIterator)
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateResultProjection()#org/apache/spark/sql/execution/aggregate/AggregationIterator$$anonfun$13/13(org.apache.spark.sql.execution.aggregate.AggregationIterator)
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateResultProjection()#org/apache/spark/sql/execution/aggregate/AggregationIterator$$anonfun$14/14(org.apache.spark.sql.execution.aggregate.AggregationIterator)
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateResultProjection()#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateProcessRow(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq)#org/apache/spark/sql/execution/aggregate/AggregationIterator$$anonfun$8/8(org.apache.spark.sql.execution.aggregate.AggregationIterator,scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateProcessRow(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateProcessRow(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq)#org/apache/spark/sql/execution/aggregate/AggregationIterator$$anonfun$generateProcessRow$1/1(org.apache.spark.sql.execution.aggregate.AggregationIterator,org.apache.spark.sql.catalyst.expressions.JoinedRow,scala.Function2[],org.apache.spark.sql.catalyst.expressions.package$MutableProjection)
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateProcessRow(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq)#scala/collection/TraversableLike/collect(scala.PartialFunction,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateProcessRow(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq)#scala/Function2/apply(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateProcessRow(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq)#org/apache/spark/sql/execution/aggregate/AggregationIterator$$anonfun$generateProcessRow$2/2(org.apache.spark.sql.execution.aggregate.AggregationIterator)
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateProcessRow(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateProcessRow(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq)#scala/collection/Seq/zipWithIndex(scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateProcessRow(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq)#scala/collection/TraversableLike/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateProcessRow(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq)#org/apache/spark/sql/execution/aggregate/AggregationIterator$$anonfun$1/1(org.apache.spark.sql.execution.aggregate.AggregationIterator,scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateProcessRow(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateProcessRow(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq)#org/apache/spark/sql/execution/aggregate/AggregationIterator$$anonfun$9/9(org.apache.spark.sql.execution.aggregate.AggregationIterator)
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateProcessRow(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq)#scala/collection/Seq/nonEmpty()
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateProcessRow(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq)#scala/collection/TraversableOnce/toArray(scala.reflect.ClassTag)
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateProcessRow(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq)#org/apache/spark/sql/catalyst/expressions/JoinedRow/JoinedRow()
org/apache/spark/sql/execution/aggregate/AggregationIterator/generateProcessRow(scala.collection.Seq,scala.collection.Seq,scala.collection.Seq)#scala/collection/Seq/flatMap(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/aggregate/AggregationIterator/initializeAggregateFunctions(scala.collection.Seq,int)#org/apache/spark/sql/catalyst/expressions/aggregate/ImperativeAggregate/withNewMutableAggBufferOffset(int)
org/apache/spark/sql/execution/aggregate/AggregationIterator/initializeAggregateFunctions(scala.collection.Seq,int)#org/apache/spark/sql/types/StructType/length()
org/apache/spark/sql/execution/aggregate/AggregationIterator/initializeAggregateFunctions(scala.collection.Seq,int)#org/apache/spark/sql/catalyst/expressions/BindReferences$/bindReference$default$3()
org/apache/spark/sql/execution/aggregate/AggregationIterator/initializeAggregateFunctions(scala.collection.Seq,int)#org/apache/spark/sql/catalyst/expressions/aggregate/AggregateExpression/mode()
org/apache/spark/sql/execution/aggregate/AggregationIterator/initializeAggregateFunctions(scala.collection.Seq,int)#scala/collection/Seq/apply(int)
org/apache/spark/sql/execution/aggregate/AggregationIterator/initializeAggregateFunctions(scala.collection.Seq,int)#org/apache/spark/sql/catalyst/expressions/aggregate/AggregateExpression/aggregateFunction()
org/apache/spark/sql/execution/aggregate/AggregationIterator/initializeAggregateFunctions(scala.collection.Seq,int)#org/apache/spark/sql/catalyst/expressions/aggregate/AggregateFunction/aggBufferSchema()
org/apache/spark/sql/execution/aggregate/AggregationIterator/initializeAggregateFunctions(scala.collection.Seq,int)#org/apache/spark/sql/catalyst/expressions/package$/AttributeSeq(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/AggregationIterator/initializeAggregateFunctions(scala.collection.Seq,int)#scala/collection/Seq/length()
org/apache/spark/sql/execution/aggregate/AggregationIterator/initializeAggregateFunctions(scala.collection.Seq,int)#org/apache/spark/sql/catalyst/expressions/BindReferences$/bindReference(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.package$AttributeSeq,boolean)
org/apache/spark/sql/execution/aggregate/AggregationIterator/initializeAggregateFunctions(scala.collection.Seq,int)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/aggregate/AggregationIterator/initializeAggregateFunctions(scala.collection.Seq,int)#org/apache/spark/sql/catalyst/expressions/aggregate/ImperativeAggregate/withNewInputAggBufferOffset(int)
org/apache/spark/sql/streaming/SourceProgress/jsonValue()#org/json4s/JsonAST$JString$/apply(java.lang.String)
org/apache/spark/sql/streaming/SourceProgress/jsonValue()#scala/Predef$/ArrowAssoc(java.lang.Object)
org/apache/spark/sql/streaming/SourceProgress/jsonValue()#org/json4s/JsonDSL$JsonListAssoc/$tilde(scala.Tuple2)
org/apache/spark/sql/streaming/SourceProgress/jsonValue()#scala/math/BigInt$/long2bigInt(long)
org/apache/spark/sql/streaming/SourceProgress/jsonValue()#scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)
org/apache/spark/sql/streaming/SourceProgress/jsonValue()#org/json4s/package$/JInt()
org/apache/spark/sql/streaming/SourceProgress/jsonValue()#scala/Predef$/$conforms()
org/apache/spark/sql/streaming/SourceProgress/jsonValue()#org/json4s/JsonDSL$/pair2Assoc(scala.Tuple2,scala.Function1)
org/apache/spark/sql/streaming/SourceProgress/jsonValue()#org/json4s/JsonDSL$/jobject2assoc(org.json4s.JsonAST$JObject)
org/apache/spark/sql/streaming/SourceProgress/jsonValue()#org/json4s/package$/JString()
org/apache/spark/sql/streaming/SourceProgress/jsonValue()#org/json4s/JsonDSL$JsonAssoc/$tilde(scala.Tuple2,scala.Function1)
org/apache/spark/sql/streaming/SourceProgress/jsonValue()#org/json4s/JsonAST$JInt$/apply(scala.math.BigInt)
org/apache/spark/sql/DataFrameReader/anonfun/verifyColumnNameOfCorruptRecord/1/apply$mcVI$sp(int)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/DataFrameReader/anonfun/verifyColumnNameOfCorruptRecord/1/apply$mcVI$sp(int)#org/apache/spark/sql/types/StructField/nullable()
org/apache/spark/sql/DataFrameReader/anonfun/verifyColumnNameOfCorruptRecord/1/apply$mcVI$sp(int)#org/apache/spark/sql/types/StructField/dataType()
org/apache/spark/sql/DataFrameReader/anonfun/verifyColumnNameOfCorruptRecord/1/apply$mcVI$sp(int)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/DataFrameReader/anonfun/verifyColumnNameOfCorruptRecord/1/apply$mcVI$sp(int)#org/apache/spark/sql/types/StructType/apply(int)
org/apache/spark/sql/DataFrameReader/anonfun/verifyColumnNameOfCorruptRecord/1/apply$mcVI$sp(int)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/DataFrameReader/anonfun/verifyColumnNameOfCorruptRecord/1/apply$mcVI$sp(int)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/DataFrameReader/anonfun/verifyColumnNameOfCorruptRecord/1/apply$mcVI$sp(int)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/DataFrameReader/anonfun/verifyColumnNameOfCorruptRecord/1/apply$mcVI$sp(int)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/anonfun/17/apply()#org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions$$anonfun$17/apply()
org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/anonfun/17/apply()#java/lang/Object/toString()
org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/anonfun/17/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/anonfun/17/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/anonfun/17/apply()#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/anonfun/17/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/anonfun/17/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/anonfun/17/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/anonfun/17/apply()#org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions$/JDBC_BATCH_INSERT_SIZE()
org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/anonfun/17/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/jdbc/JDBCOptions/anonfun/17/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/anonfun/7/apply(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/DecimalType/scale()
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/anonfun/7/apply(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/anonfun/7/apply(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#scala/collection/IterableLike/forall(scala.Function1)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/anonfun/7/apply(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#scala/math/package$/max(int,int)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/anonfun/7/apply(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#scala/Tuple2/_1()
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/anonfun/7/apply(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#scala/Function2/apply(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/anonfun/7/apply(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#scala/collection/Seq$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/anonfun/7/apply(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/anonfun/7/apply(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#scala/collection/IndexedSeq/lastIndexWhere(scala.Function1)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/anonfun/7/apply(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/DecimalType$/forType(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/anonfun/7/apply(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/anonfun/7/apply(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/csv/CSVInferSchema$/findTightestCommonType()
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/anonfun/7/apply(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/csv/CSVInferSchema$$anonfun$7$$anonfun$8/8(org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$7,org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/anonfun/7/apply(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#scala/collection/IndexedSeq/apply(int)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/anonfun/7/apply(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/csv/CSVInferSchema$$anonfun$7$$anonfun$apply$2/2(org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$$anonfun$7)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/anonfun/7/apply(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/anonfun/7/apply(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#scala/Tuple2/_2()
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/anonfun/7/apply(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/execution/datasources/csv/CSVInferSchema$/org$apache$spark$sql$execution$datasources$csv$CSVInferSchema$$numericPrecedence()
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/anonfun/7/apply(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/DecimalType/precision()
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/anonfun/7/apply(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/DecimalType/isWiderThan(org.apache.spark.sql.types.DataType)
org/apache/spark/sql/execution/datasources/csv/CSVInferSchema/anonfun/7/apply(org.apache.spark.sql.types.DataType,org.apache.spark.sql.types.DataType)#org/apache/spark/sql/types/DecimalType/DecimalType(int,int)
org/apache/spark/sql/Column/when(org.apache.spark.sql.Column,java.lang.Object)#org/apache/spark/sql/functions$/lit(java.lang.Object)
org/apache/spark/sql/Column/when(org.apache.spark.sql.Column,java.lang.Object)#org/apache/spark/sql/catalyst/expressions/CaseWhen/elseValue()
org/apache/spark/sql/Column/when(org.apache.spark.sql.Column,java.lang.Object)#org/apache/spark/sql/catalyst/expressions/CaseWhen/branches()
org/apache/spark/sql/Column/when(org.apache.spark.sql.Column,java.lang.Object)#org/apache/spark/sql/catalyst/expressions/CaseWhen/CaseWhen(scala.collection.Seq,scala.Option)
org/apache/spark/sql/Column/when(org.apache.spark.sql.Column,java.lang.Object)#scala/collection/Seq/$colon$plus(java.lang.Object,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/Column/when(org.apache.spark.sql.Column,java.lang.Object)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/Column/when(org.apache.spark.sql.Column,java.lang.Object)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/Column/when(org.apache.spark.sql.Column,java.lang.Object)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/Column/when(org.apache.spark.sql.Column,java.lang.Object)#scala/Tuple2/Tuple2(java.lang.Object,java.lang.Object)
org/apache/spark/sql/Column/when(org.apache.spark.sql.Column,java.lang.Object)#org/apache/spark/sql/catalyst/expressions/CaseWhen$/apply$default$2()
org/apache/spark/sql/Column/named()#org/apache/spark/sql/catalyst/analysis/MultiAlias/MultiAlias(org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq)
org/apache/spark/sql/Column/named()#org/apache/spark/sql/catalyst/expressions/Cast/transformUp(scala.PartialFunction)
org/apache/spark/sql/Column/named()#org/apache/spark/sql/catalyst/expressions/Alias/Alias(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String,org.apache.spark.sql.catalyst.expressions.ExprId,scala.Option,scala.Option,java.lang.Boolean)
org/apache/spark/sql/Column/named()#org/apache/spark/sql/catalyst/util/package$/usePrettyExpression(org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/Column/named()#org/apache/spark/sql/catalyst/expressions/aggregate/AggregateExpression/aggregateFunction()
org/apache/spark/sql/Column/named()#org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$4(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)
org/apache/spark/sql/Column/named()#org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$3(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)
org/apache/spark/sql/Column/named()#org/apache/spark/sql/catalyst/analysis/UnresolvedAlias/UnresolvedAlias(org.apache.spark.sql.catalyst.expressions.Expression,scala.Option)
org/apache/spark/sql/Column/named()#org/apache/spark/sql/Column$$anonfun$named$2/2(org.apache.spark.sql.Column)
org/apache/spark/sql/Column/named()#org/apache/spark/sql/Column$$anonfun$named$1/1(org.apache.spark.sql.Column)
org/apache/spark/sql/Column/named()#org/apache/spark/sql/catalyst/analysis/UnresolvedAlias$/apply$default$2()
org/apache/spark/sql/Column/named()#org/apache/spark/sql/Column$$anonfun$2/2(org.apache.spark.sql.Column)
org/apache/spark/sql/Column/named()#org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$6(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)
org/apache/spark/sql/Column/named()#org/apache/spark/sql/catalyst/expressions/Expression/sql()
org/apache/spark/sql/Column/named()#org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$5(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)
org/apache/spark/sql/Column/named()#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/Column/named()#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/anonfun/estimatePartitionStartIndices/1/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/anonfun/estimatePartitionStartIndices/1/apply()#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/anonfun/estimatePartitionStartIndices/1/apply()#org/apache/spark/sql/execution/exchange/ExchangeCoordinator$$anonfun$estimatePartitionStartIndices$1/apply()
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/anonfun/estimatePartitionStartIndices/1/apply()#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/anonfun/estimatePartitionStartIndices/1/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/anonfun/estimatePartitionStartIndices/1/apply()#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/anonfun/estimatePartitionStartIndices/1/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/anonfun/estimatePartitionStartIndices/1/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/exchange/ExchangeCoordinator/anonfun/estimatePartitionStartIndices/1/apply()#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/joins/CartesianProductExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/joins/CartesianProductExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#scala/collection/Seq/$plus$plus(scala.collection.GenTraversableOnce,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/joins/CartesianProductExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#org/apache/spark/sql/catalyst/expressions/JoinedRow/JoinedRow()
org/apache/spark/sql/execution/joins/CartesianProductExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#scala/collection/Iterator/map(scala.Function1)
org/apache/spark/sql/execution/joins/CartesianProductExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#org/apache/spark/sql/execution/joins/CartesianProductExec/newPredicate(org.apache.spark.sql.catalyst.expressions.Expression,scala.collection.Seq)
org/apache/spark/sql/execution/joins/CartesianProductExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#scala/Option/get()
org/apache/spark/sql/execution/joins/CartesianProductExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#org/apache/spark/sql/execution/SparkPlan/output()
org/apache/spark/sql/execution/joins/CartesianProductExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#org/apache/spark/sql/execution/SparkPlan/schema()
org/apache/spark/sql/execution/joins/CartesianProductExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#org/apache/spark/sql/catalyst/expressions/codegen/Predicate/initialize(int)
org/apache/spark/sql/execution/joins/CartesianProductExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#scala/Option/isDefined()
org/apache/spark/sql/execution/joins/CartesianProductExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#scala/collection/Iterator/filter(scala.Function1)
org/apache/spark/sql/execution/joins/CartesianProductExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#org/apache/spark/sql/execution/joins/CartesianProductExec$$anonfun$doExecute$1$$anonfun$apply$2/2(org.apache.spark.sql.execution.joins.CartesianProductExec$$anonfun$doExecute$1,org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowJoiner)
org/apache/spark/sql/execution/joins/CartesianProductExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#org/apache/spark/sql/catalyst/expressions/codegen/GenerateUnsafeRowJoiner$/create(org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/joins/CartesianProductExec/anonfun/doExecute/1/apply(int,scala.collection.Iterator)#org/apache/spark/sql/execution/joins/CartesianProductExec$$anonfun$doExecute$1$$anonfun$2/2(org.apache.spark.sql.execution.joins.CartesianProductExec$$anonfun$doExecute$1,org.apache.spark.sql.catalyst.expressions.codegen.Predicate,org.apache.spark.sql.catalyst.expressions.JoinedRow)
org/apache/spark/sql/execution/SortPrefixUtils/createPrefixGenerator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/SortPrefixUtils$$anon$3/3()
org/apache/spark/sql/execution/SortPrefixUtils/createPrefixGenerator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/SortPrefixUtils$/createPrefixGenerator(org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/SortPrefixUtils/createPrefixGenerator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/types/StructField/dataType()
org/apache/spark/sql/execution/SortPrefixUtils/createPrefixGenerator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/catalyst/expressions/SortOrder$/apply(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.SortDirection,scala.collection.immutable.Set)
org/apache/spark/sql/execution/SortPrefixUtils/createPrefixGenerator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/catalyst/expressions/SortPrefix/SortPrefix(org.apache.spark.sql.catalyst.expressions.SortOrder)
org/apache/spark/sql/execution/SortPrefixUtils/createPrefixGenerator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/catalyst/expressions/SortOrder$/apply$default$3()
org/apache/spark/sql/execution/SortPrefixUtils/createPrefixGenerator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/catalyst/expressions/UnsafeProjection$/create(org.apache.spark.sql.catalyst.expressions.Expression)
org/apache/spark/sql/execution/SortPrefixUtils/createPrefixGenerator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/catalyst/expressions/BoundReference/BoundReference(int,org.apache.spark.sql.types.DataType,boolean)
org/apache/spark/sql/execution/SortPrefixUtils/createPrefixGenerator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/SortPrefixUtils$$anon$2/2(org.apache.spark.sql.catalyst.expressions.SortPrefix,org.apache.spark.sql.catalyst.expressions.UnsafeProjection)
org/apache/spark/sql/execution/SortPrefixUtils/createPrefixGenerator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/types/StructType/head()
org/apache/spark/sql/execution/SortPrefixUtils/createPrefixGenerator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/types/StructType/nonEmpty()
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.catalyst.expressions.SortOrder)#org/apache/spark/sql/types/DecimalType/scale()
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.catalyst.expressions.SortOrder)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.catalyst.expressions.SortOrder)#org/apache/spark/sql/execution/SortPrefixUtils$/binaryPrefixComparator(org.apache.spark.sql.catalyst.expressions.SortOrder)
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.catalyst.expressions.SortOrder)#org/apache/spark/sql/execution/SortPrefixUtils$/getPrefixComparator(org.apache.spark.sql.catalyst.expressions.SortOrder)
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.catalyst.expressions.SortOrder)#org/apache/spark/sql/types/DecimalType/precision()
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.catalyst.expressions.SortOrder)#org/apache/spark/sql/catalyst/expressions/SortOrder/dataType()
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.catalyst.expressions.SortOrder)#org/apache/spark/sql/execution/SortPrefixUtils$/longPrefixComparator(org.apache.spark.sql.catalyst.expressions.SortOrder)
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.catalyst.expressions.SortOrder)#org/apache/spark/sql/types/Decimal$/MAX_LONG_DIGITS()
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.catalyst.expressions.SortOrder)#org/apache/spark/sql/execution/SortPrefixUtils$/stringPrefixComparator(org.apache.spark.sql.catalyst.expressions.SortOrder)
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.catalyst.expressions.SortOrder)#org/apache/spark/sql/execution/SortPrefixUtils$/doublePrefixComparator(org.apache.spark.sql.catalyst.expressions.SortOrder)
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/catalyst/expressions/BoundReference/BoundReference(int,org.apache.spark.sql.types.DataType,boolean)
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/types/StructField/nullable()
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/catalyst/expressions/SortOrder$/apply(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.expressions.SortDirection,scala.collection.immutable.Set)
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/SortPrefixUtils$/getPrefixComparator(org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/catalyst/expressions/SortOrder$/apply$default$3()
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/SortPrefixUtils$$anon$1/1()
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/types/StructField/dataType()
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/execution/SortPrefixUtils$/getPrefixComparator(org.apache.spark.sql.catalyst.expressions.SortOrder)
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/types/StructType/head()
org/apache/spark/sql/execution/SortPrefixUtils/getPrefixComparator(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/types/StructType/nonEmpty()
org/apache/spark/sql/execution/command/ShowPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/listPartitionNames(org.apache.spark.sql.catalyst.TableIdentifier,scala.Option)
org/apache/spark/sql/execution/command/ShowPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/command/ShowPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/getTableMetadata(org.apache.spark.sql.catalyst.TableIdentifier)
org/apache/spark/sql/execution/command/ShowPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/command/ShowPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/partitionColumnNames()
org/apache/spark/sql/execution/command/ShowPartitionsCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/get()
org/apache/spark/sql/execution/command/ShowPartitionsCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/immutable/Set/filterNot(scala.Function1)
org/apache/spark/sql/execution/command/ShowPartitionsCommand/run(org.apache.spark.sql.SparkSession)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/command/ShowPartitionsCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq$/canBuildFrom()
org/apache/spark/sql/execution/command/ShowPartitionsCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq/isEmpty()
org/apache/spark/sql/execution/command/ShowPartitionsCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/immutable/Set/mkString(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/command/ShowPartitionsCommand/run(org.apache.spark.sql.SparkSession)#scala/Option/isDefined()
org/apache/spark/sql/execution/command/ShowPartitionsCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/command/ShowPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/tableType()
org/apache/spark/sql/execution/command/ShowPartitionsCommand/run(org.apache.spark.sql.SparkSession)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/command/ShowPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/ShowPartitionsCommand$$anonfun$run$6/6(org.apache.spark.sql.execution.command.ShowPartitionsCommand)
org/apache/spark/sql/execution/command/ShowPartitionsCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/immutable/MapLike/keySet()
org/apache/spark/sql/execution/command/ShowPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/ShowPartitionsCommand$$anonfun$16/16(org.apache.spark.sql.execution.command.ShowPartitionsCommand,org.apache.spark.sql.catalyst.catalog.CatalogTable)
org/apache/spark/sql/execution/command/ShowPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/command/ShowPartitionsCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/ShowPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/command/ShowPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/execution/command/DDLUtils$/verifyPartitionProviderIsHive(org.apache.spark.sql.SparkSession,org.apache.spark.sql.catalyst.catalog.CatalogTable,java.lang.String)
org/apache/spark/sql/execution/command/ShowPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/command/ShowPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/TableIdentifier/quotedString()
org/apache/spark/sql/execution/command/ShowPartitionsCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/command/ShowPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTableType$/VIEW()
org/apache/spark/sql/execution/command/ShowPartitionsCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogTable/identifier()
org/apache/spark/sql/execution/command/ShowPartitionsCommand/run(org.apache.spark.sql.SparkSession)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/command/ShowPartitionsCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/immutable/Set/nonEmpty()
org/apache/spark/sql/execution/streaming/GroupStateImpl/setTimeoutTimestamp(long)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/GroupStateImpl/setTimeoutTimestamp(long)#scala/runtime/BoxesRunTime/boxToLong(long)
org/apache/spark/sql/execution/streaming/GroupStateImpl/setTimeoutTimestamp(long)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/GroupStateImpl/setTimeoutTimestamp(long)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/streaming/GroupStateImpl/setTimeoutTimestamp(long)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/GroupStateImpl/setTimeoutTimestamp(long)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/streaming/GroupStateImpl/setTimeoutTimestamp(long)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/execution/streaming/GroupStateImpl/setTimeoutTimestamp(long)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/streaming/GroupStateImpl/setTimeoutTimestamp(long)#org/apache/spark/sql/execution/streaming/GroupStateImpl$/NO_TIMESTAMP()
org/apache/spark/sql/execution/streaming/GroupStateImpl/setTimeoutTimestamp(long)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/GroupStateImpl/parseDuration(java.lang.String)#org/apache/spark/unsafe/types/CalendarInterval/fromString(java.lang.String)
org/apache/spark/sql/execution/streaming/GroupStateImpl/parseDuration(java.lang.String)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/streaming/GroupStateImpl/parseDuration(java.lang.String)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/GroupStateImpl/parseDuration(java.lang.String)#java/lang/String/startsWith(java.lang.String)
org/apache/spark/sql/execution/streaming/GroupStateImpl/parseDuration(java.lang.String)#org/apache/spark/unsafe/types/CalendarInterval/milliseconds()
org/apache/spark/sql/execution/streaming/GroupStateImpl/parseDuration(java.lang.String)#org/apache/commons/lang3/StringUtils/isBlank(java.lang.CharSequence)
org/apache/spark/sql/execution/streaming/GroupStateImpl/parseDuration(java.lang.String)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/execution/streaming/GroupStateImpl/parseDuration(java.lang.String)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/streaming/GroupStateImpl/parseDuration(java.lang.String)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/execution/streaming/GroupStateImpl/parseDuration(java.lang.String)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/spark/sql/execution/streaming/GroupStateImpl/parseDuration(java.lang.String)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/streaming/GroupStateImpl/parseDuration(java.lang.String)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/debug/package/DebugExec/anonfun/dumpStats/1/apply(scala.Tuple2)#scala/collection/JavaConverters$/asScalaSetConverter(java.util.Set)
org/apache/spark/sql/execution/debug/package/DebugExec/anonfun/dumpStats/1/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/debug/package/DebugExec/anonfun/dumpStats/1/apply(scala.Tuple2)#scala/collection/convert/Decorators$AsScala/asScala()
org/apache/spark/sql/execution/debug/package/DebugExec/anonfun/dumpStats/1/apply(scala.Tuple2)#scala/collection/TraversableOnce/mkString(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/debug/package/DebugExec/anonfun/dumpStats/1/apply(scala.Tuple2)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/debug/package/DebugExec/anonfun/dumpStats/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/Attribute/dataType()
org/apache/spark/sql/execution/debug/package/DebugExec/anonfun/dumpStats/1/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/debug/package/DebugExec/anonfun/dumpStats/1/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/Attribute/name()
org/apache/spark/sql/execution/debug/package/DebugExec/anonfun/dumpStats/1/apply(scala.Tuple2)#org/apache/spark/sql/execution/debug/package$/org$apache$spark$sql$execution$debug$package$$debugPrint(java.lang.String)
org/apache/spark/sql/execution/debug/package/DebugExec/anonfun/dumpStats/1/apply(scala.Tuple2)#org/apache/spark/sql/execution/debug/package$DebugExec$SetAccumulator/value()
org/apache/spark/sql/execution/debug/package/DebugExec/anonfun/dumpStats/1/apply(scala.Tuple2)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/debug/package/DebugExec/anonfun/dumpStats/1/apply(scala.Tuple2)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/debug/package/DebugExec/anonfun/dumpStats/1/apply(scala.Tuple2)#scala/Tuple2/_2()
org/apache/spark/sql/execution/debug/package/DebugExec/anonfun/dumpStats/1/apply(scala.Tuple2)#org/apache/spark/sql/execution/debug/package$DebugExec$ColumnMetrics/elementTypes()
org/apache/spark/sql/execution/debug/package/DebugExec/anonfun/dumpStats/1/apply(scala.Tuple2)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitManageResource/1/apply()#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitManageResource/1/apply()#java/lang/String/split(java.lang.String)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitManageResource/1/apply()#org/apache/spark/sql/execution/SparkSqlAstBuilder$$anonfun$visitManageResource$1/apply()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitManageResource/1/apply()#org/apache/spark/sql/catalyst/parser/ParserUtils$/remainder(org.antlr.v4.runtime.ParserRuleContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitManageResource/1/apply()#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitManageResource/1/apply()#org/apache/spark/sql/execution/command/ListFilesCommand$/apply$default$1()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitManageResource/1/apply()#org/apache/spark/sql/execution/command/ListJarsCommand$/apply$default$1()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitManageResource/1/apply()#org/antlr/v4/runtime/Token/getType()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitManageResource/1/apply()#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitManageResource/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$ManageResourceContext/identifier()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitManageResource/1/apply()#org/apache/spark/sql/catalyst/parser/ParserUtils$/operationNotAllowed(java.lang.String,org.antlr.v4.runtime.ParserRuleContext)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitManageResource/1/apply()#java/lang/String/trim()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitManageResource/1/apply()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitManageResource/1/apply()#org/apache/spark/sql/catalyst/parser/SqlBaseParser$IdentifierContext/getText()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitManageResource/1/apply()#java/lang/String/toLowerCase(java.util.Locale)
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitManageResource/1/apply()#java/lang/String/length()
org/apache/spark/sql/execution/SparkSqlAstBuilder/anonfun/visitManageResource/1/apply()#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/Dataset/anonfun/7/anonfun/apply/2/apply(java.lang.Object)#org/apache/spark/sql/catalyst/util/DateTimeUtils$/dateToString(int)
org/apache/spark/sql/Dataset/anonfun/7/anonfun/apply/2/apply(java.lang.Object)#scala/Predef$/genericArrayOps(java.lang.Object)
org/apache/spark/sql/Dataset/anonfun/7/anonfun/apply/2/apply(java.lang.Object)#java/lang/String/length()
org/apache/spark/sql/Dataset/anonfun/7/anonfun/apply/2/apply(java.lang.Object)#java/lang/String/substring(int,int)
org/apache/spark/sql/Dataset/anonfun/7/anonfun/apply/2/apply(java.lang.Object)#scala/collection/mutable/StringBuilder/StringBuilder()
org/apache/spark/sql/Dataset/anonfun/7/anonfun/apply/2/apply(java.lang.Object)#org/apache/spark/sql/catalyst/util/DateTimeUtils$/fromJavaDate(java.sql.Date)
org/apache/spark/sql/Dataset/anonfun/7/anonfun/apply/2/apply(java.lang.Object)#scala/runtime/ScalaRunTime$/isArray(java.lang.Object,int)
org/apache/spark/sql/Dataset/anonfun/7/anonfun/apply/2/apply(java.lang.Object)#scala/collection/mutable/StringBuilder/append(java.lang.Object)
org/apache/spark/sql/Dataset/anonfun/7/anonfun/apply/2/apply(java.lang.Object)#org/apache/spark/sql/Dataset$$anonfun$7$$anonfun$apply$2/apply(java.lang.Object)
org/apache/spark/sql/Dataset/anonfun/7/anonfun/apply/2/apply(java.lang.Object)#org/apache/spark/sql/catalyst/util/DateTimeUtils$/fromJavaTimestamp(java.sql.Timestamp)
org/apache/spark/sql/Dataset/anonfun/7/anonfun/apply/2/apply(java.lang.Object)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/Dataset/anonfun/7/anonfun/apply/2/apply(java.lang.Object)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/Dataset/anonfun/7/anonfun/apply/2/apply(java.lang.Object)#org/apache/spark/sql/catalyst/util/DateTimeUtils$/timestampToString(long,java.util.TimeZone)
org/apache/spark/sql/Dataset/anonfun/7/anonfun/apply/2/apply(java.lang.Object)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/Dataset/anonfun/7/anonfun/apply/2/apply(java.lang.Object)#java/lang/Object/toString()
org/apache/spark/sql/Dataset/anonfun/7/anonfun/apply/2/apply(java.lang.Object)#scala/Predef$/byteArrayOps(byte[])
org/apache/spark/sql/Dataset/anonfun/7/anonfun/apply/2/apply(java.lang.Object)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/Dataset/anonfun/7/anonfun/apply/2/apply(java.lang.Object)#org/apache/spark/sql/Dataset$$anonfun$7/org$apache$spark$sql$Dataset$$anonfun$$$outer()
org/apache/spark/sql/Dataset/anonfun/7/anonfun/apply/2/apply(java.lang.Object)#org/apache/spark/sql/Dataset$$anonfun$7$$anonfun$apply$2$$anonfun$8/8(org.apache.spark.sql.Dataset$$anonfun$7$$anonfun$apply$2)
org/apache/spark/sql/Dataset/anonfun/7/anonfun/apply/2/apply(java.lang.Object)#scala/collection/mutable/ArrayOps/mkString(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/Dataset/anonfun/7/anonfun/apply/2/apply(java.lang.Object)#scala/collection/Seq/mkString(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/Dataset/anonfun/7/anonfun/apply/2/apply(java.lang.Object)#scala/collection/mutable/StringBuilder/toString()
org/apache/spark/sql/execution/datasources/csv/CSVDataSource/makeSafeHeader(java.lang.String[],boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVDataSource$$anonfun$makeSafeHeader$2/2(org.apache.spark.sql.execution.datasources.csv.CSVDataSource)
org/apache/spark/sql/execution/datasources/csv/CSVDataSource/makeSafeHeader(java.lang.String[],boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/collection/mutable/ArrayOps/map(scala.Function1,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/csv/CSVDataSource/makeSafeHeader(java.lang.String[],boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVDataSource$$anonfun$makeSafeHeader$1/1(org.apache.spark.sql.execution.datasources.csv.CSVDataSource,boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions,java.lang.String[])
org/apache/spark/sql/execution/datasources/csv/CSVDataSource/makeSafeHeader(java.lang.String[],boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/collection/mutable/ArrayOps/zipWithIndex(scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/csv/CSVDataSource/makeSafeHeader(java.lang.String[],boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/collection/mutable/ArrayOps/distinct()
org/apache/spark/sql/execution/datasources/csv/CSVDataSource/makeSafeHeader(java.lang.String[],boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/datasources/csv/CSVDataSource/makeSafeHeader(java.lang.String[],boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/csv/CSVDataSource/makeSafeHeader(java.lang.String[],boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/csv/CSVDataSource/makeSafeHeader(java.lang.String[],boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVDataSource$$anonfun$2/2(org.apache.spark.sql.execution.datasources.csv.CSVDataSource,boolean)
org/apache/spark/sql/execution/datasources/csv/CSVDataSource/makeSafeHeader(java.lang.String[],boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/collection/mutable/ArrayOps/diff(scala.collection.GenSeq)
org/apache/spark/sql/execution/datasources/csv/CSVDataSource/makeSafeHeader(java.lang.String[],boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/csv/CSVDataSource/makeSafeHeader(java.lang.String[],boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#org/apache/spark/sql/execution/datasources/csv/CSVDataSource$$anonfun$1/1(org.apache.spark.sql.execution.datasources.csv.CSVDataSource)
org/apache/spark/sql/execution/datasources/csv/CSVDataSource/makeSafeHeader(java.lang.String[],boolean,org.apache.spark.sql.execution.datasources.csv.CSVOptions)#scala/collection/mutable/ArrayOps/filter(scala.Function1)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/2/apply(scala.Tuple2)#scala/Predef$/augmentString(java.lang.String)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/2/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/2/apply(scala.Tuple2)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/2/apply(scala.Tuple2)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/2/apply(scala.Tuple2)#org/apache/spark/sql/catalyst/expressions/codegen/CodegenContext/addNewFunction(java.lang.String,java.lang.String)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/2/apply(scala.Tuple2)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/2/apply(scala.Tuple2)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/2/apply(scala.Tuple2)#scala/MatchError/MatchError(java.lang.Object)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/2/apply(scala.Tuple2)#scala/collection/immutable/StringOps/StringOps(java.lang.String)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/2/apply(scala.Tuple2)#scala/Tuple2/_2$mcI$sp()
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/2/apply(scala.Tuple2)#scala/collection/immutable/StringOps/stripMargin()
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/2/apply(scala.Tuple2)#scala/collection/Seq/mkString(java.lang.String)
org/apache/spark/sql/execution/columnar/GenerateColumnAccessor/anonfun/2/apply(scala.Tuple2)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/processInputs()#org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/groupingAttributes()
org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/processInputs()#scala/collection/Seq/isEmpty()
org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/processInputs()#org/apache/spark/sql/catalyst/expressions/UnsafeProjection/apply(org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/processInputs()#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/processInputs()#scala/collection/Iterator/hasNext()
org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/processInputs()#org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator$$anonfun$processInputs$1/1(org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator)
org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/processInputs()#org/apache/spark/sql/catalyst/expressions/package$Projection/apply(java.lang.Object)
org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/processInputs()#org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/logInfo(scala.Function0)
org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/processInputs()#org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/groupingProjection()
org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/processInputs()#org/apache/spark/sql/types/StructType$/fromAttributes(scala.collection.Seq)
org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/processInputs()#scala/collection/Iterator/next()
org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/processInputs()#org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/aggregateFunctions()
org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/processInputs()#scala/Function2/apply(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/processInputs()#org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/processRow()
org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/processInputs()#org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator$$anonfun$processInputs$2/2(org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator)
org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/initAggregationBuffer(org.apache.spark.sql.catalyst.expressions.SpecificInternalRow)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/initAggregationBuffer(org.apache.spark.sql.catalyst.expressions.SpecificInternalRow)#scala/Array$/canBuildFrom(scala.reflect.ClassTag)
org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/initAggregationBuffer(org.apache.spark.sql.catalyst.expressions.SpecificInternalRow)#org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/aggregateFunctions()
org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/initAggregationBuffer(org.apache.spark.sql.catalyst.expressions.SpecificInternalRow)#org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator$$anonfun$initAggregationBuffer$1/1(org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator)
org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/initAggregationBuffer(org.apache.spark.sql.catalyst.expressions.SpecificInternalRow)#org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator$$anonfun$initAggregationBuffer$2/2(org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator,org.apache.spark.sql.catalyst.expressions.SpecificInternalRow)
org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/initAggregationBuffer(org.apache.spark.sql.catalyst.expressions.SpecificInternalRow)#org/apache/spark/sql/catalyst/expressions/package$MutableProjection/target(org.apache.spark.sql.catalyst.InternalRow)
org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/initAggregationBuffer(org.apache.spark.sql.catalyst.expressions.SpecificInternalRow)#scala/Predef$/refArrayOps(java.lang.Object[])
org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/initAggregationBuffer(org.apache.spark.sql.catalyst.expressions.SpecificInternalRow)#org/apache/spark/sql/catalyst/expressions/package$MutableProjection/apply(java.lang.Object)
org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/initAggregationBuffer(org.apache.spark.sql.catalyst.expressions.SpecificInternalRow)#scala/collection/mutable/ArrayOps/foreach(scala.Function1)
org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/initAggregationBuffer(org.apache.spark.sql.catalyst.expressions.SpecificInternalRow)#org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/expressionAggInitialProjection()
org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/initAggregationBuffer(org.apache.spark.sql.catalyst.expressions.SpecificInternalRow)#org/apache/spark/sql/catalyst/expressions/package$/EmptyRow()
org/apache/spark/sql/execution/aggregate/ObjectAggregationIterator/initAggregationBuffer(org.apache.spark.sql.catalyst.expressions.SpecificInternalRow)#scala/collection/mutable/ArrayOps/collect(scala.PartialFunction,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/datasources/SQLHadoopMapReduceCommitProtocol/setupCommitter(org.apache.hadoop.mapreduce.TaskAttemptContext)#java/lang/Class/isAssignableFrom(java.lang.Class)
org/apache/spark/sql/execution/datasources/SQLHadoopMapReduceCommitProtocol/setupCommitter(org.apache.hadoop.mapreduce.TaskAttemptContext)#java/lang/reflect/Constructor/newInstance(java.lang.Object[])
org/apache/spark/sql/execution/datasources/SQLHadoopMapReduceCommitProtocol/setupCommitter(org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/hadoop/mapreduce/TaskAttemptContext/getConfiguration()
org/apache/spark/sql/execution/datasources/SQLHadoopMapReduceCommitProtocol/setupCommitter(org.apache.hadoop.mapreduce.TaskAttemptContext)#java/lang/Class/newInstance()
org/apache/spark/sql/execution/datasources/SQLHadoopMapReduceCommitProtocol/setupCommitter(org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/hadoop/mapreduce/TaskAttemptContext/getOutputFormatClass()
org/apache/spark/sql/execution/datasources/SQLHadoopMapReduceCommitProtocol/setupCommitter(org.apache.hadoop.mapreduce.TaskAttemptContext)#java/lang/Class/getDeclaredConstructor(java.lang.Class[])
org/apache/spark/sql/execution/datasources/SQLHadoopMapReduceCommitProtocol/setupCommitter(org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/spark/sql/execution/datasources/SQLHadoopMapReduceCommitProtocol$$anonfun$setupCommitter$1/1(org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol,java.lang.Class)
org/apache/spark/sql/execution/datasources/SQLHadoopMapReduceCommitProtocol/setupCommitter(org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/spark/sql/internal/SQLConf$/OUTPUT_COMMITTER_CLASS()
org/apache/spark/sql/execution/datasources/SQLHadoopMapReduceCommitProtocol/setupCommitter(org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/spark/internal/config/OptionalConfigEntry/key()
org/apache/spark/sql/execution/datasources/SQLHadoopMapReduceCommitProtocol/setupCommitter(org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/spark/sql/execution/datasources/SQLHadoopMapReduceCommitProtocol/logInfo(scala.Function0)
org/apache/spark/sql/execution/datasources/SQLHadoopMapReduceCommitProtocol/setupCommitter(org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/hadoop/conf/Configuration/getClass(java.lang.String,java.lang.Class,java.lang.Class)
org/apache/spark/sql/execution/datasources/SQLHadoopMapReduceCommitProtocol/setupCommitter(org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/hadoop/fs/Path/Path(java.lang.String)
org/apache/spark/sql/execution/datasources/SQLHadoopMapReduceCommitProtocol/setupCommitter(org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/hadoop/mapreduce/OutputFormat/getOutputCommitter(org.apache.hadoop.mapreduce.TaskAttemptContext)
org/apache/spark/sql/execution/datasources/SQLHadoopMapReduceCommitProtocol/setupCommitter(org.apache.hadoop.mapreduce.TaskAttemptContext)#scala/runtime/ObjectRef/create(java.lang.Object)
org/apache/spark/sql/execution/datasources/SQLHadoopMapReduceCommitProtocol/setupCommitter(org.apache.hadoop.mapreduce.TaskAttemptContext)#org/apache/spark/sql/execution/datasources/SQLHadoopMapReduceCommitProtocol$$anonfun$setupCommitter$2/2(org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol,scala.runtime.ObjectRef)
org/apache/spark/sql/execution/python/ExtractPythonUDFFromAggregate/anonfun/org/apache/spark/sql/execution/python/ExtractPythonUDFFromAggregate/extract/1/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#scala/collection/mutable/ArrayBuffer/$plus$eq(java.lang.Object)
org/apache/spark/sql/execution/python/ExtractPythonUDFFromAggregate/anonfun/org/apache/spark/sql/execution/python/ExtractPythonUDFFromAggregate/extract/1/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/expressions/Alias/Alias(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String,org.apache.spark.sql.catalyst.expressions.ExprId,scala.Option,scala.Option,java.lang.Boolean)
org/apache/spark/sql/execution/python/ExtractPythonUDFFromAggregate/anonfun/org/apache/spark/sql/execution/python/ExtractPythonUDFFromAggregate/extract/1/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/execution/python/ExtractPythonUDFFromAggregate$/org$apache$spark$sql$execution$python$ExtractPythonUDFFromAggregate$$belongAggregate(org.apache.spark.sql.catalyst.expressions.Expression,org.apache.spark.sql.catalyst.plans.logical.Aggregate)
org/apache/spark/sql/execution/python/ExtractPythonUDFFromAggregate/anonfun/org/apache/spark/sql/execution/python/ExtractPythonUDFFromAggregate/extract/1/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$6(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)
org/apache/spark/sql/execution/python/ExtractPythonUDFFromAggregate/anonfun/org/apache/spark/sql/execution/python/ExtractPythonUDFFromAggregate/extract/1/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$5(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)
org/apache/spark/sql/execution/python/ExtractPythonUDFFromAggregate/anonfun/org/apache/spark/sql/execution/python/ExtractPythonUDFFromAggregate/extract/1/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$4(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)
org/apache/spark/sql/execution/python/ExtractPythonUDFFromAggregate/anonfun/org/apache/spark/sql/execution/python/ExtractPythonUDFFromAggregate/extract/1/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/expressions/Alias$/apply$default$3(org.apache.spark.sql.catalyst.expressions.Expression,java.lang.String)
org/apache/spark/sql/execution/python/ExtractPythonUDFFromAggregate/anonfun/org/apache/spark/sql/execution/python/ExtractPythonUDFFromAggregate/extract/1/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#scala/Function1/apply(java.lang.Object)
org/apache/spark/sql/execution/python/ExtractPythonUDFFromAggregate/anonfun/org/apache/spark/sql/execution/python/ExtractPythonUDFFromAggregate/extract/1/anonfun/1/applyOrElse(org.apache.spark.sql.catalyst.expressions.Expression,scala.Function1)#org/apache/spark/sql/catalyst/expressions/NamedExpression/toAttribute()
org/apache/spark/sql/execution/datasources/text/TextFileFormat/verifySchema(org.apache.spark.sql.types.StructType)#java/lang/Object/equals(java.lang.Object)
org/apache/spark/sql/execution/datasources/text/TextFileFormat/verifySchema(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/AnalysisException/AnalysisException(java.lang.String,scala.Option,scala.Option,scala.Option,scala.Option)
org/apache/spark/sql/execution/datasources/text/TextFileFormat/verifySchema(org.apache.spark.sql.types.StructType)#scala/Predef$/wrapRefArray(java.lang.Object[])
org/apache/spark/sql/execution/datasources/text/TextFileFormat/verifySchema(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/types/StructType/apply(int)
org/apache/spark/sql/execution/datasources/text/TextFileFormat/verifySchema(org.apache.spark.sql.types.StructType)#scala/StringContext/StringContext(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/text/TextFileFormat/verifySchema(org.apache.spark.sql.types.StructType)#scala/runtime/BoxesRunTime/boxToInteger(int)
org/apache/spark/sql/execution/datasources/text/TextFileFormat/verifySchema(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$3()
org/apache/spark/sql/execution/datasources/text/TextFileFormat/verifySchema(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$4()
org/apache/spark/sql/execution/datasources/text/TextFileFormat/verifySchema(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$5()
org/apache/spark/sql/execution/datasources/text/TextFileFormat/verifySchema(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/types/StructType/size()
org/apache/spark/sql/execution/datasources/text/TextFileFormat/verifySchema(org.apache.spark.sql.types.StructType)#scala/StringContext/s(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/text/TextFileFormat/verifySchema(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/types/StructField/dataType()
org/apache/spark/sql/execution/datasources/text/TextFileFormat/verifySchema(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/types/DataType/simpleString()
org/apache/spark/sql/execution/datasources/text/TextFileFormat/verifySchema(org.apache.spark.sql.types.StructType)#org/apache/spark/sql/AnalysisException$/$lessinit$greater$default$2()
org/apache/spark/sql/execution/datasources/text/TextFileFormat/verifySchema(org.apache.spark.sql.types.StructType)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/DescribeDatabaseCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)
org/apache/spark/sql/execution/command/DescribeDatabaseCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogDatabase/properties()
org/apache/spark/sql/execution/command/DescribeDatabaseCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/SessionCatalog/getDatabaseMetadata(java.lang.String)
org/apache/spark/sql/execution/command/DescribeDatabaseCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/immutable/Map/toSeq()
org/apache/spark/sql/execution/command/DescribeDatabaseCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/Row$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/command/DescribeDatabaseCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/immutable/List/$colon$colon(java.lang.Object)
org/apache/spark/sql/execution/command/DescribeDatabaseCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogDatabase/description()
org/apache/spark/sql/execution/command/DescribeDatabaseCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/Seq/mkString(java.lang.String,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/command/DescribeDatabaseCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogUtils$/URIToString(java.net.URI)
org/apache/spark/sql/execution/command/DescribeDatabaseCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/immutable/List$/canBuildFrom()
org/apache/spark/sql/execution/command/DescribeDatabaseCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogDatabase/name()
org/apache/spark/sql/execution/command/DescribeDatabaseCommand/run(org.apache.spark.sql.SparkSession)#scala/Predef$/genericWrapArray(java.lang.Object)
org/apache/spark/sql/execution/command/DescribeDatabaseCommand/run(org.apache.spark.sql.SparkSession)#org/apache/spark/sql/catalyst/catalog/CatalogDatabase/locationUri()
org/apache/spark/sql/execution/command/DescribeDatabaseCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/immutable/Map/isEmpty()
org/apache/spark/sql/execution/command/DescribeDatabaseCommand/run(org.apache.spark.sql.SparkSession)#scala/collection/immutable/List/$colon$plus(java.lang.Object,scala.collection.generic.CanBuildFrom)
org/apache/spark/sql/execution/streaming/SinkFileStatus/apply(org.apache.hadoop.fs.FileStatus)#org/apache/spark/sql/execution/streaming/FileStreamSinkLog$/ADD_ACTION()
org/apache/spark/sql/execution/streaming/SinkFileStatus/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/FileStatus/getReplication()
org/apache/spark/sql/execution/streaming/SinkFileStatus/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/FileStatus/getPath()
org/apache/spark/sql/execution/streaming/SinkFileStatus/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/FileStatus/getBlockSize()
org/apache/spark/sql/execution/streaming/SinkFileStatus/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/FileStatus/getModificationTime()
org/apache/spark/sql/execution/streaming/SinkFileStatus/apply(org.apache.hadoop.fs.FileStatus)#java/net/URI/toString()
org/apache/spark/sql/execution/streaming/SinkFileStatus/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/Path/toUri()
org/apache/spark/sql/execution/streaming/SinkFileStatus/apply(org.apache.hadoop.fs.FileStatus)#org/apache/spark/sql/execution/streaming/SinkFileStatus$/apply(org.apache.hadoop.fs.FileStatus)
org/apache/spark/sql/execution/streaming/SinkFileStatus/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/FileStatus/isDirectory()
org/apache/spark/sql/execution/streaming/SinkFileStatus/apply(org.apache.hadoop.fs.FileStatus)#org/apache/hadoop/fs/FileStatus/getLen()
org/apache/spark/sql/execution/SparkStrategies/SpecialLimits/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/expressions/IntegerLiteral$/unapply(java.lang.Object)
org/apache/spark/sql/execution/SparkStrategies/SpecialLimits/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Project/projectList()
org/apache/spark/sql/execution/SparkStrategies/SpecialLimits/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Tuple2/_2()
org/apache/spark/sql/execution/SparkStrategies/SpecialLimits/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Option/isEmpty()
org/apache/spark/sql/execution/SparkStrategies/SpecialLimits/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/ReturnAnswer/child()
org/apache/spark/sql/execution/SparkStrategies/SpecialLimits/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Sort/order()
org/apache/spark/sql/execution/SparkStrategies/SpecialLimits/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/LogicalPlan/output()
org/apache/spark/sql/execution/SparkStrategies/SpecialLimits/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/execution/SparkStrategies$SpecialLimits$/planLater(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)
org/apache/spark/sql/execution/SparkStrategies/SpecialLimits/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Sort/global()
org/apache/spark/sql/execution/SparkStrategies/SpecialLimits/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/runtime/BoxesRunTime/unboxToInt(java.lang.Object)
org/apache/spark/sql/execution/SparkStrategies/SpecialLimits/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Sort/child()
org/apache/spark/sql/execution/SparkStrategies/SpecialLimits/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Option/get()
org/apache/spark/sql/execution/SparkStrategies/SpecialLimits/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/collection/immutable/Nil$/$colon$colon(java.lang.Object)
org/apache/spark/sql/execution/SparkStrategies/SpecialLimits/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Limit$/unapply(org.apache.spark.sql.catalyst.plans.logical.GlobalLimit)
org/apache/spark/sql/execution/SparkStrategies/SpecialLimits/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#scala/Tuple2/_1()
org/apache/spark/sql/execution/SparkStrategies/SpecialLimits/apply(org.apache.spark.sql.catalyst.plans.logical.LogicalPlan)#org/apache/spark/sql/catalyst/plans/logical/Project/child()
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/runBatch/2/anonfun/apply/7/apply(scala.Tuple2)#scala/Predef$/ArrowAssoc(java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/runBatch/2/anonfun/apply/7/apply(scala.Tuple2)#scala/Tuple2/_1()
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/runBatch/2/anonfun/apply/7/apply(scala.Tuple2)#scala/Tuple2/_2()
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/runBatch/2/anonfun/apply/7/apply(scala.Tuple2)#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch$2$$anonfun$apply$7$$anonfun$apply$8/8(org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch$2$$anonfun$apply$7,org.apache.spark.sql.execution.streaming.Offset)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/runBatch/2/anonfun/apply/7/apply(scala.Tuple2)#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch$2/org$apache$spark$sql$execution$streaming$StreamExecution$$anonfun$$$outer()
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/runBatch/2/anonfun/apply/7/apply(scala.Tuple2)#scala/Option/map(scala.Function1)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/runBatch/2/anonfun/apply/7/apply(scala.Tuple2)#scala/Predef$ArrowAssoc$/$minus$greater$extension(java.lang.Object,java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/runBatch/2/anonfun/apply/7/apply(scala.Tuple2)#scala/runtime/BoxesRunTime/unboxToBoolean(java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/runBatch/2/anonfun/apply/7/apply(scala.Tuple2)#scala/Some/Some(java.lang.Object)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/runBatch/2/anonfun/apply/7/apply(scala.Tuple2)#scala/Option/getOrElse(scala.Function0)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/runBatch/2/anonfun/apply/7/apply(scala.Tuple2)#scala/Option$/option2Iterable(scala.Option)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/runBatch/2/anonfun/apply/7/apply(scala.Tuple2)#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch$2$$anonfun$apply$7$$anonfun$apply$3/3(org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch$2$$anonfun$apply$7)
org/apache/spark/sql/execution/streaming/StreamExecution/anonfun/org/apache/spark/sql/execution/streaming/StreamExecution/runBatch/2/anonfun/apply/7/apply(scala.Tuple2)#org/apache/spark/sql/execution/streaming/StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch$2$$anonfun$apply$7$$anonfun$apply$9/9(org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$org$apache$spark$sql$execution$streaming$StreamExecution$$runBatch$2$$anonfun$apply$7,org.apache.spark.sql.execution.streaming.Source,org.apache.spark.sql.execution.streaming.Offset,scala.Option)
org/apache/spark/sql/execution/datasources/json/JsonFileFormat/buildReader(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/types/StructType/getFieldIndex(java.lang.String)
org/apache/spark/sql/execution/datasources/json/JsonFileFormat/buildReader(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/catalyst/json/JSONOptions/columnNameOfCorruptRecord()
org/apache/spark/sql/execution/datasources/json/JsonFileFormat/buildReader(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/types/StructType$/apply(scala.collection.Seq)
org/apache/spark/sql/execution/datasources/json/JsonFileFormat/buildReader(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/util/SerializableConfiguration/SerializableConfiguration(org.apache.hadoop.conf.Configuration)
org/apache/spark/sql/execution/datasources/json/JsonFileFormat/buildReader(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/types/StructType/filterNot(scala.Function1)
org/apache/spark/sql/execution/datasources/json/JsonFileFormat/buildReader(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#scala/reflect/ClassTag$/apply(java.lang.Class)
org/apache/spark/sql/execution/datasources/json/JsonFileFormat/buildReader(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/datasources/json/JsonFileFormat$$anonfun$buildReader$2/2(org.apache.spark.sql.execution.datasources.json.JsonFileFormat,org.apache.spark.sql.types.StructType,org.apache.spark.broadcast.Broadcast,org.apache.spark.sql.catalyst.json.JSONOptions,org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/datasources/json/JsonFileFormat/buildReader(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#scala/Option/foreach(scala.Function1)
org/apache/spark/sql/execution/datasources/json/JsonFileFormat/buildReader(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/datasources/json/JsonFileFormat$$anonfun$1/1(org.apache.spark.sql.execution.datasources.json.JsonFileFormat,org.apache.spark.sql.catalyst.json.JSONOptions)
org/apache/spark/sql/execution/datasources/json/JsonFileFormat/buildReader(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/SparkContext/broadcast(java.lang.Object,scala.reflect.ClassTag)
org/apache/spark/sql/execution/datasources/json/JsonFileFormat/buildReader(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/internal/SQLConf/sessionLocalTimeZone()
org/apache/spark/sql/execution/datasources/json/JsonFileFormat/buildReader(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/execution/datasources/json/JsonFileFormat$$anonfun$buildReader$1/1(org.apache.spark.sql.execution.datasources.json.JsonFileFormat,org.apache.spark.sql.types.StructType)
org/apache/spark/sql/execution/datasources/json/JsonFileFormat/buildReader(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/catalyst/json/JSONOptions/JSONOptions(scala.collection.immutable.Map,java.lang.String,java.lang.String)
org/apache/spark/sql/execution/datasources/json/JsonFileFormat/buildReader(org.apache.spark.sql.SparkSession,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,org.apache.spark.sql.types.StructType,scala.collection.Seq,scala.collection.immutable.Map,org.apache.hadoop.conf.Configuration)#org/apache/spark/sql/internal/SQLConf/columnNameOfCorruptRecord()
