org/apache/parquet/hadoop/codec/SnappyCompressor/setInput(byte[],int,int)#java/nio/ByteBuffer/position()
org/apache/parquet/hadoop/codec/SnappyCompressor/setInput(byte[],int,int)#java/nio/ByteBuffer/put(java.nio.ByteBuffer)
org/apache/parquet/hadoop/codec/SnappyCompressor/setInput(byte[],int,int)#org/apache/parquet/Preconditions/checkArgument(boolean,java.lang.String)
org/apache/parquet/hadoop/codec/SnappyCompressor/setInput(byte[],int,int)#java/nio/ByteBuffer/capacity()
org/apache/parquet/hadoop/codec/SnappyCompressor/setInput(byte[],int,int)#java/nio/ByteBuffer/rewind()
org/apache/parquet/hadoop/codec/SnappyCompressor/setInput(byte[],int,int)#java/nio/ByteBuffer/hasRemaining()
org/apache/parquet/hadoop/codec/SnappyCompressor/setInput(byte[],int,int)#java/nio/ByteBuffer/allocateDirect(int)
org/apache/parquet/hadoop/codec/SnappyCompressor/setInput(byte[],int,int)#java/nio/ByteBuffer/put(byte[],int,int)
org/apache/parquet/hadoop/codec/SnappyCompressor/setInput(byte[],int,int)#java/nio/ByteBuffer/limit(int)
org/apache/parquet/hadoop/codec/SnappyCompressor/compress(byte[],int,int)#java/nio/ByteBuffer/position()
org/apache/parquet/hadoop/codec/SnappyCompressor/compress(byte[],int,int)#org/xerial/snappy/Snappy/maxCompressedLength(int)
org/apache/parquet/hadoop/codec/SnappyCompressor/compress(byte[],int,int)#java/nio/ByteBuffer/rewind()
org/apache/parquet/hadoop/codec/SnappyCompressor/compress(byte[],int,int)#java/nio/ByteBuffer/allocateDirect(int)
org/apache/parquet/hadoop/codec/SnappyCompressor/compress(byte[],int,int)#java/lang/Math/min(int,int)
org/apache/parquet/hadoop/codec/SnappyCompressor/compress(byte[],int,int)#java/nio/ByteBuffer/limit(int)
org/apache/parquet/hadoop/codec/SnappyCompressor/compress(byte[],int,int)#org/xerial/snappy/Snappy/compress(java.nio.ByteBuffer,java.nio.ByteBuffer)
org/apache/parquet/hadoop/codec/SnappyCompressor/compress(byte[],int,int)#java/nio/ByteBuffer/get(byte[],int,int)
org/apache/parquet/hadoop/codec/SnappyCompressor/compress(byte[],int,int)#java/nio/ByteBuffer/clear()
org/apache/parquet/hadoop/codec/SnappyCompressor/compress(byte[],int,int)#java/nio/ByteBuffer/capacity()
org/apache/parquet/hadoop/codec/SnappyCompressor/compress(byte[],int,int)#java/nio/ByteBuffer/remaining()
org/apache/parquet/hadoop/codec/SnappyCompressor/compress(byte[],int,int)#java/nio/ByteBuffer/hasRemaining()
org/apache/parquet/hadoop/codec/SnappyCompressor/compress(byte[],int,int)#java/nio/ByteBuffer/position(int)
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/generateSplitInfo(java.util.List,org.apache.hadoop.fs.BlockLocation[],long,long)#java/util/ArrayList/ArrayList()
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/generateSplitInfo(java.util.List,org.apache.hadoop.fs.BlockLocation[],long,long)#org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy$HDFSBlocks/getCurrentBlock()
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/generateSplitInfo(java.util.List,org.apache.hadoop.fs.BlockLocation[],long,long)#java/util/Iterator/hasNext()
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/generateSplitInfo(java.util.List,org.apache.hadoop.fs.BlockLocation[],long,long)#org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy$SplitInfo/SplitInfo(org.apache.hadoop.fs.BlockLocation)
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/generateSplitInfo(java.util.List,org.apache.hadoop.fs.BlockLocation[],long,long)#java/util/Iterator/next()
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/generateSplitInfo(java.util.List,org.apache.hadoop.fs.BlockLocation[],long,long)#java/lang/StringBuilder/append(long)
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/generateSplitInfo(java.util.List,org.apache.hadoop.fs.BlockLocation[],long,long)#org/apache/parquet/io/ParquetDecodingException/ParquetDecodingException(java.lang.String)
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/generateSplitInfo(java.util.List,org.apache.hadoop.fs.BlockLocation[],long,long)#org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy$SplitInfo/getRowGroupCount()
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/generateSplitInfo(java.util.List,org.apache.hadoop.fs.BlockLocation[],long,long)#java/util/List/get(int)
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/generateSplitInfo(java.util.List,org.apache.hadoop.fs.BlockLocation[],long,long)#org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy$SplitInfo/access$200(org.apache.parquet.hadoop.ClientSideMetadataSplitStrategy$SplitInfo,org.apache.parquet.hadoop.metadata.BlockMetaData)
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/generateSplitInfo(java.util.List,org.apache.hadoop.fs.BlockLocation[],long,long)#java/util/List/iterator()
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/generateSplitInfo(java.util.List,org.apache.hadoop.fs.BlockLocation[],long,long)#java/lang/StringBuilder/toString()
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/generateSplitInfo(java.util.List,org.apache.hadoop.fs.BlockLocation[],long,long)#org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy$HDFSBlocks/access$100(org.apache.parquet.hadoop.ClientSideMetadataSplitStrategy$HDFSBlocks,org.apache.parquet.hadoop.metadata.BlockMetaData)
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/generateSplitInfo(java.util.List,org.apache.hadoop.fs.BlockLocation[],long,long)#java/lang/StringBuilder/append(java.lang.String)
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/generateSplitInfo(java.util.List,org.apache.hadoop.fs.BlockLocation[],long,long)#java/lang/StringBuilder/StringBuilder()
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/generateSplitInfo(java.util.List,org.apache.hadoop.fs.BlockLocation[],long,long)#org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy$SplitInfo/getCompressedByteSize()
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/generateSplitInfo(java.util.List,org.apache.hadoop.fs.BlockLocation[],long,long)#java/util/List/add(java.lang.Object)
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/generateSplitInfo(java.util.List,org.apache.hadoop.fs.BlockLocation[],long,long)#org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy$HDFSBlocks/HDFSBlocks(org.apache.hadoop.fs.BlockLocation[],org.apache.parquet.hadoop.ClientSideMetadataSplitStrategy$1)
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/getSplits(org.apache.hadoop.conf.Configuration,java.util.List,long,long,org.apache.parquet.hadoop.api.ReadSupport$ReadContext)#org/apache/hadoop/fs/FileSystem/getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/getSplits(org.apache.hadoop.conf.Configuration,java.util.List,long,long,org.apache.parquet.hadoop.api.ReadSupport$ReadContext)#java/util/Iterator/hasNext()
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/getSplits(org.apache.hadoop.conf.Configuration,java.util.List,long,long,org.apache.parquet.hadoop.api.ReadSupport$ReadContext)#org/apache/hadoop/fs/FileSystem/getFileStatus(org.apache.hadoop.fs.Path)
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/getSplits(org.apache.hadoop.conf.Configuration,java.util.List,long,long,org.apache.parquet.hadoop.api.ReadSupport$ReadContext)#java/util/List/size()
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/getSplits(org.apache.hadoop.conf.Configuration,java.util.List,long,long,org.apache.parquet.hadoop.api.ReadSupport$ReadContext)#java/util/List/addAll(java.util.Collection)
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/getSplits(org.apache.hadoop.conf.Configuration,java.util.List,long,long,org.apache.parquet.hadoop.api.ReadSupport$ReadContext)#java/util/Iterator/next()
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/getSplits(org.apache.hadoop.conf.Configuration,java.util.List,long,long,org.apache.parquet.hadoop.api.ReadSupport$ReadContext)#java/lang/Integer/valueOf(int)
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/getSplits(org.apache.hadoop.conf.Configuration,java.util.List,long,long,org.apache.parquet.hadoop.api.ReadSupport$ReadContext)#org/apache/parquet/hadoop/api/ReadSupport$ReadContext/getRequestedSchema()
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/getSplits(org.apache.hadoop.conf.Configuration,java.util.List,long,long,org.apache.parquet.hadoop.api.ReadSupport$ReadContext)#org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/getSplits(org.apache.hadoop.conf.Configuration,java.util.List,long,long,org.apache.parquet.hadoop.api.ReadSupport$ReadContext)#java/util/ArrayList/ArrayList()
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/getSplits(org.apache.hadoop.conf.Configuration,java.util.List,long,long,org.apache.parquet.hadoop.api.ReadSupport$ReadContext)#java/lang/Long/valueOf(long)
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/getSplits(org.apache.hadoop.conf.Configuration,java.util.List,long,long,org.apache.parquet.hadoop.api.ReadSupport$ReadContext)#org/slf4j/Logger/debug(java.lang.String,java.lang.Object)
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/getSplits(org.apache.hadoop.conf.Configuration,java.util.List,long,long,org.apache.parquet.hadoop.api.ReadSupport$ReadContext)#java/util/List/isEmpty()
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/getSplits(org.apache.hadoop.conf.Configuration,java.util.List,long,long,org.apache.parquet.hadoop.api.ReadSupport$ReadContext)#org/apache/hadoop/fs/FileStatus/getLen()
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/getSplits(org.apache.hadoop.conf.Configuration,java.util.List,long,long,org.apache.parquet.hadoop.api.ReadSupport$ReadContext)#org/slf4j/Logger/info(java.lang.String,java.lang.Object,java.lang.Object)
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/getSplits(org.apache.hadoop.conf.Configuration,java.util.List,long,long,org.apache.parquet.hadoop.api.ReadSupport$ReadContext)#org/slf4j/Logger/info(java.lang.String)
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/getSplits(org.apache.hadoop.conf.Configuration,java.util.List,long,long,org.apache.parquet.hadoop.api.ReadSupport$ReadContext)#org/apache/parquet/schema/MessageType/toString()
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/getSplits(org.apache.hadoop.conf.Configuration,java.util.List,long,long,org.apache.parquet.hadoop.api.ReadSupport$ReadContext)#java/util/List/iterator()
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/getSplits(org.apache.hadoop.conf.Configuration,java.util.List,long,long,org.apache.parquet.hadoop.api.ReadSupport$ReadContext)#org/apache/parquet/hadoop/api/ReadSupport$ReadContext/getReadSupportMetadata()
org/apache/parquet/hadoop/ParquetInputFormat/getFooters(org.apache.hadoop.mapreduce.JobContext)#java/util/Map/put(java.lang.Object,java.lang.Object)
org/apache/parquet/hadoop/ParquetInputFormat/getFooters(org.apache.hadoop.mapreduce.JobContext)#java/util/ArrayList/ArrayList(int)
org/apache/parquet/hadoop/ParquetInputFormat/getFooters(org.apache.hadoop.mapreduce.JobContext)#org/apache/parquet/hadoop/ParquetInputFormat$FootersCacheValue/getFooter()
org/apache/parquet/hadoop/ParquetInputFormat/getFooters(org.apache.hadoop.mapreduce.JobContext)#java/util/Iterator/hasNext()
org/apache/parquet/hadoop/ParquetInputFormat/getFooters(org.apache.hadoop.mapreduce.JobContext)#java/lang/Math/max(int,int)
org/apache/parquet/hadoop/ParquetInputFormat/getFooters(org.apache.hadoop.mapreduce.JobContext)#java/util/Collections/emptyList()
org/apache/parquet/hadoop/ParquetInputFormat/getFooters(org.apache.hadoop.mapreduce.JobContext)#java/util/Iterator/next()
org/apache/parquet/hadoop/ParquetInputFormat/getFooters(org.apache.hadoop.mapreduce.JobContext)#java/util/List/add(java.lang.Object)
org/apache/parquet/hadoop/ParquetInputFormat/getFooters(org.apache.hadoop.mapreduce.JobContext)#java/lang/Integer/valueOf(int)
org/apache/parquet/hadoop/ParquetInputFormat/getFooters(org.apache.hadoop.mapreduce.JobContext)#org/apache/parquet/hadoop/ParquetInputFormat$FootersCacheValue/FootersCacheValue(org.apache.parquet.hadoop.ParquetInputFormat$FileStatusWrapper,org.apache.parquet.hadoop.Footer)
org/apache/parquet/hadoop/ParquetInputFormat/getFooters(org.apache.hadoop.mapreduce.JobContext)#java/util/List/iterator()
org/apache/parquet/hadoop/ParquetInputFormat/getFooters(org.apache.hadoop.mapreduce.JobContext)#java/util/LinkedHashMap/LinkedHashMap()
org/apache/parquet/hadoop/ParquetInputFormat/getFooters(org.apache.hadoop.mapreduce.JobContext)#java/lang/StringBuilder/toString()
org/apache/parquet/hadoop/ParquetInputFormat/getFooters(org.apache.hadoop.mapreduce.JobContext)#org/slf4j/Logger/debug(java.lang.String)
org/apache/parquet/hadoop/ParquetInputFormat/getFooters(org.apache.hadoop.mapreduce.JobContext)#org/slf4j/Logger/debug(java.lang.String,java.lang.Object,java.lang.Object)
org/apache/parquet/hadoop/ParquetInputFormat/getFooters(org.apache.hadoop.mapreduce.JobContext)#org/apache/hadoop/fs/FileStatus/getPath()
org/apache/parquet/hadoop/ParquetInputFormat/getFooters(org.apache.hadoop.mapreduce.JobContext)#java/util/Set/size()
org/apache/parquet/hadoop/ParquetInputFormat/getFooters(org.apache.hadoop.mapreduce.JobContext)#java/lang/StringBuilder/append(java.lang.String)
org/apache/parquet/hadoop/ParquetInputFormat/getFooters(org.apache.hadoop.mapreduce.JobContext)#java/util/Map$Entry/getKey()
org/apache/parquet/hadoop/ParquetInputFormat/getFooters(org.apache.hadoop.mapreduce.JobContext)#java/util/List/isEmpty()
org/apache/parquet/hadoop/ParquetInputFormat/getFooters(org.apache.hadoop.mapreduce.JobContext)#java/util/Map/get(java.lang.Object)
org/apache/parquet/hadoop/ParquetInputFormat/getFooters(org.apache.hadoop.mapreduce.JobContext)#java/util/List/size()
org/apache/parquet/hadoop/ParquetInputFormat/getFooters(org.apache.hadoop.mapreduce.JobContext)#java/util/Map/size()
org/apache/parquet/hadoop/ParquetInputFormat/getFooters(org.apache.hadoop.mapreduce.JobContext)#java/util/Set/isEmpty()
org/apache/parquet/hadoop/ParquetInputFormat/getFooters(org.apache.hadoop.mapreduce.JobContext)#java/lang/StringBuilder/StringBuilder()
org/apache/parquet/hadoop/ParquetInputFormat/getFooters(org.apache.hadoop.mapreduce.JobContext)#java/util/HashMap/HashMap(int)
org/apache/parquet/hadoop/ParquetInputFormat/getFooters(org.apache.hadoop.mapreduce.JobContext)#java/util/Set/iterator()
org/apache/parquet/hadoop/ParquetInputFormat/getFooters(org.apache.hadoop.mapreduce.JobContext)#java/lang/StringBuilder/append(java.lang.Object)
org/apache/parquet/hadoop/ParquetInputFormat/getFooters(org.apache.hadoop.mapreduce.JobContext)#java/util/Map$Entry/getValue()
org/apache/parquet/hadoop/ParquetInputFormat/getFooters(org.apache.hadoop.mapreduce.JobContext)#org/apache/parquet/hadoop/ParquetInputFormat$FileStatusWrapper/FileStatusWrapper(org.apache.hadoop.fs.FileStatus)
org/apache/parquet/hadoop/ParquetInputFormat/getFooters(org.apache.hadoop.mapreduce.JobContext)#java/util/HashSet/HashSet()
org/apache/parquet/hadoop/ParquetInputFormat/getFooters(org.apache.hadoop.mapreduce.JobContext)#org/slf4j/Logger/isDebugEnabled()
org/apache/parquet/hadoop/ParquetInputFormat/getFooters(org.apache.hadoop.mapreduce.JobContext)#java/util/Map/entrySet()
org/apache/parquet/hadoop/ParquetInputFormat/getFooters(org.apache.hadoop.mapreduce.JobContext)#java/util/Set/add(java.lang.Object)
org/apache/parquet/hadoop/ParquetInputFormat/getSplits(org.apache.hadoop.mapreduce.JobContext)#org/apache/hadoop/mapreduce/lib/input/FileInputFormat/getSplits(org.apache.hadoop.mapreduce.JobContext)
org/apache/parquet/hadoop/ParquetInputFormat/getSplits(org.apache.hadoop.mapreduce.JobContext)#java/util/ArrayList/ArrayList()
org/apache/parquet/hadoop/ParquetInputFormat/getSplits(org.apache.hadoop.mapreduce.JobContext)#java/util/Iterator/hasNext()
org/apache/parquet/hadoop/ParquetInputFormat/getSplits(org.apache.hadoop.mapreduce.JobContext)#java/lang/StringBuilder/append(java.lang.String)
org/apache/parquet/hadoop/ParquetInputFormat/getSplits(org.apache.hadoop.mapreduce.JobContext)#java/util/List/addAll(java.util.Collection)
org/apache/parquet/hadoop/ParquetInputFormat/getSplits(org.apache.hadoop.mapreduce.JobContext)#java/util/Iterator/next()
org/apache/parquet/hadoop/ParquetInputFormat/getSplits(org.apache.hadoop.mapreduce.JobContext)#org/apache/parquet/Preconditions/checkArgument(boolean,java.lang.String)
org/apache/parquet/hadoop/ParquetInputFormat/getSplits(org.apache.hadoop.mapreduce.JobContext)#java/lang/StringBuilder/StringBuilder()
org/apache/parquet/hadoop/ParquetInputFormat/getSplits(org.apache.hadoop.mapreduce.JobContext)#java/util/List/add(java.lang.Object)
org/apache/parquet/hadoop/ParquetInputFormat/getSplits(org.apache.hadoop.mapreduce.JobContext)#java/lang/StringBuilder/append(java.lang.Object)
org/apache/parquet/hadoop/ParquetInputFormat/getSplits(org.apache.hadoop.mapreduce.JobContext)#java/lang/StringBuilder/toString()
org/apache/parquet/hadoop/ParquetInputFormat/getSplits(org.apache.hadoop.mapreduce.JobContext)#java/util/List/iterator()
org/apache/parquet/hadoop/ParquetInputFormat/getAllFileRecursively(java.util.List,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/fs/FileStatus/getPath()
org/apache/parquet/hadoop/ParquetInputFormat/getAllFileRecursively(java.util.List,org.apache.hadoop.conf.Configuration)#java/util/Iterator/hasNext()
org/apache/parquet/hadoop/ParquetInputFormat/getAllFileRecursively(java.util.List,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/fs/FileStatus/isDir()
org/apache/parquet/hadoop/ParquetInputFormat/getAllFileRecursively(java.util.List,org.apache.hadoop.conf.Configuration)#java/util/List/size()
org/apache/parquet/hadoop/ParquetInputFormat/getAllFileRecursively(java.util.List,org.apache.hadoop.conf.Configuration)#java/util/Iterator/next()
org/apache/parquet/hadoop/ParquetInputFormat/getAllFileRecursively(java.util.List,org.apache.hadoop.conf.Configuration)#java/util/List/add(java.lang.Object)
org/apache/parquet/hadoop/ParquetInputFormat/getAllFileRecursively(java.util.List,org.apache.hadoop.conf.Configuration)#java/lang/Integer/valueOf(int)
org/apache/parquet/hadoop/ParquetInputFormat/getAllFileRecursively(java.util.List,org.apache.hadoop.conf.Configuration)#org/slf4j/Logger/info(java.lang.String,java.lang.Object)
org/apache/parquet/hadoop/ParquetInputFormat/getAllFileRecursively(java.util.List,org.apache.hadoop.conf.Configuration)#java/util/List/iterator()
org/apache/parquet/hadoop/ParquetInputFormat/getAllFileRecursively(java.util.List,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)
org/apache/parquet/hadoop/ParquetInputFormat/getAllFileRecursively(java.util.List,org.apache.hadoop.conf.Configuration)#java/util/ArrayList/ArrayList()
org/apache/parquet/hadoop/ParquetInputFormat/getSplits(org.apache.hadoop.conf.Configuration,java.util.List)#java/lang/StringBuilder/append(java.lang.String)
org/apache/parquet/hadoop/ParquetInputFormat/getSplits(org.apache.hadoop.conf.Configuration,java.util.List)#java/lang/Math/max(long,long)
org/apache/parquet/hadoop/ParquetInputFormat/getSplits(org.apache.hadoop.conf.Configuration,java.util.List)#java/lang/StringBuilder/append(long)
org/apache/parquet/hadoop/ParquetInputFormat/getSplits(org.apache.hadoop.conf.Configuration,java.util.List)#java/lang/StringBuilder/StringBuilder()
org/apache/parquet/hadoop/ParquetInputFormat/getSplits(org.apache.hadoop.conf.Configuration,java.util.List)#org/apache/parquet/io/ParquetDecodingException/ParquetDecodingException(java.lang.String)
org/apache/parquet/hadoop/ParquetInputFormat/getSplits(org.apache.hadoop.conf.Configuration,java.util.List)#java/lang/StringBuilder/toString()
org/apache/parquet/hadoop/ParquetInputFormat/getSplits(org.apache.hadoop.conf.Configuration,java.util.List)#org/apache/hadoop/conf/Configuration/getLong(java.lang.String,long)
org/apache/parquet/hadoop/ParquetInputFormat/getSplits(org.apache.hadoop.conf.Configuration,java.util.List)#org/apache/parquet/hadoop/ParquetInputFormat/getFormatMinSplitSize()
org/apache/parquet/hadoop/ParquetInputFormat/getSplits(org.apache.hadoop.conf.Configuration,java.util.List)#org/apache/hadoop/conf/Configuration/getBoolean(java.lang.String,boolean)
org/apache/parquet/hadoop/codec/SnappyDecompressor/decompress(byte[],int,int)#java/nio/ByteBuffer/position()
org/apache/parquet/hadoop/codec/SnappyDecompressor/decompress(byte[],int,int)#java/nio/ByteBuffer/get(byte[],int,int)
org/apache/parquet/hadoop/codec/SnappyDecompressor/decompress(byte[],int,int)#org/xerial/snappy/Snappy/uncompressedLength(java.nio.ByteBuffer)
org/apache/parquet/hadoop/codec/SnappyDecompressor/decompress(byte[],int,int)#org/apache/parquet/Preconditions/checkArgument(boolean,java.lang.String)
org/apache/parquet/hadoop/codec/SnappyDecompressor/decompress(byte[],int,int)#java/nio/ByteBuffer/rewind()
org/apache/parquet/hadoop/codec/SnappyDecompressor/decompress(byte[],int,int)#java/nio/ByteBuffer/hasRemaining()
org/apache/parquet/hadoop/codec/SnappyDecompressor/decompress(byte[],int,int)#java/nio/ByteBuffer/allocateDirect(int)
org/apache/parquet/hadoop/codec/SnappyDecompressor/decompress(byte[],int,int)#java/lang/Math/min(int,int)
org/apache/parquet/hadoop/codec/SnappyDecompressor/decompress(byte[],int,int)#java/nio/ByteBuffer/limit(int)
org/apache/parquet/hadoop/codec/SnappyDecompressor/decompress(byte[],int,int)#org/xerial/snappy/Snappy/uncompress(java.nio.ByteBuffer,java.nio.ByteBuffer)
org/apache/parquet/hadoop/codec/SnappyDecompressor/decompress(byte[],int,int)#java/nio/ByteBuffer/clear()
org/apache/parquet/hadoop/codec/SnappyDecompressor/decompress(byte[],int,int)#java/nio/ByteBuffer/capacity()
org/apache/parquet/hadoop/codec/SnappyDecompressor/decompress(byte[],int,int)#java/nio/ByteBuffer/remaining()
org/apache/parquet/hadoop/ParquetFileWriter/appendRowGroup(org.apache.parquet.io.SeekableInputStream,org.apache.parquet.hadoop.metadata.BlockMetaData,boolean)#org/apache/hadoop/fs/FSDataOutputStream/getPos()
org/apache/parquet/hadoop/ParquetFileWriter/appendRowGroup(org.apache.parquet.io.SeekableInputStream,org.apache.parquet.hadoop.metadata.BlockMetaData,boolean)#java/util/Iterator/hasNext()
org/apache/parquet/hadoop/ParquetFileWriter/appendRowGroup(org.apache.parquet.io.SeekableInputStream,org.apache.parquet.hadoop.metadata.BlockMetaData,boolean)#java/lang/String/format(java.lang.String,java.lang.Object[])
org/apache/parquet/hadoop/ParquetFileWriter/appendRowGroup(org.apache.parquet.io.SeekableInputStream,org.apache.parquet.hadoop.metadata.BlockMetaData,boolean)#org/apache/parquet/column/ColumnDescriptor/getPath()
org/apache/parquet/hadoop/ParquetFileWriter/appendRowGroup(org.apache.parquet.io.SeekableInputStream,org.apache.parquet.hadoop.metadata.BlockMetaData,boolean)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/parquet/hadoop/ParquetFileWriter/appendRowGroup(org.apache.parquet.io.SeekableInputStream,org.apache.parquet.hadoop.metadata.BlockMetaData,boolean)#java/util/Map/put(java.lang.Object,java.lang.Object)
org/apache/parquet/hadoop/ParquetFileWriter/appendRowGroup(org.apache.parquet.io.SeekableInputStream,org.apache.parquet.hadoop.metadata.BlockMetaData,boolean)#java/util/ArrayList/ArrayList()
org/apache/parquet/hadoop/ParquetFileWriter/appendRowGroup(org.apache.parquet.io.SeekableInputStream,org.apache.parquet.hadoop.metadata.BlockMetaData,boolean)#org/apache/parquet/schema/MessageType/getColumns()
org/apache/parquet/hadoop/ParquetFileWriter/appendRowGroup(org.apache.parquet.io.SeekableInputStream,org.apache.parquet.hadoop.metadata.BlockMetaData,boolean)#java/util/Map/isEmpty()
org/apache/parquet/hadoop/ParquetFileWriter/appendRowGroup(org.apache.parquet.io.SeekableInputStream,org.apache.parquet.hadoop.metadata.BlockMetaData,boolean)#java/util/List/size()
org/apache/parquet/hadoop/ParquetFileWriter/appendRowGroup(org.apache.parquet.io.SeekableInputStream,org.apache.parquet.hadoop.metadata.BlockMetaData,boolean)#org/apache/parquet/hadoop/metadata/ColumnPath/toDotString()
org/apache/parquet/hadoop/ParquetFileWriter/appendRowGroup(org.apache.parquet.io.SeekableInputStream,org.apache.parquet.hadoop.metadata.BlockMetaData,boolean)#java/util/Iterator/next()
org/apache/parquet/hadoop/ParquetFileWriter/appendRowGroup(org.apache.parquet.io.SeekableInputStream,org.apache.parquet.hadoop.metadata.BlockMetaData,boolean)#java/util/HashMap/HashMap()
org/apache/parquet/hadoop/ParquetFileWriter/appendRowGroup(org.apache.parquet.io.SeekableInputStream,org.apache.parquet.hadoop.metadata.BlockMetaData,boolean)#java/util/Map/keySet()
org/apache/parquet/hadoop/ParquetFileWriter/appendRowGroup(org.apache.parquet.io.SeekableInputStream,org.apache.parquet.hadoop.metadata.BlockMetaData,boolean)#java/util/List/add(java.lang.Object)
org/apache/parquet/hadoop/ParquetFileWriter/appendRowGroup(org.apache.parquet.io.SeekableInputStream,org.apache.parquet.hadoop.metadata.BlockMetaData,boolean)#org/apache/parquet/Strings/join(java.lang.Iterable,java.lang.String)
org/apache/parquet/hadoop/ParquetFileWriter/appendRowGroup(org.apache.parquet.io.SeekableInputStream,org.apache.parquet.hadoop.metadata.BlockMetaData,boolean)#java/util/List/get(int)
org/apache/parquet/hadoop/ParquetFileWriter/appendRowGroup(org.apache.parquet.io.SeekableInputStream,org.apache.parquet.hadoop.metadata.BlockMetaData,boolean)#java/util/List/iterator()
org/apache/parquet/hadoop/ParquetFileWriter/appendRowGroup(org.apache.parquet.io.SeekableInputStream,org.apache.parquet.hadoop.metadata.BlockMetaData,boolean)#org/apache/parquet/hadoop/metadata/ColumnPath/get(java.lang.String[])
org/apache/parquet/hadoop/ParquetFileWriter/appendRowGroup(org.apache.parquet.io.SeekableInputStream,org.apache.parquet.hadoop.metadata.BlockMetaData,boolean)#java/util/Map/remove(java.lang.Object)
org/apache/parquet/hadoop/ParquetFileWriter/writeDataPage(int,int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding)#org/slf4j/Logger/debug(java.lang.String,java.lang.Object,java.lang.Object)
org/apache/parquet/hadoop/ParquetFileWriter/writeDataPage(int,int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding)#org/apache/hadoop/fs/FSDataOutputStream/getPos()
org/apache/parquet/hadoop/ParquetFileWriter/writeDataPage(int,int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding)#org/apache/parquet/hadoop/ParquetFileWriter$STATE/write()
org/apache/parquet/hadoop/ParquetFileWriter/writeDataPage(int,int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding)#java/lang/Long/valueOf(long)
org/apache/parquet/hadoop/ParquetFileWriter/writeDataPage(int,int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding)#org/apache/parquet/column/EncodingStats$Builder/addDataEncoding(org.apache.parquet.column.Encoding)
org/apache/parquet/hadoop/ParquetFileWriter/writeDataPage(int,int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding)#org/apache/parquet/bytes/BytesInput/writeAllTo(java.io.OutputStream)
org/apache/parquet/hadoop/ParquetFileWriter/writeDataPage(int,int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding)#org/apache/parquet/bytes/BytesInput/size()
org/apache/parquet/hadoop/ParquetFileWriter/writeDataPage(int,int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding)#java/lang/Integer/valueOf(int)
org/apache/parquet/hadoop/ParquetFileWriter/writeDataPage(int,int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding)#java/util/Set/add(java.lang.Object)
org/apache/parquet/hadoop/ParquetFileWriter/writeDataPages(org.apache.parquet.bytes.BytesInput,long,long,org.apache.parquet.column.statistics.Statistics,java.util.Set,java.util.Set,java.util.List)#org/apache/hadoop/fs/FSDataOutputStream/getPos()
org/apache/parquet/hadoop/ParquetFileWriter/writeDataPages(org.apache.parquet.bytes.BytesInput,long,long,org.apache.parquet.column.statistics.Statistics,java.util.Set,java.util.Set,java.util.List)#org/apache/parquet/hadoop/ParquetFileWriter$STATE/write()
org/apache/parquet/hadoop/ParquetFileWriter/writeDataPages(org.apache.parquet.bytes.BytesInput,long,long,org.apache.parquet.column.statistics.Statistics,java.util.Set,java.util.Set,java.util.List)#org/apache/parquet/column/EncodingStats$Builder/addDataEncodings(java.util.Collection)
org/apache/parquet/hadoop/ParquetFileWriter/writeDataPages(org.apache.parquet.bytes.BytesInput,long,long,org.apache.parquet.column.statistics.Statistics,java.util.Set,java.util.Set,java.util.List)#java/util/Set/addAll(java.util.Collection)
org/apache/parquet/hadoop/ParquetFileWriter/writeDataPages(org.apache.parquet.bytes.BytesInput,long,long,org.apache.parquet.column.statistics.Statistics,java.util.Set,java.util.Set,java.util.List)#org/apache/parquet/column/EncodingStats$Builder/withV2Pages()
org/apache/parquet/hadoop/ParquetFileWriter/writeDataPages(org.apache.parquet.bytes.BytesInput,long,long,org.apache.parquet.column.statistics.Statistics,java.util.Set,java.util.Set,java.util.List)#java/util/Set/isEmpty()
org/apache/parquet/hadoop/ParquetFileWriter/writeDataPages(org.apache.parquet.bytes.BytesInput,long,long,org.apache.parquet.column.statistics.Statistics,java.util.Set,java.util.Set,java.util.List)#org/apache/parquet/bytes/BytesInput/writeAllTo(java.io.OutputStream)
org/apache/parquet/hadoop/ParquetFileWriter/writeDataPages(org.apache.parquet.bytes.BytesInput,long,long,org.apache.parquet.column.statistics.Statistics,java.util.Set,java.util.Set,java.util.List)#org/apache/parquet/bytes/BytesInput/size()
org/apache/parquet/hadoop/ParquetFileWriter/writeDataPages(org.apache.parquet.bytes.BytesInput,long,long,org.apache.parquet.column.statistics.Statistics,java.util.Set,java.util.Set,java.util.List)#java/lang/Long/valueOf(long)
org/apache/parquet/hadoop/ParquetFileWriter/writeDataPages(org.apache.parquet.bytes.BytesInput,long,long,org.apache.parquet.column.statistics.Statistics,java.util.Set,java.util.Set,java.util.List)#org/slf4j/Logger/debug(java.lang.String,java.lang.Object)
org/apache/parquet/hadoop/ParquetFileWriter/mergeFooters(org.apache.hadoop.fs.Path,java.util.List)#java/util/Iterator/hasNext()
org/apache/parquet/hadoop/ParquetFileWriter/mergeFooters(org.apache.hadoop.fs.Path,java.util.List)#java/lang/String/length()
org/apache/parquet/hadoop/ParquetFileWriter/mergeFooters(org.apache.hadoop.fs.Path,java.util.List)#java/util/Iterator/next()
org/apache/parquet/hadoop/ParquetFileWriter/mergeFooters(org.apache.hadoop.fs.Path,java.util.List)#java/net/URI/getPath()
org/apache/parquet/hadoop/ParquetFileWriter/mergeFooters(org.apache.hadoop.fs.Path,java.util.List)#java/lang/StringBuilder/StringBuilder()
org/apache/parquet/hadoop/ParquetFileWriter/mergeFooters(org.apache.hadoop.fs.Path,java.util.List)#org/apache/parquet/io/ParquetEncodingException/ParquetEncodingException(java.lang.String)
org/apache/parquet/hadoop/ParquetFileWriter/mergeFooters(org.apache.hadoop.fs.Path,java.util.List)#java/lang/String/substring(int)
org/apache/parquet/hadoop/ParquetFileWriter/mergeFooters(org.apache.hadoop.fs.Path,java.util.List)#java/util/List/add(java.lang.Object)
org/apache/parquet/hadoop/ParquetFileWriter/mergeFooters(org.apache.hadoop.fs.Path,java.util.List)#java/util/List/iterator()
org/apache/parquet/hadoop/ParquetFileWriter/mergeFooters(org.apache.hadoop.fs.Path,java.util.List)#java/lang/StringBuilder/toString()
org/apache/parquet/hadoop/ParquetFileWriter/mergeFooters(org.apache.hadoop.fs.Path,java.util.List)#java/util/ArrayList/ArrayList()
org/apache/parquet/hadoop/ParquetFileWriter/mergeFooters(org.apache.hadoop.fs.Path,java.util.List)#java/lang/StringBuilder/append(java.lang.String)
org/apache/parquet/hadoop/ParquetFileWriter/mergeFooters(org.apache.hadoop.fs.Path,java.util.List)#org/apache/hadoop/fs/Path/toUri()
org/apache/parquet/hadoop/ParquetFileWriter/mergeFooters(org.apache.hadoop.fs.Path,java.util.List)#java/lang/String/startsWith(java.lang.String)
org/apache/parquet/hadoop/ParquetFileWriter/mergeFooters(org.apache.hadoop.fs.Path,java.util.List)#java/lang/StringBuilder/append(java.lang.Object)
org/apache/parquet/hadoop/ParquetFileWriter/writeDictionaryPage(org.apache.parquet.column.page.DictionaryPage)#org/apache/parquet/hadoop/ParquetFileWriter$STATE/write()
org/apache/parquet/hadoop/ParquetFileWriter/writeDictionaryPage(org.apache.parquet.column.page.DictionaryPage)#java/lang/Long/valueOf(long)
org/apache/parquet/hadoop/ParquetFileWriter/writeDictionaryPage(org.apache.parquet.column.page.DictionaryPage)#org/apache/parquet/column/page/DictionaryPage/getEncoding()
org/apache/parquet/hadoop/ParquetFileWriter/writeDictionaryPage(org.apache.parquet.column.page.DictionaryPage)#org/apache/parquet/bytes/BytesInput/writeAllTo(java.io.OutputStream)
org/apache/parquet/hadoop/ParquetFileWriter/writeDictionaryPage(org.apache.parquet.column.page.DictionaryPage)#org/apache/parquet/bytes/BytesInput/size()
org/apache/parquet/hadoop/ParquetFileWriter/writeDictionaryPage(org.apache.parquet.column.page.DictionaryPage)#java/lang/Integer/valueOf(int)
org/apache/parquet/hadoop/ParquetFileWriter/writeDictionaryPage(org.apache.parquet.column.page.DictionaryPage)#org/apache/parquet/column/page/DictionaryPage/getUncompressedSize()
org/apache/parquet/hadoop/ParquetFileWriter/writeDictionaryPage(org.apache.parquet.column.page.DictionaryPage)#java/util/Set/add(java.lang.Object)
org/apache/parquet/hadoop/ParquetFileWriter/writeDictionaryPage(org.apache.parquet.column.page.DictionaryPage)#org/slf4j/Logger/debug(java.lang.String,java.lang.Object,java.lang.Object)
org/apache/parquet/hadoop/ParquetFileWriter/writeDictionaryPage(org.apache.parquet.column.page.DictionaryPage)#org/apache/hadoop/fs/FSDataOutputStream/getPos()
org/apache/parquet/hadoop/ParquetFileWriter/writeDictionaryPage(org.apache.parquet.column.page.DictionaryPage)#org/apache/parquet/column/page/DictionaryPage/getBytes()
org/apache/parquet/hadoop/ParquetFileWriter/writeDictionaryPage(org.apache.parquet.column.page.DictionaryPage)#org/apache/parquet/column/EncodingStats$Builder/addDictEncoding(org.apache.parquet.column.Encoding)
org/apache/parquet/hadoop/ParquetFileWriter/writeDictionaryPage(org.apache.parquet.column.page.DictionaryPage)#org/apache/parquet/column/page/DictionaryPage/getDictionarySize()
org/apache/parquet/hadoop/ParquetFileWriter/copy(org.apache.parquet.io.SeekableInputStream,org.apache.hadoop.fs.FSDataOutputStream,long,long)#org/apache/hadoop/fs/FSDataOutputStream/getPos()
org/apache/parquet/hadoop/ParquetFileWriter/copy(org.apache.parquet.io.SeekableInputStream,org.apache.hadoop.fs.FSDataOutputStream,long,long)#org/apache/hadoop/fs/FSDataOutputStream/write(byte[],int,int)
org/apache/parquet/hadoop/ParquetFileWriter/copy(org.apache.parquet.io.SeekableInputStream,org.apache.hadoop.fs.FSDataOutputStream,long,long)#java/lang/Long/valueOf(long)
org/apache/parquet/hadoop/ParquetFileWriter/copy(org.apache.parquet.io.SeekableInputStream,org.apache.hadoop.fs.FSDataOutputStream,long,long)#java/lang/StringBuilder/append(java.lang.String)
org/apache/parquet/hadoop/ParquetFileWriter/copy(org.apache.parquet.io.SeekableInputStream,org.apache.hadoop.fs.FSDataOutputStream,long,long)#org/apache/parquet/io/SeekableInputStream/read(byte[],int,int)
org/apache/parquet/hadoop/ParquetFileWriter/copy(org.apache.parquet.io.SeekableInputStream,org.apache.hadoop.fs.FSDataOutputStream,long,long)#java/lang/ThreadLocal/get()
org/apache/parquet/hadoop/ParquetFileWriter/copy(org.apache.parquet.io.SeekableInputStream,org.apache.hadoop.fs.FSDataOutputStream,long,long)#java/lang/StringBuilder/append(long)
org/apache/parquet/hadoop/ParquetFileWriter/copy(org.apache.parquet.io.SeekableInputStream,org.apache.hadoop.fs.FSDataOutputStream,long,long)#java/lang/StringBuilder/StringBuilder()
org/apache/parquet/hadoop/ParquetFileWriter/copy(org.apache.parquet.io.SeekableInputStream,org.apache.hadoop.fs.FSDataOutputStream,long,long)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/parquet/hadoop/ParquetFileWriter/copy(org.apache.parquet.io.SeekableInputStream,org.apache.hadoop.fs.FSDataOutputStream,long,long)#java/lang/StringBuilder/toString()
org/apache/parquet/hadoop/ParquetFileWriter/copy(org.apache.parquet.io.SeekableInputStream,org.apache.hadoop.fs.FSDataOutputStream,long,long)#org/apache/parquet/io/SeekableInputStream/seek(long)
org/apache/parquet/hadoop/ParquetFileWriter/copy(org.apache.parquet.io.SeekableInputStream,org.apache.hadoop.fs.FSDataOutputStream,long,long)#org/slf4j/Logger/debug(java.lang.String,java.lang.Object[])
org/apache/parquet/hadoop/ParquetFileWriter/writeDataPage(int,int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.statistics.Statistics,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding)#org/slf4j/Logger/debug(java.lang.String,java.lang.Object,java.lang.Object)
org/apache/parquet/hadoop/ParquetFileWriter/writeDataPage(int,int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.statistics.Statistics,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding)#org/apache/hadoop/fs/FSDataOutputStream/getPos()
org/apache/parquet/hadoop/ParquetFileWriter/writeDataPage(int,int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.statistics.Statistics,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding)#org/apache/parquet/hadoop/ParquetFileWriter$STATE/write()
org/apache/parquet/hadoop/ParquetFileWriter/writeDataPage(int,int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.statistics.Statistics,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding)#java/lang/Long/valueOf(long)
org/apache/parquet/hadoop/ParquetFileWriter/writeDataPage(int,int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.statistics.Statistics,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding)#org/apache/parquet/column/EncodingStats$Builder/addDataEncoding(org.apache.parquet.column.Encoding)
org/apache/parquet/hadoop/ParquetFileWriter/writeDataPage(int,int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.statistics.Statistics,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding)#org/apache/parquet/column/statistics/Statistics/mergeStatistics(org.apache.parquet.column.statistics.Statistics)
org/apache/parquet/hadoop/ParquetFileWriter/writeDataPage(int,int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.statistics.Statistics,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding)#org/apache/parquet/bytes/BytesInput/writeAllTo(java.io.OutputStream)
org/apache/parquet/hadoop/ParquetFileWriter/writeDataPage(int,int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.statistics.Statistics,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding)#org/apache/parquet/bytes/BytesInput/size()
org/apache/parquet/hadoop/ParquetFileWriter/writeDataPage(int,int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.statistics.Statistics,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding)#java/lang/Integer/valueOf(int)
org/apache/parquet/hadoop/ParquetFileWriter/writeDataPage(int,int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.statistics.Statistics,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding)#java/util/Set/add(java.lang.Object)
org/apache/parquet/hadoop/ParquetFileWriter/mergeInto(org.apache.parquet.hadoop.metadata.FileMetaData,org.apache.parquet.hadoop.metadata.GlobalMetaData,boolean)#java/util/Map/put(java.lang.Object,java.lang.Object)
org/apache/parquet/hadoop/ParquetFileWriter/mergeInto(org.apache.parquet.hadoop.metadata.FileMetaData,org.apache.parquet.hadoop.metadata.GlobalMetaData,boolean)#java/util/Iterator/hasNext()
org/apache/parquet/hadoop/ParquetFileWriter/mergeInto(org.apache.parquet.hadoop.metadata.FileMetaData,org.apache.parquet.hadoop.metadata.GlobalMetaData,boolean)#java/util/Map$Entry/getKey()
org/apache/parquet/hadoop/ParquetFileWriter/mergeInto(org.apache.parquet.hadoop.metadata.FileMetaData,org.apache.parquet.hadoop.metadata.GlobalMetaData,boolean)#java/util/Set/addAll(java.util.Collection)
org/apache/parquet/hadoop/ParquetFileWriter/mergeInto(org.apache.parquet.hadoop.metadata.FileMetaData,org.apache.parquet.hadoop.metadata.GlobalMetaData,boolean)#java/util/LinkedHashSet/LinkedHashSet()
org/apache/parquet/hadoop/ParquetFileWriter/mergeInto(org.apache.parquet.hadoop.metadata.FileMetaData,org.apache.parquet.hadoop.metadata.GlobalMetaData,boolean)#java/util/Map/putAll(java.util.Map)
org/apache/parquet/hadoop/ParquetFileWriter/mergeInto(org.apache.parquet.hadoop.metadata.FileMetaData,org.apache.parquet.hadoop.metadata.GlobalMetaData,boolean)#java/util/Map/get(java.lang.Object)
org/apache/parquet/hadoop/ParquetFileWriter/mergeInto(org.apache.parquet.hadoop.metadata.FileMetaData,org.apache.parquet.hadoop.metadata.GlobalMetaData,boolean)#org/apache/parquet/schema/MessageType/equals(java.lang.Object)
org/apache/parquet/hadoop/ParquetFileWriter/mergeInto(org.apache.parquet.hadoop.metadata.FileMetaData,org.apache.parquet.hadoop.metadata.GlobalMetaData,boolean)#java/util/Set/iterator()
org/apache/parquet/hadoop/ParquetFileWriter/mergeInto(org.apache.parquet.hadoop.metadata.FileMetaData,org.apache.parquet.hadoop.metadata.GlobalMetaData,boolean)#java/util/Map$Entry/getValue()
org/apache/parquet/hadoop/ParquetFileWriter/mergeInto(org.apache.parquet.hadoop.metadata.FileMetaData,org.apache.parquet.hadoop.metadata.GlobalMetaData,boolean)#java/util/HashSet/HashSet()
org/apache/parquet/hadoop/ParquetFileWriter/mergeInto(org.apache.parquet.hadoop.metadata.FileMetaData,org.apache.parquet.hadoop.metadata.GlobalMetaData,boolean)#java/util/Iterator/next()
org/apache/parquet/hadoop/ParquetFileWriter/mergeInto(org.apache.parquet.hadoop.metadata.FileMetaData,org.apache.parquet.hadoop.metadata.GlobalMetaData,boolean)#java/util/HashMap/HashMap()
org/apache/parquet/hadoop/ParquetFileWriter/mergeInto(org.apache.parquet.hadoop.metadata.FileMetaData,org.apache.parquet.hadoop.metadata.GlobalMetaData,boolean)#java/util/Map/entrySet()
org/apache/parquet/hadoop/ParquetFileWriter/mergeInto(org.apache.parquet.hadoop.metadata.FileMetaData,org.apache.parquet.hadoop.metadata.GlobalMetaData,boolean)#java/util/Set/add(java.lang.Object)
org/apache/parquet/hadoop/LruCache/put(java.lang.Object,org.apache.parquet.hadoop.LruCache$Value)#org/apache/parquet/hadoop/LruCache$Value/isCurrent(java.lang.Object)
org/apache/parquet/hadoop/LruCache/put(java.lang.Object,org.apache.parquet.hadoop.LruCache$Value)#org/slf4j/Logger/warn(java.lang.String,java.lang.Object)
org/apache/parquet/hadoop/LruCache/put(java.lang.Object,org.apache.parquet.hadoop.LruCache$Value)#org/slf4j/Logger/isWarnEnabled()
org/apache/parquet/hadoop/LruCache/put(java.lang.Object,org.apache.parquet.hadoop.LruCache$Value)#org/slf4j/Logger/debug(java.lang.String,java.lang.Object)
org/apache/parquet/hadoop/LruCache/put(java.lang.Object,org.apache.parquet.hadoop.LruCache$Value)#java/util/LinkedHashMap/put(java.lang.Object,java.lang.Object)
org/apache/parquet/hadoop/LruCache/put(java.lang.Object,org.apache.parquet.hadoop.LruCache$Value)#org/apache/parquet/hadoop/LruCache$Value/isNewerThan(java.lang.Object)
org/apache/parquet/hadoop/LruCache/put(java.lang.Object,org.apache.parquet.hadoop.LruCache$Value)#org/slf4j/Logger/isDebugEnabled()
org/apache/parquet/hadoop/LruCache/put(java.lang.Object,org.apache.parquet.hadoop.LruCache$Value)#java/util/LinkedHashMap/get(java.lang.Object)
org/apache/parquet/hadoop/LruCache/put(java.lang.Object,org.apache.parquet.hadoop.LruCache$Value)#org/slf4j/Logger/warn(java.lang.String,java.lang.Object,java.lang.Object)
org/apache/parquet/hadoop/ParquetInputSplit/toString()#java/lang/Class/getSimpleName()
org/apache/parquet/hadoop/ParquetInputSplit/toString()#org/apache/parquet/hadoop/ParquetInputSplit/getLocations()
org/apache/parquet/hadoop/ParquetInputSplit/toString()#org/apache/parquet/hadoop/ParquetInputSplit/getStart()
org/apache/parquet/hadoop/ParquetInputSplit/toString()#java/lang/StringBuilder/append(long)
org/apache/parquet/hadoop/ParquetInputSplit/toString()#org/apache/parquet/hadoop/ParquetInputSplit/getPath()
org/apache/parquet/hadoop/ParquetInputSplit/toString()#java/util/Arrays/toString(java.lang.Object[])
org/apache/parquet/hadoop/ParquetInputSplit/toString()#java/lang/StringBuilder/append(java.lang.String)
org/apache/parquet/hadoop/ParquetInputSplit/toString()#java/lang/StringBuilder/StringBuilder()
org/apache/parquet/hadoop/ParquetInputSplit/toString()#org/apache/parquet/hadoop/ParquetInputSplit/getLength()
org/apache/parquet/hadoop/ParquetInputSplit/toString()#java/lang/StringBuilder/append(java.lang.Object)
org/apache/parquet/hadoop/ParquetInputSplit/toString()#java/lang/Object/getClass()
org/apache/parquet/hadoop/ParquetInputSplit/toString()#java/lang/StringBuilder/toString()
org/apache/parquet/hadoop/ParquetInputSplit/toString()#java/util/Arrays/toString(long[])
org/apache/parquet/hadoop/ParquetInputSplit/write(java.io.DataOutput)#java/io/DataOutputStream/writeInt(int)
org/apache/parquet/hadoop/ParquetInputSplit/write(java.io.DataOutput)#java/util/zip/GZIPOutputStream/GZIPOutputStream(java.io.OutputStream)
org/apache/parquet/hadoop/ParquetInputSplit/write(java.io.DataOutput)#java/io/DataOutputStream/close()
org/apache/parquet/hadoop/ParquetInputSplit/write(java.io.DataOutput)#org/apache/hadoop/mapreduce/lib/input/FileSplit/write(java.io.DataOutput)
org/apache/parquet/hadoop/ParquetInputSplit/write(java.io.DataOutput)#java/io/ByteArrayOutputStream/ByteArrayOutputStream()
org/apache/parquet/hadoop/ParquetInputSplit/write(java.io.DataOutput)#java/io/DataOutputStream/writeBoolean(boolean)
org/apache/parquet/hadoop/ParquetInputSplit/write(java.io.DataOutput)#java/io/DataOutputStream/writeLong(long)
org/apache/parquet/hadoop/ParquetInputSplit/write(java.io.DataOutput)#java/io/DataOutputStream/DataOutputStream(java.io.OutputStream)
org/apache/parquet/hadoop/ParquetInputSplit/write(java.io.DataOutput)#java/io/ByteArrayOutputStream/toByteArray()
org/apache/parquet/hadoop/ParquetOutputCommitter/writeMetaDataFile(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)#org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)
org/apache/parquet/hadoop/ParquetOutputCommitter/writeMetaDataFile(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)#org/apache/hadoop/fs/Path/Path(org.apache.hadoop.fs.Path,java.lang.String)
org/apache/parquet/hadoop/ParquetOutputCommitter/writeMetaDataFile(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)#org/slf4j/Logger/warn(java.lang.String,java.lang.Throwable)
org/apache/parquet/hadoop/ParquetOutputCommitter/writeMetaDataFile(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)#org/apache/hadoop/fs/FileSystem/exists(org.apache.hadoop.fs.Path)
org/apache/parquet/hadoop/ParquetOutputCommitter/writeMetaDataFile(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)#java/lang/StringBuilder/append(java.lang.String)
org/apache/parquet/hadoop/ParquetOutputCommitter/writeMetaDataFile(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)#org/apache/hadoop/fs/FileSystem/getFileStatus(org.apache.hadoop.fs.Path)
org/apache/parquet/hadoop/ParquetOutputCommitter/writeMetaDataFile(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)#java/util/List/isEmpty()
org/apache/parquet/hadoop/ParquetOutputCommitter/writeMetaDataFile(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)#java/lang/StringBuilder/StringBuilder()
org/apache/parquet/hadoop/ParquetOutputCommitter/writeMetaDataFile(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)#java/lang/StringBuilder/append(java.lang.Object)
org/apache/parquet/hadoop/ParquetOutputCommitter/writeMetaDataFile(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)#org/apache/hadoop/fs/FileSystem/delete(org.apache.hadoop.fs.Path,boolean)
org/apache/parquet/hadoop/ParquetOutputCommitter/writeMetaDataFile(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)#org/apache/hadoop/conf/Configuration/getBoolean(java.lang.String,boolean)
org/apache/parquet/hadoop/ParquetOutputCommitter/writeMetaDataFile(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path)#java/lang/StringBuilder/toString()
org/apache/parquet/hadoop/ParquetRecordReader/checkDeltaByteArrayProblem(org.apache.parquet.hadoop.metadata.FileMetaData,org.apache.hadoop.conf.Configuration,org.apache.parquet.hadoop.metadata.BlockMetaData)#java/util/Iterator/hasNext()
org/apache/parquet/hadoop/ParquetRecordReader/checkDeltaByteArrayProblem(org.apache.parquet.hadoop.metadata.FileMetaData,org.apache.hadoop.conf.Configuration,org.apache.parquet.hadoop.metadata.BlockMetaData)#java/util/Iterator/next()
org/apache/parquet/hadoop/ParquetRecordReader/checkDeltaByteArrayProblem(org.apache.parquet.hadoop.metadata.FileMetaData,org.apache.hadoop.conf.Configuration,org.apache.parquet.hadoop.metadata.BlockMetaData)#java/util/List/iterator()
org/apache/parquet/hadoop/ParquetRecordReader/checkDeltaByteArrayProblem(org.apache.parquet.hadoop.metadata.FileMetaData,org.apache.hadoop.conf.Configuration,org.apache.parquet.hadoop.metadata.BlockMetaData)#java/util/HashSet/HashSet()
org/apache/parquet/hadoop/ParquetRecordReader/checkDeltaByteArrayProblem(org.apache.parquet.hadoop.metadata.FileMetaData,org.apache.hadoop.conf.Configuration,org.apache.parquet.hadoop.metadata.BlockMetaData)#java/util/Set/addAll(java.util.Collection)
org/apache/parquet/hadoop/ParquetRecordReader/checkDeltaByteArrayProblem(org.apache.parquet.hadoop.metadata.FileMetaData,org.apache.hadoop.conf.Configuration,org.apache.parquet.hadoop.metadata.BlockMetaData)#org/apache/parquet/CorruptDeltaByteArrays/requiresSequentialReads(java.lang.String,org.apache.parquet.column.Encoding)
org/apache/parquet/hadoop/ParquetRecordReader/checkDeltaByteArrayProblem(org.apache.parquet.hadoop.metadata.FileMetaData,org.apache.hadoop.conf.Configuration,org.apache.parquet.hadoop.metadata.BlockMetaData)#org/apache/parquet/io/ParquetDecodingException/ParquetDecodingException(java.lang.String)
org/apache/parquet/hadoop/ParquetRecordReader/checkDeltaByteArrayProblem(org.apache.parquet.hadoop.metadata.FileMetaData,org.apache.hadoop.conf.Configuration,org.apache.parquet.hadoop.metadata.BlockMetaData)#java/util/Set/iterator()
org/apache/parquet/hadoop/ParquetRecordReader/checkDeltaByteArrayProblem(org.apache.parquet.hadoop.metadata.FileMetaData,org.apache.hadoop.conf.Configuration,org.apache.parquet.hadoop.metadata.BlockMetaData)#org/apache/hadoop/conf/Configuration/getBoolean(java.lang.String,boolean)
org/apache/parquet/hadoop/ParquetRecordReader/initializeInternalReader(org.apache.parquet.hadoop.ParquetInputSplit,org.apache.hadoop.conf.Configuration)#org/apache/parquet/hadoop/ParquetInputSplit/getStart()
org/apache/parquet/hadoop/ParquetRecordReader/initializeInternalReader(org.apache.parquet.hadoop.ParquetInputSplit,org.apache.hadoop.conf.Configuration)#java/util/List/isEmpty()
org/apache/parquet/hadoop/ParquetRecordReader/initializeInternalReader(org.apache.parquet.hadoop.ParquetInputSplit,org.apache.hadoop.conf.Configuration)#java/util/List/size()
org/apache/parquet/hadoop/ParquetRecordReader/initializeInternalReader(org.apache.parquet.hadoop.ParquetInputSplit,org.apache.hadoop.conf.Configuration)#org/apache/parquet/hadoop/ParquetInputSplit/getPath()
org/apache/parquet/hadoop/ParquetRecordReader/initializeInternalReader(org.apache.parquet.hadoop.ParquetInputSplit,org.apache.hadoop.conf.Configuration)#java/lang/StringBuilder/StringBuilder()
org/apache/parquet/hadoop/ParquetRecordReader/initializeInternalReader(org.apache.parquet.hadoop.ParquetInputSplit,org.apache.hadoop.conf.Configuration)#java/lang/StringBuilder/append(java.lang.Object)
org/apache/parquet/hadoop/ParquetRecordReader/initializeInternalReader(org.apache.parquet.hadoop.ParquetInputSplit,org.apache.hadoop.conf.Configuration)#java/lang/IllegalStateException/IllegalStateException(java.lang.String)
org/apache/parquet/hadoop/ParquetRecordReader/initializeInternalReader(org.apache.parquet.hadoop.ParquetInputSplit,org.apache.hadoop.conf.Configuration)#java/lang/StringBuilder/append(java.lang.String)
org/apache/parquet/hadoop/ParquetRecordReader/initializeInternalReader(org.apache.parquet.hadoop.ParquetInputSplit,org.apache.hadoop.conf.Configuration)#java/util/List/get(int)
org/apache/parquet/hadoop/ParquetRecordReader/initializeInternalReader(org.apache.parquet.hadoop.ParquetInputSplit,org.apache.hadoop.conf.Configuration)#java/lang/StringBuilder/toString()
org/apache/parquet/hadoop/ParquetRecordReader/initializeInternalReader(org.apache.parquet.hadoop.ParquetInputSplit,org.apache.hadoop.conf.Configuration)#java/util/Arrays/toString(long[])
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#org/apache/hadoop/fs/FileSystem/getFileStatus(org.apache.hadoop.fs.Path)
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#java/lang/StringBuilder/append(int)
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#java/util/List/size()
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#org/apache/parquet/hadoop/PrintFooter$1/1(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileStatus)
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#org/apache/hadoop/fs/Path/Path(java.net.URI)
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#java/util/List/add(java.lang.Object)
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#java/lang/StringBuilder/toString()
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#org/apache/hadoop/fs/Path/getFileSystem(org.apache.hadoop.conf.Configuration)
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#java/util/ArrayList/ArrayList()
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#org/apache/hadoop/fs/Path/Path(org.apache.hadoop.fs.Path,java.lang.String)
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#org/apache/hadoop/fs/FileStatus/getPath()
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#java/util/Iterator/hasNext()
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#java/io/PrintStream/print(java.lang.String)
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#java/net/URI/URI(java.lang.String)
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#org/apache/hadoop/fs/FileSystem/exists(org.apache.hadoop.fs.Path)
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#org/apache/hadoop/conf/Configuration/Configuration()
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#java/lang/StringBuilder/append(java.lang.String)
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#java/util/Map$Entry/getKey()
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#java/io/PrintStream/println(java.lang.String)
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#org/apache/hadoop/fs/FileStatus/isDir()
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#java/util/Arrays/asList(java.lang.Object[])
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#java/util/Iterator/next()
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#java/util/Deque/isEmpty()
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#java/util/Deque/addLast(java.lang.Object)
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#java/lang/StringBuilder/append(long)
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#java/lang/StringBuilder/StringBuilder()
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#org/apache/hadoop/fs/FileSystem/listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#java/io/PrintStream/print(char)
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#java/util/concurrent/ExecutorService/shutdownNow()
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#java/util/Set/iterator()
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#java/util/Deque/removeFirst()
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#java/lang/StringBuilder/append(java.lang.Object)
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#java/util/concurrent/Executors/newFixedThreadPool(int)
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#java/util/Map$Entry/getValue()
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#java/lang/System/currentTimeMillis()
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#java/util/concurrent/Future/isDone()
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#java/util/List/iterator()
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#java/util/Deque/add(java.lang.Object)
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#java/util/Map/entrySet()
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#java/lang/StringBuilder/append(float)
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#java/util/concurrent/Future/get()
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#java/util/concurrent/ExecutorService/submit(java.util.concurrent.Callable)
org/apache/parquet/hadoop/PrintFooter/main(java.lang.String[])#java/util/concurrent/LinkedBlockingDeque/LinkedBlockingDeque()
org/apache/parquet/filter2/statisticslevel/StatisticsFilter/visit(org.apache.parquet.filter2.predicate.Operators$UserDefined,boolean)#org/apache/parquet/filter2/predicate/Operators$UserDefined/getColumn()
org/apache/parquet/filter2/statisticslevel/StatisticsFilter/visit(org.apache.parquet.filter2.predicate.Operators$UserDefined,boolean)#org/apache/parquet/filter2/predicate/UserDefinedPredicate/keep(java.lang.Comparable)
org/apache/parquet/filter2/statisticslevel/StatisticsFilter/visit(org.apache.parquet.filter2.predicate.Operators$UserDefined,boolean)#java/lang/Boolean/valueOf(boolean)
org/apache/parquet/filter2/statisticslevel/StatisticsFilter/visit(org.apache.parquet.filter2.predicate.Operators$UserDefined,boolean)#org/apache/parquet/filter2/predicate/Statistics/Statistics(java.lang.Object,java.lang.Object)
org/apache/parquet/filter2/statisticslevel/StatisticsFilter/visit(org.apache.parquet.filter2.predicate.Operators$UserDefined,boolean)#org/apache/parquet/column/statistics/Statistics/isEmpty()
org/apache/parquet/filter2/statisticslevel/StatisticsFilter/visit(org.apache.parquet.filter2.predicate.Operators$UserDefined,boolean)#org/apache/parquet/column/statistics/Statistics/genericGetMax()
org/apache/parquet/filter2/statisticslevel/StatisticsFilter/visit(org.apache.parquet.filter2.predicate.Operators$UserDefined,boolean)#org/apache/parquet/filter2/predicate/Operators$UserDefined/getUserDefinedPredicate()
org/apache/parquet/filter2/statisticslevel/StatisticsFilter/visit(org.apache.parquet.filter2.predicate.Operators$UserDefined,boolean)#org/apache/parquet/filter2/predicate/UserDefinedPredicate/canDrop(org.apache.parquet.filter2.predicate.Statistics)
org/apache/parquet/filter2/statisticslevel/StatisticsFilter/visit(org.apache.parquet.filter2.predicate.Operators$UserDefined,boolean)#org/apache/parquet/filter2/predicate/UserDefinedPredicate/inverseCanDrop(org.apache.parquet.filter2.predicate.Statistics)
org/apache/parquet/filter2/statisticslevel/StatisticsFilter/visit(org.apache.parquet.filter2.predicate.Operators$UserDefined,boolean)#org/apache/parquet/filter2/predicate/Operators$Column/getColumnPath()
org/apache/parquet/filter2/statisticslevel/StatisticsFilter/visit(org.apache.parquet.filter2.predicate.Operators$UserDefined,boolean)#org/apache/parquet/column/statistics/Statistics/genericGetMin()
org/apache/parquet/format/converter/ParquetMetadataConverter/convertEncodingStats(org.apache.parquet.column.EncodingStats)#org/apache/parquet/column/EncodingStats/getNumDataPagesEncodedAs(org.apache.parquet.column.Encoding)
org/apache/parquet/format/converter/ParquetMetadataConverter/convertEncodingStats(org.apache.parquet.column.EncodingStats)#java/util/ArrayList/ArrayList()
org/apache/parquet/format/converter/ParquetMetadataConverter/convertEncodingStats(org.apache.parquet.column.EncodingStats)#org/apache/parquet/column/EncodingStats/getDataEncodings()
org/apache/parquet/format/converter/ParquetMetadataConverter/convertEncodingStats(org.apache.parquet.column.EncodingStats)#java/util/Iterator/hasNext()
org/apache/parquet/format/converter/ParquetMetadataConverter/convertEncodingStats(org.apache.parquet.column.EncodingStats)#org/apache/parquet/column/EncodingStats/usesV2Pages()
org/apache/parquet/format/converter/ParquetMetadataConverter/convertEncodingStats(org.apache.parquet.column.EncodingStats)#java/util/Iterator/next()
org/apache/parquet/format/converter/ParquetMetadataConverter/convertEncodingStats(org.apache.parquet.column.EncodingStats)#org/apache/parquet/format/PageEncodingStats/PageEncodingStats(org.apache.parquet.format.PageType,org.apache.parquet.format.Encoding,int)
org/apache/parquet/format/converter/ParquetMetadataConverter/convertEncodingStats(org.apache.parquet.column.EncodingStats)#java/util/Set/iterator()
org/apache/parquet/format/converter/ParquetMetadataConverter/convertEncodingStats(org.apache.parquet.column.EncodingStats)#java/util/List/add(java.lang.Object)
org/apache/parquet/format/converter/ParquetMetadataConverter/convertEncodingStats(org.apache.parquet.column.EncodingStats)#org/apache/parquet/column/EncodingStats/getDictionaryEncodings()
org/apache/parquet/format/converter/ParquetMetadataConverter/convertEncodingStats(org.apache.parquet.column.EncodingStats)#org/apache/parquet/column/EncodingStats/getNumDictionaryPagesEncodedAs(org.apache.parquet.column.Encoding)
org/apache/parquet/format/converter/ParquetMetadataConverter/filterFileMetaDataByMidpoint(org.apache.parquet.format.FileMetaData,org.apache.parquet.format.converter.ParquetMetadataConverter$RangeMetadataFilter)#java/util/ArrayList/ArrayList()
org/apache/parquet/format/converter/ParquetMetadataConverter/filterFileMetaDataByMidpoint(org.apache.parquet.format.FileMetaData,org.apache.parquet.format.converter.ParquetMetadataConverter$RangeMetadataFilter)#org/apache/parquet/format/ColumnChunk/getMeta_data()
org/apache/parquet/format/converter/ParquetMetadataConverter/filterFileMetaDataByMidpoint(org.apache.parquet.format.FileMetaData,org.apache.parquet.format.converter.ParquetMetadataConverter$RangeMetadataFilter)#org/apache/parquet/format/FileMetaData/getRow_groups()
org/apache/parquet/format/converter/ParquetMetadataConverter/filterFileMetaDataByMidpoint(org.apache.parquet.format.FileMetaData,org.apache.parquet.format.converter.ParquetMetadataConverter$RangeMetadataFilter)#org/apache/parquet/format/ColumnMetaData/getTotal_compressed_size()
org/apache/parquet/format/converter/ParquetMetadataConverter/filterFileMetaDataByMidpoint(org.apache.parquet.format.FileMetaData,org.apache.parquet.format.converter.ParquetMetadataConverter$RangeMetadataFilter)#java/util/Iterator/next()
org/apache/parquet/format/converter/ParquetMetadataConverter/filterFileMetaDataByMidpoint(org.apache.parquet.format.FileMetaData,org.apache.parquet.format.converter.ParquetMetadataConverter$RangeMetadataFilter)#java/util/List/add(java.lang.Object)
org/apache/parquet/format/converter/ParquetMetadataConverter/filterFileMetaDataByMidpoint(org.apache.parquet.format.FileMetaData,org.apache.parquet.format.converter.ParquetMetadataConverter$RangeMetadataFilter)#java/util/List/get(int)
org/apache/parquet/format/converter/ParquetMetadataConverter/filterFileMetaDataByMidpoint(org.apache.parquet.format.FileMetaData,org.apache.parquet.format.converter.ParquetMetadataConverter$RangeMetadataFilter)#org/apache/parquet/format/FileMetaData/setRow_groups(java.util.List)
org/apache/parquet/format/converter/ParquetMetadataConverter/filterFileMetaDataByMidpoint(org.apache.parquet.format.FileMetaData,org.apache.parquet.format.converter.ParquetMetadataConverter$RangeMetadataFilter)#java/util/Iterator/hasNext()
org/apache/parquet/format/converter/ParquetMetadataConverter/filterFileMetaDataByMidpoint(org.apache.parquet.format.FileMetaData,org.apache.parquet.format.converter.ParquetMetadataConverter$RangeMetadataFilter)#org/apache/parquet/format/RowGroup/getColumns()
org/apache/parquet/format/converter/ParquetMetadataConverter/filterFileMetaDataByMidpoint(org.apache.parquet.format.FileMetaData,org.apache.parquet.format.converter.ParquetMetadataConverter$RangeMetadataFilter)#java/util/List/iterator()
org/apache/parquet/format/converter/ParquetMetadataConverter/filterFileMetaDataByMidpoint(org.apache.parquet.format.FileMetaData,org.apache.parquet.format.converter.ParquetMetadataConverter$RangeMetadataFilter)#org/apache/parquet/format/converter/ParquetMetadataConverter$RangeMetadataFilter/contains(long)
org/apache/parquet/format/converter/ParquetMetadataConverter/buildChildren(org.apache.parquet.schema.Types$GroupBuilder,java.util.Iterator,int)#org/apache/parquet/format/SchemaElement/isSetScale()
org/apache/parquet/format/converter/ParquetMetadataConverter/buildChildren(org.apache.parquet.schema.Types$GroupBuilder,java.util.Iterator,int)#java/util/Iterator/next()
org/apache/parquet/format/converter/ParquetMetadataConverter/buildChildren(org.apache.parquet.schema.Types$GroupBuilder,java.util.Iterator,int)#org/apache/parquet/schema/Types$Builder/named(java.lang.String)
org/apache/parquet/format/converter/ParquetMetadataConverter/buildChildren(org.apache.parquet.schema.Types$GroupBuilder,java.util.Iterator,int)#org/apache/parquet/format/SchemaElement/isSetField_id()
org/apache/parquet/format/converter/ParquetMetadataConverter/buildChildren(org.apache.parquet.schema.Types$GroupBuilder,java.util.Iterator,int)#org/apache/parquet/schema/Types$GroupBuilder/group(org.apache.parquet.schema.Type$Repetition)
org/apache/parquet/format/converter/ParquetMetadataConverter/buildChildren(org.apache.parquet.schema.Types$GroupBuilder,java.util.Iterator,int)#org/apache/parquet/format/SchemaElement/isSetPrecision()
org/apache/parquet/format/converter/ParquetMetadataConverter/buildChildren(org.apache.parquet.schema.Types$GroupBuilder,java.util.Iterator,int)#org/apache/parquet/format/SchemaElement/isSetConverted_type()
org/apache/parquet/format/converter/ParquetMetadataConverter/buildChildren(org.apache.parquet.schema.Types$GroupBuilder,java.util.Iterator,int)#org/apache/parquet/schema/Types$Builder/as(org.apache.parquet.schema.OriginalType)
org/apache/parquet/format/converter/ParquetMetadataConverter/buildChildren(org.apache.parquet.schema.Types$GroupBuilder,java.util.Iterator,int)#org/apache/parquet/schema/Types$PrimitiveBuilder/length(int)
org/apache/parquet/format/converter/ParquetMetadataConverter/buildChildren(org.apache.parquet.schema.Types$GroupBuilder,java.util.Iterator,int)#org/apache/parquet/schema/Types$PrimitiveBuilder/scale(int)
org/apache/parquet/format/converter/ParquetMetadataConverter/buildChildren(org.apache.parquet.schema.Types$GroupBuilder,java.util.Iterator,int)#org/apache/parquet/schema/Types$Builder/id(int)
org/apache/parquet/format/converter/ParquetMetadataConverter/buildChildren(org.apache.parquet.schema.Types$GroupBuilder,java.util.Iterator,int)#org/apache/parquet/format/SchemaElement/isSetType_length()
org/apache/parquet/format/converter/ParquetMetadataConverter/buildChildren(org.apache.parquet.schema.Types$GroupBuilder,java.util.Iterator,int)#org/apache/parquet/schema/Types$PrimitiveBuilder/precision(int)
org/apache/parquet/format/converter/ParquetMetadataConverter/buildChildren(org.apache.parquet.schema.Types$GroupBuilder,java.util.Iterator,int)#org/apache/parquet/schema/Types$GroupBuilder/primitive(org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName,org.apache.parquet.schema.Type$Repetition)
org/apache/parquet/format/converter/ParquetMetadataConverter/fromParquetMetadata(org.apache.parquet.format.FileMetaData)#org/apache/parquet/format/ColumnMetaData/getEncoding_stats()
org/apache/parquet/format/converter/ParquetMetadataConverter/fromParquetMetadata(org.apache.parquet.format.FileMetaData)#org/apache/parquet/format/FileMetaData/getCreated_by()
org/apache/parquet/format/converter/ParquetMetadataConverter/fromParquetMetadata(org.apache.parquet.format.FileMetaData)#org/apache/parquet/schema/Type/asPrimitiveType()
org/apache/parquet/format/converter/ParquetMetadataConverter/fromParquetMetadata(org.apache.parquet.format.FileMetaData)#org/apache/parquet/format/FileMetaData/getSchema()
org/apache/parquet/format/converter/ParquetMetadataConverter/fromParquetMetadata(org.apache.parquet.format.FileMetaData)#org/apache/parquet/io/ParquetDecodingException/ParquetDecodingException(java.lang.String)
org/apache/parquet/format/converter/ParquetMetadataConverter/fromParquetMetadata(org.apache.parquet.format.FileMetaData)#org/apache/parquet/schema/MessageType/getType(java.lang.String[])
org/apache/parquet/format/converter/ParquetMetadataConverter/fromParquetMetadata(org.apache.parquet.format.FileMetaData)#org/apache/parquet/hadoop/metadata/CompressionCodecName/fromParquet(org.apache.parquet.format.CompressionCodec)
org/apache/parquet/format/converter/ParquetMetadataConverter/fromParquetMetadata(org.apache.parquet.format.FileMetaData)#java/util/Map/put(java.lang.Object,java.lang.Object)
org/apache/parquet/format/converter/ParquetMetadataConverter/fromParquetMetadata(org.apache.parquet.format.FileMetaData)#java/util/ArrayList/ArrayList()
org/apache/parquet/format/converter/ParquetMetadataConverter/fromParquetMetadata(org.apache.parquet.format.FileMetaData)#java/util/Iterator/hasNext()
org/apache/parquet/format/converter/ParquetMetadataConverter/fromParquetMetadata(org.apache.parquet.format.FileMetaData)#org/apache/parquet/format/RowGroup/getTotal_byte_size()
org/apache/parquet/format/converter/ParquetMetadataConverter/fromParquetMetadata(org.apache.parquet.format.FileMetaData)#org/apache/parquet/format/RowGroup/getColumns()
org/apache/parquet/format/converter/ParquetMetadataConverter/fromParquetMetadata(org.apache.parquet.format.FileMetaData)#org/apache/parquet/format/ColumnChunk/getFile_path()
org/apache/parquet/format/converter/ParquetMetadataConverter/fromParquetMetadata(org.apache.parquet.format.FileMetaData)#org/apache/parquet/format/FileMetaData/getRow_groups()
org/apache/parquet/format/converter/ParquetMetadataConverter/fromParquetMetadata(org.apache.parquet.format.FileMetaData)#org/apache/parquet/hadoop/metadata/ColumnPath/toArray()
org/apache/parquet/format/converter/ParquetMetadataConverter/fromParquetMetadata(org.apache.parquet.format.FileMetaData)#java/util/Iterator/next()
org/apache/parquet/format/converter/ParquetMetadataConverter/fromParquetMetadata(org.apache.parquet.format.FileMetaData)#org/apache/parquet/format/FileMetaData/getKey_value_metadata()
org/apache/parquet/format/converter/ParquetMetadataConverter/fromParquetMetadata(org.apache.parquet.format.FileMetaData)#java/util/HashMap/HashMap()
org/apache/parquet/format/converter/ParquetMetadataConverter/fromParquetMetadata(org.apache.parquet.format.FileMetaData)#java/lang/String/equals(java.lang.Object)
org/apache/parquet/format/converter/ParquetMetadataConverter/fromParquetMetadata(org.apache.parquet.format.FileMetaData)#java/util/List/add(java.lang.Object)
org/apache/parquet/format/converter/ParquetMetadataConverter/fromParquetMetadata(org.apache.parquet.format.FileMetaData)#java/util/List/get(int)
org/apache/parquet/format/converter/ParquetMetadataConverter/fromParquetMetadata(org.apache.parquet.format.FileMetaData)#java/util/List/iterator()
org/apache/parquet/format/converter/ParquetMetadataConverter/fromParquetMetadata(org.apache.parquet.format.FileMetaData)#org/apache/parquet/schema/PrimitiveType/getPrimitiveTypeName()
org/apache/parquet/format/converter/ParquetMetadataConverter/fromParquetMetadata(org.apache.parquet.format.FileMetaData)#org/apache/parquet/format/RowGroup/getNum_rows()
org/apache/parquet/format/converter/ParquetMetadataConverter/toParquetStatistics(org.apache.parquet.column.statistics.Statistics)#org/apache/parquet/format/Statistics/setMin(byte[])
org/apache/parquet/format/converter/ParquetMetadataConverter/toParquetStatistics(org.apache.parquet.column.statistics.Statistics)#org/apache/parquet/column/statistics/Statistics/getNumNulls()
org/apache/parquet/format/converter/ParquetMetadataConverter/toParquetStatistics(org.apache.parquet.column.statistics.Statistics)#org/apache/parquet/column/statistics/Statistics/getMaxBytes()
org/apache/parquet/format/converter/ParquetMetadataConverter/toParquetStatistics(org.apache.parquet.column.statistics.Statistics)#org/apache/parquet/format/Statistics/setNull_count(long)
org/apache/parquet/format/converter/ParquetMetadataConverter/toParquetStatistics(org.apache.parquet.column.statistics.Statistics)#org/apache/parquet/column/statistics/Statistics/isEmpty()
org/apache/parquet/format/converter/ParquetMetadataConverter/toParquetStatistics(org.apache.parquet.column.statistics.Statistics)#org/apache/parquet/format/Statistics/setMax(byte[])
org/apache/parquet/format/converter/ParquetMetadataConverter/toParquetStatistics(org.apache.parquet.column.statistics.Statistics)#org/apache/parquet/column/statistics/Statistics/isSmallerThan(long)
org/apache/parquet/format/converter/ParquetMetadataConverter/toParquetStatistics(org.apache.parquet.column.statistics.Statistics)#org/apache/parquet/column/statistics/Statistics/getMinBytes()
org/apache/parquet/format/converter/ParquetMetadataConverter/toParquetStatistics(org.apache.parquet.column.statistics.Statistics)#org/apache/parquet/column/statistics/Statistics/hasNonNullValue()
org/apache/parquet/format/converter/ParquetMetadataConverter/toParquetStatistics(org.apache.parquet.column.statistics.Statistics)#org/apache/parquet/format/Statistics/Statistics()
org/apache/parquet/format/converter/ParquetMetadataConverter/toParquetMetadata(int,org.apache.parquet.hadoop.metadata.ParquetMetadata)#java/util/ArrayList/ArrayList()
org/apache/parquet/format/converter/ParquetMetadataConverter/toParquetMetadata(int,org.apache.parquet.hadoop.metadata.ParquetMetadata)#java/util/Iterator/hasNext()
org/apache/parquet/format/converter/ParquetMetadataConverter/toParquetMetadata(int,org.apache.parquet.hadoop.metadata.ParquetMetadata)#org/apache/parquet/format/FileMetaData/FileMetaData(int,java.util.List,long,java.util.List)
org/apache/parquet/format/converter/ParquetMetadataConverter/toParquetMetadata(int,org.apache.parquet.hadoop.metadata.ParquetMetadata)#java/util/Iterator/next()
org/apache/parquet/format/converter/ParquetMetadataConverter/toParquetMetadata(int,org.apache.parquet.hadoop.metadata.ParquetMetadata)#java/util/Set/iterator()
org/apache/parquet/format/converter/ParquetMetadataConverter/toParquetMetadata(int,org.apache.parquet.hadoop.metadata.ParquetMetadata)#java/util/Map$Entry/getValue()
org/apache/parquet/format/converter/ParquetMetadataConverter/toParquetMetadata(int,org.apache.parquet.hadoop.metadata.ParquetMetadata)#java/util/List/iterator()
org/apache/parquet/format/converter/ParquetMetadataConverter/toParquetMetadata(int,org.apache.parquet.hadoop.metadata.ParquetMetadata)#java/util/Map/entrySet()
org/apache/parquet/format/converter/ParquetMetadataConverter/toParquetMetadata(int,org.apache.parquet.hadoop.metadata.ParquetMetadata)#java/util/Map$Entry/getKey()
org/apache/parquet/format/converter/ParquetMetadataConverter/toParquetMetadata(int,org.apache.parquet.hadoop.metadata.ParquetMetadata)#org/apache/parquet/format/FileMetaData/setCreated_by(java.lang.String)
org/apache/parquet/format/converter/ParquetMetadataConverter/addRowGroup(org.apache.parquet.hadoop.metadata.ParquetMetadata,java.util.List,org.apache.parquet.hadoop.metadata.BlockMetaData)#org/apache/parquet/format/ColumnMetaData/ColumnMetaData(org.apache.parquet.format.Type,java.util.List,java.util.List,org.apache.parquet.format.CompressionCodec,long,long,long,long)
org/apache/parquet/format/converter/ParquetMetadataConverter/addRowGroup(org.apache.parquet.hadoop.metadata.ParquetMetadata,java.util.List,org.apache.parquet.hadoop.metadata.BlockMetaData)#org/apache/parquet/format/ColumnMetaData/setEncoding_stats(java.util.List)
org/apache/parquet/format/converter/ParquetMetadataConverter/addRowGroup(org.apache.parquet.hadoop.metadata.ParquetMetadata,java.util.List,org.apache.parquet.hadoop.metadata.BlockMetaData)#java/util/Iterator/next()
org/apache/parquet/format/converter/ParquetMetadataConverter/addRowGroup(org.apache.parquet.hadoop.metadata.ParquetMetadata,java.util.List,org.apache.parquet.hadoop.metadata.BlockMetaData)#org/apache/parquet/format/ColumnChunk/ColumnChunk(long)
org/apache/parquet/format/converter/ParquetMetadataConverter/addRowGroup(org.apache.parquet.hadoop.metadata.ParquetMetadata,java.util.List,org.apache.parquet.hadoop.metadata.BlockMetaData)#java/util/ArrayList/ArrayList()
org/apache/parquet/format/converter/ParquetMetadataConverter/addRowGroup(org.apache.parquet.hadoop.metadata.ParquetMetadata,java.util.List,org.apache.parquet.hadoop.metadata.BlockMetaData)#java/util/Iterator/hasNext()
org/apache/parquet/format/converter/ParquetMetadataConverter/addRowGroup(org.apache.parquet.hadoop.metadata.ParquetMetadata,java.util.List,org.apache.parquet.hadoop.metadata.BlockMetaData)#java/util/Arrays/asList(java.lang.Object[])
org/apache/parquet/format/converter/ParquetMetadataConverter/addRowGroup(org.apache.parquet.hadoop.metadata.ParquetMetadata,java.util.List,org.apache.parquet.hadoop.metadata.BlockMetaData)#org/apache/parquet/hadoop/metadata/ColumnPath/toArray()
org/apache/parquet/format/converter/ParquetMetadataConverter/addRowGroup(org.apache.parquet.hadoop.metadata.ParquetMetadata,java.util.List,org.apache.parquet.hadoop.metadata.BlockMetaData)#org/apache/parquet/format/RowGroup/RowGroup(java.util.List,long,long)
org/apache/parquet/format/converter/ParquetMetadataConverter/addRowGroup(org.apache.parquet.hadoop.metadata.ParquetMetadata,java.util.List,org.apache.parquet.hadoop.metadata.BlockMetaData)#org/apache/parquet/format/ColumnMetaData/setStatistics(org.apache.parquet.format.Statistics)
org/apache/parquet/format/converter/ParquetMetadataConverter/addRowGroup(org.apache.parquet.hadoop.metadata.ParquetMetadata,java.util.List,org.apache.parquet.hadoop.metadata.BlockMetaData)#org/apache/parquet/hadoop/metadata/CompressionCodecName/getParquetCompressionCodec()
org/apache/parquet/format/converter/ParquetMetadataConverter/addRowGroup(org.apache.parquet.hadoop.metadata.ParquetMetadata,java.util.List,org.apache.parquet.hadoop.metadata.BlockMetaData)#org/apache/parquet/column/statistics/Statistics/isEmpty()
org/apache/parquet/format/converter/ParquetMetadataConverter/addRowGroup(org.apache.parquet.hadoop.metadata.ParquetMetadata,java.util.List,org.apache.parquet.hadoop.metadata.BlockMetaData)#java/util/List/add(java.lang.Object)
org/apache/parquet/format/converter/ParquetMetadataConverter/addRowGroup(org.apache.parquet.hadoop.metadata.ParquetMetadata,java.util.List,org.apache.parquet.hadoop.metadata.BlockMetaData)#java/util/List/iterator()
org/apache/parquet/format/converter/ParquetMetadataConverter/convertEncodingStats(java.util.List)#java/util/Iterator/hasNext()
org/apache/parquet/format/converter/ParquetMetadataConverter/convertEncodingStats(java.util.List)#org/apache/parquet/format/PageEncodingStats/getPage_type()
org/apache/parquet/format/converter/ParquetMetadataConverter/convertEncodingStats(java.util.List)#org/apache/parquet/column/EncodingStats$Builder/addDataEncoding(org.apache.parquet.column.Encoding,int)
org/apache/parquet/format/converter/ParquetMetadataConverter/convertEncodingStats(java.util.List)#java/util/Iterator/next()
org/apache/parquet/format/converter/ParquetMetadataConverter/convertEncodingStats(java.util.List)#org/apache/parquet/column/EncodingStats$Builder/addDictEncoding(org.apache.parquet.column.Encoding,int)
org/apache/parquet/format/converter/ParquetMetadataConverter/convertEncodingStats(java.util.List)#org/apache/parquet/column/EncodingStats$Builder/build()
org/apache/parquet/format/converter/ParquetMetadataConverter/convertEncodingStats(java.util.List)#java/util/List/iterator()
org/apache/parquet/format/converter/ParquetMetadataConverter/convertEncodingStats(java.util.List)#org/apache/parquet/column/EncodingStats$Builder/Builder()
org/apache/parquet/format/converter/ParquetMetadataConverter/convertEncodingStats(java.util.List)#org/apache/parquet/column/EncodingStats$Builder/withV2Pages()
org/apache/parquet/format/converter/ParquetMetadataConverter/convertEncodingStats(java.util.List)#org/apache/parquet/format/PageType/ordinal()
org/apache/parquet/format/converter/ParquetMetadataConverter/convertEncodingStats(java.util.List)#org/apache/parquet/format/PageEncodingStats/getCount()
org/apache/parquet/format/converter/ParquetMetadataConverter/convertEncodingStats(java.util.List)#org/apache/parquet/format/PageEncodingStats/getEncoding()
org/apache/parquet/format/converter/ParquetMetadataConverter/filterFileMetaDataByStart(org.apache.parquet.format.FileMetaData,org.apache.parquet.format.converter.ParquetMetadataConverter$OffsetMetadataFilter)#org/apache/parquet/format/converter/ParquetMetadataConverter$OffsetMetadataFilter/contains(long)
org/apache/parquet/format/converter/ParquetMetadataConverter/filterFileMetaDataByStart(org.apache.parquet.format.FileMetaData,org.apache.parquet.format.converter.ParquetMetadataConverter$OffsetMetadataFilter)#java/util/ArrayList/ArrayList()
org/apache/parquet/format/converter/ParquetMetadataConverter/filterFileMetaDataByStart(org.apache.parquet.format.FileMetaData,org.apache.parquet.format.converter.ParquetMetadataConverter$OffsetMetadataFilter)#org/apache/parquet/format/FileMetaData/getRow_groups()
org/apache/parquet/format/converter/ParquetMetadataConverter/filterFileMetaDataByStart(org.apache.parquet.format.FileMetaData,org.apache.parquet.format.converter.ParquetMetadataConverter$OffsetMetadataFilter)#java/util/Iterator/next()
org/apache/parquet/format/converter/ParquetMetadataConverter/filterFileMetaDataByStart(org.apache.parquet.format.FileMetaData,org.apache.parquet.format.converter.ParquetMetadataConverter$OffsetMetadataFilter)#java/util/List/add(java.lang.Object)
org/apache/parquet/format/converter/ParquetMetadataConverter/filterFileMetaDataByStart(org.apache.parquet.format.FileMetaData,org.apache.parquet.format.converter.ParquetMetadataConverter$OffsetMetadataFilter)#java/util/List/get(int)
org/apache/parquet/format/converter/ParquetMetadataConverter/filterFileMetaDataByStart(org.apache.parquet.format.FileMetaData,org.apache.parquet.format.converter.ParquetMetadataConverter$OffsetMetadataFilter)#org/apache/parquet/format/FileMetaData/setRow_groups(java.util.List)
org/apache/parquet/format/converter/ParquetMetadataConverter/filterFileMetaDataByStart(org.apache.parquet.format.FileMetaData,org.apache.parquet.format.converter.ParquetMetadataConverter$OffsetMetadataFilter)#java/util/List/iterator()
org/apache/parquet/format/converter/ParquetMetadataConverter/filterFileMetaDataByStart(org.apache.parquet.format.FileMetaData,org.apache.parquet.format.converter.ParquetMetadataConverter$OffsetMetadataFilter)#java/util/Iterator/hasNext()
org/apache/parquet/format/converter/ParquetMetadataConverter/filterFileMetaDataByStart(org.apache.parquet.format.FileMetaData,org.apache.parquet.format.converter.ParquetMetadataConverter$OffsetMetadataFilter)#org/apache/parquet/format/RowGroup/getColumns()
org/apache/parquet/hadoop/api/InitContext/getMergedKeyValueMetaData()#java/util/Map/put(java.lang.Object,java.lang.Object)
org/apache/parquet/hadoop/api/InitContext/getMergedKeyValueMetaData()#java/util/Iterator/hasNext()
org/apache/parquet/hadoop/api/InitContext/getMergedKeyValueMetaData()#java/lang/StringBuilder/StringBuilder()
org/apache/parquet/hadoop/api/InitContext/getMergedKeyValueMetaData()#java/util/Set/iterator()
org/apache/parquet/hadoop/api/InitContext/getMergedKeyValueMetaData()#java/lang/RuntimeException/RuntimeException(java.lang.String)
org/apache/parquet/hadoop/api/InitContext/getMergedKeyValueMetaData()#java/util/Map/entrySet()
org/apache/parquet/hadoop/api/InitContext/getMergedKeyValueMetaData()#java/lang/StringBuilder/toString()
org/apache/parquet/hadoop/api/InitContext/getMergedKeyValueMetaData()#java/util/Set/size()
org/apache/parquet/hadoop/api/InitContext/getMergedKeyValueMetaData()#java/lang/StringBuilder/append(java.lang.String)
org/apache/parquet/hadoop/api/InitContext/getMergedKeyValueMetaData()#java/util/Map$Entry/getKey()
org/apache/parquet/hadoop/api/InitContext/getMergedKeyValueMetaData()#java/util/Iterator/next()
org/apache/parquet/hadoop/api/InitContext/getMergedKeyValueMetaData()#java/util/HashMap/HashMap()
org/apache/parquet/hadoop/api/InitContext/getMergedKeyValueMetaData()#java/lang/StringBuilder/append(java.lang.Object)
org/apache/parquet/hadoop/api/InitContext/getMergedKeyValueMetaData()#java/util/Map$Entry/getValue()
org/apache/parquet/hadoop/InternalParquetRecordReader/toSetMultiMap(java.util.Map)#java/util/Map/put(java.lang.Object,java.lang.Object)
org/apache/parquet/hadoop/InternalParquetRecordReader/toSetMultiMap(java.util.Map)#java/util/Iterator/hasNext()
org/apache/parquet/hadoop/InternalParquetRecordReader/toSetMultiMap(java.util.Map)#java/util/Map$Entry/getKey()
org/apache/parquet/hadoop/InternalParquetRecordReader/toSetMultiMap(java.util.Map)#java/util/Collections/unmodifiableMap(java.util.Map)
org/apache/parquet/hadoop/InternalParquetRecordReader/toSetMultiMap(java.util.Map)#java/util/Set/iterator()
org/apache/parquet/hadoop/InternalParquetRecordReader/toSetMultiMap(java.util.Map)#java/util/Map$Entry/getValue()
org/apache/parquet/hadoop/InternalParquetRecordReader/toSetMultiMap(java.util.Map)#java/util/HashSet/HashSet()
org/apache/parquet/hadoop/InternalParquetRecordReader/toSetMultiMap(java.util.Map)#java/util/Iterator/next()
org/apache/parquet/hadoop/InternalParquetRecordReader/toSetMultiMap(java.util.Map)#java/util/HashMap/HashMap()
org/apache/parquet/hadoop/InternalParquetRecordReader/toSetMultiMap(java.util.Map)#java/util/Map/entrySet()
org/apache/parquet/hadoop/InternalParquetRecordReader/toSetMultiMap(java.util.Map)#java/util/Set/add(java.lang.Object)
org/apache/parquet/hadoop/InternalParquetRecordReader/toSetMultiMap(java.util.Map)#java/util/Collections/unmodifiableSet(java.util.Set)
org/apache/parquet/hadoop/InternalParquetRecordReader/checkRead()#java/io/IOException/IOException(java.lang.String)
org/apache/parquet/hadoop/InternalParquetRecordReader/checkRead()#java/lang/StringBuilder/append(java.lang.String)
org/apache/parquet/hadoop/InternalParquetRecordReader/checkRead()#java/lang/StringBuilder/append(int)
org/apache/parquet/hadoop/InternalParquetRecordReader/checkRead()#java/lang/StringBuilder/append(long)
org/apache/parquet/hadoop/InternalParquetRecordReader/checkRead()#java/lang/StringBuilder/StringBuilder()
org/apache/parquet/hadoop/InternalParquetRecordReader/checkRead()#org/apache/parquet/io/ColumnIOFactory/getColumnIO(org.apache.parquet.schema.MessageType,org.apache.parquet.schema.MessageType,boolean)
org/apache/parquet/hadoop/InternalParquetRecordReader/checkRead()#org/slf4j/Logger/isInfoEnabled()
org/apache/parquet/hadoop/InternalParquetRecordReader/checkRead()#java/lang/StringBuilder/toString()
org/apache/parquet/hadoop/InternalParquetRecordReader/checkRead()#java/lang/Long/valueOf(long)
org/apache/parquet/hadoop/InternalParquetRecordReader/checkRead()#org/slf4j/Logger/debug(java.lang.String,java.lang.Object)
org/apache/parquet/hadoop/InternalParquetRecordReader/checkRead()#org/apache/parquet/io/MessageColumnIO/getRecordReader(org.apache.parquet.column.page.PageReadStore,org.apache.parquet.io.api.RecordMaterializer,org.apache.parquet.filter2.compat.FilterCompat$Filter)
org/apache/parquet/hadoop/InternalParquetRecordReader/checkRead()#org/slf4j/Logger/info(java.lang.String,java.lang.Object,java.lang.Object)
org/apache/parquet/hadoop/InternalParquetRecordReader/checkRead()#org/slf4j/Logger/info(java.lang.String)
org/apache/parquet/hadoop/InternalParquetRecordReader/checkRead()#org/apache/parquet/column/page/PageReadStore/getRowCount()
org/apache/parquet/hadoop/InternalParquetRecordReader/checkRead()#java/lang/System/currentTimeMillis()
org/apache/parquet/hadoop/InternalParquetRecordReader/checkRead()#java/lang/StringBuilder/append(float)
org/apache/parquet/format/converter/ParquetMetadataConverter/1/visit(org.apache.parquet.schema.PrimitiveType)#org/apache/parquet/schema/PrimitiveType/getTypeLength()
org/apache/parquet/format/converter/ParquetMetadataConverter/1/visit(org.apache.parquet.schema.PrimitiveType)#org/apache/parquet/format/SchemaElement/setRepetition_type(org.apache.parquet.format.FieldRepetitionType)
org/apache/parquet/format/converter/ParquetMetadataConverter/1/visit(org.apache.parquet.schema.PrimitiveType)#org/apache/parquet/schema/DecimalMetadata/getPrecision()
org/apache/parquet/format/converter/ParquetMetadataConverter/1/visit(org.apache.parquet.schema.PrimitiveType)#org/apache/parquet/schema/PrimitiveType/getDecimalMetadata()
org/apache/parquet/format/converter/ParquetMetadataConverter/1/visit(org.apache.parquet.schema.PrimitiveType)#org/apache/parquet/schema/PrimitiveType/getRepetition()
org/apache/parquet/format/converter/ParquetMetadataConverter/1/visit(org.apache.parquet.schema.PrimitiveType)#org/apache/parquet/format/SchemaElement/setType_length(int)
org/apache/parquet/format/converter/ParquetMetadataConverter/1/visit(org.apache.parquet.schema.PrimitiveType)#org/apache/parquet/schema/PrimitiveType/getPrimitiveTypeName()
org/apache/parquet/format/converter/ParquetMetadataConverter/1/visit(org.apache.parquet.schema.PrimitiveType)#org/apache/parquet/format/SchemaElement/setType(org.apache.parquet.format.Type)
org/apache/parquet/format/converter/ParquetMetadataConverter/1/visit(org.apache.parquet.schema.PrimitiveType)#org/apache/parquet/format/SchemaElement/SchemaElement(java.lang.String)
org/apache/parquet/format/converter/ParquetMetadataConverter/1/visit(org.apache.parquet.schema.PrimitiveType)#org/apache/parquet/format/SchemaElement/setPrecision(int)
org/apache/parquet/format/converter/ParquetMetadataConverter/1/visit(org.apache.parquet.schema.PrimitiveType)#org/apache/parquet/schema/PrimitiveType/getName()
org/apache/parquet/format/converter/ParquetMetadataConverter/1/visit(org.apache.parquet.schema.PrimitiveType)#org/apache/parquet/schema/PrimitiveType/getOriginalType()
org/apache/parquet/format/converter/ParquetMetadataConverter/1/visit(org.apache.parquet.schema.PrimitiveType)#java/util/List/add(java.lang.Object)
org/apache/parquet/format/converter/ParquetMetadataConverter/1/visit(org.apache.parquet.schema.PrimitiveType)#org/apache/parquet/format/SchemaElement/setConverted_type(org.apache.parquet.format.ConvertedType)
org/apache/parquet/format/converter/ParquetMetadataConverter/1/visit(org.apache.parquet.schema.PrimitiveType)#org/apache/parquet/format/SchemaElement/setScale(int)
org/apache/parquet/format/converter/ParquetMetadataConverter/1/visit(org.apache.parquet.schema.PrimitiveType)#org/apache/parquet/schema/DecimalMetadata/getScale()
org/apache/parquet/hadoop/ParquetOutputFormat/getRecordWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.CompressionCodecName)#java/lang/StringBuilder/append(java.lang.String)
org/apache/parquet/hadoop/ParquetOutputFormat/getRecordWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.CompressionCodecName)#org/apache/parquet/column/ParquetProperties/getMinRowCountForPageSizeCheck()
org/apache/parquet/hadoop/ParquetOutputFormat/getRecordWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.CompressionCodecName)#org/apache/parquet/column/ParquetProperties/isEnableDictionary()
org/apache/parquet/hadoop/ParquetOutputFormat/getRecordWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.CompressionCodecName)#org/apache/parquet/column/ParquetProperties/getPageSizeThreshold()
org/apache/parquet/hadoop/ParquetOutputFormat/getRecordWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.CompressionCodecName)#org/apache/parquet/column/ParquetProperties$Builder/withMaxRowCountForPageSizeCheck(int)
org/apache/parquet/hadoop/ParquetOutputFormat/getRecordWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.CompressionCodecName)#java/lang/StringBuilder/StringBuilder()
org/apache/parquet/hadoop/ParquetOutputFormat/getRecordWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.CompressionCodecName)#org/slf4j/Logger/isInfoEnabled()
org/apache/parquet/hadoop/ParquetOutputFormat/getRecordWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.CompressionCodecName)#org/apache/parquet/column/ParquetProperties$Builder/withWriterVersion(org.apache.parquet.column.ParquetProperties$WriterVersion)
org/apache/parquet/hadoop/ParquetOutputFormat/getRecordWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.CompressionCodecName)#org/apache/parquet/column/ParquetProperties$Builder/withMinRowCountForPageSizeCheck(int)
org/apache/parquet/hadoop/ParquetOutputFormat/getRecordWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.CompressionCodecName)#org/slf4j/Logger/warn(java.lang.String)
org/apache/parquet/hadoop/ParquetOutputFormat/getRecordWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.CompressionCodecName)#org/apache/hadoop/conf/Configuration/getLong(java.lang.String,long)
org/apache/parquet/hadoop/ParquetOutputFormat/getRecordWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.CompressionCodecName)#java/lang/Long/valueOf(long)
org/apache/parquet/hadoop/ParquetOutputFormat/getRecordWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.CompressionCodecName)#org/apache/parquet/column/ParquetProperties/builder()
org/apache/parquet/hadoop/ParquetOutputFormat/getRecordWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.CompressionCodecName)#org/apache/parquet/column/ParquetProperties$Builder/withPageSize(int)
org/apache/parquet/hadoop/ParquetOutputFormat/getRecordWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.CompressionCodecName)#org/apache/parquet/column/ParquetProperties$Builder/build()
org/apache/parquet/hadoop/ParquetOutputFormat/getRecordWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.CompressionCodecName)#org/apache/parquet/column/ParquetProperties/estimateNextSizeCheck()
org/apache/parquet/hadoop/ParquetOutputFormat/getRecordWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.CompressionCodecName)#java/lang/Integer/valueOf(int)
org/apache/parquet/hadoop/ParquetOutputFormat/getRecordWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.CompressionCodecName)#org/apache/parquet/column/ParquetProperties$Builder/withDictionaryPageSize(int)
org/apache/parquet/hadoop/ParquetOutputFormat/getRecordWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.CompressionCodecName)#org/apache/parquet/column/ParquetProperties/getWriterVersion()
org/apache/parquet/hadoop/ParquetOutputFormat/getRecordWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.CompressionCodecName)#org/apache/parquet/column/ParquetProperties/getDictionaryPageSizeThreshold()
org/apache/parquet/hadoop/ParquetOutputFormat/getRecordWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.CompressionCodecName)#org/apache/parquet/hadoop/api/WriteSupport$WriteContext/getSchema()
org/apache/parquet/hadoop/ParquetOutputFormat/getRecordWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.CompressionCodecName)#org/slf4j/Logger/info(java.lang.String,java.lang.Object)
org/apache/parquet/hadoop/ParquetOutputFormat/getRecordWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.CompressionCodecName)#org/apache/parquet/column/ParquetProperties$Builder/withDictionaryEncoding(boolean)
org/apache/parquet/hadoop/ParquetOutputFormat/getRecordWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.CompressionCodecName)#org/apache/parquet/column/ParquetProperties/getMaxRowCountForPageSizeCheck()
org/apache/parquet/hadoop/ParquetOutputFormat/getRecordWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.CompressionCodecName)#org/apache/parquet/hadoop/api/WriteSupport$WriteContext/getExtraMetaData()
org/apache/parquet/hadoop/ParquetOutputFormat/getRecordWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.CompressionCodecName)#org/apache/hadoop/conf/Configuration/getFloat(java.lang.String,float)
org/apache/parquet/hadoop/ParquetOutputFormat/getRecordWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.CompressionCodecName)#java/lang/StringBuilder/append(float)
org/apache/parquet/hadoop/ParquetOutputFormat/getRecordWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.CompressionCodecName)#java/lang/StringBuilder/toString()
org/apache/parquet/hadoop/ParquetOutputFormat/getRecordWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.CompressionCodecName)#org/apache/parquet/column/ParquetProperties$Builder/estimateRowCountForPageSizeCheck(boolean)
org/apache/parquet/hadoop/MemoryManager/updateAllocation()#org/slf4j/Logger/debug(java.lang.String)
org/apache/parquet/hadoop/MemoryManager/updateAllocation()#java/lang/Math/floor(double)
org/apache/parquet/hadoop/MemoryManager/updateAllocation()#org/slf4j/Logger/warn(java.lang.String)
org/apache/parquet/hadoop/MemoryManager/updateAllocation()#java/util/Map$Entry/getKey()
org/apache/parquet/hadoop/MemoryManager/updateAllocation()#java/lang/Math/max(int,int)
org/apache/parquet/hadoop/MemoryManager/updateAllocation()#org/apache/parquet/schema/MessageType/getColumns()
org/apache/parquet/hadoop/MemoryManager/updateAllocation()#java/util/Map/values()
org/apache/parquet/hadoop/MemoryManager/updateAllocation()#java/util/Set/iterator()
org/apache/parquet/hadoop/MemoryManager/updateAllocation()#java/lang/Integer/valueOf(int)
org/apache/parquet/hadoop/MemoryManager/updateAllocation()#java/util/Map$Entry/getValue()
org/apache/parquet/hadoop/MemoryManager/updateAllocation()#java/util/Collection/iterator()
org/apache/parquet/hadoop/MemoryManager/updateAllocation()#java/util/Map/entrySet()
org/apache/parquet/hadoop/MemoryManager/updateAllocation()#java/util/Iterator/hasNext()
org/apache/parquet/hadoop/MemoryManager/updateAllocation()#java/lang/Double/valueOf(double)
org/apache/parquet/hadoop/MemoryManager/updateAllocation()#org/apache/parquet/hadoop/MemoryManager$1/1(org.apache.parquet.hadoop.MemoryManager,java.lang.String)
org/apache/parquet/hadoop/MemoryManager/updateAllocation()#java/lang/Runnable/run()
org/apache/parquet/hadoop/MemoryManager/updateAllocation()#java/lang/Long/valueOf(long)
org/apache/parquet/hadoop/MemoryManager/updateAllocation()#java/lang/String/format(java.lang.String,java.lang.Object[])
org/apache/parquet/hadoop/MemoryManager/updateAllocation()#java/lang/Float/valueOf(float)
org/apache/parquet/hadoop/MemoryManager/updateAllocation()#java/util/List/size()
org/apache/parquet/hadoop/MemoryManager/updateAllocation()#java/util/Map/size()
org/apache/parquet/hadoop/MemoryManager/updateAllocation()#java/util/Iterator/next()
org/apache/parquet/hadoop/MemoryManager/updateAllocation()#java/util/Map/keySet()
org/apache/parquet/hadoop/MemoryManager/updateAllocation()#java/lang/Long/longValue()
org/apache/parquet/hadoop/ParquetFileReader/ConsecutiveChunkList/readAll(org.apache.parquet.io.SeekableInputStream)#org/apache/parquet/hadoop/ParquetFileReader$WorkaroundChunk/WorkaroundChunk(org.apache.parquet.hadoop.ParquetFileReader,org.apache.parquet.hadoop.ParquetFileReader$ChunkDescriptor,byte[],int,org.apache.parquet.io.SeekableInputStream,org.apache.parquet.hadoop.ParquetFileReader$1)
org/apache/parquet/hadoop/ParquetFileReader/ConsecutiveChunkList/readAll(org.apache.parquet.io.SeekableInputStream)#java/util/ArrayList/ArrayList(int)
org/apache/parquet/hadoop/ParquetFileReader/ConsecutiveChunkList/readAll(org.apache.parquet.io.SeekableInputStream)#org/apache/parquet/hadoop/ParquetFileReader$ChunkDescriptor/access$900(org.apache.parquet.hadoop.ParquetFileReader$ChunkDescriptor)
org/apache/parquet/hadoop/ParquetFileReader/ConsecutiveChunkList/readAll(org.apache.parquet.io.SeekableInputStream)#org/apache/parquet/io/SeekableInputStream/seek(long)
org/apache/parquet/hadoop/ParquetFileReader/ConsecutiveChunkList/readAll(org.apache.parquet.io.SeekableInputStream)#org/apache/parquet/hadoop/ParquetFileReader$Chunk/Chunk(org.apache.parquet.hadoop.ParquetFileReader,org.apache.parquet.hadoop.ParquetFileReader$ChunkDescriptor,byte[],int)
org/apache/parquet/hadoop/ParquetFileReader/ConsecutiveChunkList/readAll(org.apache.parquet.io.SeekableInputStream)#java/util/List/size()
org/apache/parquet/hadoop/ParquetFileReader/ConsecutiveChunkList/readAll(org.apache.parquet.io.SeekableInputStream)#java/util/List/get(int)
org/apache/parquet/hadoop/ParquetFileReader/ConsecutiveChunkList/readAll(org.apache.parquet.io.SeekableInputStream)#java/util/List/add(java.lang.Object)
org/apache/parquet/hadoop/ParquetFileReader/ConsecutiveChunkList/readAll(org.apache.parquet.io.SeekableInputStream)#org/apache/parquet/io/SeekableInputStream/readFully(byte[])
org/apache/parquet/format/converter/ParquetMetadataConverter/3/<clinit>()#org/apache/parquet/format/ConvertedType/ordinal()
org/apache/parquet/format/converter/ParquetMetadataConverter/3/<clinit>()#org/apache/parquet/schema/OriginalType/ordinal()
org/apache/parquet/format/converter/ParquetMetadataConverter/3/<clinit>()#org/apache/parquet/schema/OriginalType/values()
org/apache/parquet/format/converter/ParquetMetadataConverter/3/<clinit>()#org/apache/parquet/format/ConvertedType/values()
org/apache/parquet/format/converter/ParquetMetadataConverter/3/<clinit>()#org/apache/parquet/schema/PrimitiveType$PrimitiveTypeName/ordinal()
org/apache/parquet/format/converter/ParquetMetadataConverter/3/<clinit>()#org/apache/parquet/schema/PrimitiveType$PrimitiveTypeName/values()
org/apache/parquet/format/converter/ParquetMetadataConverter/3/<clinit>()#org/apache/parquet/format/PageType/ordinal()
org/apache/parquet/format/converter/ParquetMetadataConverter/3/<clinit>()#org/apache/parquet/format/Type/values()
org/apache/parquet/format/converter/ParquetMetadataConverter/3/<clinit>()#org/apache/parquet/format/Type/ordinal()
org/apache/parquet/format/converter/ParquetMetadataConverter/3/<clinit>()#org/apache/parquet/format/PageType/values()
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$UserDefined,boolean)#org/apache/parquet/filter2/predicate/Operators$UserDefined/getColumn()
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$UserDefined,boolean)#java/util/Iterator/hasNext()
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$UserDefined,boolean)#org/apache/parquet/filter2/predicate/Operators$Column/getColumnPath()
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$UserDefined,boolean)#org/apache/parquet/filter2/predicate/UserDefinedPredicate/keep(java.lang.Comparable)
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$UserDefined,boolean)#org/apache/parquet/filter2/predicate/Operators$UserDefined/getUserDefinedPredicate()
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$UserDefined,boolean)#org/slf4j/Logger/warn(java.lang.String,java.lang.Throwable)
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$UserDefined,boolean)#java/util/Iterator/next()
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$UserDefined,boolean)#java/lang/Boolean/valueOf(boolean)
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$UserDefined,boolean)#java/util/Set/iterator()
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/expandDictionary(org.apache.parquet.hadoop.metadata.ColumnChunkMetaData)#org/apache/parquet/column/Dictionary/decodeToBinary(int)
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/expandDictionary(org.apache.parquet.hadoop.metadata.ColumnChunkMetaData)#org/slf4j/Logger/warn(java.lang.String,java.lang.Object)
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/expandDictionary(org.apache.parquet.hadoop.metadata.ColumnChunkMetaData)#org/apache/parquet/column/Dictionary/decodeToInt(int)
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/expandDictionary(org.apache.parquet.hadoop.metadata.ColumnChunkMetaData)#org/apache/parquet/schema/PrimitiveType$PrimitiveTypeName/ordinal()
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/expandDictionary(org.apache.parquet.hadoop.metadata.ColumnChunkMetaData)#org/apache/parquet/column/page/DictionaryPageReadStore/readDictionaryPage(org.apache.parquet.column.ColumnDescriptor)
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/expandDictionary(org.apache.parquet.hadoop.metadata.ColumnChunkMetaData)#org/apache/parquet/column/Dictionary/decodeToDouble(int)
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/expandDictionary(org.apache.parquet.hadoop.metadata.ColumnChunkMetaData)#org/apache/parquet/column/page/DictionaryPage/getEncoding()
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/expandDictionary(org.apache.parquet.hadoop.metadata.ColumnChunkMetaData)#org/apache/parquet/column/Dictionary/getMaxId()
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/expandDictionary(org.apache.parquet.hadoop.metadata.ColumnChunkMetaData)#java/lang/Integer/valueOf(int)
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/expandDictionary(org.apache.parquet.hadoop.metadata.ColumnChunkMetaData)#org/apache/parquet/column/Encoding/initDictionary(org.apache.parquet.column.ColumnDescriptor,org.apache.parquet.column.page.DictionaryPage)
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/expandDictionary(org.apache.parquet.hadoop.metadata.ColumnChunkMetaData)#org/apache/parquet/column/Dictionary/decodeToFloat(int)
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/expandDictionary(org.apache.parquet.hadoop.metadata.ColumnChunkMetaData)#java/util/HashSet/HashSet()
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/expandDictionary(org.apache.parquet.hadoop.metadata.ColumnChunkMetaData)#org/apache/parquet/column/Dictionary/decodeToLong(int)
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/expandDictionary(org.apache.parquet.hadoop.metadata.ColumnChunkMetaData)#java/lang/Double/valueOf(double)
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/expandDictionary(org.apache.parquet.hadoop.metadata.ColumnChunkMetaData)#java/lang/Long/valueOf(long)
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/expandDictionary(org.apache.parquet.hadoop.metadata.ColumnChunkMetaData)#org/apache/parquet/hadoop/metadata/ColumnPath/toArray()
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/expandDictionary(org.apache.parquet.hadoop.metadata.ColumnChunkMetaData)#java/lang/Float/valueOf(float)
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/expandDictionary(org.apache.parquet.hadoop.metadata.ColumnChunkMetaData)#org/apache/parquet/column/ColumnDescriptor/ColumnDescriptor(java.lang.String[],org.apache.parquet.schema.PrimitiveType$PrimitiveTypeName,int,int)
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/expandDictionary(org.apache.parquet.hadoop.metadata.ColumnChunkMetaData)#java/util/Set/add(java.lang.Object)
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$Lt)#org/slf4j/Logger/warn(java.lang.String,java.lang.Throwable)
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$Lt)#org/apache/parquet/filter2/predicate/Operators$Column/getColumnPath()
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$Lt)#org/apache/parquet/filter2/predicate/Operators$Lt/getValue()
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$Lt)#org/apache/parquet/filter2/predicate/Operators$Lt/getColumn()
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$Lt)#java/util/Iterator/hasNext()
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$Lt)#java/lang/Comparable/compareTo(java.lang.Object)
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$Lt)#java/util/Iterator/next()
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$Lt)#java/lang/Boolean/valueOf(boolean)
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$Lt)#java/util/Set/iterator()
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$GtEq)#org/apache/parquet/filter2/predicate/Operators$Column/getColumnPath()
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$GtEq)#org/apache/parquet/filter2/predicate/Operators$GtEq/getValue()
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$GtEq)#org/apache/parquet/filter2/predicate/Operators$GtEq/getColumn()
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$GtEq)#org/slf4j/Logger/warn(java.lang.String,java.lang.Throwable)
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$GtEq)#java/util/Iterator/hasNext()
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$GtEq)#java/lang/Comparable/compareTo(java.lang.Object)
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$GtEq)#java/util/Iterator/next()
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$GtEq)#java/lang/Boolean/valueOf(boolean)
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$GtEq)#java/util/Set/iterator()
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$LtEq)#org/apache/parquet/filter2/predicate/Operators$LtEq/getColumn()
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$LtEq)#org/slf4j/Logger/warn(java.lang.String,java.lang.Throwable)
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$LtEq)#org/apache/parquet/filter2/predicate/Operators$Column/getColumnPath()
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$LtEq)#java/util/Iterator/hasNext()
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$LtEq)#java/lang/Comparable/compareTo(java.lang.Object)
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$LtEq)#java/util/Iterator/next()
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$LtEq)#java/lang/Boolean/valueOf(boolean)
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$LtEq)#java/util/Set/iterator()
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$LtEq)#org/apache/parquet/filter2/predicate/Operators$LtEq/getValue()
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$Gt)#org/slf4j/Logger/warn(java.lang.String,java.lang.Throwable)
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$Gt)#org/apache/parquet/filter2/predicate/Operators$Column/getColumnPath()
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$Gt)#org/apache/parquet/filter2/predicate/Operators$Gt/getColumn()
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$Gt)#org/apache/parquet/filter2/predicate/Operators$Gt/getValue()
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$Gt)#java/util/Iterator/hasNext()
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$Gt)#java/lang/Comparable/compareTo(java.lang.Object)
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$Gt)#java/util/Iterator/next()
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$Gt)#java/lang/Boolean/valueOf(boolean)
org/apache/parquet/filter2/dictionarylevel/DictionaryFilter/visit(org.apache.parquet.filter2.predicate.Operators$Gt)#java/util/Set/iterator()
org/apache/parquet/hadoop/ColumnChunkPageReadStore/ColumnChunkPageReader/1/visit(org.apache.parquet.column.page.DataPageV2)#org/apache/parquet/column/page/DataPageV2/getRowCount()
org/apache/parquet/hadoop/ColumnChunkPageReadStore/ColumnChunkPageReader/1/visit(org.apache.parquet.column.page.DataPageV2)#org/apache/parquet/column/page/DataPageV2/isCompressed()
org/apache/parquet/hadoop/ColumnChunkPageReadStore/ColumnChunkPageReader/1/visit(org.apache.parquet.column.page.DataPageV2)#org/apache/parquet/column/page/DataPageV2/getUncompressedSize()
org/apache/parquet/hadoop/ColumnChunkPageReadStore/ColumnChunkPageReader/1/visit(org.apache.parquet.column.page.DataPageV2)#org/apache/parquet/Ints/checkedCast(long)
org/apache/parquet/hadoop/ColumnChunkPageReadStore/ColumnChunkPageReader/1/visit(org.apache.parquet.column.page.DataPageV2)#org/apache/parquet/column/page/DataPageV2/getDataEncoding()
org/apache/parquet/hadoop/ColumnChunkPageReadStore/ColumnChunkPageReader/1/visit(org.apache.parquet.column.page.DataPageV2)#org/apache/parquet/column/page/DataPageV2/getDefinitionLevels()
org/apache/parquet/hadoop/ColumnChunkPageReadStore/ColumnChunkPageReader/1/visit(org.apache.parquet.column.page.DataPageV2)#org/apache/parquet/column/page/DataPageV2/getStatistics()
org/apache/parquet/hadoop/ColumnChunkPageReadStore/ColumnChunkPageReader/1/visit(org.apache.parquet.column.page.DataPageV2)#org/apache/parquet/hadoop/ColumnChunkPageReadStore$ColumnChunkPageReader$1/visit(org.apache.parquet.column.page.DataPageV2)
org/apache/parquet/hadoop/ColumnChunkPageReadStore/ColumnChunkPageReader/1/visit(org.apache.parquet.column.page.DataPageV2)#org/apache/parquet/bytes/BytesInput/size()
org/apache/parquet/hadoop/ColumnChunkPageReadStore/ColumnChunkPageReader/1/visit(org.apache.parquet.column.page.DataPageV2)#org/apache/parquet/column/page/DataPageV2/getData()
org/apache/parquet/hadoop/ColumnChunkPageReadStore/ColumnChunkPageReader/1/visit(org.apache.parquet.column.page.DataPageV2)#org/apache/parquet/hadoop/CodecFactory$BytesDecompressor/decompress(org.apache.parquet.bytes.BytesInput,int)
org/apache/parquet/hadoop/ColumnChunkPageReadStore/ColumnChunkPageReader/1/visit(org.apache.parquet.column.page.DataPageV2)#org/apache/parquet/column/page/DataPageV2/getValueCount()
org/apache/parquet/hadoop/ColumnChunkPageReadStore/ColumnChunkPageReader/1/visit(org.apache.parquet.column.page.DataPageV2)#org/apache/parquet/column/page/DataPageV2/getNullCount()
org/apache/parquet/hadoop/ColumnChunkPageReadStore/ColumnChunkPageReader/1/visit(org.apache.parquet.column.page.DataPageV2)#org/apache/parquet/column/page/DataPageV2/getRepetitionLevels()
org/apache/parquet/hadoop/ColumnChunkPageReadStore/ColumnChunkPageReader/1/visit(org.apache.parquet.column.page.DataPageV2)#org/apache/parquet/column/page/DataPageV2/uncompressed(int,int,int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.Encoding,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.statistics.Statistics)
org/apache/parquet/hadoop/ColumnChunkPageReadStore/ColumnChunkPageReader/1/visit(org.apache.parquet.column.page.DataPageV2)#org/apache/parquet/io/ParquetDecodingException/ParquetDecodingException(java.lang.String,java.lang.Throwable)
org/apache/parquet/hadoop/ColumnChunkPageReadStore/ColumnChunkPageReader/1/visit(org.apache.parquet.column.page.DataPageV2)#org/apache/parquet/hadoop/ColumnChunkPageReadStore$ColumnChunkPageReader/access$000(org.apache.parquet.hadoop.ColumnChunkPageReadStore$ColumnChunkPageReader)
org/apache/parquet/hadoop/ColumnChunkPageReadStore/ColumnChunkPageReader/1/visit(org.apache.parquet.column.page.DataPageV1)#org/apache/parquet/column/page/DataPageV1/DataPageV1(org.apache.parquet.bytes.BytesInput,int,int,org.apache.parquet.column.statistics.Statistics,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding)
org/apache/parquet/hadoop/ColumnChunkPageReadStore/ColumnChunkPageReader/1/visit(org.apache.parquet.column.page.DataPageV1)#org/apache/parquet/column/page/DataPageV1/getValueEncoding()
org/apache/parquet/hadoop/ColumnChunkPageReadStore/ColumnChunkPageReader/1/visit(org.apache.parquet.column.page.DataPageV1)#org/apache/parquet/column/page/DataPageV1/getValueCount()
org/apache/parquet/hadoop/ColumnChunkPageReadStore/ColumnChunkPageReader/1/visit(org.apache.parquet.column.page.DataPageV1)#org/apache/parquet/column/page/DataPageV1/getRlEncoding()
org/apache/parquet/hadoop/ColumnChunkPageReadStore/ColumnChunkPageReader/1/visit(org.apache.parquet.column.page.DataPageV1)#org/apache/parquet/hadoop/CodecFactory$BytesDecompressor/decompress(org.apache.parquet.bytes.BytesInput,int)
org/apache/parquet/hadoop/ColumnChunkPageReadStore/ColumnChunkPageReader/1/visit(org.apache.parquet.column.page.DataPageV1)#org/apache/parquet/column/page/DataPageV1/getDlEncoding()
org/apache/parquet/hadoop/ColumnChunkPageReadStore/ColumnChunkPageReader/1/visit(org.apache.parquet.column.page.DataPageV1)#org/apache/parquet/column/page/DataPageV1/getUncompressedSize()
org/apache/parquet/hadoop/ColumnChunkPageReadStore/ColumnChunkPageReader/1/visit(org.apache.parquet.column.page.DataPageV1)#org/apache/parquet/column/page/DataPageV1/getBytes()
org/apache/parquet/hadoop/ColumnChunkPageReadStore/ColumnChunkPageReader/1/visit(org.apache.parquet.column.page.DataPageV1)#org/apache/parquet/column/page/DataPageV1/getStatistics()
org/apache/parquet/hadoop/ColumnChunkPageReadStore/ColumnChunkPageReader/1/visit(org.apache.parquet.column.page.DataPageV1)#org/apache/parquet/hadoop/ColumnChunkPageReadStore$ColumnChunkPageReader$1/visit(org.apache.parquet.column.page.DataPageV1)
org/apache/parquet/hadoop/ColumnChunkPageReadStore/ColumnChunkPageReader/1/visit(org.apache.parquet.column.page.DataPageV1)#org/apache/parquet/io/ParquetDecodingException/ParquetDecodingException(java.lang.String,java.lang.Throwable)
org/apache/parquet/hadoop/ColumnChunkPageReadStore/ColumnChunkPageReader/1/visit(org.apache.parquet.column.page.DataPageV1)#org/apache/parquet/hadoop/ColumnChunkPageReadStore$ColumnChunkPageReader/access$000(org.apache.parquet.hadoop.ColumnChunkPageReadStore$ColumnChunkPageReader)
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#org/apache/parquet/format/DataPageHeader/getEncoding()
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#org/apache/parquet/column/page/DataPageV1/DataPageV1(org.apache.parquet.bytes.BytesInput,int,int,org.apache.parquet.column.statistics.Statistics,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding)
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#org/apache/parquet/hadoop/ParquetFileReader$ChunkDescriptor/access$700(org.apache.parquet.hadoop.ParquetFileReader$ChunkDescriptor)
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#org/apache/parquet/format/DataPageHeaderV2/getRepetition_levels_byte_length()
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#java/util/List/size()
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#org/apache/parquet/format/PageHeader/getUncompressed_page_size()
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#java/lang/StringBuilder/StringBuilder()
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#org/apache/parquet/format/DataPageHeaderV2/getNum_rows()
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#org/apache/parquet/format/DataPageHeaderV2/getEncoding()
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#org/apache/parquet/format/DataPageHeaderV2/isIs_compressed()
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#org/slf4j/Logger/debug(java.lang.String,java.lang.Object,java.lang.Object)
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#org/apache/parquet/format/DataPageHeader/getDefinition_level_encoding()
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#java/util/ArrayList/ArrayList()
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#org/apache/parquet/hadoop/ParquetFileReader$Chunk/pos()
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#org/apache/parquet/format/DataPageHeader/getRepetition_level_encoding()
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#org/apache/parquet/format/DataPageHeaderV2/getNum_values()
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#java/io/IOException/IOException(java.lang.String)
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#org/apache/parquet/format/PageHeader/getData_page_header_v2()
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#org/apache/parquet/format/DataPageHeaderV2/getDefinition_levels_byte_length()
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#java/lang/StringBuilder/append(java.lang.String)
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#org/apache/parquet/hadoop/ParquetFileReader$Chunk/skip(long)
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#org/apache/parquet/hadoop/ParquetFileReader$ChunkDescriptor/access$400(org.apache.parquet.hadoop.ParquetFileReader$ChunkDescriptor)
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#org/apache/parquet/format/DataPageHeader/getNum_values()
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#org/apache/parquet/column/page/DictionaryPage/DictionaryPage(org.apache.parquet.bytes.BytesInput,int,int,org.apache.parquet.column.Encoding)
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#java/lang/StringBuilder/append(int)
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#org/apache/parquet/hadoop/ParquetFileReader$ChunkDescriptor/access$300(org.apache.parquet.hadoop.ParquetFileReader$ChunkDescriptor)
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#org/apache/parquet/hadoop/ParquetFileReader$Chunk/readAsBytesInput(int)
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#org/apache/parquet/format/PageType/ordinal()
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#org/apache/parquet/format/PageHeader/getData_page_header()
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#org/apache/parquet/format/DataPageHeaderV2/getStatistics()
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#org/apache/parquet/format/DictionaryPageHeader/getEncoding()
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#org/apache/parquet/schema/Type/asPrimitiveType()
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#org/apache/parquet/format/DataPageHeader/getStatistics()
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#java/lang/StringBuilder/append(long)
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#org/apache/parquet/column/page/DataPageV2/DataPageV2(int,int,int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.Encoding,org.apache.parquet.bytes.BytesInput,int,org.apache.parquet.column.statistics.Statistics,boolean)
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#org/apache/parquet/column/ColumnDescriptor/getPath()
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#org/apache/parquet/io/ParquetDecodingException/ParquetDecodingException(java.lang.String)
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#org/apache/parquet/format/DictionaryPageHeader/getNum_values()
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#org/apache/parquet/hadoop/ColumnChunkPageReadStore$ColumnChunkPageReader/ColumnChunkPageReader(org.apache.parquet.hadoop.CodecFactory$BytesDecompressor,java.util.List,org.apache.parquet.column.page.DictionaryPage)
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#java/util/List/add(java.lang.Object)
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#org/apache/parquet/format/PageHeader/getCompressed_page_size()
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#java/lang/Integer/valueOf(int)
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#org/apache/parquet/hadoop/ParquetFileReader$Chunk/readPageHeader()
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#org/apache/parquet/format/PageHeader/getDictionary_page_header()
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#java/lang/StringBuilder/append(java.lang.Object)
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#org/apache/parquet/schema/MessageType/getType(java.lang.String[])
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#org/apache/parquet/format/PageHeader/getType()
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#java/lang/StringBuilder/toString()
org/apache/parquet/hadoop/ParquetFileReader/Chunk/readAllPages()#org/apache/parquet/format/DataPageHeaderV2/getNum_nulls()
org/apache/parquet/hadoop/util/H1SeekableInputStream/readFullyDirectBuffer(org.apache.hadoop.fs.FSDataInputStream,java.nio.ByteBuffer,byte[])#java/lang/StringBuilder/append(java.lang.String)
org/apache/parquet/hadoop/util/H1SeekableInputStream/readFullyDirectBuffer(org.apache.hadoop.fs.FSDataInputStream,java.nio.ByteBuffer,byte[])#java/lang/StringBuilder/append(int)
org/apache/parquet/hadoop/util/H1SeekableInputStream/readFullyDirectBuffer(org.apache.hadoop.fs.FSDataInputStream,java.nio.ByteBuffer,byte[])#org/apache/hadoop/fs/FSDataInputStream/read(byte[],int,int)
org/apache/parquet/hadoop/util/H1SeekableInputStream/readFullyDirectBuffer(org.apache.hadoop.fs.FSDataInputStream,java.nio.ByteBuffer,byte[])#java/io/EOFException/EOFException(java.lang.String)
org/apache/parquet/hadoop/util/H1SeekableInputStream/readFullyDirectBuffer(org.apache.hadoop.fs.FSDataInputStream,java.nio.ByteBuffer,byte[])#java/nio/ByteBuffer/remaining()
org/apache/parquet/hadoop/util/H1SeekableInputStream/readFullyDirectBuffer(org.apache.hadoop.fs.FSDataInputStream,java.nio.ByteBuffer,byte[])#java/lang/StringBuilder/StringBuilder()
org/apache/parquet/hadoop/util/H1SeekableInputStream/readFullyDirectBuffer(org.apache.hadoop.fs.FSDataInputStream,java.nio.ByteBuffer,byte[])#java/lang/Math/min(int,int)
org/apache/parquet/hadoop/util/H1SeekableInputStream/readFullyDirectBuffer(org.apache.hadoop.fs.FSDataInputStream,java.nio.ByteBuffer,byte[])#java/nio/ByteBuffer/put(byte[],int,int)
org/apache/parquet/hadoop/util/H1SeekableInputStream/readFullyDirectBuffer(org.apache.hadoop.fs.FSDataInputStream,java.nio.ByteBuffer,byte[])#java/lang/StringBuilder/toString()
org/apache/parquet/hadoop/ColumnChunkPageWriteStore/ColumnChunkPageWriter/writePageV2(int,int,int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.Encoding,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.statistics.Statistics)#org/apache/parquet/hadoop/CodecFactory$BytesCompressor/compress(org.apache.parquet.bytes.BytesInput)
org/apache/parquet/hadoop/ColumnChunkPageWriteStore/ColumnChunkPageWriter/writePageV2(int,int,int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.Encoding,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.statistics.Statistics)#org/apache/parquet/bytes/ConcatenatingByteArrayCollector/collect(org.apache.parquet.bytes.BytesInput)
org/apache/parquet/hadoop/ColumnChunkPageWriteStore/ColumnChunkPageWriter/writePageV2(int,int,int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.Encoding,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.statistics.Statistics)#org/apache/parquet/bytes/BytesInput/concat(org.apache.parquet.bytes.BytesInput[])
org/apache/parquet/hadoop/ColumnChunkPageWriteStore/ColumnChunkPageWriter/writePageV2(int,int,int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.Encoding,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.statistics.Statistics)#java/io/ByteArrayOutputStream/reset()
org/apache/parquet/hadoop/ColumnChunkPageWriteStore/ColumnChunkPageWriter/writePageV2(int,int,int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.Encoding,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.statistics.Statistics)#org/apache/parquet/bytes/BytesInput/from(java.io.ByteArrayOutputStream)
org/apache/parquet/hadoop/ColumnChunkPageWriteStore/ColumnChunkPageWriter/writePageV2(int,int,int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.Encoding,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.statistics.Statistics)#org/apache/parquet/column/statistics/Statistics/mergeStatistics(org.apache.parquet.column.statistics.Statistics)
org/apache/parquet/hadoop/ColumnChunkPageWriteStore/ColumnChunkPageWriter/writePageV2(int,int,int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.Encoding,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.statistics.Statistics)#org/apache/parquet/bytes/BytesInput/size()
org/apache/parquet/hadoop/ColumnChunkPageWriteStore/ColumnChunkPageWriter/writePageV2(int,int,int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.Encoding,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.statistics.Statistics)#org/apache/parquet/hadoop/ColumnChunkPageWriteStore$ColumnChunkPageWriter/toIntWithCheck(long)
org/apache/parquet/hadoop/ColumnChunkPageWriteStore/ColumnChunkPageWriter/writePageV2(int,int,int,org.apache.parquet.bytes.BytesInput,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.Encoding,org.apache.parquet.bytes.BytesInput,org.apache.parquet.column.statistics.Statistics)#java/util/List/add(java.lang.Object)
org/apache/parquet/hadoop/ColumnChunkPageWriteStore/ColumnChunkPageWriter/writeToFileWriter(org.apache.parquet.hadoop.ParquetFileWriter)#org/slf4j/Logger/debug(java.lang.String)
org/apache/parquet/hadoop/ColumnChunkPageWriteStore/ColumnChunkPageWriter/writeToFileWriter(org.apache.parquet.hadoop.ParquetFileWriter)#java/lang/String/format(java.lang.String,java.lang.Object[])
org/apache/parquet/hadoop/ColumnChunkPageWriteStore/ColumnChunkPageWriter/writeToFileWriter(org.apache.parquet.hadoop.ParquetFileWriter)#org/apache/parquet/bytes/ConcatenatingByteArrayCollector/size()
org/apache/parquet/hadoop/ColumnChunkPageWriteStore/ColumnChunkPageWriter/writeToFileWriter(org.apache.parquet.hadoop.ParquetFileWriter)#java/lang/StringBuilder/StringBuilder()
org/apache/parquet/hadoop/ColumnChunkPageWriteStore/ColumnChunkPageWriter/writeToFileWriter(org.apache.parquet.hadoop.ParquetFileWriter)#java/util/List/clear()
org/apache/parquet/hadoop/ColumnChunkPageWriteStore/ColumnChunkPageWriter/writeToFileWriter(org.apache.parquet.hadoop.ParquetFileWriter)#org/apache/parquet/column/page/DictionaryPage/getDictionarySize()
org/apache/parquet/hadoop/ColumnChunkPageWriteStore/ColumnChunkPageWriter/writeToFileWriter(org.apache.parquet.hadoop.ParquetFileWriter)#java/lang/Integer/valueOf(int)
org/apache/parquet/hadoop/ColumnChunkPageWriteStore/ColumnChunkPageWriter/writeToFileWriter(org.apache.parquet.hadoop.ParquetFileWriter)#org/apache/parquet/column/page/DictionaryPage/getUncompressedSize()
org/apache/parquet/hadoop/ColumnChunkPageWriteStore/ColumnChunkPageWriter/writeToFileWriter(org.apache.parquet.hadoop.ParquetFileWriter)#org/apache/parquet/hadoop/CodecFactory$BytesCompressor/getCodecName()
org/apache/parquet/hadoop/ColumnChunkPageWriteStore/ColumnChunkPageWriter/writeToFileWriter(org.apache.parquet.hadoop.ParquetFileWriter)#java/util/HashSet/HashSet(java.util.Collection)
org/apache/parquet/hadoop/ColumnChunkPageWriteStore/ColumnChunkPageWriter/writeToFileWriter(org.apache.parquet.hadoop.ParquetFileWriter)#java/lang/StringBuilder/toString()
org/apache/parquet/hadoop/ColumnChunkPageWriteStore/ColumnChunkPageWriter/writeToFileWriter(org.apache.parquet.hadoop.ParquetFileWriter)#java/lang/Long/valueOf(long)
org/apache/parquet/hadoop/ColumnChunkPageWriteStore/ColumnChunkPageWriter/writeToFileWriter(org.apache.parquet.hadoop.ParquetFileWriter)#java/lang/StringBuilder/append(java.lang.String)
org/apache/parquet/hadoop/ColumnChunkPageWriteStore/ColumnChunkPageWriter/writeToFileWriter(org.apache.parquet.hadoop.ParquetFileWriter)#java/util/Set/clear()
org/apache/parquet/hadoop/ColumnChunkPageWriteStore/ColumnChunkPageWriter/writeToFileWriter(org.apache.parquet.hadoop.ParquetFileWriter)#org/slf4j/Logger/isDebugEnabled()
org/apache/parquet/hadoop/ColumnChunkPageWriteStore/ColumnChunkPageWriter/writePage(org.apache.parquet.bytes.BytesInput,int,org.apache.parquet.column.statistics.Statistics,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding)#org/apache/parquet/hadoop/CodecFactory$BytesCompressor/compress(org.apache.parquet.bytes.BytesInput)
org/apache/parquet/hadoop/ColumnChunkPageWriteStore/ColumnChunkPageWriter/writePage(org.apache.parquet.bytes.BytesInput,int,org.apache.parquet.column.statistics.Statistics,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding)#org/apache/parquet/bytes/ConcatenatingByteArrayCollector/collect(org.apache.parquet.bytes.BytesInput)
org/apache/parquet/hadoop/ColumnChunkPageWriteStore/ColumnChunkPageWriter/writePage(org.apache.parquet.bytes.BytesInput,int,org.apache.parquet.column.statistics.Statistics,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding)#java/lang/StringBuilder/append(java.lang.String)
org/apache/parquet/hadoop/ColumnChunkPageWriteStore/ColumnChunkPageWriter/writePage(org.apache.parquet.bytes.BytesInput,int,org.apache.parquet.column.statistics.Statistics,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding)#org/apache/parquet/bytes/BytesInput/concat(org.apache.parquet.bytes.BytesInput[])
org/apache/parquet/hadoop/ColumnChunkPageWriteStore/ColumnChunkPageWriter/writePage(org.apache.parquet.bytes.BytesInput,int,org.apache.parquet.column.statistics.Statistics,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding)#java/lang/StringBuilder/StringBuilder()
org/apache/parquet/hadoop/ColumnChunkPageWriteStore/ColumnChunkPageWriter/writePage(org.apache.parquet.bytes.BytesInput,int,org.apache.parquet.column.statistics.Statistics,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding)#org/apache/parquet/io/ParquetEncodingException/ParquetEncodingException(java.lang.String)
org/apache/parquet/hadoop/ColumnChunkPageWriteStore/ColumnChunkPageWriter/writePage(org.apache.parquet.bytes.BytesInput,int,org.apache.parquet.column.statistics.Statistics,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding)#org/apache/parquet/bytes/BytesInput/size()
org/apache/parquet/hadoop/ColumnChunkPageWriteStore/ColumnChunkPageWriter/writePage(org.apache.parquet.bytes.BytesInput,int,org.apache.parquet.column.statistics.Statistics,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding)#java/util/List/add(java.lang.Object)
org/apache/parquet/hadoop/ColumnChunkPageWriteStore/ColumnChunkPageWriter/writePage(org.apache.parquet.bytes.BytesInput,int,org.apache.parquet.column.statistics.Statistics,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding)#java/util/Set/add(java.lang.Object)
org/apache/parquet/hadoop/ColumnChunkPageWriteStore/ColumnChunkPageWriter/writePage(org.apache.parquet.bytes.BytesInput,int,org.apache.parquet.column.statistics.Statistics,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding)#java/lang/StringBuilder/toString()
org/apache/parquet/hadoop/ColumnChunkPageWriteStore/ColumnChunkPageWriter/writePage(org.apache.parquet.bytes.BytesInput,int,org.apache.parquet.column.statistics.Statistics,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding)#org/apache/parquet/bytes/BytesInput/from(java.io.ByteArrayOutputStream)
org/apache/parquet/hadoop/ColumnChunkPageWriteStore/ColumnChunkPageWriter/writePage(org.apache.parquet.bytes.BytesInput,int,org.apache.parquet.column.statistics.Statistics,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding)#org/apache/parquet/column/statistics/Statistics/mergeStatistics(org.apache.parquet.column.statistics.Statistics)
org/apache/parquet/hadoop/ColumnChunkPageWriteStore/ColumnChunkPageWriter/writePage(org.apache.parquet.bytes.BytesInput,int,org.apache.parquet.column.statistics.Statistics,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding)#java/lang/StringBuilder/append(long)
org/apache/parquet/hadoop/ColumnChunkPageWriteStore/ColumnChunkPageWriter/writePage(org.apache.parquet.bytes.BytesInput,int,org.apache.parquet.column.statistics.Statistics,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding,org.apache.parquet.column.Encoding)#java/io/ByteArrayOutputStream/reset()
org/apache/parquet/hadoop/metadata/GlobalMetaData/merge()#java/util/Map/put(java.lang.Object,java.lang.Object)
org/apache/parquet/hadoop/metadata/GlobalMetaData/merge()#java/lang/Object/toString()
org/apache/parquet/hadoop/metadata/GlobalMetaData/merge()#java/util/Iterator/hasNext()
org/apache/parquet/hadoop/metadata/GlobalMetaData/merge()#java/lang/StringBuilder/StringBuilder()
org/apache/parquet/hadoop/metadata/GlobalMetaData/merge()#java/util/Set/iterator()
org/apache/parquet/hadoop/metadata/GlobalMetaData/merge()#java/lang/RuntimeException/RuntimeException(java.lang.String)
org/apache/parquet/hadoop/metadata/GlobalMetaData/merge()#java/util/Map/entrySet()
org/apache/parquet/hadoop/metadata/GlobalMetaData/merge()#java/lang/StringBuilder/toString()
org/apache/parquet/hadoop/metadata/GlobalMetaData/merge()#java/util/Set/size()
org/apache/parquet/hadoop/metadata/GlobalMetaData/merge()#java/lang/StringBuilder/append(java.lang.String)
org/apache/parquet/hadoop/metadata/GlobalMetaData/merge()#java/util/Map$Entry/getKey()
org/apache/parquet/hadoop/metadata/GlobalMetaData/merge()#java/util/Iterator/next()
org/apache/parquet/hadoop/metadata/GlobalMetaData/merge()#java/util/HashMap/HashMap()
org/apache/parquet/hadoop/metadata/GlobalMetaData/merge()#java/lang/StringBuilder/append(java.lang.Object)
org/apache/parquet/hadoop/metadata/GlobalMetaData/merge()#java/util/Map$Entry/getValue()
org/apache/parquet/hadoop/ParquetFileReader/readFooter(org.apache.parquet.format.converter.ParquetMetadataConverter,long,java.lang.String,org.apache.parquet.io.SeekableInputStream,org.apache.parquet.format.converter.ParquetMetadataConverter$MetadataFilter)#org/apache/parquet/io/SeekableInputStream/seek(long)
org/apache/parquet/hadoop/ParquetFileReader/readFooter(org.apache.parquet.format.converter.ParquetMetadataConverter,long,java.lang.String,org.apache.parquet.io.SeekableInputStream,org.apache.parquet.format.converter.ParquetMetadataConverter$MetadataFilter)#org/apache/parquet/bytes/BytesUtils/readIntLittleEndian(java.io.InputStream)
org/apache/parquet/hadoop/ParquetFileReader/readFooter(org.apache.parquet.format.converter.ParquetMetadataConverter,long,java.lang.String,org.apache.parquet.io.SeekableInputStream,org.apache.parquet.format.converter.ParquetMetadataConverter$MetadataFilter)#java/lang/StringBuilder/append(java.lang.String)
org/apache/parquet/hadoop/ParquetFileReader/readFooter(org.apache.parquet.format.converter.ParquetMetadataConverter,long,java.lang.String,org.apache.parquet.io.SeekableInputStream,org.apache.parquet.format.converter.ParquetMetadataConverter$MetadataFilter)#java/util/Arrays/equals(byte[],byte[])
org/apache/parquet/hadoop/ParquetFileReader/readFooter(org.apache.parquet.format.converter.ParquetMetadataConverter,long,java.lang.String,org.apache.parquet.io.SeekableInputStream,org.apache.parquet.format.converter.ParquetMetadataConverter$MetadataFilter)#java/util/Arrays/toString(byte[])
org/apache/parquet/hadoop/ParquetFileReader/readFooter(org.apache.parquet.format.converter.ParquetMetadataConverter,long,java.lang.String,org.apache.parquet.io.SeekableInputStream,org.apache.parquet.format.converter.ParquetMetadataConverter$MetadataFilter)#java/lang/StringBuilder/StringBuilder()
org/apache/parquet/hadoop/ParquetFileReader/readFooter(org.apache.parquet.format.converter.ParquetMetadataConverter,long,java.lang.String,org.apache.parquet.io.SeekableInputStream,org.apache.parquet.format.converter.ParquetMetadataConverter$MetadataFilter)#org/apache/parquet/io/SeekableInputStream/readFully(byte[])
org/apache/parquet/hadoop/ParquetFileReader/readFooter(org.apache.parquet.format.converter.ParquetMetadataConverter,long,java.lang.String,org.apache.parquet.io.SeekableInputStream,org.apache.parquet.format.converter.ParquetMetadataConverter$MetadataFilter)#java/lang/Integer/valueOf(int)
org/apache/parquet/hadoop/ParquetFileReader/readFooter(org.apache.parquet.format.converter.ParquetMetadataConverter,long,java.lang.String,org.apache.parquet.io.SeekableInputStream,org.apache.parquet.format.converter.ParquetMetadataConverter$MetadataFilter)#java/lang/RuntimeException/RuntimeException(java.lang.String)
org/apache/parquet/hadoop/ParquetFileReader/readFooter(org.apache.parquet.format.converter.ParquetMetadataConverter,long,java.lang.String,org.apache.parquet.io.SeekableInputStream,org.apache.parquet.format.converter.ParquetMetadataConverter$MetadataFilter)#java/lang/StringBuilder/toString()
org/apache/parquet/hadoop/ParquetFileReader/readFooter(org.apache.parquet.format.converter.ParquetMetadataConverter,long,java.lang.String,org.apache.parquet.io.SeekableInputStream,org.apache.parquet.format.converter.ParquetMetadataConverter$MetadataFilter)#org/slf4j/Logger/debug(java.lang.String,java.lang.Object,java.lang.Object)
org/apache/parquet/hadoop/ParquetFileReader/readFooter(org.apache.parquet.format.converter.ParquetMetadataConverter,long,java.lang.String,org.apache.parquet.io.SeekableInputStream,org.apache.parquet.format.converter.ParquetMetadataConverter$MetadataFilter)#java/lang/Long/valueOf(long)
org/apache/parquet/hadoop/ParquetFileReader/readFooter(org.apache.parquet.format.converter.ParquetMetadataConverter,long,java.lang.String,org.apache.parquet.io.SeekableInputStream,org.apache.parquet.format.converter.ParquetMetadataConverter$MetadataFilter)#org/slf4j/Logger/debug(java.lang.String,java.lang.Object)
org/apache/parquet/hadoop/ParquetFileReader/runAllInParallel(int,java.util.List)#java/util/ArrayList/ArrayList()
org/apache/parquet/hadoop/ParquetFileReader/runAllInParallel(int,java.util.List)#java/util/ArrayList/ArrayList(int)
org/apache/parquet/hadoop/ParquetFileReader/runAllInParallel(int,java.util.List)#java/util/Iterator/hasNext()
org/apache/parquet/hadoop/ParquetFileReader/runAllInParallel(int,java.util.List)#java/util/List/size()
org/apache/parquet/hadoop/ParquetFileReader/runAllInParallel(int,java.util.List)#java/util/Iterator/next()
org/apache/parquet/hadoop/ParquetFileReader/runAllInParallel(int,java.util.List)#java/util/concurrent/ExecutorService/shutdownNow()
org/apache/parquet/hadoop/ParquetFileReader/runAllInParallel(int,java.util.List)#java/util/List/add(java.lang.Object)
org/apache/parquet/hadoop/ParquetFileReader/runAllInParallel(int,java.util.List)#java/lang/Integer/valueOf(int)
org/apache/parquet/hadoop/ParquetFileReader/runAllInParallel(int,java.util.List)#java/lang/RuntimeException/RuntimeException(java.lang.String,java.lang.Throwable)
org/apache/parquet/hadoop/ParquetFileReader/runAllInParallel(int,java.util.List)#java/util/List/iterator()
org/apache/parquet/hadoop/ParquetFileReader/runAllInParallel(int,java.util.List)#java/util/concurrent/Executors/newFixedThreadPool(int)
org/apache/parquet/hadoop/ParquetFileReader/runAllInParallel(int,java.util.List)#org/slf4j/Logger/info(java.lang.String,java.lang.Object)
org/apache/parquet/hadoop/ParquetFileReader/runAllInParallel(int,java.util.List)#java/util/concurrent/Future/get()
org/apache/parquet/hadoop/ParquetFileReader/runAllInParallel(int,java.util.List)#java/util/concurrent/ExecutorService/submit(java.util.concurrent.Callable)
org/apache/parquet/hadoop/ParquetFileReader/readAllFootersInParallel(org.apache.hadoop.conf.Configuration,java.util.List,boolean)#java/util/ArrayList/ArrayList()
org/apache/parquet/hadoop/ParquetFileReader/readAllFootersInParallel(org.apache.hadoop.conf.Configuration,java.util.List,boolean)#org/apache/parquet/hadoop/ParquetFileReader$2/2(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.conf.Configuration,boolean)
org/apache/parquet/hadoop/ParquetFileReader/readAllFootersInParallel(org.apache.hadoop.conf.Configuration,java.util.List,boolean)#java/io/IOException/IOException(java.lang.String,java.lang.Throwable)
org/apache/parquet/hadoop/ParquetFileReader/readAllFootersInParallel(org.apache.hadoop.conf.Configuration,java.util.List,boolean)#java/util/Iterator/hasNext()
org/apache/parquet/hadoop/ParquetFileReader/readAllFootersInParallel(org.apache.hadoop.conf.Configuration,java.util.List,boolean)#java/lang/StringBuilder/append(java.lang.String)
org/apache/parquet/hadoop/ParquetFileReader/readAllFootersInParallel(org.apache.hadoop.conf.Configuration,java.util.List,boolean)#java/util/concurrent/ExecutionException/getMessage()
org/apache/parquet/hadoop/ParquetFileReader/readAllFootersInParallel(org.apache.hadoop.conf.Configuration,java.util.List,boolean)#java/util/concurrent/ExecutionException/getCause()
org/apache/parquet/hadoop/ParquetFileReader/readAllFootersInParallel(org.apache.hadoop.conf.Configuration,java.util.List,boolean)#java/util/Iterator/next()
org/apache/parquet/hadoop/ParquetFileReader/readAllFootersInParallel(org.apache.hadoop.conf.Configuration,java.util.List,boolean)#java/lang/StringBuilder/StringBuilder()
org/apache/parquet/hadoop/ParquetFileReader/readAllFootersInParallel(org.apache.hadoop.conf.Configuration,java.util.List,boolean)#java/util/List/add(java.lang.Object)
org/apache/parquet/hadoop/ParquetFileReader/readAllFootersInParallel(org.apache.hadoop.conf.Configuration,java.util.List,boolean)#java/util/List/iterator()
org/apache/parquet/hadoop/ParquetFileReader/readAllFootersInParallel(org.apache.hadoop.conf.Configuration,java.util.List,boolean)#org/apache/hadoop/conf/Configuration/getInt(java.lang.String,int)
org/apache/parquet/hadoop/ParquetFileReader/readAllFootersInParallel(org.apache.hadoop.conf.Configuration,java.util.List,boolean)#java/lang/StringBuilder/toString()
org/apache/parquet/hadoop/ParquetFileReader/footersFromSummaryFile(org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.ParquetMetadata)#org/apache/hadoop/fs/Path/Path(org.apache.hadoop.fs.Path,java.lang.String)
org/apache/parquet/hadoop/ParquetFileReader/footersFromSummaryFile(org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.ParquetMetadata)#java/util/Iterator/hasNext()
org/apache/parquet/hadoop/ParquetFileReader/footersFromSummaryFile(org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.ParquetMetadata)#java/util/Map/get(java.lang.Object)
org/apache/parquet/hadoop/ParquetFileReader/footersFromSummaryFile(org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.ParquetMetadata)#java/util/Set/iterator()
org/apache/parquet/hadoop/ParquetFileReader/footersFromSummaryFile(org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.ParquetMetadata)#java/util/List/add(java.lang.Object)
org/apache/parquet/hadoop/ParquetFileReader/footersFromSummaryFile(org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.ParquetMetadata)#java/util/Map$Entry/getValue()
org/apache/parquet/hadoop/ParquetFileReader/footersFromSummaryFile(org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.ParquetMetadata)#java/util/Map/entrySet()
org/apache/parquet/hadoop/ParquetFileReader/footersFromSummaryFile(org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.ParquetMetadata)#java/util/Map/put(java.lang.Object,java.lang.Object)
org/apache/parquet/hadoop/ParquetFileReader/footersFromSummaryFile(org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.ParquetMetadata)#java/util/ArrayList/ArrayList()
org/apache/parquet/hadoop/ParquetFileReader/footersFromSummaryFile(org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.ParquetMetadata)#java/util/Map$Entry/getKey()
org/apache/parquet/hadoop/ParquetFileReader/footersFromSummaryFile(org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.ParquetMetadata)#java/util/Iterator/next()
org/apache/parquet/hadoop/ParquetFileReader/footersFromSummaryFile(org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.ParquetMetadata)#java/util/HashMap/HashMap()
org/apache/parquet/hadoop/ParquetFileReader/footersFromSummaryFile(org.apache.hadoop.fs.Path,org.apache.parquet.hadoop.metadata.ParquetMetadata)#java/util/List/iterator()
org/apache/parquet/hadoop/ParquetFileReader/readDictionary(org.apache.parquet.hadoop.metadata.ColumnChunkMetaData)#org/apache/parquet/column/page/DictionaryPage/getBytes()
org/apache/parquet/hadoop/ParquetFileReader/readDictionary(org.apache.parquet.hadoop.metadata.ColumnChunkMetaData)#org/apache/parquet/format/Util/readPageHeader(java.io.InputStream)
org/apache/parquet/hadoop/ParquetFileReader/readDictionary(org.apache.parquet.hadoop.metadata.ColumnChunkMetaData)#org/apache/parquet/column/page/DictionaryPage/DictionaryPage(org.apache.parquet.bytes.BytesInput,int,org.apache.parquet.column.Encoding)
org/apache/parquet/hadoop/ParquetFileReader/readDictionary(org.apache.parquet.hadoop.metadata.ColumnChunkMetaData)#org/apache/parquet/io/SeekableInputStream/getPos()
org/apache/parquet/hadoop/ParquetFileReader/readDictionary(org.apache.parquet.hadoop.metadata.ColumnChunkMetaData)#org/apache/parquet/column/page/DictionaryPage/getDictionarySize()
org/apache/parquet/hadoop/ParquetFileReader/readDictionary(org.apache.parquet.hadoop.metadata.ColumnChunkMetaData)#org/apache/parquet/format/PageHeader/isSetDictionary_page_header()
org/apache/parquet/hadoop/ParquetFileReader/readDictionary(org.apache.parquet.hadoop.metadata.ColumnChunkMetaData)#org/apache/parquet/column/page/DictionaryPage/getUncompressedSize()
org/apache/parquet/hadoop/ParquetFileReader/readDictionary(org.apache.parquet.hadoop.metadata.ColumnChunkMetaData)#org/apache/parquet/hadoop/CodecFactory$BytesDecompressor/decompress(org.apache.parquet.bytes.BytesInput,int)
org/apache/parquet/hadoop/ParquetFileReader/readDictionary(org.apache.parquet.hadoop.metadata.ColumnChunkMetaData)#org/apache/parquet/io/SeekableInputStream/seek(long)
org/apache/parquet/hadoop/ParquetFileReader/readDictionary(org.apache.parquet.hadoop.metadata.ColumnChunkMetaData)#org/apache/parquet/column/page/DictionaryPage/getEncoding()
org/apache/parquet/hadoop/ParquetFileReader/readDictionary(org.apache.parquet.hadoop.metadata.ColumnChunkMetaData)#java/util/Set/contains(java.lang.Object)
org/apache/parquet/hadoop/ParquetFileReader/readAllFootersInParallelUsingSummaryFiles(org.apache.hadoop.conf.Configuration,java.util.Collection,boolean)#java/util/ArrayList/ArrayList()
org/apache/parquet/hadoop/ParquetFileReader/readAllFootersInParallelUsingSummaryFiles(org.apache.hadoop.conf.Configuration,java.util.Collection,boolean)#java/util/ArrayList/ArrayList(int)
org/apache/parquet/hadoop/ParquetFileReader/readAllFootersInParallelUsingSummaryFiles(org.apache.hadoop.conf.Configuration,java.util.Collection,boolean)#org/apache/hadoop/fs/FileStatus/getPath()
org/apache/parquet/hadoop/ParquetFileReader/readAllFootersInParallelUsingSummaryFiles(org.apache.hadoop.conf.Configuration,java.util.Collection,boolean)#java/io/IOException/IOException(java.lang.String,java.lang.Throwable)
org/apache/parquet/hadoop/ParquetFileReader/readAllFootersInParallelUsingSummaryFiles(org.apache.hadoop.conf.Configuration,java.util.Collection,boolean)#java/util/Iterator/hasNext()
org/apache/parquet/hadoop/ParquetFileReader/readAllFootersInParallelUsingSummaryFiles(org.apache.hadoop.conf.Configuration,java.util.Collection,boolean)#java/util/Map/putAll(java.util.Map)
org/apache/parquet/hadoop/ParquetFileReader/readAllFootersInParallelUsingSummaryFiles(org.apache.hadoop.conf.Configuration,java.util.Collection,boolean)#java/util/Map/get(java.lang.Object)
org/apache/parquet/hadoop/ParquetFileReader/readAllFootersInParallelUsingSummaryFiles(org.apache.hadoop.conf.Configuration,java.util.Collection,boolean)#java/util/List/addAll(java.util.Collection)
org/apache/parquet/hadoop/ParquetFileReader/readAllFootersInParallelUsingSummaryFiles(org.apache.hadoop.conf.Configuration,java.util.Collection,boolean)#java/util/Set/iterator()
org/apache/parquet/hadoop/ParquetFileReader/readAllFootersInParallelUsingSummaryFiles(org.apache.hadoop.conf.Configuration,java.util.Collection,boolean)#java/lang/Integer/valueOf(int)
org/apache/parquet/hadoop/ParquetFileReader/readAllFootersInParallelUsingSummaryFiles(org.apache.hadoop.conf.Configuration,java.util.Collection,boolean)#org/slf4j/Logger/info(java.lang.String,java.lang.Object)
org/apache/parquet/hadoop/ParquetFileReader/readAllFootersInParallelUsingSummaryFiles(org.apache.hadoop.conf.Configuration,java.util.Collection,boolean)#java/util/HashSet/HashSet()
org/apache/parquet/hadoop/ParquetFileReader/readAllFootersInParallelUsingSummaryFiles(org.apache.hadoop.conf.Configuration,java.util.Collection,boolean)#org/apache/hadoop/fs/Path/getParent()
org/apache/parquet/hadoop/ParquetFileReader/readAllFootersInParallelUsingSummaryFiles(org.apache.hadoop.conf.Configuration,java.util.Collection,boolean)#org/apache/parquet/hadoop/ParquetFileReader$1/1(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,boolean,java.util.Collection)
org/apache/parquet/hadoop/ParquetFileReader/readAllFootersInParallelUsingSummaryFiles(org.apache.hadoop.conf.Configuration,java.util.Collection,boolean)#java/util/List/size()
org/apache/parquet/hadoop/ParquetFileReader/readAllFootersInParallelUsingSummaryFiles(org.apache.hadoop.conf.Configuration,java.util.Collection,boolean)#java/util/Collection/size()
org/apache/parquet/hadoop/ParquetFileReader/readAllFootersInParallelUsingSummaryFiles(org.apache.hadoop.conf.Configuration,java.util.Collection,boolean)#java/util/Iterator/next()
org/apache/parquet/hadoop/ParquetFileReader/readAllFootersInParallelUsingSummaryFiles(org.apache.hadoop.conf.Configuration,java.util.Collection,boolean)#java/util/HashMap/HashMap()
org/apache/parquet/hadoop/ParquetFileReader/readAllFootersInParallelUsingSummaryFiles(org.apache.hadoop.conf.Configuration,java.util.Collection,boolean)#java/util/List/add(java.lang.Object)
org/apache/parquet/hadoop/ParquetFileReader/readAllFootersInParallelUsingSummaryFiles(org.apache.hadoop.conf.Configuration,java.util.Collection,boolean)#java/util/List/iterator()
org/apache/parquet/hadoop/ParquetFileReader/readAllFootersInParallelUsingSummaryFiles(org.apache.hadoop.conf.Configuration,java.util.Collection,boolean)#java/util/Collection/iterator()
org/apache/parquet/hadoop/ParquetFileReader/readAllFootersInParallelUsingSummaryFiles(org.apache.hadoop.conf.Configuration,java.util.Collection,boolean)#org/apache/hadoop/conf/Configuration/getInt(java.lang.String,int)
org/apache/parquet/hadoop/ParquetFileReader/readAllFootersInParallelUsingSummaryFiles(org.apache.hadoop.conf.Configuration,java.util.Collection,boolean)#java/util/Set/add(java.lang.Object)
org/apache/parquet/hadoop/ParquetFileReader/readNextRowGroup()#java/util/ArrayList/ArrayList()
org/apache/parquet/hadoop/ParquetFileReader/readNextRowGroup()#java/util/Iterator/hasNext()
org/apache/parquet/hadoop/ParquetFileReader/readNextRowGroup()#java/util/List/size()
org/apache/parquet/hadoop/ParquetFileReader/readNextRowGroup()#java/util/Iterator/next()
org/apache/parquet/hadoop/ParquetFileReader/readNextRowGroup()#java/util/List/add(java.lang.Object)
org/apache/parquet/hadoop/ParquetFileReader/readNextRowGroup()#java/util/List/get(int)
org/apache/parquet/hadoop/ParquetFileReader/readNextRowGroup()#org/apache/parquet/hadoop/ParquetFileReader$ConsecutiveChunkList/ConsecutiveChunkList(org.apache.parquet.hadoop.ParquetFileReader,long)
org/apache/parquet/hadoop/ParquetFileReader/readNextRowGroup()#org/apache/parquet/hadoop/ParquetFileReader$Chunk/readAllPages()
org/apache/parquet/hadoop/ParquetFileReader/readNextRowGroup()#org/apache/parquet/hadoop/ParquetFileReader$ConsecutiveChunkList/readAll(org.apache.parquet.io.SeekableInputStream)
org/apache/parquet/hadoop/ParquetFileReader/readNextRowGroup()#org/apache/parquet/hadoop/ParquetFileReader$ConsecutiveChunkList/addChunk(org.apache.parquet.hadoop.ParquetFileReader$ChunkDescriptor)
org/apache/parquet/hadoop/ParquetFileReader/readNextRowGroup()#org/apache/parquet/hadoop/ParquetFileReader$ChunkDescriptor/access$300(org.apache.parquet.hadoop.ParquetFileReader$ChunkDescriptor)
org/apache/parquet/hadoop/ParquetFileReader/readNextRowGroup()#org/apache/parquet/hadoop/ParquetFileReader$Chunk/access$200(org.apache.parquet.hadoop.ParquetFileReader$Chunk)
org/apache/parquet/hadoop/ParquetFileReader/readNextRowGroup()#java/util/Map/get(java.lang.Object)
org/apache/parquet/hadoop/ParquetFileReader/readNextRowGroup()#org/apache/parquet/hadoop/ParquetFileReader$ConsecutiveChunkList/endPos()
org/apache/parquet/hadoop/ParquetFileReader/readNextRowGroup()#java/lang/RuntimeException/RuntimeException(java.lang.String)
org/apache/parquet/hadoop/ParquetFileReader/readNextRowGroup()#java/util/List/iterator()
org/apache/parquet/hadoop/ParquetFileReader/readNextRowGroup()#org/apache/parquet/hadoop/ParquetFileReader$ChunkDescriptor/ChunkDescriptor(org.apache.parquet.column.ColumnDescriptor,org.apache.parquet.hadoop.metadata.ColumnChunkMetaData,long,int,org.apache.parquet.hadoop.ParquetFileReader$1)
org/apache/parquet/hadoop/util/ContextUtil/<clinit>()#java/lang/Class/forName(java.lang.String)
org/apache/parquet/hadoop/util/ContextUtil/<clinit>()#java/util/Map/put(java.lang.Object,java.lang.Object)
org/apache/parquet/hadoop/util/ContextUtil/<clinit>()#java/lang/Class/getMethod(java.lang.String,java.lang.Class[])
org/apache/parquet/hadoop/util/ContextUtil/<clinit>()#java/lang/Class/getDeclaredField(java.lang.String)
org/apache/parquet/hadoop/util/ContextUtil/<clinit>()#java/util/HashMap/HashMap()
org/apache/parquet/hadoop/util/ContextUtil/<clinit>()#java/lang/Class/getDeclaredConstructor(java.lang.Class[])
org/apache/parquet/hadoop/util/ContextUtil/<clinit>()#java/lang/Class/getConstructor(java.lang.Class[])
org/apache/parquet/hadoop/util/ContextUtil/<clinit>()#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String,java.lang.Throwable)
org/apache/parquet/hadoop/util/ContextUtil/<clinit>()#java/lang/reflect/Constructor/setAccessible(boolean)
org/apache/parquet/hadoop/util/ContextUtil/<clinit>()#java/lang/reflect/Field/setAccessible(boolean)
org/apache/parquet/hadoop/ColumnChunkPageReadStore/getPageReader(org.apache.parquet.column.ColumnDescriptor)#java/lang/StringBuilder/append(java.lang.String)
org/apache/parquet/hadoop/ColumnChunkPageReadStore/getPageReader(org.apache.parquet.column.ColumnDescriptor)#java/util/Map/get(java.lang.Object)
org/apache/parquet/hadoop/ColumnChunkPageReadStore/getPageReader(org.apache.parquet.column.ColumnDescriptor)#java/util/Map/keySet()
org/apache/parquet/hadoop/ColumnChunkPageReadStore/getPageReader(org.apache.parquet.column.ColumnDescriptor)#java/lang/StringBuilder/append(long)
org/apache/parquet/hadoop/ColumnChunkPageReadStore/getPageReader(org.apache.parquet.column.ColumnDescriptor)#java/lang/StringBuilder/StringBuilder()
org/apache/parquet/hadoop/ColumnChunkPageReadStore/getPageReader(org.apache.parquet.column.ColumnDescriptor)#java/util/Map/containsKey(java.lang.Object)
org/apache/parquet/hadoop/ColumnChunkPageReadStore/getPageReader(org.apache.parquet.column.ColumnDescriptor)#java/lang/IllegalArgumentException/IllegalArgumentException(java.lang.String)
org/apache/parquet/hadoop/ColumnChunkPageReadStore/getPageReader(org.apache.parquet.column.ColumnDescriptor)#java/lang/StringBuilder/append(java.lang.Object)
org/apache/parquet/hadoop/ColumnChunkPageReadStore/getPageReader(org.apache.parquet.column.ColumnDescriptor)#java/lang/StringBuilder/toString()
org/apache/parquet/hadoop/util/SerializationUtil/readObjectFromConfAsBase64(java.lang.String,org.apache.hadoop.conf.Configuration)#java/util/zip/GZIPInputStream/GZIPInputStream(java.io.InputStream)
org/apache/parquet/hadoop/util/SerializationUtil/readObjectFromConfAsBase64(java.lang.String,org.apache.hadoop.conf.Configuration)#java/io/IOException/IOException(java.lang.String,java.lang.Throwable)
org/apache/parquet/hadoop/util/SerializationUtil/readObjectFromConfAsBase64(java.lang.String,org.apache.hadoop.conf.Configuration)#java/lang/StringBuilder/append(java.lang.String)
org/apache/parquet/hadoop/util/SerializationUtil/readObjectFromConfAsBase64(java.lang.String,org.apache.hadoop.conf.Configuration)#java/io/ByteArrayInputStream/ByteArrayInputStream(byte[])
org/apache/parquet/hadoop/util/SerializationUtil/readObjectFromConfAsBase64(java.lang.String,org.apache.hadoop.conf.Configuration)#java/io/ObjectInputStream/readObject()
org/apache/parquet/hadoop/util/SerializationUtil/readObjectFromConfAsBase64(java.lang.String,org.apache.hadoop.conf.Configuration)#java/lang/StringBuilder/StringBuilder()
org/apache/parquet/hadoop/util/SerializationUtil/readObjectFromConfAsBase64(java.lang.String,org.apache.hadoop.conf.Configuration)#java/lang/String/getBytes(java.lang.String)
org/apache/parquet/hadoop/util/SerializationUtil/readObjectFromConfAsBase64(java.lang.String,org.apache.hadoop.conf.Configuration)#org/apache/commons/codec/binary/Base64/decodeBase64(byte[])
org/apache/parquet/hadoop/util/SerializationUtil/readObjectFromConfAsBase64(java.lang.String,org.apache.hadoop.conf.Configuration)#java/io/ObjectInputStream/ObjectInputStream(java.io.InputStream)
org/apache/parquet/hadoop/util/SerializationUtil/readObjectFromConfAsBase64(java.lang.String,org.apache.hadoop.conf.Configuration)#org/apache/parquet/Closeables/close(java.io.Closeable)
org/apache/parquet/hadoop/util/SerializationUtil/readObjectFromConfAsBase64(java.lang.String,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/conf/Configuration/get(java.lang.String)
org/apache/parquet/hadoop/util/SerializationUtil/readObjectFromConfAsBase64(java.lang.String,org.apache.hadoop.conf.Configuration)#java/lang/StringBuilder/toString()
org/apache/parquet/hadoop/util/SerializationUtil/writeObjectToConfAsBase64(java.lang.String,java.lang.Object,org.apache.hadoop.conf.Configuration)#java/util/zip/GZIPOutputStream/GZIPOutputStream(java.io.OutputStream)
org/apache/parquet/hadoop/util/SerializationUtil/writeObjectToConfAsBase64(java.lang.String,java.lang.Object,org.apache.hadoop.conf.Configuration)#org/apache/hadoop/conf/Configuration/set(java.lang.String,java.lang.String)
org/apache/parquet/hadoop/util/SerializationUtil/writeObjectToConfAsBase64(java.lang.String,java.lang.Object,org.apache.hadoop.conf.Configuration)#java/lang/String/String(byte[],java.lang.String)
org/apache/parquet/hadoop/util/SerializationUtil/writeObjectToConfAsBase64(java.lang.String,java.lang.Object,org.apache.hadoop.conf.Configuration)#java/io/ObjectOutputStream/writeObject(java.lang.Object)
org/apache/parquet/hadoop/util/SerializationUtil/writeObjectToConfAsBase64(java.lang.String,java.lang.Object,org.apache.hadoop.conf.Configuration)#java/io/ByteArrayOutputStream/ByteArrayOutputStream()
org/apache/parquet/hadoop/util/SerializationUtil/writeObjectToConfAsBase64(java.lang.String,java.lang.Object,org.apache.hadoop.conf.Configuration)#org/apache/commons/codec/binary/Base64/encodeBase64(byte[])
org/apache/parquet/hadoop/util/SerializationUtil/writeObjectToConfAsBase64(java.lang.String,java.lang.Object,org.apache.hadoop.conf.Configuration)#org/apache/parquet/Closeables/close(java.io.Closeable)
org/apache/parquet/hadoop/util/SerializationUtil/writeObjectToConfAsBase64(java.lang.String,java.lang.Object,org.apache.hadoop.conf.Configuration)#java/io/ObjectOutputStream/ObjectOutputStream(java.io.OutputStream)
org/apache/parquet/hadoop/util/SerializationUtil/writeObjectToConfAsBase64(java.lang.String,java.lang.Object,org.apache.hadoop.conf.Configuration)#java/io/ByteArrayOutputStream/toByteArray()
org/apache/parquet/hadoop/ParquetFileReader/1/call()#org/apache/parquet/hadoop/ParquetFileReader$1/call()
org/apache/parquet/hadoop/ParquetFileReader/1/call()#org/apache/hadoop/fs/Path/getName()
org/apache/parquet/hadoop/ParquetFileReader/1/call()#java/util/Iterator/hasNext()
org/apache/parquet/hadoop/ParquetFileReader/1/call()#java/util/List/add(java.lang.Object)
org/apache/parquet/hadoop/ParquetFileReader/1/call()#java/util/Map/put(java.lang.Object,java.lang.Object)
org/apache/parquet/hadoop/ParquetFileReader/1/call()#java/util/ArrayList/ArrayList()
org/apache/parquet/hadoop/ParquetFileReader/1/call()#java/util/Collections/emptyMap()
org/apache/parquet/hadoop/ParquetFileReader/1/call()#org/apache/hadoop/fs/FileStatus/getPath()
org/apache/parquet/hadoop/ParquetFileReader/1/call()#org/apache/hadoop/fs/Path/Path(org.apache.hadoop.fs.Path,java.lang.String)
org/apache/parquet/hadoop/ParquetFileReader/1/call()#java/util/Iterator/next()
org/apache/parquet/hadoop/ParquetFileReader/1/call()#java/util/HashMap/HashMap()
org/apache/parquet/hadoop/ParquetFileReader/1/call()#java/util/List/iterator()
org/apache/parquet/hadoop/ParquetFileReader/1/call()#java/util/Collection/iterator()
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/SplitInfo/getParquetInputSplit(org.apache.hadoop.fs.FileStatus,java.lang.String,java.util.Map)#org/apache/parquet/schema/MessageType/containsPath(java.lang.String[])
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/SplitInfo/getParquetInputSplit(org.apache.hadoop.fs.FileStatus,java.lang.String,java.util.Map)#org/apache/hadoop/fs/FileStatus/getPath()
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/SplitInfo/getParquetInputSplit(org.apache.hadoop.fs.FileStatus,java.lang.String,java.util.Map)#java/util/Iterator/hasNext()
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/SplitInfo/getParquetInputSplit(org.apache.hadoop.fs.FileStatus,java.lang.String,java.util.Map)#org/apache/hadoop/fs/BlockLocation/getOffset()
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/SplitInfo/getParquetInputSplit(org.apache.hadoop.fs.FileStatus,java.lang.String,java.util.Map)#org/apache/hadoop/fs/BlockLocation/getHosts()
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/SplitInfo/getParquetInputSplit(org.apache.hadoop.fs.FileStatus,java.lang.String,java.util.Map)#org/apache/parquet/schema/MessageTypeParser/parseMessageType(java.lang.String)
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/SplitInfo/getParquetInputSplit(org.apache.hadoop.fs.FileStatus,java.lang.String,java.util.Map)#java/util/Iterator/next()
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/SplitInfo/getParquetInputSplit(org.apache.hadoop.fs.FileStatus,java.lang.String,java.util.Map)#org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy$SplitInfo/getRowGroupCount()
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/SplitInfo/getParquetInputSplit(org.apache.hadoop.fs.FileStatus,java.lang.String,java.util.Map)#java/util/List/get(int)
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/SplitInfo/getParquetInputSplit(org.apache.hadoop.fs.FileStatus,java.lang.String,java.util.Map)#org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy$SplitInfo/getRowGroups()
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/SplitInfo/getParquetInputSplit(org.apache.hadoop.fs.FileStatus,java.lang.String,java.util.Map)#org/apache/parquet/hadoop/metadata/ColumnPath/toArray()
org/apache/parquet/hadoop/ClientSideMetadataSplitStrategy/SplitInfo/getParquetInputSplit(org.apache.hadoop.fs.FileStatus,java.lang.String,java.util.Map)#java/util/List/iterator()
org/apache/parquet/hadoop/DictionaryPageReader/readDictionaryPage(org.apache.parquet.column.ColumnDescriptor)#java/util/Map/put(java.lang.Object,java.lang.Object)
org/apache/parquet/hadoop/DictionaryPageReader/readDictionaryPage(org.apache.parquet.column.ColumnDescriptor)#java/lang/StringBuilder/append(java.lang.String)
org/apache/parquet/hadoop/DictionaryPageReader/readDictionaryPage(org.apache.parquet.column.ColumnDescriptor)#java/util/Map/get(java.lang.Object)
org/apache/parquet/hadoop/DictionaryPageReader/readDictionaryPage(org.apache.parquet.column.ColumnDescriptor)#org/apache/parquet/column/ColumnDescriptor/getPath()
org/apache/parquet/hadoop/DictionaryPageReader/readDictionaryPage(org.apache.parquet.column.ColumnDescriptor)#java/lang/StringBuilder/StringBuilder()
org/apache/parquet/hadoop/DictionaryPageReader/readDictionaryPage(org.apache.parquet.column.ColumnDescriptor)#java/util/Map/containsKey(java.lang.Object)
org/apache/parquet/hadoop/DictionaryPageReader/readDictionaryPage(org.apache.parquet.column.ColumnDescriptor)#org/apache/parquet/Strings/join(java.lang.String[],java.lang.String)
org/apache/parquet/hadoop/DictionaryPageReader/readDictionaryPage(org.apache.parquet.column.ColumnDescriptor)#java/lang/StringBuilder/toString()
org/apache/parquet/hadoop/DictionaryPageReader/readDictionaryPage(org.apache.parquet.column.ColumnDescriptor)#org/apache/parquet/io/ParquetDecodingException/ParquetDecodingException(java.lang.String)
org/apache/parquet/hadoop/DictionaryPageReader/readDictionaryPage(org.apache.parquet.column.ColumnDescriptor)#org/apache/parquet/io/ParquetDecodingException/ParquetDecodingException(java.lang.String,java.lang.Throwable)
